[{"link": "https://arxiv.org/abs/2106.06555", "abstract": "Knowledge Graph (KG) completion research usually focuses on densely connected\nbenchmark datasets that are not representative of real KGs. We curate two KG\ndatasets that include biomedical and encyclopedic knowledge and use an existing\ncommonsense KG dataset to explore KG completion in the more realistic setting\nwhere dense connectivity is not guaranteed. We develop a deep convolutional\nnetwork that utilizes textual entity representations and demonstrate that our\nmodel outperforms recent KG completion methods in this challenging setting. We\nfind that our model's performance improvements stem primarily from its\nrobustness to sparsity. We then distill the knowledge from the convolutional\nnetwork into a student network that re-ranks promising candidate entities. This\nre-ranking stage leads to further improvements in performance and demonstrates\nthe effectiveness of entity re-ranking for KG completion.", "title": "Robust Knowledge Graph Completion with Stacked Convolutions and a  Student Re-Ranking Network"}, {"link": "https://arxiv.org/abs/2106.06560", "abstract": "High-resolution representations (HR) are essential for dense prediction tasks\nsuch as segmentation, detection, and pose estimation. Learning HR\nrepresentations is typically ignored in previous Neural Architecture Search\n(NAS) methods that focus on image classification. This work proposes a novel\nNAS method, called HR-NAS, which is able to find efficient and accurate\nnetworks for different tasks, by effectively encoding multiscale contextual\ninformation while maintaining high-resolution representations. In HR-NAS, we\nrenovate the NAS search space as well as its searching strategy. To better\nencode multiscale image contexts in the search space of HR-NAS, we first\ncarefully design a lightweight transformer, whose computational complexity can\nbe dynamically changed with respect to different objective functions and\ncomputation budgets. To maintain high-resolution representations of the learned\nnetworks, HR-NAS adopts a multi-branch architecture that provides convolutional\nencoding of multiple feature resolutions, inspired by HRNet. Last, we proposed\nan efficient fine-grained search strategy to train HR-NAS, which effectively\nexplores the search space, and finds optimal architectures given various tasks\nand computation resources. HR-NAS is capable of achieving state-of-the-art\ntrade-offs between performance and FLOPs for three dense prediction tasks and\nan image classification task, given only small computational budgets. For\nexample, HR-NAS surpasses SqueezeNAS that is specially designed for semantic\nsegmentation while improving efficiency by 45.9%. Code is available at\nhttps://github.com/dingmyu/HR-NAS", "title": "HR-NAS: Searching Efficient High-Resolution Neural Architectures with  Lightweight Transformers"}, {"link": "https://arxiv.org/abs/2106.06561", "abstract": "We show how to learn a map that takes a content code, derived from a face\nimage, and a randomly chosen style code to an anime image. We derive an\nadversarial loss from our simple and effective definitions of style and\ncontent. This adversarial loss guarantees the map is diverse -- a very wide\nrange of anime can be produced from a single content code. Under plausible\nassumptions, the map is not just diverse, but also correctly represents the\nprobability of an anime, conditioned on an input face. In contrast, current\nmultimodal generation procedures cannot capture the complex styles that appear\nin anime. Extensive quantitative experiments support the idea the map is\ncorrect. Extensive qualitative results show that the method can generate a much\nmore diverse range of styles than SOTA comparisons. Finally, we show that our\nformalization of content and style allows us to perform video to video\ntranslation without ever training on videos.", "title": "GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation  (works for videos too!)"}, {"link": "https://arxiv.org/abs/2106.06566", "abstract": "Neural models excel at extracting statistical patterns from large amounts of\ndata, but struggle to learn patterns or reason about language from only a few\nexamples. In this paper, we ask: Can we learn explicit rules that generalize\nwell from only a few examples? We explore this question using program\nsynthesis. We develop a synthesis model to learn phonology rules as programs in\na domain-specific language. We test the ability of our models to generalize\nfrom few training examples using our new dataset of problems from the\nLinguistics Olympiad, a challenging set of tasks that require strong linguistic\nreasoning ability. In addition to being highly sample-efficient, our approach\ngenerates human-readable programs, and allows control over the generalizability\nof the learnt programs.", "title": "Sample-efficient Linguistic Generalizations through Program Synthesis:  Experiments with Phonology Problems"}, {"link": "https://arxiv.org/abs/2106.06575", "abstract": "While maximizing deep neural networks' (DNNs') acceleration efficiency\nrequires a joint search/design of three different yet highly coupled aspects,\nincluding the networks, bitwidths, and accelerators, the challenges associated\nwith such a joint search have not yet been fully understood and addressed. The\nkey challenges include (1) the dilemma of whether to explode the memory\nconsumption due to the huge joint space or achieve sub-optimal designs, (2) the\ndiscrete nature of the accelerator design space that is coupled yet different\nfrom that of the networks and bitwidths, and (3) the chicken and egg problem\nassociated with network-accelerator co-search, i.e., co-search requires\noperation-wise hardware cost, which is lacking during search as the optimal\naccelerator depending on the whole network is still unknown during search. To\ntackle these daunting challenges towards optimal and fast development of DNN\naccelerators, we propose a framework dubbed Auto-NBA to enable jointly\nsearching for the Networks, Bitwidths, and Accelerators, by efficiently\nlocalizing the optimal design within the huge joint design space for each\ntarget dataset and acceleration specification. Our Auto-NBA integrates a\nheterogeneous sampling strategy to achieve unbiased search with constant memory\nconsumption, and a novel joint-search pipeline equipped with a generic\ndifferentiable accelerator search engine. Extensive experiments and ablation\nstudies validate that both Auto-NBA generated networks and accelerators\nconsistently outperform state-of-the-art designs (including\nco-search/exploration techniques, hardware-aware NAS methods, and DNN\naccelerators), in terms of search time, task accuracy, and accelerator\nefficiency. Our codes are available at: https://github.com/RICE-EIC/Auto-NBA.", "title": "Auto-NBA: Efficient and Effective Search Over the Joint Space of  Networks, Bitwidths, and Accelerators"}, {"link": "https://arxiv.org/abs/2106.06577", "abstract": "Driven by the explosive interest in applying deep reinforcement learning\n(DRL) agents to numerous real-time control and decision-making applications,\nthere has been a growing demand to deploy DRL agents to empower daily-life\nintelligent devices, while the prohibitive complexity of DRL stands at odds\nwith limited on-device resources. In this work, we propose an Automated Agent\nAccelerator Co-Search (A3C-S) framework, which to our best knowledge is the\nfirst to automatically co-search the optimally matched DRL agents and\naccelerators that maximize both test scores and hardware efficiency. Extensive\nexperiments consistently validate the superiority of our A3C-S over\nstate-of-the-art techniques.", "title": "A3C-S: Automated Agent Accelerator Co-Search towards Efficient Deep  Reinforcement Learning"}, {"link": "https://arxiv.org/abs/2106.06579", "abstract": "As neural networks get widespread adoption in resource-constrained embedded\ndevices, there is a growing need for low-power neural systems. Spiking Neural\nNetworks (SNNs)are emerging to be an energy-efficient alternative to the\ntraditional Artificial Neural Networks (ANNs) which are known to be\ncomputationally intensive. From an application perspective, as federated\nlearning involves multiple energy-constrained devices, there is a huge scope to\nleverage energy efficiency provided by SNNs. Despite its importance, there has\nbeen little attention on training SNNs on a large-scale distributed system like\nfederated learning. In this paper, we bring SNNs to a more realistic federated\nlearning scenario. Specifically, we propose a federated learning framework for\ndecentralized and privacy-preserving training of SNNs. To validate the proposed\nfederated learning framework, we experimentally evaluate the advantages of SNNs\non various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks.\nWe observe that SNNs outperform ANNs in terms of overall accuracy by over 15%\nwhen the data is distributed across a large number of clients in the federation\nwhile providing up to5.3x energy efficiency. In addition to efficiency, we also\nanalyze the sensitivity of the proposed federated SNN framework to data\ndistribution among the clients, stragglers, and gradient noise and perform a\ncomprehensive comparison with ANNs.", "title": "Federated Learning with Spiking Neural Networks"}, {"link": "https://arxiv.org/abs/2106.06580", "abstract": "Boolean functions can be represented in many ways including logical forms,\ntruth tables, and polynomials. Additionally, Boolean functions have different\ncanonical representations such as minimal disjunctive normal forms. Other\ncanonical representation is based on the polynomial representation of Boolean\nfunctions where they can be written as a nested product of canalizing layers\nand a polynomial that contains the noncanalizing variables. In this paper we\nstudy the problem of identifying the canalizing layers format of Boolean\nfunctions. First, we show that the problem of finding the canalizing layers is\nNP-hard. Second, we present several algorithms for finding the canalizing\nlayers of a Boolean function, discuss their complexities, and compare their\nperformances. Third, we show applications where the computation of canalizing\nlayers can be used for finding a disjunctive normal form of a nested canalizing\nfunction. Another application deals with the reverse engineering of Boolean\nnetworks with a prescribed layering format. Finally, implementations of our\nalgorithms in Python and in the computer algebra system Macaulay2 are available\nat https://github.com/ckadelka/BooleanCanalization.", "title": "Revealing the canalizing structure of Boolean functions: Algorithms and  applications"}, {"link": "https://arxiv.org/abs/2106.06583", "abstract": "We present the Deception Detection and Physiological Monitoring (DDPM)\ndataset and initial baseline results on this dataset. Our application context\nis an interview scenario in which the interviewee attempts to deceive the\ninterviewer on selected responses. The interviewee is recorded in RGB,\nnear-infrared, and long-wave infrared, along with cardiac pulse, blood\noxygenation, and audio. After collection, data were annotated for\ninterviewer/interviewee, curated, ground-truthed, and organized into train /\ntest parts for a set of canonical deception detection experiments. Baseline\nexperiments found random accuracy for micro-expressions as an indicator of\ndeception, but that saccades can give a statistically significant response. We\nalso estimated subject heart rates from face videos (remotely) with a mean\nabsolute error as low as 3.16 bpm. The database contains almost 13 hours of\nrecordings of 70 subjects, and over 8 million visible-light, near-infrared, and\nthermal video frames, along with appropriate meta, audio and pulse oximeter\ndata. To our knowledge, this is the only collection offering recordings of five\nmodalities in an interview scenario that can be used in both deception\ndetection and remote photoplethysmography research.", "title": "Deception Detection and Remote Physiological Monitoring: A Dataset and  Baseline Experimental Results"}, {"link": "https://arxiv.org/abs/2106.06585", "abstract": "The goal of the present paper is to understand the impact of numerical\nschemes for the reconstruction of data at cell faces in finite-volume methods,\nand to assess their interaction with the quadrature rule used to compute the\naverage over the cell volume. Here, third-, fifth- and seventh-order WENO-Z\nschemes are investigated. On a problem with a smooth solution, the theoretical\norder of convergence rate for each method is retrieved, and changing the order\nof the reconstruction at cell faces does not impact the results, whereas for a\nshock-driven problem all the methods collapse to first-order. Study of the\ndecay of compressible homogeneous isotropic turbulence reveals that using a\nhigh-order quadrature rule to compute the average over a finite volume cell\ndoes not improve the spectral accuracy and that all methods present a\nsecond-order convergence rate. However the choice of the numerical method to\nreconstruct data at cell faces is found to be critical to correctly capture\nturbulent spectra. In the context of simulations with finite-volume methods of\npractical flows encountered in engineering applications, it becomes apparent\nthat an efficient strategy is to perform the average integration with a\nlow-order quadrature rule on a fine mesh resolution, whereas high-order schemes\nshould be used to reconstruct data at cell faces.", "title": "On the numerical accuracy in finite-volume methods to accurately capture  turbulence in compressible flows"}, {"link": "https://arxiv.org/abs/2106.06586", "abstract": "Graph neural networks (GNNs) have achieved tremendous success on multiple\ngraph-based learning tasks by fusing network structure and node features.\nModern GNN models are built upon iterative aggregation of neighbor's/proximity\nfeatures by message passing. Its prediction performance has been shown to be\nstrongly bounded by assortative mixing in the graph, a key property wherein\nnodes with similar attributes mix/connect with each other. We observe that real\nworld networks exhibit heterogeneous or diverse mixing patterns and the\nconventional global measurement of assortativity, such as global assortativity\ncoefficient, may not be a representative statistic in quantifying this mixing.\nWe adopt a generalized concept, node-level assortativity, one that is based at\nthe node level to better represent the diverse patterns and accurately quantify\nthe learnability of GNNs. We find that the prediction performance of a wide\nrange of GNN models is highly correlated with the node level assortativity. To\nbreak this limit, in this work, we focus on transforming the input graph into a\ncomputation graph which contains both proximity and structural information as\ndistinct type of edges. The resulted multi-relational graph has an enhanced\nlevel of assortativity and, more importantly, preserves rich information from\nthe original graph. We then propose to run GNNs on this computation graph and\nshow that adaptively choosing between structure and proximity leads to improved\nperformance under diverse mixing. Empirically, we show the benefits of adopting\nour transformation framework for semi-supervised node classification task on a\nvariety of real world graph learning benchmarks.", "title": "Breaking the Limit of Graph Neural Networks by Improving the  Assortativity of Graphs with Local Mixing Patterns"}, {"link": "https://arxiv.org/abs/2106.06588", "abstract": "Robust visualization of complex data is critical for the effective use of NLP\nfor event classification, as the volume of data is large and the\nhigh-dimensional structure of text makes data challenging to summarize\nsuccinctly. In event extraction tasks in particular, visualization can aid in\nunderstanding and illustrating the textual relationships from which machine\nlearning tools produce insights. Through our case study which seeks to identify\npotential triggers of state-led mass killings from news articles using NLP, we\ndemonstrate how visualizations can aid in each stage, from exploratory analysis\nof raw data, to machine learning training analysis, and finally post-inference\nvalidation.", "title": "Visualization Techniques to Enhance Automated Event Extraction"}, {"link": "https://arxiv.org/abs/2106.06592", "abstract": "Introduction: Mobile apps, through artificial vision, are capable of\nrecognizing vegetable species in real time. However, the existing species\nrecognition apps do not take in consideration the wide variety of endemic and\nnative (Chilean) species, which leads to wrong species predictions. This study\nintroduces the development of a chilean species dataset and an optimized\nclassification model implemented to a mobile app. Method: the data set was\nbuilt by putting together pictures of several species captured on the field and\nby selecting some pictures available from other datasets available online.\nConvolutional neural networks were used in order to develop the images\nprediction models. The networks were trained by performing a sensitivity\nanalysis, validating with k-fold cross validation and performing tests with\ndifferent hyper-parameters, optimizers, convolutional layers, and learning\nrates in order to identify and choose the best models and then put them\ntogether in one classification model. Results: The final data set was\ncompounded by 46 species, including native species, endemic and exotic from\nChile, with 6120 training pictures and 655 testing pictures. The best models\nwere implemented on a mobile app, obtaining a 95% correct prediction rate with\nrespect to the set of tests. Conclusion: The app developed in this study is\ncapable of classifying species with a high level of accuracy, depending on the\nstate of the art of the artificial vision and it can also show relevant\ninformation related to the classified species.", "title": "Dise\u00f1o y desarrollo de aplicaci\u00f3n m\u00f3vil para la clasificaci\u00f3n de  flora nativa chilena utilizando redes neuronales convolucionales"}, {"link": "https://arxiv.org/abs/2106.06593", "abstract": "Virtual try-on methods aim to generate images of fashion models wearing\narbitrary combinations of garments. This is a challenging task because the\ngenerated image must appear realistic and accurately display the interaction\nbetween garments. Prior works produce images that are filled with artifacts and\nfail to capture important visual details necessary for commercial applications.\nWe propose Outfit Visualization Net (OVNet) to capture these important details\n(e.g. buttons, shading, textures, realistic hemlines, and interactions between\ngarments) and produce high quality multiple-garment virtual try-on images.\nOVNet consists of 1) a semantic layout generator and 2) an image generation\npipeline using multiple coordinated warps. We train the warper to output\nmultiple warps using a cascade loss, which refines each successive warp to\nfocus on poorly generated regions of a previous warp and yields consistent\nimprovements in detail. In addition, we introduce a method for matching outfits\nwith the most suitable model and produce significant improvements for both our\nand other previous try-on methods. Through quantitative and qualitative\nanalysis, we demonstrate our method generates substantially higher-quality\nstudio images compared to prior works for multi-garment outfits. An interactive\ninterface powered by this method has been deployed on fashion e-commerce\nwebsites and received overwhelmingly positive feedback.", "title": "Toward Accurate and Realistic Outfits Visualization with Attention to  Details"}, {"link": "https://arxiv.org/abs/2106.06595", "abstract": "Pressure and temperature profile are key data for safe production in oil and\ngas wells. In this paper, a bucket-brigade inspired sensor network protocol is\nproposed which can be used to extract sensed data profile from the nanoscale up\nto kilometer long structures. The PHY/MAC layers are discussed. This protocol\nis best suited for low data rate exchanges in small fixed-size packets, named\nbuckets, transmitted as time-domain bursts among high-precision smart sensors\ndeployed as a queue. There is only one coordinator, which is not directly\naccessible by most of the sensor nodes. The coordinator is responsible for\ncollecting the measurement profile and send it to a supervisory node. There is\nno need for complex routing mechanism, as the network topology is determined\nduring deployment. There are many applications which require sensors to be\ndeployed as a long queue and sensed data could be transmitted at low data\nrates. Examples of such monitoring applications are: neural connected\nartificial skin, oil/gas/water pipeline integrity, power transmission line\ntower integrity, (rail)road/highway lighting and integrity, individualized\nmonitoring in vineyard or re-foresting or plantation, underwater\ntelecommunications cable integrity, oil/gas riser integrity, oil/gas well\ntemperature and pressure profile, among others. For robustness and reduced\nelectromagnetic interference, wired network is preferred. Besides in some harsh\nenvironment wireless is not feasible. To reduce wiring, communications can be\ncarried out in the same cable used to supply electrical power.", "title": "Bucket-brigade inspired power line network protocol for sensed quantity  profile acquisition with smart sensors deployed as a queue in harsh  environment"}, {"link": "https://arxiv.org/abs/2106.06596", "abstract": "The \"cold posterior effect\" (CPE) in Bayesian deep learning describes the\nuncomforting observation that the predictive performance of Bayesian neural\nnetworks can be significantly improved if the Bayes posterior is artificially\nsharpened using a temperature parameter T<1. The CPE is problematic in theory\nand practice and since the effect was identified many researchers have proposed\nhypotheses to explain the phenomenon. However, despite this intensive research\neffort the effect remains poorly understood. In this work we provide novel and\nnuanced evidence relevant to existing explanations for the cold posterior\neffect, disentangling three hypotheses: 1. The dataset curation hypothesis of\nAitchison (2020): we show empirically that the CPE does not arise in a real\ncurated data set but can be produced in a controlled experiment with varying\ncuration strength. 2. The data augmentation hypothesis of Izmailov et al.\n(2021) and Fortuin et al. (2021): we show empirically that data augmentation is\nsufficient but not necessary for the CPE to be present. 3. The bad prior\nhypothesis of Wenzel et al. (2020): we use a simple experiment evaluating the\nrelative importance of the prior and the likelihood, strongly linking the CPE\nto the prior. Our results demonstrate how the CPE can arise in isolation from\nsynthetic curation, data augmentation, and bad priors. Cold posteriors observed\n\"in the wild\" are therefore unlikely to arise from a single simple cause; as a\nresult, we do not expect a simple \"fix\" for cold posteriors.", "title": "Disentangling the Roles of Curation, Data-Augmentation and the Prior in  the Cold Posterior Effect"}, {"link": "https://arxiv.org/abs/2106.06598", "abstract": "In this paper, we explore the use of pre-trained language models to learn\nsentiment information of written texts for speech sentiment analysis. First, we\ninvestigate how useful a pre-trained language model would be in a 2-step\npipeline approach employing Automatic Speech Recognition (ASR) and\ntranscripts-based sentiment analysis separately. Second, we propose a pseudo\nlabel-based semi-supervised training strategy using a language model on an\nend-to-end speech sentiment approach to take advantage of a large, but\nunlabeled speech dataset for training. Although spoken and written texts have\ndifferent linguistic characteristics, they can complement each other in\nunderstanding sentiment. Therefore, the proposed system can not only model\nacoustic characteristics to bear sentiment-specific information in speech\nsignals, but learn latent information to carry sentiments in the text\nrepresentation. In these experiments, we demonstrate the proposed approaches\nimprove F1 scores consistently compared to systems without a language model.\nMoreover, we also show that the proposed framework can reduce 65% of human\nsupervision by leveraging a large amount of data without human sentiment\nannotation and boost performance in a low-resource condition where the human\nsentiment annotation is not available enough.", "title": "Leveraging Pre-trained Language Model for Speech Sentiment Analysis"}, {"link": "https://arxiv.org/abs/2106.06600", "abstract": "We consider repair tasks: given a critic (e.g., compiler) that assesses the\nquality of an input, the goal is to train a fixer that converts a bad example\n(e.g., code with syntax errors) into a good one (e.g., code with no errors).\nExisting works create training data consisting of (bad, good) pairs by\ncorrupting good examples using heuristics (e.g., dropping tokens). However,\nfixers trained on this synthetically-generated data do not extrapolate well to\nthe real distribution of bad inputs. To bridge this gap, we propose a new\ntraining approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use\nthe critic to check a fixer's output on real bad inputs and add good (fixed)\noutputs to the training data, and (ii) we train a breaker to generate realistic\nbad code from good code. Based on these ideas, we iteratively update the\nbreaker and the fixer while using them in conjunction to generate more paired\ndata. We evaluate BIFI on two code repair datasets: GitHub-Python, a new\ndataset we introduce where the goal is to repair Python code with AST parse\nerrors; and DeepFix, where the goal is to repair C code with compiler errors.\nBIFI outperforms existing methods, obtaining 90.5% repair accuracy on\nGitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not\nrequire any labeled data; we hope it will be a strong starting point for\nunsupervised learning of various repair tasks.", "title": "Break-It-Fix-It: Unsupervised Learning for Program Repair"}, {"link": "https://arxiv.org/abs/2106.06603", "abstract": "ldp deployments are vulnerable to inference attacks as an adversary can link\nthe noisy responses to their identity and subsequently, auxiliary information\nusing the order of the data. An alternative model, shuffle DP, prevents this by\nshuffling the noisy responses uniformly at random. However, this limits the\ndata learnability -- only symmetric functions (input order agnostic) can be\nlearned. In this paper, we strike a balance and propose a generalized shuffling\nframework that interpolates between the two deployment models. We show that\nsystematic shuffling of the noisy responses can thwart specific inference\nattacks while retaining some meaningful data learnability. To this end, we\npropose a novel privacy guarantee, d-sigma privacy, that captures the privacy\nof the order of a data sequence. d-sigma privacy allows tuning the granularity\nat which the ordinal information is maintained, which formalizes the degree the\nresistance to inference attacks trading it off with data learnability.\nAdditionally, we propose a novel shuffling mechanism that can achieve d-sigma\nprivacy and demonstrate the practicality of our mechanism via evaluation on\nreal-world datasets.", "title": "A Shuffling Framework for Local Differential Privacy"}, {"link": "https://arxiv.org/abs/2106.06604", "abstract": "We present a tool-supported approach for the synthesis, verification and\nvalidation of the control software responsible for the safety of the\nhuman-robot interaction in manufacturing processes that use collaborative\nrobots. In human-robot collaboration, software-based safety controllers are\nused to improve operational safety, e.g., by triggering shutdown mechanisms or\nemergency stops to avoid accidents. Complex robotic tasks and increasingly\nclose human-robot interaction pose new challenges to controller developers and\ncertification authorities. Key among these challenges is the need to assure the\ncorrectness of safety controllers under explicit (and preferably weak)\nassumptions. Our controller synthesis, verification and validation approach is\ninformed by the process, risk analysis, and relevant safety regulations for the\ntarget application. Controllers are selected from a design space of feasible\ncontrollers according to a set of optimality criteria, are formally verified\nagainst correctness criteria, and are translated into executable code and\nvalidated in a digital twin. The resulting controller can detect the occurrence\nof hazards, move the process into a safe state, and, in certain circumstances,\nreturn the process to an operational state from which it can resume its\noriginal task. We show the effectiveness of our software engineering approach\nthrough a case study involving the development of a safety controller for a\nmanufacturing work cell equipped with a collaborative robot.", "title": "Verified Synthesis of Optimal Safety Controllers for Human-Robot  Collaboration"}, {"link": "https://arxiv.org/abs/2106.06605", "abstract": "While there is an abundance of popular writing targeted to podcast creators\non how to speak in ways that engage their listeners, there has been little\ndata-driven analysis of podcasts that relates linguistic style with listener\nengagement. In this paper, we investigate how various factors -- vocabulary\ndiversity, distinctiveness, emotion, and syntax, among others -- correlate with\nengagement, based on analysis of the creators' written descriptions and\ntranscripts of the audio. We build models with different textual\nrepresentations, and show that the identified features are highly predictive of\nengagement. Our analysis tests popular wisdom about stylistic elements in\nhigh-engagement podcasts, corroborating some aspects, and adding new\nperspectives on others.", "title": "Modeling Language Usage and Listener Engagement in Podcasts"}, {"link": "https://arxiv.org/abs/2106.06607", "abstract": "The invariance principle from causality is at the heart of notable approaches\nsuch as invariant risk minimization (IRM) that seek to address\nout-of-distribution (OOD) generalization failures. Despite the promising\ntheory, invariance principle-based approaches fail in common classification\ntasks, where invariant (causal) features capture all the information about the\nlabel. Are these failures due to the methods failing to capture the invariance?\nOr is the invariance principle itself insufficient? To answer these questions,\nwe revisit the fundamental assumptions in linear regression tasks, where\ninvariance-based approaches were shown to provably generalize OOD. In contrast\nto the linear regression tasks, we show that for linear classification tasks we\nneed much stronger restrictions on the distribution shifts, or otherwise OOD\ngeneralization is impossible. Furthermore, even with appropriate restrictions\non distribution shifts in place, we show that the invariance principle alone is\ninsufficient. We prove that a form of the information bottleneck constraint\nalong with invariance helps address key failures when invariant features\ncapture all the information about the label and also retains the existing\nsuccess when they do not. We propose an approach that incorporates both of\nthese principles and demonstrate its effectiveness in several experiments.", "title": "Invariance Principle Meets Information Bottleneck for  Out-of-Distribution Generalization"}, {"link": "https://arxiv.org/abs/2106.06610", "abstract": "There has been enormous progress in the last few years in designing\nconceivable (though not always practical) neural networks that respect the\ngauge symmetries -- or coordinate freedom -- of physical law. Some of these\nframeworks make use of irreducible representations, some make use of higher\norder tensor objects, and some apply symmetry-enforcing constraints. Different\nphysical laws obey different combinations of fundamental symmetries, but a\nlarge fraction (possibly all) of classical physics is equivariant to\ntranslation, rotation, reflection (parity), boost (relativity), and\npermutations. Here we show that it is simple to parameterize universally\napproximating polynomial functions that are equivariant under these symmetries,\nor under the Euclidean, Lorentz, and Poincar\\'e groups, at any dimensionality\n$d$. The key observation is that nonlinear O($d$)-equivariant (and\nrelated-group-equivariant) functions can be expressed in terms of a lightweight\ncollection of scalars -- scalar products and scalar contractions of the scalar,\nvector, and tensor inputs. These results demonstrate theoretically that\ngauge-invariant deep learning models for classical physics with good scaling\nfor large problems are feasible right now.", "title": "Scalars are universal: Gauge-equivariant machine learning, structured  like classical physics"}, {"link": "https://arxiv.org/abs/2106.06613", "abstract": "In many coordination problems, independently reasoning humans are able to\ndiscover mutually compatible policies. In contrast, independently trained\nself-play policies are often mutually incompatible. Zero-shot coordination\n(ZSC) has recently been proposed as a new frontier in multi-agent reinforcement\nlearning to address this fundamental issue. Prior work approaches the ZSC\nproblem by assuming players can agree on a shared learning algorithm but not on\nlabels for actions and observations, and proposes other-play as an optimal\nsolution. However, until now, this \"label-free\" problem has only been\ninformally defined. We formalize this setting as the label-free coordination\n(LFC) problem by defining the label-free coordination game. We show that\nother-play is not an optimal solution to the LFC problem as it fails to\nconsistently break ties between incompatible maximizers of the other-play\nobjective. We introduce an extension of the algorithm, other-play with\ntie-breaking, and prove that it is optimal in the LFC problem and an\nequilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the\nZSC setting aims to prevent, we conclude that the LFC problem does not reflect\nthe aims of ZSC. To address this, we introduce an alternative informal\noperationalization of ZSC as a starting point for future work.", "title": "A New Formalism, Method and Open Issues for Zero-Shot Coordination"}, {"link": "https://arxiv.org/abs/2106.06614", "abstract": "We reprove the countable splitting lemma by adapting Nawrotzki's algorithm\nwhich produces a sequence that converges to a solution. Our algorithm combines\nNawrotzki's approach with taking finite cuts. It is constructive in the sense\nthat each term of the iteratively built approximating sequence as well as the\nerror between the approximants and the solution is computable with finitely\nmany algebraic operations.", "title": "Nawrotzki's Algorithm for the Countable Splitting Lemma, Constructively"}, {"link": "https://arxiv.org/abs/2106.06615", "abstract": "Recent works on Bayesian neural networks (BNNs) have highlighted the need to\nbetter understand the implications of using Gaussian priors in combination with\nthe compositional structure of the network architecture. Similar in spirit to\nthe kind of analysis that has been developed to devise better initialization\nschemes for neural networks (cf. He- or Xavier initialization), we derive a\nprecise characterization of the prior predictive distribution of finite-width\nReLU networks with Gaussian weights. While theoretical results have been\nobtained for their heavy-tailedness, the full characterization of the prior\npredictive distribution (i.e. its density, CDF and moments), remained unknown\nprior to this work. Our analysis, based on the Meijer-G function, allows us to\nquantify the influence of architectural choices such as the width or depth of\nthe network on the resulting shape of the prior predictive distribution. We\nalso formally connect our results to previous work in the infinite width\nsetting, demonstrating that the moments of the distribution converge to those\nof a normal log-normal mixture in the infinite depth limit. Finally, our\nresults provide valuable guidance on prior design: for instance, controlling\nthe predictive variance with depth- and width-informed priors on the weights of\nthe network.", "title": "Precise characterization of the prior predictive distribution of deep  ReLU networks"}, {"link": "https://arxiv.org/abs/2106.06616", "abstract": "The sharing of scarce resources among multiple rational agents is one of the\nclassical problems in economics. In exchange economies, which are used to model\nsuch situations, agents begin with an initial endowment of resources and\nexchange them in a way that is mutually beneficial until they reach a\ncompetitive equilibrium (CE). CE allocations are Pareto efficient and fair.\nConsequently, they are used widely in designing mechanisms for fair division.\nHowever, computing CEs requires the knowledge of agent preferences which are\nunknown in several applications of interest. In this work, we explore a new\nonline learning mechanism, which, on each round, allocates resources to the\nagents and collects stochastic feedback on their experience in using that\nallocation. Its goal is to learn the agent utilities via this feedback and\nimitate the allocations at a CE in the long run. We quantify CE behavior via\ntwo losses and propose a randomized algorithm which achieves\n$\\bigOtilde(\\sqrt{T})$ loss after $T$ rounds under both criteria. Empirically,\nwe demonstrate the effectiveness of this mechanism through numerical\nsimulations.", "title": "Online Learning of Competitive Equilibria in Exchange Economies"}, {"link": "https://arxiv.org/abs/2106.06617", "abstract": "Loops are pervasive in robotics problems, appearing in mapping and\nlocalization, where one is interested in finding loop closure constraints to\nbetter approximate robot poses or other estimated quantities, as well as\nplanning and prediction, where one is interested in the homotopy classes of the\nspace through which a robot is moving. We generalize the standard topological\ndefinition of a loop to cases where a trajectory passes close to itself, but\ndoesn't necessarily touch, giving a definition that is more practical for real\nrobotics problems. This relaxation leads to new and useful properties of\ninexact loops, such as their ability to be partitioned into topologically\nconnected sets closely matching the concept of a \"loop closure\", and the\nexistence of simple and nonsimple loops. Building from these ideas, we\nintroduce several ways to measure properties and quantities of inexact loops on\na trajectory, such as the trajectory's \"loop area\" and \"loop density\", and use\nthem to compare strategies for sampling representative inexact loops to build\nconstraints in mapping and localization problems.", "title": "Inexact Loops in Robotics Problems"}, {"link": "https://arxiv.org/abs/2106.06620", "abstract": "A fundamental challenge in artificial intelligence is learning useful\nrepresentations of data that yield good performance on a downstream task,\nwithout overfitting to spurious input features. Extracting such task-relevant\npredictive information is particularly difficult for real-world datasets. In\nthis work, we propose Contrastive Input Morphing (CIM), a representation\nlearning framework that learns input-space transformations of the data to\nmitigate the effect of irrelevant input features on downstream performance. Our\nmethod leverages a perceptual similarity metric via a triplet loss to ensure\nthat the transformation preserves task-relevant information.Empirically, we\ndemonstrate the efficacy of our approach on tasks which typically suffer from\nthe presence of spurious correlations: classification with nuisance\ninformation, out-of-distribution generalization, and preservation of subgroup\naccuracies. We additionally show that CIM is complementary to other mutual\ninformation-based representation learning techniques, and demonstrate that it\nimproves the performance of variational information bottleneck (VIB) when used\ntogether.", "title": "Robust Representation Learning via Perceptual Similarity Metrics"}, {"link": "https://arxiv.org/abs/2106.06621", "abstract": "Neural networks are a popular tool for modeling sequential data but they\ngenerally do not treat time as a continuous variable. Neural ODEs represent an\nimportant exception: they parameterize the time derivative of a hidden state\nwith a neural network and then integrate over arbitrary amounts of time. But\nthese parameterizations, which have arbitrary curvature, can be hard to\nintegrate and thus train and evaluate. In this paper, we propose making a\npiecewise-constant approximation to Neural ODEs to mitigate these issues. Our\nmodel can be integrated exactly via Euler integration and can generate\nautoregressive samples in 3-20 times fewer steps than comparable RNN and\nODE-RNN models. We evaluate our model on several synthetic physics tasks and a\nplanning task inspired by the game of billiards. We find that it matches the\nperformance of baseline approaches while requiring less time to train and\nevaluate.", "title": "Piecewise-constant Neural ODEs"}, {"link": "https://arxiv.org/abs/2106.06623", "abstract": "Deep learning methods such as convolutional neural networks (CNNs) are\ndifficult to directly utilize to analyze whole slide images (WSIs) due to the\nlarge image dimensions. We overcome this limitation by proposing a novel\ntwo-stage approach. First, we extract a set of representative patches (called\nmosaic) from a WSI. Each patch of a mosaic is encoded to a feature vector using\na deep network. The feature extractor model is fine-tuned using hierarchical\ntarget labels of WSIs, i.e., anatomic site and primary diagnosis. In the second\nstage, a set of encoded patch-level features from a WSI is used to compute the\nprimary diagnosis probability through the proposed Pay Attention with Focus\nscheme, an attention-weighted averaging of predicted probabilities for all\npatches of a mosaic modulated by a trainable focal factor. Experimental results\nshow that the proposed model can be robust, and effective for the\nclassification of WSIs.", "title": "Pay Attention with Focus: A Novel Learning Scheme for Classification of  Whole Slide Images"}, {"link": "https://arxiv.org/abs/2106.06624", "abstract": "Certifiable local robustness, which rigorously precludes small-norm\nadversarial examples, has received significant attention as a means of\naddressing security concerns in deep learning. However, for some classification\nproblems, local robustness is not a natural objective, even in the presence of\nadversaries; for example, if an image contains two classes of subjects, the\ncorrect label for the image may be considered arbitrary between the two, and\nthus enforcing strict separation between them is unnecessary. In this work, we\nintroduce two relaxed safety properties for classifiers that address this\nobservation: (1) relaxed top-k robustness, which serves as the analogue of\ntop-k accuracy; and (2) affinity robustness, which specifies which sets of\nlabels must be separated by a robustness margin, and which can be\n$\\epsilon$-close in $\\ell_p$ space. We show how to construct models that can be\nefficiently certified against each relaxed robustness property, and trained\nwith very little overhead relative to standard gradient descent. Finally, we\ndemonstrate experimentally that these relaxed variants of robustness are\nwell-suited to several significant classification problems, leading to lower\nrejection rates and higher certified accuracies than can be obtained when\ncertifying \"standard\" local robustness.", "title": "Relaxing Local Robustness"}, {"link": "https://arxiv.org/abs/2106.06627", "abstract": "With the rapid growth in mobile computing, massive amounts of data and\ncomputing resources are now located at the edge. To this end, Federated\nlearning (FL) is becoming a widely adopted distributed machine learning (ML)\nparadigm, which aims to harness this expanding skewed data locally in order to\ndevelop rich and informative models. In centralized FL, a collection of devices\ncollaboratively solve a ML task under the coordination of a central server.\nHowever, existing FL frameworks make an over-simplistic assumption about\nnetwork connectivity and ignore the communication bandwidth of the different\nlinks in the network. In this paper, we present and study a novel FL algorithm,\nin which devices mostly collaborate with other devices in a pairwise manner.\nOur nonparametric approach is able to exploit network topology to reduce\ncommunication bottlenecks. We evaluate our approach on various FL benchmarks\nand demonstrate that our method achieves 10X better communication efficiency\nand around 8% increase in accuracy compared to the centralized approach.", "title": "Efficient and Less Centralized Federated Learning"}, {"link": "https://arxiv.org/abs/2106.06629", "abstract": "Despite recent progress in depth sensing and 3D reconstruction, mirror\nsurfaces are a significant source of errors. To address this problem, we create\nthe Mirror3D dataset: a 3D mirror plane dataset based on three RGBD datasets\n(Matterport3D, NYUv2 and ScanNet) containing 7,011 mirror instance masks and 3D\nplanes. We then develop Mirror3DNet: a module that refines raw sensor depth or\nestimated depth to correct errors on mirror surfaces. Our key idea is to\nestimate the 3D mirror plane based on RGB input and surrounding depth context,\nand use this estimate to directly regress mirror surface depth. Our experiments\nshow that Mirror3DNet significantly mitigates errors from a variety of input\ndepth data, including raw sensor depth and depth estimation or completion\nmethods.", "title": "Mirror3D: Depth Refinement for Mirror Surfaces"}, {"link": "https://arxiv.org/abs/2106.06630", "abstract": "We study the adversarial robustness in offline reinforcement learning. Given\na batch dataset consisting of tuples $(s, a, r, s')$, an adversary is allowed\nto arbitrarily modify $\\epsilon$ fraction of the tuples. From the corrupted\ndataset the learner aims to robustly identify a near-optimal policy. We first\nshow that a worst-case $\\Omega(d\\epsilon)$ optimality gap is unavoidable in\nlinear MDP of dimension $d$, even if the adversary only corrupts the reward\nelement in a tuple. This contrasts with dimension-free results in robust\nsupervised learning and best-known lower-bound in the online RL setting with\ncorruption. Next, we propose robust variants of the Least-Square Value\nIteration (LSVI) algorithm utilizing robust supervised learning oracles, which\nachieve near-matching performances in cases both with and without full data\ncoverage. The algorithm requires the knowledge of $\\epsilon$ to design the\npessimism bonus in the no-coverage case. Surprisingly, in this case, the\nknowledge of $\\epsilon$ is necessary, as we show that being adaptive to unknown\n$\\epsilon$ is impossible.This again contrasts with recent results on\ncorruption-robust online RL and implies that robust offline RL is a strictly\nharder problem.", "title": "Corruption-Robust Offline Reinforcement Learning"}, {"link": "https://arxiv.org/abs/2106.06631", "abstract": "Counterfactual explanations are usually generated through heuristics that are\nsensitive to the search's initial conditions. The absence of guarantees of\nperformance and robustness hinders trustworthiness. In this paper, we take a\ndisciplined approach towards counterfactual explanations for tree ensembles. We\nadvocate for a model-based search aiming at \"optimal\" explanations and propose\nefficient mixed-integer programming approaches. We show that isolation forests\ncan be modeled within our framework to focus the search on plausible\nexplanations with a low outlier score. We provide comprehensive coverage of\nadditional constraints that model important objectives, heterogeneous data\ntypes, structural constraints on the feature space, along with resource and\nactionability restrictions. Our experimental analyses demonstrate that the\nproposed search approach requires a computational effort that is orders of\nmagnitude smaller than previous mathematical programming algorithms. It scales\nup to large data sets and tree ensembles, where it provides, within seconds,\nsystematic explanations grounded on well-defined models solved to optimality.", "title": "Optimal Counterfactual Explanations in Tree Ensembles"}, {"link": "https://arxiv.org/abs/2106.06633", "abstract": "On the topic of probabilistic rewriting, there are several works studying\nboth termination and confluence of different systems. While working with a\nlambda calculus modelling quantum computation, we found a system with\nprobabilistic rewriting rules and strongly normalizing terms. We examine the\neffect of small modifications in probabilistic rewriting, affine variables, and\nstrategies on the overall confluence in this strongly normalizing probabilistic\ncalculus.", "title": "A note on confluence in typed probabilistic lambda calculi"}, {"link": "https://arxiv.org/abs/2106.06635", "abstract": "We consider a cache-aided wireless device-to-device (D2D) network under the\nconstraint of one-shot delivery, where the placement phase is orchestrated by a\ncentral server. We assume that the devices' caches are filled with uncoded\ndata, and the whole file database at the server is made available in the\ncollection of caches. Following this phase, the files requested by the users\nare serviced by inter-device multicast communication. For such a system\nsetting, we provide the exact characterization of load-memory trade-off, by\nderiving both the minimum average and the minimum peak sum-loads of links\nbetween devices, for a given individual memory size at disposal of each user.", "title": "On D2D Caching with Uncoded Cache Placement"}, {"link": "https://arxiv.org/abs/2106.06636", "abstract": "Simultaneous speech-to-text translation is widely useful in many scenarios.\nThe conventional cascaded approach uses a pipeline of streaming ASR followed by\nsimultaneous MT, but suffers from error propagation and extra latency. To\nalleviate these issues, recent efforts attempt to directly translate the source\nspeech into target text simultaneously, but this is much harder due to the\ncombination of two separate tasks. We instead propose a new paradigm with the\nadvantages of both cascaded and end-to-end approaches. The key idea is to use\ntwo separate, but synchronized, decoders on streaming ASR and direct\nspeech-to-text translation (ST), respectively, and the intermediate results of\nASR guide the decoding policy of (but is not fed as input to) ST. During\ntraining time, we use multitask learning to jointly learn these two tasks with\na shared encoder. En-to-De and En-to-Es experiments on the MuSTC dataset\ndemonstrate that our proposed technique achieves substantially better\ntranslation quality at similar levels of latency.", "title": "Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized  Streaming ASR"}, {"link": "https://arxiv.org/abs/2106.06637", "abstract": "Image registration is a fundamental building block for various applications\nin medical image analysis. To better explore the correlation between the fixed\nand moving images and improve registration performance, we propose a novel deep\nlearning network, Co-Attention guided Registration Network (CAR-Net). CAR-Net\nemploys a co-attention block to learn a new representation of the inputs, which\ndrives the registration of the fixed and moving images. Experiments on UK\nBiobank cardiac cine-magnetic resonance image data demonstrate that CAR-Net\nobtains higher registration accuracy and smoother deformation fields than\nstate-of-the-art unsupervised registration methods, while achieving comparable\nor better registration performance than corresponding weakly-supervised\nvariants. In addition, our approach can provide critical structural information\nof the input fixed and moving images simultaneously in a completely\nunsupervised manner.", "title": "CAR-Net: Unsupervised Co-Attention Guided Registration Network for Joint  Registration and Structure Learning"}, {"link": "https://arxiv.org/abs/2106.06639", "abstract": "Federated Learning (FL) trains a shared model across distributed devices\nwhile keeping the training data on the devices. Most FL schemes are\nsynchronous: they perform a synchronized aggregation of model updates from\nindividual devices. Synchronous training can be slow because of late-arriving\ndevices (stragglers). On the other hand, completely asynchronous training makes\nFL less private because of incompatibility with secure aggregation. In this\nwork, we propose a model aggregation scheme, FedBuff, that combines the best\nproperties of synchronous and asynchronous FL. Similar to synchronous FL,\nFedBuff is compatible with secure aggregation. Similar to asynchronous FL,\nFedBuff is robust to stragglers. In FedBuff, clients trains asynchronously and\nsend updates to the server. The server aggregates client updates in a private\nbuffer until updates have been received, at which point a server model update\nis immediately performed. We provide theoretical convergence guarantees for\nFedBuff in a non-convex setting. Empirically, FedBuff converges up to 3.8x\nfaster than previous proposals for synchronous FL (e.g., FedAvgM), and up to\n2.5x faster than previous proposals for asynchronous FL (e.g., FedAsync). We\nshow that FedBuff is robust to different staleness distributions and is more\nscalable than synchronous FL techniques.", "title": "Federated Learning with Buffered Asynchronous Aggregation"}, {"link": "https://arxiv.org/abs/2106.06641", "abstract": "Conservative symmetric second-order one-step schemes are derived for\ndynamical systems describing various many-body systems using the Discrete\nMultiplier Method. This includes conservative schemes for the $n$-species\nLotka-Volterra system, the $n$-body problem with radially symmetric potential\nand the $n$-point vortex models in the plane and on the sphere. In particular,\nwe recover Greenspan-Labudde's conservative schemes for the $n$-body problem.\nNumerical experiments are shown verifying the conservative property of the\nschemes and second-order accuracy.", "title": "Conservative Integrators for Many-body Problems"}, {"link": "https://arxiv.org/abs/2106.06646", "abstract": "We apply the coded caching scheme proposed by Maddah-Ali and Niesen to a\nmultipoint multicasting video paradigm. Partially caching the video files on\nthe wireless devices provides an opportunity to decrease data traffic load in\npeak hours via sending multicast coded messages to users. In this paper, we\npropose a two-hop wireless network for video multicasting, where the common\ncoded multicast message is transmitted through different single antenna Edge\nNodes (ENs) to multiple antenna users. Each user can decide to decode any EN by\nusing a zero forcing receiver. Motivated by Scalable Video Coding (SVC), we\nconsider successive refinement source coding in order to provide a ``softer''\ntradeoff between the number of decoded ENs and the source distortion at each\nuser receiver. The resulting coding scheme can be seen as the concatenation of\nMaddah-Ali and Niesen coded caching for each source-coded layer, and multiple\ndescription coding. Using stochastic geometry, we investigate the tradeoff\nbetween delivery time and per-user average source distortion. The proposed\nsystem is spatially scalable in the sense that, for given users' and ENs'\nspatial density, the achieved distortion-delivery time performance is\nindependent of the coverage area (for in the limit of large area).", "title": "Spatially Scalable Lossy Coded Caching"}, {"link": "https://arxiv.org/abs/2106.06649", "abstract": "Video Instance Segmentation (VIS) is a multi-task problem performing\ndetection, segmentation, and tracking simultaneously. Extended from image set\napplications, video data additionally induces the temporal information, which,\nif handled appropriately, is very useful to identify and predict object\nmotions. In this work, we design a unified model to mutually learn these tasks.\nSpecifically, we propose two modules, named Temporally Correlated Instance\nSegmentation (TCIS) and Bidirectional Tracking (BiTrack), to take the benefit\nof the temporal correlation between the object's instance masks across adjacent\nframes. On the other hand, video data is often redundant due to the frame's\noverlap. Our analysis shows that this problem is particularly severe for the\nYoutubeVOS-VIS2021 data. Therefore, we propose a Multi-Source Data (MSD)\ntraining mechanism to compensate for the data deficiency. By combining these\ntechniques with a bag of tricks, the network performance is significantly\nboosted compared to the baseline, and outperforms other methods by a\nconsiderable margin on the YoutubeVOS-VIS 2019 and 2021 datasets.", "title": "1st Place Solution for YouTubeVOS Challenge 2021:Video Instance  Segmentation"}, {"link": "https://arxiv.org/abs/2106.06650", "abstract": "Existing approaches to unsupervised object discovery (UOD) do not scale up to\nlarge datasets without approximations which compromise their performance. We\npropose a novel formulation of UOD as a ranking problem, amenable to the\narsenal of distributed methods available for eigenvalue problems and link\nanalysis. Extensive experiments with COCO and OpenImages demonstrate that, in\nthe single-object discovery setting where a single prominent object is sought\nin each image, the proposed LOD (Large-scale Object Discovery) approach is on\npar with, or better than the state of the art for medium-scale datasets (up to\n120K images), and over 37% better than the only other algorithms capable of\nscaling up to 1.7M images. In the multi-object discovery setting where multiple\nobjects are sought in each image, the proposed LOD is over 14% better in\naverage precision (AP) than all other methods for datasets ranging from 20K to\n1.7M images.", "title": "Large-Scale Unsupervised Object Discovery"}, {"link": "https://arxiv.org/abs/2106.06652", "abstract": "When optimizing software for the cloud, monolithic applications need to be\npartitioned into many smaller *microservices*. While many tools have been\nproposed for this task, we warn that the evaluation of those approaches has\nbeen incomplete; e.g. minimal prior exploration of hyperparameter optimization.\nUsing a set of open source Java EE applications, we show here that (a) such\noptimization can significantly improve microservice partitioning; and that (b)\nan open issue for future work is how to find which optimizer works best for\ndifferent problems. To facilitate that future work, see\n[https://github.com/yrahul3910/ase-tuned-mono2micro](https://github.com/yrahul3910/ase-tuned-mono2micro)\nfor a reproduction package for this research.", "title": "Lessons learned from hyper-parameter tuning for microservice candidate  identification"}, {"link": "https://arxiv.org/abs/2106.06654", "abstract": "When data is publicly released for human consumption, it is unclear how to\nprevent its unauthorized usage for machine learning purposes. Successful model\ntraining may be preventable with carefully designed dataset modifications, and\nwe present a proof-of-concept approach for the image classification setting. We\npropose methods based on the notion of adversarial shortcuts, which encourage\nmodels to rely on non-robust signals rather than semantic features, and our\nexperiments demonstrate that these measures successfully prevent deep learning\nmodels from achieving high accuracy on real, unmodified data examples.", "title": "Disrupting Model Training with Adversarial Shortcuts"}, {"link": "https://arxiv.org/abs/2106.06655", "abstract": "Assessing the performance of human movements during teleoperation and virtual\nreality is a challenging problem, particularly in 3D space due to complex\nspatial settings. Despite the presence of a multitude of metrics, a compelling\nstandardized 3D metric is yet missing, aggravating inter-study comparability\nbetween different studies. Hence, evaluating human performance in virtual\nenvironments is a long-standing research goal, and a performance metric that\ncombines two or more metrics under one formulation remains largely unexplored,\nparticularly in higher dimensions. The absence of such a metric is primarily\nattributed to the discrepancies between pointing and manipulation, the complex\nspatial variables in 3D, and the combination of translational and rotational\nmovements altogether. In this work, four experiments were designed and\nconducted with progressively higher spatial complexity to study and compare\nexisting metrics thoroughly. The research goal was to quantify the difficulty\nof these 3D tasks and model human performance sufficiently in full 3D\nperipersonal space. Consequently, a new model extension has been proposed and\nits applicability has been validated across all the experimental results,\nshowing improved modelling and representation of human performance in combined\nmovements of 3D object pointing and manipulation tasks than existing work.\nLastly, the implications on 3D interaction, teleoperation and object task\ndesign in virtual reality are discussed.", "title": "Metrics for 3D Object Pointing and Manipulation in Virtual Reality"}, {"link": "https://arxiv.org/abs/2106.06657", "abstract": "This paper studies zero-shot domain adaptation where each domain is indexed\non a multi-dimensional array, and we only have data from a small subset of\ndomains. Our goal is to produce predictors that perform well on \\emph{unseen}\ndomains. We propose a model which consists of a domain-invariant latent\nrepresentation layer and a domain-specific linear prediction layer with a\nlow-rank tensor structure. Theoretically, we present explicit sample complexity\nbounds to characterize the prediction error on unseen domains in terms of the\nnumber of domains with training data and the number of data per domain. To our\nknowledge, this is the first finite-sample guarantee for zero-shot domain\nadaptation. In addition, we provide experiments on two-way MNIST and four-way\nfiber sensing datasets to demonstrate the effectiveness of our proposed model.", "title": "Provable Adaptation across Multiway Domains via Representation Learning"}, {"link": "https://arxiv.org/abs/2106.06662", "abstract": "Pixelizations of Platonic solids such as the cube and icosahedron have been\nwidely used to represent spherical data, from climate records to Cosmic\nMicrowave Background maps. Platonic solids have well-known global symmetries.\nOnce we pixelize each face of the solid, each face also possesses its own local\nsymmetries in the form of Euclidean isometries. One way to combine these\nsymmetries is through a hierarchy. However, this approach does not adequately\nmodel the interplay between the two levels of symmetry transformations. We show\nhow to model this interplay using ideas from group theory, identify the\nequivariant linear maps, and introduce equivariant padding that respects these\nsymmetries. Deep networks that use these maps as their building blocks\ngeneralize gauge equivariant CNNs on pixelized spheres. These deep networks\nachieve state-of-the-art results on semantic segmentation for climate data and\nomnidirectional image processing. Code is available at https://git.io/JGiZA.", "title": "Equivariant Networks for Pixelized Spheres"}, {"link": "https://arxiv.org/abs/2106.06663", "abstract": "Graph Neural Networks (GNNs) have achieved promising performance in various\nreal-world applications. However, recent studies have shown that GNNs are\nvulnerable to adversarial attacks. In this paper, we study a\nrecently-introduced realistic attack scenario on graphs -- graph injection\nattack (GIA). In the GIA scenario, the adversary is not able to modify the\nexisting link structure and node attributes of the input graph, instead the\nattack is performed by injecting adversarial nodes into it. We present an\nanalysis on the topological vulnerability of GNNs under GIA setting, based on\nwhich we propose the Topological Defective Graph Injection Attack (TDGIA) for\neffective injection attacks. TDGIA first introduces the topological defective\nedge selection strategy to choose the original nodes for connecting with the\ninjected ones. It then designs the smooth feature optimization objective to\ngenerate the features for the injected nodes. Extensive experiments on\nlarge-scale datasets show that TDGIA can consistently and significantly\noutperform various attack baselines in attacking dozens of defense GNN models.\nNotably, the performance drop on target GNNs resultant from TDGIA is more than\ndouble the damage brought by the best attack solution among hundreds of\nsubmissions on KDD-CUP 2020.", "title": "TDGIA:Effective Injection Attacks on Graph Neural Networks"}, {"link": "https://arxiv.org/abs/2106.06666", "abstract": "HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their\npotential in modeling high-order relations preserved in graph structured data.\nHowever, most existing convolution filters are localized and determined by the\npre-defined initial hypergraph topology, neglecting to explore implicit and\nlong-ange relations in real-world data. In this paper, we propose the first\nlearning-based method tailored for constructing adaptive hypergraph structure,\ntermed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic\nplug-in-play module for improving the representational power of HGCNNs.\nSpecifically, HERALD adaptively optimizes the adjacency relationship between\nhypernodes and hyperedges in an end-to-end manner and thus the task-aware\nhypergraph is learned. Furthermore, HERALD employs the self-attention mechanism\nto capture the non-local paired-nodes relation. Extensive experiments on\nvarious popular hypergraph datasets for node classification and graph\nclassification tasks demonstrate that our approach obtains consistent and\nconsiderable performance enhancement, proving its effectiveness and\ngeneralization ability.", "title": "Learnable Hypergraph Laplacian for Hypergraph Learning"}, {"link": "https://arxiv.org/abs/2106.06667", "abstract": "Transfer learning eases the burden of training a well-performed model from\nscratch, especially when training data is scarce and computation power is\nlimited. In deep learning, a typical strategy for transfer learning is to\nfreeze the early layers of a pre-trained model and fine-tune the rest of its\nlayers on the target domain. Previous work focuses on the accuracy of the\ntransferred model but neglects the transfer of adversarial robustness. In this\nwork, we first show that transfer learning improves the accuracy on the target\ndomain but degrades the inherited robustness of the target model. To address\nsuch a problem, we propose a novel cooperative adversarially-robust transfer\nlearning (CARTL) by pre-training the model via feature distance minimization\nand fine-tuning the pre-trained model with non-expansive fine-tuning for target\ndomain tasks. Empirical results show that CARTL improves the inherited\nrobustness by about 28% at most compared with the baseline with the same degree\nof accuracy. Furthermore, we study the relationship between the batch\nnormalization (BN) layers and the robustness in the context of transfer\nlearning, and we reveal that freezing BN layers can further boost the\nrobustness transfer.", "title": "CARTL: Cooperative Adversarially-Robust Transfer Learning"}, {"link": "https://arxiv.org/abs/2106.06672", "abstract": "Capturing contextual dependencies has proven useful to improve the\nrepresentational power of deep neural networks. Recent approaches that focus on\nmodeling global context, such as self-attention and non-local operation,\nachieve this goal by enabling unconstrained pairwise interactions between\nelements. In this work, we consider learning representations for deformable\nobjects which can benefit from context exploitation by modeling the structural\ndependencies that the data intrinsically possesses. To this end, we provide a\nnovel structure-regularized attention mechanism, which formalizes feature\ninteraction as structural factorization through the use of a pair of\nlight-weight operations. The instantiated building blocks can be directly\nincorporated into modern convolutional neural networks, to boost the\nrepresentational power in an efficient manner. Comprehensive studies on\nmultiple tasks and empirical comparisons with modern attention mechanisms\ndemonstrate the gains brought by our method in terms of both performance and\nmodel complexity. We further investigate its effect on feature representations,\nshowing that our trained models can capture diversified representations\ncharacterizing object parts without resorting to extra supervision.", "title": "Structure-Regularized Attention for Deformable Object Representation"}, {"link": "https://arxiv.org/abs/2106.06673", "abstract": "This work investigates the application of sampling methods for sentiment\nanalysis on two different highly imbalanced datasets. One dataset contains\nonline user reviews from the cooking platform Epicurious and the other contains\ncomments given to the Planned Parenthood organization. In both these datasets,\nthe classes of interest are rare. Word n-grams were used as features from these\ndatasets. A feature selection technique based on information gain is first\napplied to reduce the number of features to a manageable space. A number of\ndifferent sampling methods were then applied to mitigate the class imbalance\nproblem which are then analyzed.", "title": "Study of sampling methods in sentiment analysis of imbalanced data"}, {"link": "https://arxiv.org/abs/2106.06677", "abstract": "With spatial analytic, econometric, and visualization tools, this book\nchapter investigates greenhouse gas emissions for the on-road passenger vehicle\ntransport sector in the Boston metropolitan area in 2014. It compares\ngreenhouse gas emission estimations from both the production-based and\nconsumption-based perspectives with two large-scale administrative datasets:\nthe vehicle odometer readings from individual vehicle annual inspection, and\nthe road inventory data containing road segment level geospatial and traffic\ninformation. Based on spatial econometric models that examine socioeconomic and\nbuilt environment factors contributing to the vehicle miles traveled at the\ncensus tract level, it offers insights to help cities reduce VMT and carbon\nfootprint for passenger vehicle travel. Finally, it recommends a pathway for\ncities and towns in the Boston metropolitan area to curb VMT and mitigate\ncarbon emissions to achieve climate goals of carbon neutrality.", "title": "Examining Passenger Vehicle Miles Traveled and Carbon Emissions in the  Boston Metropolitan Area"}, {"link": "https://arxiv.org/abs/2106.06678", "abstract": "An accurate and reliable technique for predicting Remaining Useful Life (RUL)\nfor battery cells proves helpful in battery-operated IoT devices, especially in\nremotely operated sensor nodes. Data-driven methods have proved to be the most\neffective methods until now. These IoT devices have low computational\ncapabilities to save costs, but Data-Driven battery health techniques often\nrequire a comparatively large amount of computational power to predict SOH and\nRUL due to most methods being feature-heavy. This issue calls for ways to\npredict RUL with the least amount of calculations and memory. This paper\nproposes an effective and novel peak extraction method to reduce computation\nand memory needs and provide accurate prediction methods using the least number\nof features while performing all calculations on-board. The model can\nself-sustain, requires minimal external interference, and hence operate\nremotely much longer. Experimental results prove the accuracy and reliability\nof this method. The Absolute Error (AE), Relative error (RE), and Root Mean\nSquare Error (RMSE) are calculated to compare effectiveness. The training of\nthe GPR model takes less than 2 seconds, and the correlation between SOH from\npeak extraction and RUL is 0.97.", "title": "iThing: Designing Next-Generation Things with Battery Health  Self-Monitoring Capabilities for Sustainable IoT in Smart Cities"}, {"link": "https://arxiv.org/abs/2106.06680", "abstract": "We consider the problem of constrained Markov Decision Process (CMDP) where\nan agent interacts with a unichain Markov Decision Process. At every\ninteraction, the agent obtains a reward. Further, there are $K$ cost functions.\nThe agent aims to maximize the long-term average reward while simultaneously\nkeeping the $K$ long-term average costs lower than a certain threshold. In this\npaper, we propose CMDP-PSRL, a posterior sampling based algorithm using which\nthe agent can learn optimal policies to interact with the CMDP. Further, for\nMDP with $S$ states, $A$ actions, and diameter $D$, we prove that following\nCMDP-PSRL algorithm, the agent can bound the regret of not accumulating rewards\nfrom optimal policy by $\\Tilde{O}(poly(DSA)\\sqrt{T})$. Further, we show that\nthe violations for any of the $K$ constraints is also bounded by\n$\\Tilde{O}(poly(DSA)\\sqrt{T})$. To the best of our knowledge, this is the first\nwork which obtains a $\\Tilde{O}(\\sqrt{T})$ regret bounds for ergodic MDPs with\nlong-term average constraints.", "title": "Markov Decision Processes with Long-Term Average Constraints"}, {"link": "https://arxiv.org/abs/2106.06682", "abstract": "This paper proposes a mesh-free computational framework and machine learning\ntheory for solving elliptic PDEs on unknown manifolds, identified with point\nclouds, based on diffusion maps (DM) and deep learning. The PDE solver is\nformulated as a supervised learning task to solve a least-squares regression\nproblem that imposes an algebraic equation approximating a PDE (and boundary\nconditions if applicable). This algebraic equation involves a graph-Laplacian\ntype matrix obtained via DM asymptotic expansion, which is a consistent\nestimator of second-order elliptic differential operators. The resulting\nnumerical method is to solve a highly non-convex empirical risk minimization\nproblem subjected to a solution from a hypothesis space of neural-network type\nfunctions. In a well-posed elliptic PDE setting, when the hypothesis space\nconsists of feedforward neural networks with either infinite width or depth, we\nshow that the global minimizer of the empirical loss function is a consistent\nsolution in the limit of large training data. When the hypothesis space is a\ntwo-layer neural network, we show that for a sufficiently large width, the\ngradient descent method can identify a global minimizer of the empirical loss\nfunction. Supporting numerical examples demonstrate the convergence of the\nsolutions and the effectiveness of the proposed solver in avoiding numerical\nissues that hampers the traditional approach when a large data set becomes\navailable, e.g., large matrix inversion.", "title": "Solving PDEs on Unknown Manifolds with Machine Learning"}, {"link": "https://arxiv.org/abs/2106.06684", "abstract": "This work presents a novel approach to improve the results of pose estimation\nby detecting and distinguishing between the occurrence of True and False\nPositive results. It achieves this by training a binary classifier on the\noutput of an arbitrary pose estimation algorithm, and returns a binary label\nindicating the validity of the result. We demonstrate that our approach\nimproves upon a state-of-the-art pose estimation result on the Sil\\'eane\ndataset, outperforming a variation of the alternative CullNet method by 4.15%\nin average class accuracy and 0.73% in overall accuracy at validation. Applying\nour method can also improve the pose estimation average precision results of\nOp-Net by 6.06% on average.", "title": "Multistream ValidNet: Improving 6D Object Pose Estimation by Automatic  Multistream Validation"}, {"link": "https://arxiv.org/abs/2106.06685", "abstract": "Adversarial robustness has become a topic of growing interest in machine\nlearning since it was observed that neural networks tend to be brittle. We\npropose an information-geometric formulation of adversarial defense and\nintroduce FIRE, a new Fisher-Rao regularization for the categorical\ncross-entropy loss, which is based on the geodesic distance between natural and\nperturbed input features. Based on the information-geometric properties of the\nclass of softmax distributions, we derive an explicit characterization of the\nFisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some\ninteresting properties as well as connections with standard regularization\nmetrics. Furthermore, for a simple linear and Gaussian model, we show that all\nPareto-optimal points in the accuracy-robustness region can be reached by FIRE\nwhile other state-of-the-art methods fail. Empirically, we evaluate the\nperformance of various classifiers trained with the proposed loss on standard\ndatasets, showing up to 2\\% of improvements in terms of robustness while\nreducing the training time by 20\\% over the best-performing methods.", "title": "Adversarial Robustness via Fisher-Rao Regularization"}, {"link": "https://arxiv.org/abs/2106.06688", "abstract": "Several Convolutional Deep Learning models have been proposed to classify the\ncognitive states utilizing several neuro-imaging domains. These models have\nachieved significant results, but they are heavily designed with millions of\nparameters, which increases train and test time, making the model complex and\nless suitable for real-time analysis. This paper proposes a simple, lightweight\nCNN model to classify cognitive states from Electroencephalograph (EEG)\nrecordings. We develop a novel pipeline to learn distinct cognitive\nrepresentation consisting of two stages. The first stage is to generate the 2D\nspectral images from neural time series signals in a particular frequency band.\nImages are generated to preserve the relationship between the neighboring\nelectrodes and the spectral property of the cognitive events. The second is to\ndevelop a time-efficient, computationally less loaded, and high-performing\nmodel. We design a network containing 4 blocks and major components include\nstandard and depth-wise convolution for increasing the performance and followed\nby separable convolution to decrease the number of parameters which maintains\nthe tradeoff between time and performance. We experiment on open access EEG\nmeditation dataset comprising expert, nonexpert meditative, and control states.\nWe compare performance with six commonly used machine learning classifiers and\nfour state of the art deep learning models. We attain comparable performance\nutilizing less than 4\\% of the parameters of other models. This model can be\nemployed in a real-time computation environment such as neurofeedback.", "title": "BRAIN2DEPTH: Lightweight CNN Model for Classification of Cognitive  States from EEG Recordings"}, {"link": "https://arxiv.org/abs/2106.06689", "abstract": "We propose two fast neural combinatory models for constituency parsing:\nbinary and multi-branching. Our models decompose the bottom-up parsing process\ninto 1) classification of tags, labels, and binary orientations or chunks and\n2) vector composition based on the computed orientations or chunks. These\nmodels have theoretical sub-quadratic complexity and empirical linear\ncomplexity. The binary model achieves an F1 score of 92.54 on Penn Treebank,\nspeeding at 1327.2 sents/sec. Both the models with XLNet provide near\nstate-of-the-art accuracies for English. Syntactic branching tendency and\nheadedness of a language are observed during the training and inference\nprocesses for Penn Treebank, Chinese Treebank, and Keyaki Treebank (Japanese).", "title": "Neural Combinatory Constituency Parsing"}, {"link": "https://arxiv.org/abs/2106.06692", "abstract": "Future generation wireless networks are designed with extremely low delay\nrequirements which makes even small contributed delays important. On the other\nhand, software defined networking (SDN) has been introduced as a key enabler of\nfuture wireless and cellular networks in order to make them more flexible. In\nSDN, a central controller manages all network equipments by setting the\nmatch-action pairs in flow tables of the devices. However, these flow tables\nhave limited capacity and thus are not capable of storing the rules of all the\nusers. In this paper, we consider an SDN-enabled base station (SD-BS) in a cell\nequipped with a limited capacity flow table. We analyze the expected delay\nincurred in processing of the incoming packets to the SD-BS and present a\nmathematical expression for it in terms of density of the users and cell area.", "title": "Delay Analysis of Base Station Flow Table in SDN-enabled Radio Access  Networks"}, {"link": "https://arxiv.org/abs/2106.06694", "abstract": "We analyze egocentric views of attended objects from infants. This paper\nshows 1) empirical evidence that children's egocentric views have more diverse\ndistributions compared to adults' views, 2) we can computationally simulate the\ninfants' distribution, and 3) the distribution is beneficial for training more\ngeneralized image classifiers not only for infant egocentric vision but for\nthird-person computer vision.", "title": "Reverse-engineer the Distributional Structure of Infant Egocentric Views  for Training Generalizable Image Classifiers"}, {"link": "https://arxiv.org/abs/2106.06695", "abstract": "State-of-the-art methods for scalable Gaussian processes use iterative\nalgorithms, requiring fast matrix vector multiplies (MVMs) with the covariance\nkernel. The Structured Kernel Interpolation (SKI) framework accelerates these\nMVMs by performing efficient MVMs on a grid and interpolating back to the\noriginal space. In this work, we develop a connection between SKI and the\npermutohedral lattice used for high-dimensional fast bilateral filtering. Using\na sparse simplicial grid instead of a dense rectangular one, we can perform GP\ninference exponentially faster in the dimension than SKI. Our approach,\nSimplex-GP, enables scaling SKI to high dimensions, while maintaining strong\npredictive performance. We additionally provide a CUDA implementation of\nSimplex-GP, which enables significant GPU acceleration of MVM based inference.", "title": "SKIing on Simplices: Kernel Interpolation on the Permutohedral Lattice  for Scalable Gaussian Processes"}, {"link": "https://arxiv.org/abs/2106.06697", "abstract": "Despite the high accuracy offered by state-of-the-art deep natural-language\nmodels (e.g. LSTM, BERT), their application in real-life settings is still\nwidely limited, as they behave like a black-box to the end-user. Hence,\nexplainability is rapidly becoming a fundamental requirement of\nfuture-generation data-driven systems based on deep-learning approaches.\nSeveral attempts to fulfill the existing gap between accuracy and\ninterpretability have been done. However, robust and specialized xAI\n(Explainable Artificial Intelligence) solutions tailored to deep\nnatural-language models are still missing. We propose a new framework, named\nT-EBAnO, which provides innovative prediction-local and class-based\nmodel-global explanation strategies tailored to black-box deep natural-language\nmodels. Given a deep NLP model and the textual input data, T-EBAnO provides an\nobjective, human-readable, domain-specific assessment of the reasons behind the\nautomatic decision-making process. Specifically, the framework extracts sets of\ninterpretable features mining the inner knowledge of the model. Then, it\nquantifies the influence of each feature during the prediction process by\nexploiting the novel normalized Perturbation Influence Relation index at the\nlocal level and the novel Global Absolute Influence and Global Relative\nInfluence indexes at the global level. The effectiveness and the quality of the\nlocal and global explanations obtained with T-EBAnO are proved on (i) a\nsentiment analysis task performed by a fine-tuned BERT model, and (ii) a toxic\ncomment classification task performed by an LSTM model.", "title": "Explaining the Deep Natural Language Processing by Mining Textual  Interpretable Features"}, {"link": "https://arxiv.org/abs/2106.06703", "abstract": "We learn, in an unsupervised way, an embedding from sequences of radar images\nthat is suitable for solving place recognition problem using complex radar\ndata. We experiment on 280 km of data and show performance exceeding\nstate-of-the-art supervised approaches, localising correctly 98.38% of the time\nwhen using just the nearest database candidate.", "title": "Unsupervised Place Recognition with Deep Embedding Learning over Radar  Videos"}, {"link": "https://arxiv.org/abs/2106.06707", "abstract": "Various recent proposals increase the distinguishing power of Graph Neural\nNetworks GNNs by propagating features between $k$-tuples of vertices. The\ndistinguishing power of these \"higher-order'' GNNs is known to be bounded by\nthe $k$-dimensional Weisfeiler-Leman (WL) test, yet their $\\mathcal O(n^k)$\nmemory requirements limit their applicability. Other proposals infuse GNNs with\nlocal higher-order graph structural information from the start, hereby\ninheriting the desirable $\\mathcal O(n)$ memory requirement from GNNs at the\ncost of a one-time, possibly non-linear, preprocessing step. We propose local\ngraph parameter enabled GNNs as a framework for studying the latter kind of\napproaches and precisely characterize their distinguishing power, in terms of a\nvariant of the WL test, and in terms of the graph structural properties that\nthey can take into account. Local graph parameters can be added to any GNN\narchitecture, and are cheap to compute. In terms of expressive power, our\nproposal lies in the middle of GNNs and their higher-order counterparts.\nFurther, we propose several techniques to aide in choosing the right local\ngraph parameters. Our results connect GNNs with deep results in finite model\ntheory and finite variable logics. Our experimental evaluation shows that\nadding local graph parameters often has a positive effect for a variety of\nGNNs, datasets and graph learning tasks.", "title": "Graph Neural Networks with Local Graph Parameters"}, {"link": "https://arxiv.org/abs/2106.06708", "abstract": "In this paper, we consider some aspects of the numerical analysis of the\nmathematical model of fractional Duffing with a derivative of variable\nfractional order of the Riemann-Liouville type. Using numerical methods: an\nexplicit finite-difference scheme based on the Grunwald-Letnikov and\nAdams-Bashford-Moulton approximations (predictor-corrector), the proposed\nnumerical model is found. These methods have been verified with a test case. It\nis shown that the predictor-corrector method has a faster convergence than the\nmethod according to the explicit finite-difference scheme. For these schemes,\nusing Runge's rule, estimates of the computational accuracy were made, which\ntended to unity with an increase in the number of calculated grid nodes.", "title": "Some Aspects of the Numerical Analysis of a Fractional Duffing  Oscillator with a Fractional Variable Order Derivative of the  Riemann-Liouville Type"}, {"link": "https://arxiv.org/abs/2106.06712", "abstract": "We consider the stochastic combinatorial semi-bandit problem with adversarial\ncorruptions. We provide a simple combinatorial algorithm that can achieve a\nregret of $\\tilde{O}\\left(C+d^2K/\\Delta_{min}\\right)$ where $C$ is the total\namount of corruptions, $d$ is the maximal number of arms one can play in each\nround, $K$ is the number of arms. If one selects only one arm in each round, we\nachieves a regret of $\\tilde{O}\\left(C+\\sum_{\\Delta_i>0}(1/\\Delta_i)\\right)$.\nOur algorithm is combinatorial and improves on the previous combinatorial\nalgorithm by [Gupta et al., COLT2019] (their bound is\n$\\tilde{O}\\left(KC+\\sum_{\\Delta_i>0}(1/\\Delta_i)\\right)$), and almost matches\nthe best known bounds obtained by [Zimmert et al., ICML2019] and [Zimmert and\nSeldin, AISTATS2019] (up to logarithmic factor). Note that the algorithms in\n[Zimmert et al., ICML2019] and [Zimmert and Seldin, AISTATS2019] require one to\nsolve complex convex programs while our algorithm is combinatorial, very easy\nto implement, requires weaker assumptions and has very low oracle complexity\nand running time. We also study the setting where we only get access to an\napproximation oracle for the stochastic combinatorial semi-bandit problem. Our\nalgorithm achieves an (approximation) regret bound of\n$\\tilde{O}\\left(d\\sqrt{KT}\\right)$. Our algorithm is very simple, only worse\nthan the best known regret bound by $\\sqrt{d}$, and has much lower oracle\ncomplexity than previous work.", "title": "Simple Combinatorial Algorithms for Combinatorial Bandits: Corruptions  and Approximations"}, {"link": "https://arxiv.org/abs/2106.06713", "abstract": "Designing an effective loss function plays a crucial role in training deep\nrecommender systems. Most existing works often leverage a predefined and fixed\nloss function that could lead to suboptimal recommendation quality and training\nefficiency. Some recent efforts rely on exhaustively or manually searched\nweights to fuse a group of candidate loss functions, which is exceptionally\ncostly in computation and time. They also neglect the various convergence\nbehaviors of different data examples. In this work, we propose an AutoLoss\nframework that can automatically and adaptively search for the appropriate loss\nfunction from a set of candidates. To be specific, we develop a novel\ncontroller network, which can dynamically adjust the loss probabilities in a\ndifferentiable manner. Unlike existing algorithms, the proposed controller can\nadaptively generate the loss probabilities for different data examples\naccording to their varied convergence behaviors. Such design improves the\nmodel's generalizability and transferability between deep recommender systems\nand datasets. We evaluate the proposed framework on two benchmark datasets. The\nresults show that AutoLoss outperforms representative baselines. Further\nexperiments have been conducted to deepen our understandings of AutoLoss,\nincluding its transferability, components and training efficiency.", "title": "AutoLoss: Automated Loss Function Search in Recommendations"}, {"link": "https://arxiv.org/abs/2106.06716", "abstract": "Automatic medical image segmentation has made great progress benefit from the\ndevelopment of deep learning. However, most existing methods are based on\nconvolutional neural networks (CNNs), which fail to build long-range\ndependencies and global context connections due to the limitation of receptive\nfield in convolution operation. Inspired by the success of Transformer in\nmodeling the long-range contextual information, some researchers have expended\nconsiderable efforts in designing the robust variants of Transformer-based\nU-Net. Moreover, the patch division used in vision transformers usually ignores\nthe pixel-level intrinsic structural features inside each patch. To alleviate\nthese problems, we propose a novel deep medical image segmentation framework\ncalled Dual Swin Transformer U-Net (DS-TransUNet), which might be the first\nattempt to concurrently incorporate the advantages of hierarchical Swin\nTransformer into both encoder and decoder of the standard U-shaped architecture\nto enhance the semantic segmentation quality of varying medical images. Unlike\nmany prior Transformer-based solutions, the proposed DS-TransUNet first adopts\ndual-scale encoder subnetworks based on Swin Transformer to extract the coarse\nand fine-grained feature representations of different semantic scales. As the\ncore component for our DS-TransUNet, a well-designed Transformer Interactive\nFusion (TIF) module is proposed to effectively establish global dependencies\nbetween features of different scales through the self-attention mechanism.\nFurthermore, we also introduce the Swin Transformer block into decoder to\nfurther explore the long-range contextual information during the up-sampling\nprocess. Extensive experiments across four typical tasks for medical image\nsegmentation demonstrate the effectiveness of DS-TransUNet, and show that our\napproach significantly outperforms the state-of-the-art methods.", "title": "DS-TransUNet:Dual Swin Transformer U-Net for Medical Image Segmentation"}, {"link": "https://arxiv.org/abs/2106.06719", "abstract": "Dialogue topic segmentation is critical in several dialogue modeling\nproblems. However, popular unsupervised approaches only exploit surface\nfeatures in assessing topical coherence among utterances. In this work, we\naddress this limitation by leveraging supervisory signals from the\nutterance-pair coherence scoring task. First, we present a simple yet effective\nstrategy to generate a training corpus for utterance-pair coherence scoring.\nThen, we train a BERT-based neural utterance-pair coherence model with the\nobtained training corpus. Finally, such model is used to measure the topical\nrelevance between utterances, acting as the basis of the segmentation\ninference. Experiments on three public datasets in English and Chinese\ndemonstrate that our proposal outperforms the state-of-the-art baselines.", "title": "Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair  Coherence Scoring"}, {"link": "https://arxiv.org/abs/2106.06720", "abstract": "Infectious disease outbreak has a significant impact on morbidity, mortality\nand can cause economic instability of many countries. As global trade is\ngrowing, goods and individuals are expected to travel across the border, an\ninfected epidemic area carrier can pose a great danger to his hostile. If a\ndisease outbreak is recognized promptly, then commercial products and travelers\n(traders/visitors) will be effectively vaccinated, and therefore the disease\nstopped. Early detection of outbreaks plays an important role here, and beware\nof the rapid implementation of control measures by citizens, public health\norganizations, and government. Many indicators have valuable information, such\nas online news sources (RSS) and social media sources (Twitter, Facebook) that\ncan be used, but are unstructured and bulky, to extract information about\ndisease outbreaks. Few early warning outbreak systems exist with some\nlimitation of linguistic (Urdu) and covering areas (Pakistan). In Pakistan, few\nchannels are published the outbreak news in Urdu or English. The aim is to\nprocure information from Pakistan's English and Urdu news channels and then\ninvestigate process, integrate, and visualize the disease epidemic. Urdu\nontology is not existed before to match extracted diseases, so we also build\nthat ontology of disease.", "title": "BIOPAK Flasher: Epidemic disease monitoring and detection in Pakistan  using text mining"}, {"link": "https://arxiv.org/abs/2106.06721", "abstract": "The growth of video streaming has stretched the Internet to its limitation.\nIn other words, the Internet was originally devised to connect a limited number\nof computers so that they can share network resources, so the Internet cannot\nhandle a large amount of traffic at a time, which leads to network congestion.\nTo overcome this, CDNs are built on top of the Internet as an overlay to\nefficiently store and swiftly disseminate contents to users by placing many\nservers and data centers around the globe. The topic of CDNs has been\nextensively studied in the last several decades. However, there is still a\ncertain gap between theories in academia and current technologies in industry.\nIn this paper, we take a close look at the design, implementation, solution,\nand performance of a CDN system by analyzing its raw log files. Specifically,\nits infrastructure and system design are first presented, and then we conduct a\ntrace-based study to understand user access patterns, the sources of requests,\nsystem performance, and how such information can be used to improve the whole\nCDN system.", "title": "A use case of Content Delivery Network raw logfile analysis"}, {"link": "https://arxiv.org/abs/2106.06726", "abstract": "Regularization and data augmentation methods have been widely used and become\nincreasingly indispensable in deep learning training. Researchers who devote\nthemselves to this have considered various possibilities. But so far, there has\nbeen little discussion about regularizing outputs of the model. This paper\nbegins with empirical observations that better performances are significantly\nassociated with output distributions, that have smaller average values and\nvariances. By audaciously assuming there is causality involved, we propose a\nnovel regularization term, called Output Decay, that enforces the model to\nassign smaller and similar output values on each class. Though being\ncounter-intuitive, such a small modification result in a remarkable improvement\non performance. Extensive experiments demonstrate the wide applicability,\nversatility, and compatibility of Output Decay.", "title": "Go Small and Similar: A Simple Output Decay Brings Better Performance"}, {"link": "https://arxiv.org/abs/2106.06731", "abstract": "Punctuation restoration is an important post-processing step in automatic\nspeech recognition. Among other kinds of external information, part-of-speech\n(POS) taggers provide informative tags, suggesting each input token's syntactic\nrole, which has been shown to be beneficial for the punctuation restoration\ntask. In this work, we incorporate an external POS tagger and fuse its\npredicted labels into the existing language model to provide syntactic\ninformation. Besides, we propose sequence boundary sampling (SBS) to learn\npunctuation positions more efficiently as a sequence tagging task. Experimental\nresults show that our methods can consistently obtain performance gains and\nachieve a new state-of-the-art on the common IWSLT benchmark. Further ablation\nstudies illustrate that both large pre-trained language models and the external\nPOS tagger take essential parts to improve the model's performance.", "title": "Incorporating External POS Tagger for Punctuation Restoration"}, {"link": "https://arxiv.org/abs/2106.06733", "abstract": "Radiation therapy treatment planning is a complex process, as the target dose\nprescription and normal tissue sparing are conflicting objectives. Automated\nand accurate dose prediction for radiation therapy planning is in high demand.\nIn this study, we propose a novel learning-based ensemble approach, named\nLE-NAS, which integrates neural architecture search (NAS) with knowledge\ndistillation for 3D radiotherapy dose prediction. Specifically, the prediction\nnetwork first exhaustively searches each block from enormous architecture\nspace. Then, multiple architectures are selected with promising performance and\ndiversity. To reduce the inference time, we adopt the teacher-student paradigm\nby treating the combination of diverse outputs from multiple searched networks\nas supervisions to guide the student network training. In addition, we apply\nadversarial learning to optimize the student network to recover the knowledge\nin teacher networks. To the best of our knowledge, we are the first to\ninvestigate the combination of NAS and knowledge distillation. The proposed\nmethod has been evaluated on the public OpenKBP dataset, and experimental\nresults demonstrate the effectiveness of our method and its superior\nperformance to the state-of-the-art method.", "title": "LE-NAS: Learning-based Ensenble with NAS for Dose Prediction"}, {"link": "https://arxiv.org/abs/2106.06736", "abstract": "Event classification is inherently sequential and multimodal. Therefore, deep\nneural models need to dynamically focus on the most relevant time window and/or\nmodality of a video. In this study, we propose the Multi-level Attention Fusion\nnetwork (MAFnet), an architecture that can dynamically fuse visual and audio\ninformation for event recognition. Inspired by prior studies in neuroscience,\nwe couple both modalities at different levels of visual and audio paths.\nFurthermore, the network dynamically highlights a modality at a given time\nwindow relevant to classify events. Experimental results in AVE (Audio-Visual\nEvent), UCF51, and Kinetics-Sounds datasets show that the approach can\neffectively improve the accuracy in audio-visual event classification. Code is\navailable at: https://github.com/numediart/MAFnet", "title": "Multi-level Attention Fusion Network for Audio-visual Event Recognition"}, {"link": "https://arxiv.org/abs/2106.06738", "abstract": "Training deep learning models with limited labelled data is an attractive\nscenario for many NLP tasks, including document classification. While with the\nrecent emergence of BERT, deep learning language models can achieve reasonably\ngood performance in document classification with few labelled instances, there\nis a lack of evidence in the utility of applying BERT-like models on long\ndocument classification. This work introduces a long-text-specific model -- the\nHierarchical BERT Model (HBM) -- that learns sentence-level features of the\ntext and works well in scenarios with limited labelled data. Various evaluation\nexperiments have demonstrated that HBM can achieve higher performance in\ndocument classification than the previous state-of-the-art methods with only 50\nto 200 labelled instances, especially when documents are long. Also, as an\nextra benefit of HBM, the salient sentences identified by learned HBM are\nuseful as explanations for labelling documents based on a user study.", "title": "A Sentence-level Hierarchical BERT Model for Document Classification  with Limited Labelled Data"}, {"link": "https://arxiv.org/abs/2106.06739", "abstract": "We propose a large, scalable engineering knowledge graph, comprising sets of\n(entity, relationship, entity) triples that are real-world engineering facts\nfound in the patent database. We apply a set of rules based on the syntactic\nand lexical properties of claims in a patent document to extract facts. We\naggregate these facts within each patent document and integrate the aggregated\nsets of facts across the patent database to obtain the engineering knowledge\ngraph. Such a knowledge graph is expected to support inference, reasoning, and\nrecalling in various engineering tasks. The knowledge graph has a greater size\nand coverage in comparison with the previously used knowledge graphs and\nsemantic networks in the engineering literature.", "title": "Engineering Knowledge Graph from Patent Database"}, {"link": "https://arxiv.org/abs/2106.06742", "abstract": "The core problem of Magnetic Resonance Imaging (MRI) is the trade off between\nacceleration and image quality. Image reconstruction and super-resolution are\ntwo crucial techniques in Magnetic Resonance Imaging (MRI). Current methods are\ndesigned to perform these tasks separately, ignoring the correlations between\nthem. In this work, we propose an end-to-end task transformer network\n(T$^2$Net) for joint MRI reconstruction and super-resolution, which allows\nrepresentations and feature transmission to be shared between multiple task to\nachieve higher-quality, super-resolved and motion-artifacts-free images from\nhighly undersampled and degenerated MRI data. Our framework combines both\nreconstruction and super-resolution, divided into two sub-branches, whose\nfeatures are expressed as queries and keys. Specifically, we encourage joint\nfeature learning between the two tasks, thereby transferring accurate task\ninformation. We first use two separate CNN branches to extract task-specific\nfeatures. Then, a task transformer module is designed to embed and synthesize\nthe relevance between the two tasks. Experimental results show that our\nmulti-task model significantly outperforms advanced sequential methods, both\nquantitatively and qualitatively.", "title": "Task Transformer Network for Joint MRI Reconstruction and  Super-Resolution"}, {"link": "https://arxiv.org/abs/2106.06744", "abstract": "Lung cancer is the leading cause of cancer death worldwide. The critical\nreason for the deaths is delayed diagnosis and poor prognosis. With the\naccelerated development of deep learning techniques, it has been successfully\napplied extensively in many real-world applications, including health sectors\nsuch as medical image interpretation and disease diagnosis. By combining more\nmodalities that being engaged in the processing of information, multimodal\nlearning can extract better features and improve predictive ability. The\nconventional methods for lung cancer survival analysis normally utilize\nclinical data and only provide a statistical probability. To improve the\nsurvival prediction accuracy and help prognostic decision-making in clinical\npractice for medical experts, we for the first time propose a multimodal deep\nlearning method for non-small cell lung cancer (NSCLC) survival analysis, named\nDeepMMSA. This method leverages CT images in combination with clinical data,\nenabling the abundant information hold within medical images to be associate\nwith lung cancer survival information. We validate our method on the data of\n422 NSCLC patients from The Cancer Imaging Archive (TCIA). Experimental results\nsupport our hypothesis that there is an underlying relationship between\nprognostic information and radiomic images. Besides, quantitative results\nshowing that the established multimodal model can be applied to traditional\nmethod and has the potential to break bottleneck of existing methods and\nincrease the the percentage of concordant pairs(right predicted pairs) in\noverall population by 4%.", "title": "DeepMMSA: A Novel Multimodal Deep Learning Method for Non-small Cell  Lung Cancer Survival Analysis"}, {"link": "https://arxiv.org/abs/2106.06745", "abstract": "While many applications of automata in formal methods can use\nnondeterministic automata, some applications, most notably synthesis, need\ndeterministic or good-for-games(GFG) automata. The latter are nondeterministic\nautomata that can resolve their nondeterministic choices in a way that only\ndepends on the past. The minimization problem for deterministic B\\\"uchi and\nco-B\\\"uchi word automata is NP-complete. In particular, no canonical minimal\ndeterministic automaton exists, and a language may have different minimal\ndeterministic automata. We describe a polynomial minimization algorithm for GFG\nco-B\\\"uchi word automata with transition-based acceptance. Thus, a run is\naccepting if it traverses a set $\\alpha$ of designated transitions only\nfinitely often. Our algorithm is based on a sequence of transformations we\napply to the automaton, on top of which a minimal quotient automaton is\ndefined. We use our minimization algorithm to show canonicity for\ntransition-based GFG co-B\\\"uchi word automata: all minimal automata have\nisomorphic safe components (namely components obtained by restricting the\ntransitions to these not in $\\alpha$) and once we saturate the automata with\n$\\alpha$-transitions, we get full isomorphism.", "title": "Minimization and Canonization of GFG Transition-Based Automata"}, {"link": "https://arxiv.org/abs/2106.06747", "abstract": "The increasing interest in open source software has led to the emergence of\nlarge package distributions of reusable software libraries, such as npm and\nRubyGems. These software packages can be subject to security vulnerabilities\nthat may expose dependent packages through explicitly declared dependencies.\nThis article empirically studies security vulnerabilities affecting npm and\nRubyGems packages. We analyse how and when these vulnerabilities are discovered\nand fixed, and how their prevalence changes over time. We also analyse how\nvulnerable packages expose their direct and indirect dependents to\nvulnerabilities. We distinguish between two types of dependents: packages\ndistributed via the package manager, and external GitHub projects. Compared to\nRubyGems, we observe that the number of vulnerabilities is increasing faster in\nnpm, but vulnerabilities are also discovered faster in npm. For both package\ndistributions, the time required to discover vulnerabilities is increasing, but\nnpm is improving the time needed to fix vulnerabilities. A large proportion of\nexternal GitHub projects are exposed to vulnerabilities coming from direct or\nindirect dependencies. Around one out of three direct vulnerable dependencies\nto which projects or packages are exposed could be avoided, if software\ndevelopers would update their dependencies to more recent releases within the\nsame major release range.", "title": "On the Impact of Security Vulnerabilities in the npm and RubyGems  Dependency Networks"}, {"link": "https://arxiv.org/abs/2106.06749", "abstract": "Currently, researchers have proposed the adaptive gradient descent algorithm\nand its variants, such as AdaGrad, RMSProp, Adam, AmsGrad, etc. Although these\nalgorithms have a faster speed in the early stage, the generalization ability\nin the later stage of training is often not as good as the stochastic gradient\ndescent. Recently, some researchers have combined the adaptive gradient descent\nand stochastic gradient descent to obtain the advantages of both and achieved\ngood results. Based on this research, we propose a decreasing scaling\ntransition from adaptive gradient descent to stochastic gradient descent\nmethod(DSTAda). For the training stage of the stochastic gradient descent, we\nuse a learning rate that decreases linearly with the number of iterations\ninstead of a constant learning rate. We achieve a smooth and stable transition\nfrom adaptive gradient descent to stochastic gradient descent through scaling.\nAt the same time, we give a theoretical proof of the convergence of DSTAda\nunder the framework of online learning. Our experimental results show that the\nDSTAda algorithm has a faster convergence speed, higher accuracy, and better\nstability and robustness. Our implementation is available at:\nhttps://github.com/kunzeng/DSTAdam.", "title": "Decreasing scaling transition from adaptive gradient descent to  stochastic gradient descent"}, {"link": "https://arxiv.org/abs/2106.06751", "abstract": "Although teacher forcing has become the main training paradigm for neural\nmachine translation, it usually makes predictions only conditioned on past\ninformation, and hence lacks global planning for the future. To address this\nproblem, we introduce another decoder, called seer decoder, into the\nencoder-decoder framework during training, which involves future information in\ntarget predictions. Meanwhile, we force the conventional decoder to simulate\nthe behaviors of the seer decoder via knowledge distillation. In this way, at\ntest the conventional decoder can perform like the seer decoder without the\nattendance of it. Experiment results on the Chinese-English, English-German and\nEnglish-Romanian translation tasks show our method can outperform competitive\nbaselines significantly and achieves greater improvements on the bigger data\nsets. Besides, the experiments also prove knowledge distillation the best way\nto transfer knowledge from the seer decoder to the conventional decoder\ncompared to adversarial learning and L2 regularization.", "title": "Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation"}, {"link": "https://arxiv.org/abs/2106.06753", "abstract": "The plain stochastic gradient descent and momentum stochastic gradient\ndescent have extremely wide applications in deep learning due to their simple\nsettings and low computational complexity. The momentum stochastic gradient\ndescent uses the accumulated gradient as the updated direction of the current\nparameters, which has a faster training speed. Because the direction of the\nplain stochastic gradient descent has not been corrected by the accumulated\ngradient. For the parameters that currently need to be updated, it is the\noptimal direction, and its update is more accurate. We combine the advantages\nof the momentum stochastic gradient descent with fast training speed and the\nplain stochastic gradient descent with high accuracy, and propose a scaling\ntransition from momentum stochastic gradient descent to plain stochastic\ngradient descent(TSGD) method. At the same time, a learning rate that decreases\nlinearly with the iterations is used instead of a constant learning rate. The\nTSGD algorithm has a larger step size in the early stage to speed up the\ntraining, and training with a smaller step size in the later stage can steadily\nconverge. Our experimental results show that the TSGD algorithm has faster\ntraining speed, higher accuracy and better stability. Our implementation is\navailable at: https://github.com/kunzeng/TSGD.", "title": "Scaling transition from momentum stochastic gradient descent to plain  stochastic gradient descent"}, {"link": "https://arxiv.org/abs/2106.06755", "abstract": "In this work, we study the socially fair $k$-median/$k$-means problem. We are\ngiven a set of points $P$ in a metric space $\\mathcal{X}$ with a distance\nfunction $d(.,.)$. There are $\\ell$ groups: $P_1,\\dotsc,P_{\\ell} \\subseteq P$.\nWe are also given a set $F$ of feasible centers in $\\mathcal{X}$. The goal of\nthe socially fair $k$-median problem is to find a set $C \\subseteq F$ of $k$\ncenters that minimizes the maximum average cost over all the groups. That is,\nfind $C$ that minimizes the objective function $\\Phi(C,P) \\equiv \\max_{j}\n\\sum_{x \\in P_j} d(C,x)/|P_j|$, where $d(C,x)$ is the distance of $x$ to the\nclosest center in $C$. The socially fair $k$-means problem is defined similarly\nby using squared distances, i.e., $d^{2}(.,.)$ instead of $d(.,.)$. In this\nwork, we design $(5+\\varepsilon)$ and $(33 + \\varepsilon)$ approximation\nalgorithms for the socially fair $k$-median and $k$-means problems,\nrespectively. For the parameters: $k$ and $\\ell$, the algorithms have an FPT\n(fixed parameter tractable) running time of $f(k,\\ell,\\varepsilon) \\cdot n$ for\n$f(k,\\ell,\\varepsilon) = 2^{{O}(k \\, \\ell/\\varepsilon)}$ and $n = |P \\cup F|$.\nWe also study a special case of the problem where the centers are allowed to be\nchosen from the point set $P$, i.e., $P \\subseteq F$. For this special case,\nour algorithms give better approximation guarantees of $(4+\\varepsilon)$ and\n$(18+\\varepsilon)$ for the socially fair $k$-median and $k$-means problems,\nrespectively. Furthermore, we convert these algorithms to constant pass\nlog-space streaming algorithms. Lastly, we show FPT hardness of approximation\nresults for the problem with a small gap between our upper and lower bounds.", "title": "FPT Approximation for Socially Fair Clustering"}, {"link": "https://arxiv.org/abs/2106.06758", "abstract": "Previous work on review summarization focused on measuring the sentiment\ntoward the main aspects of the reviewed product or business, or on creating a\ntextual summary. These approaches provide only a partial view of the data:\naspect-based sentiment summaries lack sufficient explanation or justification\nfor the aspect rating, while textual summaries do not quantify the significance\nof each element, and are not well-suited for representing conflicting views.\nRecently, Key Point Analysis (KPA) has been proposed as a summarization\nframework that provides both textual and quantitative summary of the main\npoints in the data. We adapt KPA to review data by introducing Collective Key\nPoint Mining for better key point extraction; integrating sentiment analysis\ninto KPA; identifying good key point candidates for review summaries; and\nleveraging the massive amount of available reviews and their metadata. We show\nempirically that these novel extensions of KPA substantially improve its\nperformance. We demonstrate that promising results can be achieved without any\ndomain-specific annotation, while human supervision can lead to further\nimprovement.", "title": "Every Bite Is an Experience: Key Point Analysis of Business Reviews"}, {"link": "https://arxiv.org/abs/2106.06761", "abstract": "The ensemble methods are meta-algorithms that combine several base machine\nlearning techniques to increase the effectiveness of the classification. Many\nexisting committees of classifiers use the classifier selection process to\ndetermine the optimal set of base classifiers. In this article, we propose the\nclassifiers selection framework with relearning base classifiers. Additionally,\nwe use in the proposed framework the new generated feature, which can be\nobtained after the relearning process. The proposed technique was compared with\nstate-of-the-art ensemble methods using three benchmark datasets and one\nsynthetic dataset. Four classification performance measures are used to\nevaluate the proposed method.", "title": "Relearning ensemble selection based on new generated features"}, {"link": "https://arxiv.org/abs/2106.06765", "abstract": "Data Distribution Service (DDS) is an innovative approach towards\ncommunication in ICS/IoT infrastructure and robotics. Being based on the\ncross-platform and cross-language API to be applicable in any computerised\ndevice, it offers the benefits of modern programming languages and the\nopportunities to develop more complex and advanced systems. However, the DDS\ncomplexity equally increases its vulnerability, while the existing security\nmeasures are limited to plug-ins and static rules, with the rest of the\nsecurity provided by third-party applications and operating system.\nSpecifically, traditional intrusion detection systems (IDS) do not detect any\nanomalies in the publish/subscribe method. With the exponentially growing\nglobal communication exchange, securing DDS is of the utmost importance to\nfutureproofing industrial, public, and even personal devices and systems. This\nreport presents an experimental work on the simulation of several specific\nattacks against DDS, and the application of Deep Learning for their detection.\nThe findings show that even though Deep Learning allows to detect all simulated\nattacks using only metadata analysis, their detection level varies, with some\nof the advanced attacks being harder to detect. The limitations imposed by the\nattempts to preserve privacy significantly decrease the detection rate. The\nreport also reviews the drawbacks and limitations of the Deep Learning approach\nand proposes a set of selected solutions and configurations, that can further\nimprove the DDS security.", "title": "Towards a Privacy-preserving Deep Learning-based Network Intrusion  Detection in Data Distribution Services"}, {"link": "https://arxiv.org/abs/2106.06766", "abstract": "Multilingual sentence representations pose a great advantage for low-resource\nlanguages that do not have enough data to build monolingual models on their\nown. These multilingual sentence representations have been separately exploited\nby few research for document and sentence alignment. However, most of the\nlow-resource languages are under-represented in these pre-trained models. Thus,\nin the context of low-resource languages, these models have to be fine-tuned\nfor the task at hand, using additional data sources. This paper presents a\nweighting mechanism that makes use of available small-scale parallel corpora to\nimprove the performance of multilingual sentence representations on document\nand sentence alignment. Experiments are conducted with respect to two\nlow-resource languages, Sinhala and Tamil. Results on a newly created dataset\nof Sinhala-English, Tamil-English, and Sinhala-Tamil show that this new\nweighting mechanism significantly improves both document and sentence\nalignment. This dataset, as well as the source-code, is publicly released.", "title": "Exploiting Parallel Corpora to Improve Multilingual Embedding based  Document and Sentence Alignment"}, {"link": "https://arxiv.org/abs/2106.06769", "abstract": "Working memory (WM) is a basic part of human cognition, which plays an\nimportant role in the study of human cognitive load. Among various brain\nimaging techniques, electroencephalography has shown its advantage on easy\naccess and reliability. However, one of the critical challenges is that\nindividual difference may cause the ineffective results, especially when the\nestablished model meets an unfamiliar subject. In this work, we propose a\ncross-subject deep adaptation model with spatial attention (CS-DASA) to\ngeneralize the workload classifications across subjects. First, we transform\ntime-series EEG data into multi-frame EEG images incorporating more\nspatio-temporal information. First, the subject-shared module in CS-DASA\nreceives multi-frame EEG image data from both source and target subjects and\nlearns the common feature representations. Then, in subject-specific module,\nthe maximum mean discrepancy is implemented to measure the domain distribution\ndivergence in a reproducing kernel Hilbert space, which can add an effective\npenalty loss for domain adaptation. Additionally, the subject-to-subject\nspatial attention mechanism is employed to focus on the most discriminative\nspatial feature in EEG image data. Experiments conducted on a public WM EEG\ndataset containing 13 subjects show that the proposed model is capable of\nachieve better performance than existing state-of-the art methods.", "title": "Cross-Subject Domain Adaptation for Multi-Frame EEG Images"}, {"link": "https://arxiv.org/abs/2106.06770", "abstract": "For certain infinitely-wide neural networks, the neural tangent kernel (NTK)\ntheory fully characterizes generalization. However, for the networks used in\npractice, the empirical NTK represents only a rough first-order approximation\nof these architectures. Still, a growing body of work keeps leveraging this\napproximation to successfully analyze important deep learning phenomena and\nderive algorithms for new applications. In our work, we provide strong\nempirical evidence to determine the practical validity of such approximation by\nconducting a systematic comparison of the behaviour of different neural\nnetworks and their linear approximations on different tasks. We show that the\nlinear approximations can indeed rank the learning complexity of certain tasks\nfor neural networks, albeit with important nuances. Specifically, we discover\nthat, in contrast to what was previously observed, neural networks do not\nalways perform better than their kernel approximations, and reveal that their\nperformance gap heavily depends on architecture, number of samples and training\ntask. In fact, we show that during training, deep networks increase the\nalignment of their empirical NTK with the target task, which explains why\nlinear approximations at the end of training can better explain the dynamics of\ndeep networks. Overall, our work provides concrete examples of novel deep\nlearning phenomena which can inspire future theoretical research, as well as\nprovides a new perspective on the use of the NTK approximation in deep\nlearning.", "title": "What can linearized neural networks actually say about generalization?"}, {"link": "https://arxiv.org/abs/2106.06772", "abstract": "In this work, we address a task allocation problem for human multi-robot\nsettings. Given a set of tasks to perform, we formulate a general Mixed-Integer\nLinear Programming (MILP) problem aiming at minimizing the overall execution\ntime while optimizing the quality of the executed tasks as well as human and\nrobotic workload. Different skills of the agents, both human and robotic, are\ntaken into account and human operators are enabled to either directly execute\ntasks or play supervisory roles; moreover, multiple manipulators can tightly\ncollaborate if required to carry out a task. Finally, as realistic in human\ncontexts, human parameters are assumed to vary over time, e.g., due to\nincreasing human level of fatigue. Therefore, online monitoring is required and\nre-allocation is performed if needed. Simulations in a realistic scenario with\ntwo manipulators and a human operator performing an assembly task validate the\neffectiveness of the approach.", "title": "A Mixed-Integer Linear Programming Formulation for Human Multi-Robot  Task Allocation"}, {"link": "https://arxiv.org/abs/2106.06776", "abstract": "In this work, the issue of estimation of reachable sets in continuous bimodal\npiecewise affine systems is studied. A new method is proposed, in the framework\nof ellipsoidal bounding, using piecewise quadratic Lyapunov functions. Although\nbimodal piecewise affine systems can be seen as a special class of affine\nhybrid systems, reachability methods developed for affine hybrid systems might\nbe inappropriately complex for bimodal dynamics. This work goes in the\ndirection of exploiting the dynamical structure of the system to propose a\nsimpler approach. More specifically, because of the piecewise nature of the\nLyapunov function, we first derive conditions to ensure that a given quadratic\nfunction is positive on half spaces. Then, we exploit the property of bimodal\npiecewise quadratic functions being continuous on a given hyperplane. Finally,\nlinear matrix characterizations of the estimate of the reachable set are\nderived.", "title": "A piecewise ellipsoidal reachable set estimation method for continuous  bimodal piecewise affine systems"}, {"link": "https://arxiv.org/abs/2106.06777", "abstract": "We study reinforcement learning for the optimal control of Branching Markov\nDecision Processes (BMDPs), a natural extension of (multitype) Branching Markov\nChains (BMCs). The state of a (discrete-time) BMCs is a collection of entities\nof various types that, while spawning other entities, generate a payoff. In\ncomparison with BMCs, where the evolution of a each entity of the same type\nfollows the same probabilistic pattern, BMDPs allow an external controller to\npick from a range of options. This permits us to study the best/worst behaviour\nof the system. We generalise model-free reinforcement learning techniques to\ncompute an optimal control strategy of an unknown BMDP in the limit. We present\nresults of an implementation that demonstrate the practicality of the approach.", "title": "Model-free Reinforcement Learning for Branching Markov Decision  Processes"}, {"link": "https://arxiv.org/abs/2106.06778", "abstract": "Convolutional networks (ConvNets) have shown impressive capability to solve\nvarious vision tasks. Nevertheless, the trade-off between performance and\nefficiency is still a challenge for a feasible model deployment on\nresource-constrained platforms. In this paper, we introduce a novel concept\ntermed multi-path fully connected pattern (MPFC) to rethink the\ninterdependencies of topology pattern, accuracy and efficiency for ConvNets.\nInspired by MPFC, we further propose a dual-branch module named dynamic clone\ntransformer (DCT) where one branch generates multiple replicas from inputs and\nanother branch reforms those clones through a series of difference vectors\nconditional on inputs itself to produce more variants. This operation allows\nthe self-expansion of channel-wise information in a data-driven way with little\ncomputational cost while providing sufficient learning capacity, which is a\npotential unit to replace computationally expensive pointwise convolution as an\nexpansion layer in the bottleneck structure.", "title": "Dynamic Clone Transformer for Efficient Convolutional Neural Netwoks"}, {"link": "https://arxiv.org/abs/2106.06780", "abstract": "Multi-Context Systems (MCS) model in Computational Logic distributed systems\ncomposed of heterogeneous sources, or \"contexts\", interacting via special rules\ncalled \"bridge rules\". In this paper, we consider how to enhance flexibility\nand generality in bridge-rules definition and application. In particular, we\nintroduce and discuss some formal extensions of MCSs useful for a practical use\nin dynamic environments, and we try to provide guidelines for implementations", "title": "Multi-Context Systems: Dynamics and Evolution (Pre-Print of  \"Multi-context systems in dynamic environments\")"}, {"link": "https://arxiv.org/abs/2106.06781", "abstract": "This paper considers a scenario where a robot and a human operator share the\nsame workspace, and the robot is able to both carry out autonomous tasks and\nphysically interact with the human in order to achieve common goals. In this\ncontext, both intentional and accidental contacts between human and robot might\noccur due to the complexity of tasks and environment, to the uncertainty of\nhuman behavior, and to the typical lack of awareness of each other actions.\nHere, a two stage strategy based on Recurrent Neural Networks (RNNs) is\ndesigned to detect intentional and accidental contacts: the occurrence of a\ncontact with the human is detected at the first stage, while the classification\nbetween intentional and accidental is performed at the second stage. An\nadmittance control strategy or an evasive action is then performed by the\nrobot, respectively. The approach also works in the case the robot\nsimultaneously interacts with the human and the environment, where the\ninteraction wrench of the latter is modeled via Gaussian Mixture Models (GMMs).\nControl Barrier Functions (CBFs) are included, at the control level, to\nguarantee the satisfaction of robot and task constraints while performing the\nproper interaction strategy. The approach has been validated on a real setup\ncomposed of a Kinova Jaco2 robot.", "title": "A Data-Driven Approach for Contact Detection, Classification and  Reaction in Physical Human-Robot Collaboration"}, {"link": "https://arxiv.org/abs/2106.06783", "abstract": "State estimation with sensors is essential for mobile robots. Due to sensors\nhave different performance in different environments, how to fuse measurements\nof various sensors is a problem. In this paper, we propose a tightly-coupled\nmulti-sensor fusion framework, Lvio-Fusion, which fuses stereo camera, Lidar,\nIMU, and GPS based on the graph optimization. Especially for urban traffic\nscenes, we introduce a segmented global pose graph optimization with GPS and\nloop-closure, which can eliminate accumulated drifts. Additionally, we\ncreatively use a actor-critic method in reinforcement learning to adaptively\nadjust sensors' weight. After training, actor-critic agent can provide the\nsystem with better and dynamic sensors' weight. We evaluate the performance of\nour system on public datasets and compare it with other state-of-the-art\nmethods, showing that the proposed method achieves high estimation accuracy and\nrobustness to various environments. And our implementations are open source and\nhighly scalable.", "title": "Lvio-Fusion: A Self-adaptive Multi-sensor Fusion SLAM Framework Using  Actor-critic Method"}, {"link": "https://arxiv.org/abs/2106.06786", "abstract": "Japan is a unique country with a distinct cultural heritage, which is\nreflected in billions of historical documents that have been preserved.\nHowever, the change in Japanese writing system in 1900 made these documents\ninaccessible for the general public. A major research project has been to make\nthese historical documents accessible and understandable. An increasing amount\nof research has focused on the character recognition task and the location of\ncharacters on image, yet less research has focused on how to predict the\nsequential ordering of the characters. This is because sequence in classical\nJapanese is very different from modern Japanese. Ordering characters into a\nsequence is important for making the document text easily readable and\nsearchable. Additionally, it is a necessary step for any kind of natural\nlanguage processing on the data (e.g. machine translation, language modeling,\nand word embeddings). We explore a few approaches to the task of predicting the\nsequential ordering of the characters: one using simple hand-crafted rules,\nanother using hand-crafted rules with adaptive thresholds, and another using a\ndeep recurrent sequence model trained with teacher forcing. We provide a\nquantitative and qualitative comparison of these techniques as well as their\ndistinct trade-offs. Our best-performing system has an accuracy of 98.65\\% and\nhas a perfect accuracy on 49\\% of the books in our dataset, suggesting that the\ntechnique is able to predict the order of the characters well enough for many\ntasks.", "title": "Predicting the Ordering of Characters in Japanese Historical Documents"}, {"link": "https://arxiv.org/abs/2106.06787", "abstract": "This paper develops manifold learning techniques for the numerical solution\nof PDE-constrained Bayesian inverse problems on manifolds with boundaries. We\nintroduce graphical Mat\\'ern-type Gaussian field priors that enable flexible\nmodeling near the boundaries, representing boundary values by superposition of\nharmonic functions with appropriate Dirichlet boundary conditions. We also\ninvestigate the graph-based approximation of forward models from PDE parameters\nto observed quantities. In the construction of graph-based prior and forward\nmodels, we leverage the ghost point diffusion map algorithm to approximate\nsecond-order elliptic operators with classical boundary conditions. Numerical\nresults validate our graph-based approach and demonstrate the need to design\nprior covariance models that account for boundary conditions.", "title": "Graph-based Prior and Forward Models for Inverse Problems on Manifolds  with Boundaries"}, {"link": "https://arxiv.org/abs/2106.06788", "abstract": "Although deep learning has made significant progress on fixed large-scale\ndatasets, it typically encounters challenges regarding improperly detecting\nnew/unseen classes in the open-world classification, over-parametrized, and\noverfitting small samples. In contrast, biological systems can overcome the\nabove difficulties very well. Individuals inherit an innate gene from\ncollective creatures that have evolved over hundreds of millions of years, and\ncan learn new skills through a few examples. Inspired by this, we propose a\npractical collective-individual paradigm where open-world tasks are trained in\nsequence using an evolution (expandable) network. To be specific, we\ninnovatively introduce learngene that inherits the meta-knowledge from the\ncollective model and reconstructs a new lightweight individual model for the\ntarget task, to realize the collective-individual paradigm. Particularly, we\npresent a novel criterion that can discover the learngene in the collective\nmodel, according to the gradient information. Finally, the individual model is\ntrained only with a few samples in the absence of the source data. We\ndemonstrate the effectiveness of our approach in an extensive empirical study\nand theoretical analysis.", "title": "Learngene: From Open-World to Your Learning Task"}, {"link": "https://arxiv.org/abs/2106.06792", "abstract": "Visual surface inspection is a challenging task owing to the highly diverse\nappearance of target surfaces and defective regions. Previous attempts heavily\nrely on vast quantities of training examples with manual annotation. However,\nin some practical cases, it is difficult to obtain a large number of samples\nfor inspection. To combat it, we propose a hierarchical texture-perceiving\ngenerative adversarial network (HTP-GAN) that is learned from the one-shot\nnormal image in an unsupervised scheme. Specifically, the HTP-GAN contains a\npyramid of convolutional GANs that can capture the global structure and\nfine-grained representation of an image simultaneously. This innovation helps\ndistinguishing defective surface regions from normal ones. In addition, in the\ndiscriminator, a texture-perceiving module is devised to capture the spatially\ninvariant representation of normal image via directional convolutions, making\nit more sensitive to defective areas. Experiments on a variety of datasets\nconsistently demonstrate the effectiveness of our method.", "title": "A One-Shot Texture-Perceiving Generative Adversarial Network for  Unsupervised Surface Inspection"}, {"link": "https://arxiv.org/abs/2106.06795", "abstract": "We propose a novel approach for class incremental online learning in a\nlimited data setting. This problem setting is challenging because of the\nfollowing constraints: (1) Classes are given incrementally, which necessitates\na class incremental learning approach; (2) Data for each class is given in an\nonline fashion, i.e., each training example is seen only once during training;\n(3) Each class has very few training examples; and (4) We do not use or assume\naccess to any replay/memory to store data from previous classes. Therefore, in\nthis setting, we have to handle twofold problems of catastrophic forgetting and\noverfitting. In our approach, we learn robust representations that are\ngeneralizable across tasks without suffering from the problems of catastrophic\nforgetting and overfitting to accommodate future classes with limited samples.\nOur proposed method leverages the meta-learning framework with knowledge\nconsolidation. The meta-learning framework helps the model for rapid learning\nwhen samples appear in an online fashion. Simultaneously, knowledge\nconsolidation helps to learn a robust representation against forgetting under\nonline updates to facilitate future learning. Our approach significantly\noutperforms other methods on several benchmarks.", "title": "Knowledge Consolidation based Class Incremental Online Learning with  Limited Data"}, {"link": "https://arxiv.org/abs/2106.06796", "abstract": "The performance of federated learning (FL) over wireless networks depend on\nthe reliability of the client-server connectivity and clients' local\ncomputation capabilities. In this article we investigate the problem of client\nscheduling and resource block (RB) allocation to enhance the performance of\nmodel training using FL, over a pre-defined training duration under imperfect\nchannel state information (CSI) and limited local computing resources. First,\nwe analytically derive the gap between the training losses of FL with clients\nscheduling and a centralized training method for a given training duration.\nThen, we formulate the gap of the training loss minimization over client\nscheduling and RB allocation as a stochastic optimization problem and solve it\nusing Lyapunov optimization. A Gaussian process regression-based channel\nprediction method is leveraged to learn and track the wireless channel, in\nwhich, the clients' CSI predictions and computing power are incorporated into\nthe scheduling decision. Using an extensive set of simulations, we validate the\nrobustness of the proposed method under both perfect and imperfect CSI over an\narray of diverse data distributions. Results show that the proposed method\nreduces the gap of the training accuracy loss by up to 40.7% compared to\nstate-of-theart client scheduling and RB allocation methods.", "title": "Joint Client Scheduling and Resource Allocation under Channel  Uncertainty in Federated Learning"}, {"link": "https://arxiv.org/abs/2106.06797", "abstract": "State-of-the-art machine translation (MT) systems are typically trained to\ngenerate the \"standard\" target language; however, many languages have multiple\nvarieties (regional varieties, dialects, sociolects, non-native varieties) that\nare different from the standard language. Such varieties are often\nlow-resource, and hence do not benefit from contemporary NLP solutions, MT\nincluded. We propose a general framework to rapidly adapt MT systems to\ngenerate language varieties that are close to, but different from, the standard\ntarget language, using no parallel (source--variety) data. This also includes\nadaptation of MT systems to low-resource typologically-related target\nlanguages. We experiment with adapting an English--Russian MT system to\ngenerate Ukrainian and Belarusian, an English--Norwegian Bokm{\\aa}l system to\ngenerate Nynorsk, and an English--Arabic system to generate four Arabic\ndialects, obtaining significant improvements over competitive baselines.", "title": "Machine Translation into Low-resource Language Varieties"}, {"link": "https://arxiv.org/abs/2106.06799", "abstract": "Differentiable neural architecture search (NAS) has attracted significant\nattention in recent years due to its ability to quickly discover promising\narchitectures of deep neural networks even in very large search spaces. Despite\nits success, DARTS lacks robustness in certain cases, e.g. it may degenerate to\ntrivial architectures with excessive parametric-free operations such as skip\nconnection or random noise, leading to inferior performance. In particular,\noperation selection based on the magnitude of architectural parameters was\nrecently proven to be fundamentally wrong showcasing the need to rethink this\naspect. On the other hand, zero-cost proxies have been recently studied in the\ncontext of sample-based NAS showing promising results -- speeding up the search\nprocess drastically in some cases but also failing on some of the large search\nspaces typical for differentiable NAS. In this work we propose a novel\noperation selection paradigm in the context of differentiable NAS which\nutilises zero-cost proxies. Our perturbation-based zero-cost operation\nselection (Zero-Cost-PT) improves searching time and, in many cases, accuracy\ncompared to the best available differentiable architecture search, regardless\nof the search space size. Specifically, we are able to find comparable\narchitectures to DARTS-PT on the DARTS CNN search space while being over 40x\nfaster (total searching time 25 minutes on a single GPU).", "title": "Zero-Cost Proxies Meet Differentiable Architecture Search"}, {"link": "https://arxiv.org/abs/2106.06801", "abstract": "Contrastive Learning (CL) is a recent representation learning approach, which\nachieves promising results by encouraging inter-class separability and\nintra-class compactness in learned image representations. Because medical\nimages often contain multiple classes of interest per image, a standard\nimage-level CL for these images is not applicable. In this work, we present a\nnovel semi-supervised 2D medical segmentation solution that applies CL on image\npatches, instead of full images. These patches are meaningfully constructed\nusing the semantic information of different classes obtained via pseudo\nlabeling. We also propose a novel consistency regularization scheme, which\nworks in synergy with contrastive learning. It addresses the problem of\nconfirmation bias often observed in semi-supervised settings, and encourages\nbetter clustering in the feature space. We evaluate our method on four public\nmedical segmentation datasets along with a novel histopathology dataset that we\nintroduce. Our method obtains consistent improvements over the state-of-the-art\nsemi-supervised segmentation approaches for all datasets.", "title": "Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation"}, {"link": "https://arxiv.org/abs/2106.06804", "abstract": "Explainable artificial intelligence has rapidly emerged since lawmakers have\nstarted requiring interpretable models for safety-critical domains.\nConcept-based neural networks have arisen as explainable-by-design methods as\nthey leverage human-understandable symbols (i.e. concepts) to predict class\nmemberships. However, most of these approaches focus on the identification of\nthe most relevant concepts but do not provide concise, formal explanations of\nhow such concepts are leveraged by the classifier to make predictions. In this\npaper, we propose a novel end-to-end differentiable approach enabling the\nextraction of logic explanations from neural networks using the formalism of\nFirst-Order Logic. The method relies on an entropy-based criterion which\nautomatically identifies the most relevant concepts. We consider four different\ncase studies to demonstrate that: (i) this entropy-based criterion enables the\ndistillation of concise logic explanations in safety-critical domains from\nclinical data to computer vision; (ii) the proposed approach outperforms\nstate-of-the-art white-box models in terms of classification accuracy.", "title": "Entropy-based Logic Explanations of Neural Networks"}, {"link": "https://arxiv.org/abs/2106.06807", "abstract": "We present a new approach for redirected walking in static and dynamic scenes\nthat uses techniques from robot motion planning to compute the redirection\ngains that steer the user on collision-free paths in the physical space. Our\nfirst contribution is a mathematical framework for redirected walking using\nconcepts from motion planning and configuration spaces. This framework\nhighlights various geometric and perceptual constraints that tend to make\ncollision-free redirected walking difficult. We use our framework to propose an\nefficient solution to the redirection problem that uses the notion of\nvisibility polygons to compute the free spaces in the physical environment and\nthe virtual environment. The visibility polygon provides a concise\nrepresentation of the entire space that is visible, and therefore walkable, to\nthe user from their position within an environment. Using this representation\nof walkable space, we apply redirected walking to steer the user to regions of\nthe visibility polygon in the physical environment that closely match the\nregion that the user occupies in the visibility polygon in the virtual\nenvironment. We show that our algorithm is able to steer the user along paths\nthat result in significantly fewer red{resets than existing state-of-the-art\nalgorithms in both static and dynamic scenes.", "title": "Redirected Walking in Static and Dynamic Scenes Using Visibility  Polygons"}, {"link": "https://arxiv.org/abs/2106.06811", "abstract": "COVID-19 pandemic has generated what public health officials called an\ninfodemic of misinformation. As social distancing and stay-at-home orders came\ninto effect, many turned to social media for socializing. This increase in\nsocial media usage has made it a prime vehicle for the spreading of\nmisinformation. This paper presents a mechanism to detect COVID-19\nhealth-related misinformation in social media following an interdisciplinary\napproach. Leveraging social psychology as a foundation and existing\nmisinformation frameworks, we defined misinformation themes and associated\nkeywords incorporated into the misinformation detection mechanism using applied\nmachine learning techniques. Next, using the Twitter dataset, we explored the\nperformance of the proposed methodology using multiple state-of-the-art machine\nlearning classifiers. Our method shows promising results with at most 78%\naccuracy in classifying health-related misinformation versus true information\nusing uni-gram-based NLP feature generations from tweets and the Decision Tree\nclassifier. We also provide suggestions on alternatives for countering\nmisinformation and ethical consideration for the study.", "title": "Case Study on Detecting COVID-19 Health-Related Misinformation in Social  Media"}, {"link": "https://arxiv.org/abs/2106.06815", "abstract": "Dimension reduction of data sets is a standard problem in the realm of\nmachine learning and knowledge reasoning. They affect patterns in and\ndependencies on data dimensions and ultimately influence any decision-making\nprocesses. Therefore, a wide variety of reduction procedures are in use, each\npursuing different objectives. A so far not considered criterion is the\nconceptual continuity of the reduction mapping, i.e., the preservation of the\nconceptual structure with respect to the original data set. Based on the notion\nscale-measure from formal concept analysis we present in this work a) the\ntheoretical foundations to detect and quantify conceptual errors in data\nscalings; b) an experimental investigation of our approach on eleven data sets\nthat were respectively treated with a variant of non-negative matrix\nfactorization.", "title": "Quantifying the Conceptual Error in Dimensionality Reduction"}, {"link": "https://arxiv.org/abs/2106.06816", "abstract": "The robust disturbance rejection controller has been the subject of intensive\nresearch due to its undeniable importance for automation. Modern control theory\ntends to use model-based approaches versus model-free approaches, especially\nwhen it comes to highly modern applications. The backbone of the dissertation\nis based on the systematic modeling of dynamic systems and the development of\nadvanced control methods. Accordingly, the dissertation begins with the\ninvestigation of nonlinearities in dynamic systems. The extension of classic\nsubspace algorithms for linear systems in the frequency domain is tackled using\nthe new local polynomial approach. Next, the problem of disturbance control is\naddressed, namely modeling of uncertainties and non-modeled high-order\ndynamics, fragility of the controller and observer systems, and the\nnon-linearities are analyzed separately.", "title": "System Identification and Model-based Robust Nonlinear Disturbance  Rejection Control"}, {"link": "https://arxiv.org/abs/2106.06819", "abstract": "Conditional generative models of high-dimensional images have many\napplications, but supervision signals from conditions to images can be\nexpensive to acquire. This paper describes Diffusion-Decoding models with\nContrastive representations (D2C), a paradigm for training unconditional\nvariational autoencoders (VAEs) for few-shot conditional image generation. D2C\nuses a learned diffusion-based prior over the latent representations to improve\ngeneration and contrastive self-supervised learning to improve representation\nquality. D2C can adapt to novel generation tasks conditioned on labels or\nmanipulation constraints, by learning from as few as 100 labeled examples. On\nconditional generation from new labels, D2C achieves superior performance over\nstate-of-the-art VAEs and diffusion models. On conditional image manipulation,\nD2C generations are two orders of magnitude faster to produce over StyleGAN2\nones and are preferred by 50% - 60% of the human evaluators in a double-blind\nstudy.", "title": "D2C: Diffusion-Denoising Models for Few-shot Conditional Generation"}, {"link": "https://arxiv.org/abs/2106.06822", "abstract": "Automatic International Classification of Diseases (ICD) coding is defined as\na kind of text multi-label classification problem, which is difficult because\nthe number of labels is very large and the distribution of labels is\nunbalanced. The label-wise attention mechanism is widely used in automatic ICD\ncoding because it can assign weights to every word in full Electronic Medical\nRecords (EMR) for different ICD codes. However, the label-wise attention\nmechanism is computational redundant and costly. In this paper, we propose a\npseudo label-wise attention mechanism to tackle the problem. Instead of\ncomputing different attention modes for different ICD codes, the pseudo\nlabel-wise attention mechanism automatically merges similar ICD codes and\ncomputes only one attention mode for the similar ICD codes, which greatly\ncompresses the number of attention modes and improves the predicted accuracy.\nIn addition, we apply a more convenient and effective way to obtain the ICD\nvectors, and thus our model can predict new ICD codes by calculating the\nsimilarities between EMR vectors and ICD vectors. Extensive experiments show\nthe superior performance of our model. On the public MIMIC-III dataset and\nprivate Xiangya dataset, our model achieves micro f1 of 0.575 and 0.796,\nrespectively, which outperforms other competing models. Furthermore, we verify\nthe ability of our model in predicting new ICD codes. The case study shows how\npseudo label-wise attention works, and demonstrates the effectiveness of pseudo\nlabel-wise attention mechanism.", "title": "A Pseudo Label-wise Attention Network for Automatic ICD Coding"}, {"link": "https://arxiv.org/abs/2106.06823", "abstract": "Many commonsense reasoning NLP tasks involve choosing between one or more\npossible answers to a question or prompt based on knowledge that is often\nimplicit. Large pretrained language models (PLMs) can achieve near-human\nperformance on such tasks, while providing little human-interpretable evidence\nof the underlying reasoning they use. In this work, we show how to use these\nsame models to generate such evidence: inspired by the contrastive nature of\nhuman explanations, we use PLMs to complete explanation prompts which contrast\nalternatives according to the key attribute(s) required to justify the correct\nanswer (for example, peanuts are usually salty while raisins are sweet).\nConditioning model decisions on these explanations improves performance on two\ncommonsense reasoning benchmarks, as compared to previous non-contrastive\nalternatives. These explanations are also judged by humans to be more relevant\nfor solving the task, and facilitate a novel method to evaluate explanation\nfaithfulfness.", "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks"}, {"link": "https://arxiv.org/abs/2106.06828", "abstract": "Trust region methods are widely applied in single-agent reinforcement\nlearning problems due to their monotonic performance-improvement guarantee at\nevery iteration. Nonetheless, when applied in multi-agent settings, the\nguarantee of trust region methods no longer holds because an agent's payoff is\nalso affected by other agents' adaptive behaviors. To tackle this problem, we\nconduct a game-theoretical analysis in the policy space, and propose a\nmulti-agent trust region learning method (MATRL), which enables trust region\noptimization for multi-agent learning. Specifically, MATRL finds a stable\nimprovement direction that is guided by the solution concept of Nash\nequilibrium at the meta-game level. We derive the monotonic improvement\nguarantee in multi-agent settings and empirically show the local convergence of\nMATRL to stable fixed points in the two-player rotational differential game. To\ntest our method, we evaluate MATRL in both discrete and continuous multiplayer\ngeneral-sum games including checker and switch grid worlds, multi-agent MuJoCo,\nand Atari games. Results suggest that MATRL significantly outperforms strong\nmulti-agent reinforcement learning baselines.", "title": "A Game-Theoretic Approach to Multi-Agent Trust Region Optimization"}, {"link": "https://arxiv.org/abs/2106.06830", "abstract": "Retrieval is a core component for open-domain NLP tasks. In open-domain\ntasks, multiple entities can share a name, making disambiguation an inherent\nyet under-explored problem. We propose an evaluation benchmark for assessing\nthe entity disambiguation capabilities of these retrievers, which we call\nAmbiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection\nof entities that share a name along with queries about those entities. By\ncovering the set of entities for polysemous names, AmbER sets act as a\nchallenging test of entity disambiguation. We create AmbER sets for three\npopular open-domain tasks: fact checking, slot filling, and question answering,\nand evaluate a diverse set of retrievers. We find that the retrievers exhibit\npopularity bias, significantly under-performing on rarer entities that share a\nname, e.g., they are twice as likely to retrieve erroneous documents on queries\nfor the less popular entity under the same name. These experiments on AmbER\nsets show their utility as an evaluation tool and highlight the weaknesses of\npopular retrieval systems.", "title": "Evaluating Entity Disambiguation and the Role of Popularity in  Retrieval-Based NLP"}, {"link": "https://arxiv.org/abs/2106.06831", "abstract": "Digitization of historical documents is a challenging task in many digital\nhumanities projects. A popular approach for digitization is to scan the\ndocuments into images, and then convert images into text using Optical\nCharacter Recognition (OCR) algorithms. However, the outcome of OCR processing\nof historical documents is usually inaccurate and requires post-processing\nerror correction. This study investigates how crowdsourcing can be utilized to\ncorrect OCR errors in historical text collections, and which crowdsourcing\nmethodology is the most effective in different scenarios and for various\nresearch objectives. A series of experiments with different micro-task's\nstructures and text lengths was conducted with 753 workers on the Amazon's\nMechanical Turk platform. The workers had to fix OCR errors in a selected\nhistorical text. To analyze the results, new accuracy and efficiency measures\nhave been devised. The analysis suggests that in terms of accuracy, the optimal\ntext length is medium (paragraph-size) and the optimal structure of the\nexperiment is two-phase with a scanned image. In terms of efficiency, the best\nresults were obtained when using longer text in the single-stage structure with\nno image. The study provides practical recommendations to researchers on how to\nbuild the optimal crowdsourcing task for OCR post-correction. The developed\nmethodology can also be utilized to create golden standard historical texts for\nautomatic OCR post-correction. This is the first attempt to systematically\ninvestigate the influence of various factors on crowdsourcing-based OCR\npost-correction and propose an optimal strategy for this process.", "title": "Toward the Optimized Crowdsourcing Strategy for OCR Post-Correction"}, {"link": "https://arxiv.org/abs/2106.06836", "abstract": "We introduce a general framework for the modeling and analysis of vehicular\nnetworks by defining street systems as random 1D subsets of $\\mathbb{R}^{2}$.\nThe street system, in turn, specifies the random intensity measure of a Cox\nprocess of vehicles, i.e., vehicles form independent 1D Poisson point processes\non each street. Models in this Coxian framework can characterize streets of\ndifferent lengths and orientations forming intersections or T-junctions. The\nlengths of the streets can be infinite or finite and mutually independent or\ndependent. We analyze the reliability of communication for different models,\nwhere reliability is the probability that a vehicle at an intersection, a\nT-junction, or a general location can receive a message successfully from a\ntransmitter at a certain distance. Further, we introduce a notion of\nequivalence between vehicular models, which means that a representative model\ncan be used as a proxy for other models in terms of reliability. Specifically,\nwe prove that the Poisson stick process-based vehicular network is equivalent\nto the Poisson line process-based and Poisson lilypond model-based vehicular\nnetworks, and their rotational variants.", "title": "Cox Models for Vehicular Networks: SIR Performance and Equivalence"}, {"link": "https://arxiv.org/abs/2106.06837", "abstract": "We investigate the convergence of the Crouzeix-Raviart finite element method\nfor variational problems with non-autonomous integrands that exhibit\nnon-standard growth conditions. While conforming schemes fail due to the\nLavrentiev gap phenomenon, we prove that the solution of the Crouzeix-Raviart\nscheme converges to a global minimiser. Numerical experiments illustrate the\nperformance of the scheme and give additional analytical insights.", "title": "Crouzeix-Raviart finite element method for non-autonomous variational  problems with Lavrentiev gap"}, {"link": "https://arxiv.org/abs/2106.06838", "abstract": "In this paper, we presents a low-complexity deep learning frameworks for\nacoustic scene classification (ASC). The proposed framework can be separated\ninto three main steps: Front-end spectrogram extraction, back-end\nclassification, and late fusion of predicted probabilities. First, we use Mel\nfilter, Gammatone filter and Constant Q Transfrom (CQT) to transform raw audio\nsignal into spectrograms, where both frequency and temporal features are\npresented. Three spectrograms are then fed into three individual back-end\nconvolutional neural networks (CNNs), classifying into ten urban scenes.\nFinally, a late fusion of three predicted probabilities obtained from three\nCNNs is conducted to achieve the final classification result. To reduce the\ncomplexity of our proposed CNN network, we apply two model compression\ntechniques: model restriction and decomposed convolution. Our extensive\nexperiments, which are conducted on DCASE 2021 (IEEE AASP Challenge on\nDetection and Classification of Acoustic Scenes and Events) Task 1A development\ndataset, achieve a low-complexity CNN based framework with 128 KB trainable\nparameters and the best classification accuracy of 66.7%, improving DCASE\nbaseline by 19.0%", "title": "A Low-Compexity Deep Learning Framework For Acoustic Scene  Classification"}, {"link": "https://arxiv.org/abs/2106.06839", "abstract": "This paper addresses the ability to enable machines to automatically detect\nfailures on machine tool components as well as estimating the severity of the\nfailures, which is a critical step towards autonomous production machines.\nExtracting information about the severity of failures has been a substantial\npart of classical, as well as Machine Learning based machine vision systems.\nEfforts have been undertaken to automatically predict the severity of failures\non machine tool components for predictive maintenance purposes. Though, most\napproaches only partly cover a completely automatic system from detecting\nfailures to the prognosis of their future severity. To the best of the authors\nknowledge, this is the first time a vision-based system for defect detection\nand prognosis of failures on metallic surfaces in general and on Ball Screw\nDrives in specific has been proposed. The authors show that they can do both,\ndetect and prognose the evolution of a failure on the surface of a Ball Screw\nDrive.", "title": "Intelligent Vision Based Wear Forecasting on Surfaces of Machine Tool  Elements"}, {"link": "https://arxiv.org/abs/2106.06840", "abstract": "In this paper, we present deep learning frameworks for audio-visual scene\nclassification (SC) and indicate how individual visual and audio features as\nwell as their combination affect SC performance. Our extensive experiments,\nwhich are conducted on DCASE (IEEE AASP Challenge on Detection and\nClassification of Acoustic Scenes and Events) Task 1B development dataset,\nachieve the best classification accuracy of 82.2%, 91.1%, and 93.9% with audio\ninput only, visual input only, and both audio-visual input, respectively. The\nhighest classification accuracy of 93.9%, obtained from an ensemble of\naudio-based and visual-based frameworks, shows an improvement of 16.5% compared\nwith DCASE baseline.", "title": "Deep Learning Frameworks Applied For Audio-Visual Scene Classification"}, {"link": "https://arxiv.org/abs/2106.06842", "abstract": "The Reinforcement Learning (RL) building blocks, i.e. Q-functions and policy\nnetworks, usually take elements from the cartesian product of two domains as\ninput. In particular, the input of the Q-function is both the state and the\naction, and in multi-task problems (Meta-RL) the policy can take a state and a\ncontext. Standard architectures tend to ignore these variables' underlying\ninterpretations and simply concatenate their features into a single vector. In\nthis work, we argue that this choice may lead to poor gradient estimation in\nactor-critic algorithms and high variance learning steps in Meta-RL algorithms.\nTo consider the interaction between the input variables, we suggest using a\nHypernetwork architecture where a primary network determines the weights of a\nconditional dynamic network. We show that this approach improves the gradient\napproximation and reduces the learning step variance, which both accelerates\nlearning and improves the final performance. We demonstrate a consistent\nimprovement across different locomotion tasks and different algorithms both in\nRL (TD3 and SAC) and in Meta-RL (MAML and PEARL).", "title": "Recomposing the Reinforcement Learning Building Blocks with  Hypernetworks"}, {"link": "https://arxiv.org/abs/2106.06843", "abstract": "Federated learning is an emerging distributed machine learning framework for\nprivacy preservation. However, models trained in federated learning usually\nhave worse performance than those trained in the standard centralized learning\nmode, especially when the training data are not independent and identically\ndistributed (Non-IID) on the local devices. In this survey, we pro-vide a\ndetailed analysis of the influence of Non-IID data on both parametric and\nnon-parametric machine learning models in both horizontal and vertical\nfederated learning. In addition, cur-rent research work on handling challenges\nof Non-IID data in federated learning are reviewed, and both advantages and\ndisadvantages of these approaches are discussed. Finally, we suggest several\nfuture research directions before concluding the paper.", "title": "Federated Learning on Non-IID Data: A Survey"}, {"link": "https://arxiv.org/abs/2106.06844", "abstract": "In recent years, numerous studies have used 'data subject access requests' in\na collective manner, to tackle information asymmetries and shed light on data\ncollection and privacy practices of organizations. While successful at\nincreasing transparency, such studies are quite hard to conduct for the simple\nfact that right of access is an individual right. This means that researchers\nhave to recruit participants and guide them through the often-cumbersome\nprocess of access. In this paper, we present an alternative method: to ask\nparticipants to delegate their right of access to the researchers. We discuss\nthe legal grounds for doing this, the advantages it can bring to both\nresearchers and data subjects, and present a procedural and technical design to\nexecute it in a manner that ensures data subjects stay informed and in charge\nduring the process. We tested our method in a pilot study in the Netherlands,\nand found that it creates a win-win for both the researchers and the\nparticipants. We also noted differences in how data controllers from various\nsectors react to such requests and discuss some remaining challenges.", "title": "Amplifying Privacy: Scaling Up Transparency Research Through Delegated  Access Requests"}, {"link": "https://arxiv.org/abs/2106.06845", "abstract": "Heterogeneity in medical data, e.g., from data collected at different sites\nand with different protocols in a clinical study, is a fundamental hurdle for\naccurate prediction using machine learning models, as such models often fail to\ngeneralize well. This paper presents a normalizing-flow-based method to perform\ncounterfactual inference upon a structural causal model (SCM) to harmonize such\ndata. We formulate a causal model for observed effects (brain magnetic\nresonance imaging data) that result from known confounders (site, gender and\nage) and exogenous noise variables. Our method exploits the bijection induced\nby flow for harmonization. We can infer the posterior of exogenous variables,\nintervene on observations, and draw samples from the resultant SCM to obtain\ncounterfactuals. We evaluate on multiple, large, real-world medical datasets to\nobserve that this method leads to better cross-domain generalization compared\nto state-of-the-art algorithms. Further experiments that evaluate the quality\nof confounder-independent data generated by our model using regression and\nclassification tasks are provided.", "title": "Harmonization with Flow-based Causal Inference"}, {"link": "https://arxiv.org/abs/2106.06847", "abstract": "Video super-resolution (VSR), with the aim to restore a high-resolution video\nfrom its corresponding low-resolution version, is a spatial-temporal sequence\nprediction problem. Recently, Transformer has been gaining popularity due to\nits parallel computing ability for sequence-to-sequence modeling. Thus, it\nseems to be straightforward to apply the vision Transformer to solve VSR.\nHowever, the typical block design of Transformer with a fully connected\nself-attention layer and a token-wise feed-forward layer does not fit well for\nVSR due to the following two reasons. First, the fully connected self-attention\nlayer neglects to exploit the data locality because this layer relies on linear\nlayers to compute attention maps. Second, the token-wise feed-forward layer\nlacks the feature alignment which is important for VSR since this layer\nindependently processes each of the input token embeddings without any\ninteraction among them. In this paper, we make the first attempt to adapt\nTransformer for VSR. Specifically, to tackle the first issue, we present a\nspatial-temporal convolutional self-attention layer with a theoretical\nunderstanding to exploit the locality information. For the second issue, we\ndesign a bidirectional optical flow-based feed-forward layer to discover the\ncorrelations across different video frames and also align features. Extensive\nexperiments on several benchmark datasets demonstrate the effectiveness of our\nproposed method. The code will be available at\nhttps://github.com/caojiezhang/VSR-Transformer.", "title": "Video Super-Resolution Transformer"}, {"link": "https://arxiv.org/abs/2106.06848", "abstract": "We consider the problem of finding, through adaptive sampling, which of n\npopulations (arms) has the largest mean. Our objective is to determine a rule\nwhich identifies the best population with a fixed minimum confidence using as\nfew observations as possible, i.e. fixed-confidence (FC) best arm\nidentification (BAI) in multi-armed bandits. We study such problems under the\nBayesian setting with both Bernoulli and Gaussian populations. We propose to\nuse the classical vector at a time (VT) rule, which samples each alive\npopulation once in each round. We show how VT can be implemented and analyzed\nin our Bayesian setting and be improved by early elimination. We also propose\nand analyze a variant of the classical play the winner (PW) algorithm.\nNumerical results show that these rules compare favorably with state-of-art\nalgorithms.", "title": "Guaranteed Fixed-Confidence Best Arm Identification in Multi-Armed  Bandit"}, {"link": "https://arxiv.org/abs/2106.06849", "abstract": "Transformer-based language models (LMs) continue to advance state-of-the-art\nperformance on NLP benchmark tasks, including tasks designed to mimic\nhuman-inspired \"commonsense\" competencies. To better understand the degree to\nwhich LMs can be said to have certain linguistic reasoning skills, researchers\nare beginning to adapt the tools and concepts of the field of psychometrics.\nBut to what extent can the benefits flow in the other direction? I.e., can LMs\nbe of use in predicting what the psychometric properties of test items will be\nwhen those items are given to human participants? We gather responses from\nnumerous human participants and LMs (transformer and non-transformer-based) on\na broad diagnostic test of linguistic competencies. We then use the responses\nto calculate standard psychometric properties of the items in the diagnostic\ntest, using the human responses and the LM responses separately. We then\ndetermine how well these two sets of predictions match. We find cases in which\ntransformer-based LMs predict psychometric properties consistently well in\ncertain categories but consistently poorly in others, thus providing new\ninsights into fundamental similarities and differences between human and LM\nreasoning.", "title": "Can Transformer Language Models Predict Psychometric Properties?"}, {"link": "https://arxiv.org/abs/2106.06854", "abstract": "Marginalized importance sampling (MIS), which measures the density ratio\nbetween the state-action occupancy of a target policy and that of a sampling\ndistribution, is a promising approach for off-policy evaluation. However,\ncurrent state-of-the-art MIS methods rely on complex optimization tricks and\nsucceed mostly on simple toy problems. We bridge the gap between MIS and deep\nreinforcement learning by observing that the density ratio can be computed from\nthe successor representation of the target policy. The successor representation\ncan be trained through deep reinforcement learning methodology and decouples\nthe reward optimization from the dynamics of the environment, making the\nresulting algorithm stable and applicable to high-dimensional domains. We\nevaluate the empirical performance of our approach on a variety of challenging\nAtari and MuJoCo environments.", "title": "A Deep Reinforcement Learning Approach to Marginalized Importance  Sampling with the Successor Representation"}, {"link": "https://arxiv.org/abs/2106.06856", "abstract": "Multi-Camera Multiple Object Tracking (MC-MOT) is a significant computer\nvision problem due to its emerging applicability in several real-world\napplications. Despite a large number of existing works, solving the data\nassociation problem in any MC-MOT pipeline is arguably one of the most\nchallenging tasks. Developing a robust MC-MOT system, however, is still highly\nchallenging due to many practical issues such as inconsistent lighting\nconditions, varying object movement patterns, or the trajectory occlusions of\nthe objects between the cameras. To address these problems, this work,\ntherefore, proposes a new Dynamic Graph Model with Link Prediction (DyGLIP)\napproach to solve the data association task. Compared to existing methods, our\nnew model offers several advantages, including better feature representations\nand the ability to recover from lost tracks during camera transitions.\nMoreover, our model works gracefully regardless of the overlapping ratios\nbetween the cameras. Experimental results show that we outperform existing\nMC-MOT algorithms by a large margin on several practical datasets. Notably, our\nmodel works favorably on online settings but can be extended to an incremental\napproach for large-scale datasets.", "title": "DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate  Multi-Camera Multiple Object Tracking"}, {"link": "https://arxiv.org/abs/2106.06860", "abstract": "Offline reinforcement learning (RL) defines the task of learning from a fixed\nbatch of data. Due to errors in value estimation from out-of-distribution\nactions, most offline RL algorithms take the approach of constraining or\nregularizing the policy with the actions contained in the dataset. Built on\npre-existing RL algorithms, modifications to make an RL algorithm work offline\ncomes at the cost of additional complexity. Offline RL algorithms introduce new\nhyperparameters and often leverage secondary components such as generative\nmodels, while adjusting the underlying RL algorithm. In this paper we aim to\nmake a deep RL algorithm work while making minimal changes. We find that we can\nmatch the performance of state-of-the-art offline RL algorithms by simply\nadding a behavior cloning term to the policy update of an online RL algorithm\nand normalizing the data. The resulting algorithm is a simple to implement and\ntune baseline, while more than halving the overall run time by removing the\nadditional computational overheads of previous methods.", "title": "A Minimalist Approach to Offline Reinforcement Learning"}, {"link": "https://arxiv.org/abs/2106.06863", "abstract": "To date, various speech technology systems have adopted the vocoder approach,\na method for synthesizing speech waveform that shows a major role in the\nperformance of statistical parametric speech synthesis. WaveNet one of the best\nmodels that nearly resembles the human voice, has to generate a waveform in a\ntime consuming sequential manner with an extremely complex structure of its\nneural networks.", "title": "Continuous Wavelet Vocoder-based Decomposition of Parametric Speech  Waveform Synthesis"}, {"link": "https://arxiv.org/abs/2106.06866", "abstract": "Fonts are ubiquitous across documents and come in a variety of styles. They\nare either represented in a native vector format or rasterized to produce fixed\nresolution images. In the first case, the non-standard representation prevents\nbenefiting from latest network architectures for neural representations; while,\nin the latter case, the rasterized representation, when encoded via networks,\nresults in loss of data fidelity, as font-specific discontinuities like edges\nand corners are difficult to represent using neural networks. Based on the\nobservation that complex fonts can be represented by a superposition of a set\nof simpler occupancy functions, we introduce \\textit{multi-implicits} to\nrepresent fonts as a permutation-invariant set of learned implict functions,\nwithout losing features (e.g., edges and corners). However, while\nmulti-implicits locally preserve font features, obtaining supervision in the\nform of ground truth multi-channel signals is a problem in itself. Instead, we\npropose how to train such a representation with only local supervision, while\nthe proposed neural architecture directly finds globally consistent\nmulti-implicits for font families. We extensively evaluate the proposed\nrepresentation for various tasks including reconstruction, interpolation, and\nsynthesis to demonstrate clear advantages with existing alternatives.\nAdditionally, the representation naturally enables glyph completion, wherein a\nsingle characteristic font is used to synthesize a whole font family in the\ntarget style.", "title": "A Multi-Implicit Neural Representation for Fonts"}, {"link": "https://arxiv.org/abs/2106.06867", "abstract": "Ant species such as Temnothorax albipennis select a new nest site in a\ndistributed fashion that, if modeled correctly, can serve as useful information\nfor site selection algorithms for robotic swarms and other applications.\nStudying and replicating the ants' house hunting behavior will also illuminate\nuseful distributed strategies that have evolved in nature. Many of the existing\nmodels of househunting behaviour for T. albipennis make the assumption that all\ncandidate nest sites are equally distant from the ants' home nest, or that an\nant has an equal probability of finding each candidate nest site. However,\nrealistically this is not the case, as nests that are further away from the\nhome nest and nests that are difficult to access are less likely to be found,\neven if they are of higher quality. We extend previous house-hunting models to\naccount for a pairwise distance metric between nests, compare our results to\nthose of real colonies, and use our results to examine the effects of house\nhunting in nests of different spatial orientations. Our incorporation of\ndistances in the ant model appear to match empirical data in situations where a\ndistance-quality tradeoff between nests is relevant. Furthermore, the model\ncontinues to be on par with previous house-hunting models in experiments where\nall candidate nests are equidistant from the home nest, as is typically\nassumed.", "title": "A Spatially Dependent Probabilistic Model for House Hunting in Ant  Colonies"}, {"link": "https://arxiv.org/abs/2106.06868", "abstract": "Accurate mechanisms for forecasting solar irradiance and insolation provide\nimportant information for the planning of renewable energy and agriculture\nprojects as well as for environmental and socio-economical studies. This\nresearch introduces a pipeline for the one-day ahead forecasting of solar\nirradiance and insolation that only requires solar irradiance historical data\nfor training. Furthermore, our approach is able to deal with missing data since\nit includes a data imputation state. In the prediction stage, we consider four\ndata-driven approaches: Autoregressive Integrated Moving Average (ARIMA),\nSingle Layer Feed Forward Network (SL-FNN), Multiple Layer Feed Forward Network\n(FL-FNN), and Long Short-Term Memory (LSTM). The experiments are performed in a\nreal-world dataset collected with 12 Automatic Weather Stations (AWS) located\nin the Nari\\~no - Colombia. The results show that the neural network-based\nmodels outperform ARIMA in most cases. Furthermore, LSTM exhibits better\nperformance in cloudy environments (where more randomness is expected).", "title": "Short-term forecasting of global solar irradiance with incomplete data"}, {"link": "https://arxiv.org/abs/2106.06873", "abstract": "Graphs are widely used to model the relational structure of data, and the\nresearch of graph machine learning (ML) has a wide spectrum of applications\nranging from drug design in molecular graphs to friendship recommendation in\nsocial networks. Prevailing approaches for graph ML typically require abundant\nlabeled instances in achieving satisfactory results, which is commonly\ninfeasible in real-world scenarios since labeled data for newly emerged\nconcepts (e.g., new categorizations of nodes) on graphs is limited. Though\nmeta-learning has been applied to different few-shot graph learning problems,\nmost existing efforts predominately assume that all the data from those seen\nclasses is gold-labeled, while those methods may lose their efficacy when the\nseen data is weakly-labeled with severe label noise. As such, we aim to\ninvestigate a novel problem of weakly-supervised graph meta-learning for\nimproving the model robustness in terms of knowledge transfer. To achieve this\ngoal, we propose a new graph meta-learning framework -- Graph Hallucination\nNetworks (Meta-GHN) in this paper. Based on a new robustness-enhanced episodic\ntraining, Meta-GHN is meta-learned to hallucinate clean node representations\nfrom weakly-labeled data and extracts highly transferable meta-knowledge, which\nenables the model to quickly adapt to unseen tasks with few labeled instances.\nExtensive experiments demonstrate the superiority of Meta-GHN over existing\ngraph meta-learning studies on the task of weakly-supervised few-shot node\nclassification.", "title": "Weakly-supervised Graph Meta-learning for Few-shot Node Classification"}, {"link": "https://arxiv.org/abs/2106.06875", "abstract": "High-performing machine translation (MT) systems can help overcome language\nbarriers while making it possible for everyone to communicate and use language\ntechnologies in the language of their choice. However, such systems require\nlarge amounts of parallel sentences for training, and translators can be\ndifficult to find and expensive. Here, we present a data collection strategy\nfor MT which, in contrast, is cheap and simple, as it does not require\nbilingual speakers. Based on the insight that humans pay specific attention to\nmovements, we use graphics interchange formats (GIFs) as a pivot to collect\nparallel sentences from monolingual annotators. We use our strategy to collect\ndata in Hindi, Tamil and English. As a baseline, we also collect data using\nimages as a pivot. We perform an intrinsic evaluation by manually evaluating a\nsubset of the sentence pairs and an extrinsic evaluation by finetuning mBART on\nthe collected data. We find that sentences collected via GIFs are indeed of\nhigher quality.", "title": "Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine  Translation Data"}, {"link": "https://arxiv.org/abs/2106.06876", "abstract": "A new class of test functions for black box optimization is introduced.\nAffine OneMax (AOM) functions are defined as compositions of OneMax and\ninvertible affine maps on bit vectors. The black box complexity of the class is\nupper bounded by a polynomial of large degree in the dimension. The proof\nrelies on discrete Fourier analysis and the Kushilevitz-Mansour algorithm.\nTunable complexity is achieved by expressing invertible linear maps as finite\nproducts of transvections. The black box complexity of sub-classes of AOM\nfunctions is studied. Finally, experimental results are given to illustrate the\nperformance of search algorithms on AOM functions.", "title": "Affine OneMax"}, {"link": "https://arxiv.org/abs/2106.06878", "abstract": "In probabilistic nonadaptive group testing (PGT), we aim to characterize the\nnumber of pooled tests necessary to identify a random $k$-sparse vector of\ndefectives with high probability. Recent work has shown that $n$ tests are\nnecessary when $k =\\omega(n/\\log n)$. It is also known that $O(k \\log n)$ tests\nare necessary and sufficient in other regimes. This leaves open the important\nsparsity regime where the probability of a defective item is $\\sim 1/\\log n$\n(or $k = \\Theta(n/\\log n)$) where the number of tests required is linear in\n$n$. In this work we aim to exactly characterize the number of tests in this\nsparsity regime. In particular, we seek to determine the number of defectives\n$\\lambda(\\alpha)n / \\log n$ that can be identified if the number of tests is\n$\\alpha n$. In the process, we give upper and lower bounds on the exact point\nat which individual testing becomes suboptimal, and the use of a carefully\nconstructed pooled test design is beneficial.", "title": "Probabilistic Group Testing with a Linear Number of Tests"}, {"link": "https://arxiv.org/abs/2106.06880", "abstract": "Recently, there has been much interest in studying the convergence rates of\nwithout-replacement SGD, and proving that it is faster than with-replacement\nSGD in the worst case. However, these works ignore or do not provide tight\nbounds in terms of the problem's geometry, including its condition number.\nPerhaps surprisingly, we prove that when the condition number is taken into\naccount, without-replacement SGD \\emph{does not} significantly improve on\nwith-replacement SGD in terms of worst-case bounds, unless the number of epochs\n(passes over the data) is larger than the condition number. Since many problems\nin machine learning and other areas are both ill-conditioned and involve large\ndatasets, this indicates that without-replacement does not necessarily improve\nover with-replacement sampling for realistic iteration budgets. We show this by\nproviding new lower and upper bounds which are tight (up to log factors), for\nquadratic problems with commuting quadratic terms, precisely quantifying the\ndependence on the problem parameters.", "title": "Random Shuffling Beats SGD Only After Many Epochs on Ill-Conditioned  Problems"}, {"link": "https://arxiv.org/abs/2106.06882", "abstract": "Bird's Eye View (BEV) is a popular representation for processing 3D point\nclouds, and by its nature is fundamentally sparse. Motivated by the\ncomputational limitations of mobile robot platforms, we take a fast\nhigh-performance BEV 3D object detector - PointPillars - and modify its\nbackbone to exploit this sparsity, leading to decreased runtimes. We present\npreliminary results demonstrating decreased runtimes with either the same\nperformance or a modest decrease in performance, which we anticipate will be\nremedied by model specific hyperparameter tuning. Our work is a first step\ntowards a new class of 3D object detectors that exploit sparsity throughout\ntheir entire pipeline in order to reduce runtime and resource usage while\nmaintaining good detection performance.", "title": "Sparse PointPillars: Exploiting Sparsity in Birds-Eye-View Object  Detection"}, {"link": "https://arxiv.org/abs/2106.06885", "abstract": "Inspired by the demands of real-time climate and weather forecasting, we\ndevelop optimistic online learning algorithms that require no parameter tuning\nand have optimal regret guarantees under delayed feedback. Our algorithms --\nDORM, DORMP, and AdaHedgeD -- arise from a novel reduction of delayed online\nlearning to optimistic online learning that reveals how optimistic hints can\nmitigate the regret penalty caused by delay. We pair this delay-as-optimism\nperspective with a new analysis of optimistic learning that exposes its\nrobustness to hinting errors and a new meta-algorithm for learning effective\nhinting strategies in the presence of delay. We conclude by benchmarking our\nalgorithms on four subseasonal climate forecasting tasks, demonstrating low\nregret relative to state-of-the-art forecasting models.", "title": "Online Learning with Optimism and Delay"}, {"link": "https://arxiv.org/abs/2106.06887", "abstract": "Event cameras, inspired by biological vision systems, provide a natural and\ndata efficient representation of visual information. Visual information is\nacquired in the form of events that are triggered by local brightness changes.\nEach pixel location of the camera's sensor records events asynchronously and\nindependently with very high temporal resolution. However, because most\nbrightness changes are triggered by relative motion of the camera and the\nscene, the events recorded at a single sensor location seldom correspond to the\nsame world point. To extract meaningful information from event cameras, it is\nhelpful to register events that were triggered by the same underlying world\npoint. In this work we propose a new model of event data that captures its\nnatural spatio-temporal structure. We start by developing a model for aligned\nevent data. That is, we develop a model for the data as though it has been\nperfectly registered already. In particular, we model the aligned data as a\nspatio-temporal Poisson point process. Based on this model, we develop a\nmaximum likelihood approach to registering events that are not yet aligned.\nThat is, we find transformations of the observed events that make them as\nlikely as possible under our model. In particular we extract the camera\nrotation that leads to the best event alignment. We show new state of the art\naccuracy for rotational velocity estimation on the DAVIS 240C dataset. In\naddition, our method is also faster and has lower computational complexity than\nseveral competing methods.", "title": "The Spatio-Temporal Poisson Point Process: A Simple Model for the  Alignment of Event Camera Data"}, {"link": "https://arxiv.org/abs/2106.06889", "abstract": "Text analytics directly on compression (TADOC) has proven to be a promising\ntechnology for big data analytics. GPUs are extremely popular accelerators for\ndata analytics systems. Unfortunately, no work so far shows how to utilize GPUs\nto accelerate TADOC. We describe G-TADOC, the first framework that provides\nGPU-based text analytics directly on compression, effectively enabling\nefficient text analytics on GPUs without decompressing the input data. G-TADOC\nsolves three major challenges. First, TADOC involves a large amount of\ndependencies, which makes it difficult to exploit massive parallelism on a GPU.\nWe develop a novel fine-grained thread-level workload scheduling strategy for\nGPU threads, which partitions heavily-dependent loads adaptively in a\nfine-grained manner. Second, in developing G-TADOC, thousands of GPU threads\nwriting to the same result buffer leads to inconsistency while directly using\nlocks and atomic operations lead to large synchronization overheads. We develop\na memory pool with thread-safe data structures on GPUs to handle such\ndifficulties. Third, maintaining the sequence information among words is\nessential for lossless compression. We design a sequence-support strategy,\nwhich maintains high GPU parallelism while ensuring sequence information. Our\nexperimental evaluations show that G-TADOC provides 31.1x average speedup\ncompared to state-of-the-art TADOC.", "title": "G-TADOC: Enabling Efficient GPU-Based Text Analytics without  Decompression"}, {"link": "https://arxiv.org/abs/2106.06892", "abstract": "Matching is one of the most fundamental and broadly applicable problems\nacross many domains. In these diverse real-world applications, there is often a\ndegree of uncertainty in the input which has led to the study of stochastic\nmatching models. Here, each edge in the graph has a known, independent\nprobability of existing derived from some prediction. Algorithms must probe\nedges to determine existence and match them irrevocably if they exist. Further,\neach vertex may have a patience constraint denoting how many of its neighboring\nedges can be probed. We present new ordered contention resolution schemes\nyielding improved approximation guarantees for some of the foundational\nproblems studied in this area. For stochastic matching with patience\nconstraints in general graphs, we provide a 0.382-approximate algorithm,\nsignificantly improving over the previous best 0.31-approximation of Baveja et\nal. (2018). When the vertices do not have patience constraints, we describe a\n0.432-approximate random order probing algorithm with several corollaries such\nas an improved guarantee for the Prophet Secretary problem under Edge Arrivals.\nFinally, for the special case of bipartite graphs with unit patience\nconstraints on one of the partitions, we show a 0.632-approximate algorithm\nthat improves on the recent $1/3$-guarantee of Hikima et al. (2021).", "title": "Improved Guarantees for Offline Stochastic Matching via new Ordered  Contention Resolution Schemes"}, {"link": "https://arxiv.org/abs/2106.06895", "abstract": "Convolutional Neural Networks (CNN) have shown impressive performance in\ncomputer vision, natural language processing, and many other applications, but\nthey exhibit high computations and substantial memory requirements. To address\nthese limitations, especially in resource-constrained devices, the use of cloud\ncomputing for CNNs is becoming more popular. This comes with privacy and\nlatency concerns that have motivated the designers to develop embedded hardware\naccelerators for CNNs. However, designing a specialized accelerator increases\nthe time-to-market and cost of production. Therefore, to reduce the\ntime-to-market and access to state-of-the-art techniques, CNN hardware mapping\nand deployment on embedded accelerators are often outsourced to untrusted third\nparties, which is going to be more prevalent in futuristic artificial\nintelligence of things (AIoT) systems. These AIoT systems anticipate horizontal\ncollaboration among different resource-constrained AIoT node devices, where CNN\nlayers are partitioned and these devices collaboratively compute complex CNN\ntasks Therefore, there is a dire need to explore this attack surface for\ndesigning secure embedded hardware accelerators for CNNs. Towards this goal, in\nthis paper, we exploited this attack surface to propose an HT-based attack\ncalled FeSHI. This attack exploits the statistical distribution i.e., Gaussian\ndistribution, of the layer-by-layer feature maps of the CNN to design two\ntriggers for stealthy HT with a very low probability of triggering. To\nillustrate the effectiveness of the proposed attack, we deployed the LeNet and\nLeNet-3D on PYNQ to classify the MNIST and CIFAR-10 datasets, respectively, and\ntested FeSHI. The experimental results show that FeSHI utilizes up to 2% extra\nLUTs, and the overall resource overhead is less than 1% compared to the\noriginal designs", "title": "FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack"}, {"link": "https://arxiv.org/abs/2106.06896", "abstract": "The monitoring of coastal wetlands is of great importance to the protection\nof marine and terrestrial ecosystems. However, due to the complex environment,\nsevere vegetation mixture, and difficulty of access, it is impossible to\naccurately classify coastal wetlands and identify their species with\ntraditional classifiers. Despite the integration of multisource remote sensing\ndata for performance enhancement, there are still challenges with acquiring and\nexploiting the complementary merits from multisource data. In this paper, the\nDeepwise Feature Interaction Network (DFINet) is proposed for wetland\nclassification. A depthwise cross attention module is designed to extract\nself-correlation and cross-correlation from multisource feature pairs. In this\nway, meaningful complementary information is emphasized for classification.\nDFINet is optimized by coordinating consistency loss, discrimination loss, and\nclassification loss. Accordingly, DFINet reaches the standard solution-space\nunder the regularity of loss functions, while the spatial consistency and\nfeature discrimination are preserved. Comprehensive experimental results on two\nhyperspectral and multispectral wetland datasets demonstrate that the proposed\nDFINet outperforms other competitive methods in terms of overall accuracy.", "title": "Hyperspectral and Multispectral Classification for Coastal Wetland Using  Depthwise Feature Interaction Network"}, {"link": "https://arxiv.org/abs/2106.06898", "abstract": "Chaotic systems are notoriously challenging to predict because of their\ninstability. Small errors accumulate in the simulation of each time step,\nresulting in completely different trajectories. However, the trajectories of\nmany prominent chaotic systems live in a low-dimensional subspace (attractor).\nIf the system is Markovian, the attractor is uniquely determined by the Markov\noperator that maps the evolution of infinitesimal time steps. This makes it\npossible to predict the behavior of the chaotic system by learning the Markov\noperator even if we cannot predict the exact trajectory. Recently, a new\nframework for learning resolution-invariant solution operators for PDEs was\nproposed, known as neural operators. In this work, we train a Markov neural\noperator (MNO) with only the local one-step evolution information. We then\ncompose the learned operator to obtain the global attractor and invariant\nmeasure. Such a Markov neural operator forms a discrete semigroup and we\nempirically observe that does not collapse or blow up. Experiments show neural\noperators are more accurate and stable compared to previous methods on chaotic\nsystems such as the Kuramoto-Sivashinsky and Navier-Stokes equations.", "title": "Markov Neural Operators for Learning Chaotic Systems"}, {"link": "https://arxiv.org/abs/2106.06899", "abstract": "Following the success of dot-product attention in Transformers, numerous\napproximations have been recently proposed to address its quadratic complexity\nwith respect to the input length. While these variants are memory and compute\nefficient, it is not possible to directly use them with popular pre-trained\nlanguage models trained using vanilla attention, without an expensive\ncorrective pre-training stage. In this work, we propose a simple yet highly\naccurate approximation for vanilla attention. We process the queries in chunks,\nand for each query, compute the top-$k$ scores with respect to the keys. Our\napproach offers several advantages: (a) its memory usage is linear in the input\nsize, similar to linear attention variants, such as Performer and RFA (b) it is\na drop-in replacement for vanilla attention that does not require any\ncorrective pre-training, and (c) it can also lead to significant memory savings\nin the feed-forward layers after casting them into the familiar query-key-value\nframework. We evaluate the quality of top-$k$ approximation for multi-head\nattention layers on the Long Range Arena Benchmark, and for feed-forward layers\nof T5 and UnifiedQA on multiple QA datasets. We show our approach leads to\naccuracy that is nearly-identical to vanilla attention in multiple setups\nincluding training from scratch, fine-tuning, and zero-shot inference.", "title": "Memory-efficient Transformers via Top-$k$ Attention"}, {"link": "https://arxiv.org/abs/2106.06900", "abstract": "Group recommender systems are widely used in current web applications. In\nthis paper, we propose a novel group recommender system based on the deep\nreinforcement learning. We introduce the MovieLens data at first and generate\none random group dataset, MovieLens-Rand, from it. This randomly generated\ndataset is described and analyzed. We also present experimental settings and\ntwo state-of-art baselines, AGREE and GroupIM. The framework of our novel\nmodel, the Deep Reinforcement learning based Group Recommender system (DRGR),\nis proposed. Actor-critic networks are implemented with the deep deterministic\npolicy gradient algorithm. The DRGR model is applied on the MovieLens-Rand\ndataset with two baselines. Compared with baselines, we conclude that DRGR\nperforms better than GroupIM due to long interaction histories but worse than\nAGREE because of the self-attention mechanism. We express advantages and\nshortcomings of DRGR and also give future improvement directions at the end.", "title": "Deep Reinforcement Learning based Group Recommender System"}, {"link": "https://arxiv.org/abs/2106.06901", "abstract": "Extremely large-scale multiple-input multiple-output (XL-MIMO) communication\naims to further boost the antenna size significantly than current massive MIMO\nsystems, for which conventional far-field assumption with uniform plane wave\n(UPW) model may become invalid. This paper studies the modelling and\nperformance analysis for multi-user XL-MIMO communication. With the spherical\nwavefront phase modelling, and also by taking into account the variations of\nsignal amplitude and projected aperture across array elements, the performance\nof the three typical beamforming schemes are analyzed, namely the maximal-ratio\ncombining (MRC), zero-forcing (ZF), and minimum mean-square error (MMSE)\nbeamforming. For the special case of two-users, we analytically show that the\nsignal-to-interference-plus-noise ratio (SINR) of all the three beamforming\nschemes increases as the channels' correlation coefficient decreases.\nFurthermore, compared to existing UPW model where inter-user interference (IUI)\ncan only be suppressed in angular domain, XL-MIMO enables a new\ndegree-of-freedom (DoF) for IUI suppression by distance separation, even for\nusers along the same direction. Simulation results are provided to validate the\nmodelling and performance analysis of multi-user XL-MIMO communications.", "title": "Multi-User Communication with Extremely Large-Scale MIMO"}, {"link": "https://arxiv.org/abs/2106.06905", "abstract": "E-commerce companies have to face abnormal sellers who sell potentially-risky\nproducts. Typically, the risk can be identified by jointly considering product\ncontent (e.g., title and image) and seller behavior. This work focuses on\nbehavior feature extraction as behavior sequences can provide valuable clues\nfor the risk discovery by reflecting the sellers' operation habits. Traditional\nfeature extraction techniques heavily depend on domain experts and adapt poorly\nto new tasks. In this paper, we propose a self-supervised method InfoBehavior\nto automatically extract meaningful representations from ultra-long raw\nbehavior sequences instead of the costly feature selection procedure.\nInfoBehavior utilizes Bidirectional Transformer as feature encoder due to its\nexcellent capability in modeling long-term dependency. However, it is\nintractable for commodity GPUs because the time and memory required by\nTransformer grow quadratically with the increase of sequence length. Thus, we\npropose a hierarchical grouping strategy to aggregate ultra-long raw behavior\nsequences to length-processable high-level embedding sequences. Moreover, we\nintroduce two types of pretext tasks. Sequence-related pretext task defines a\ncontrastive-based training objective to correctly select the masked-out\ncoarse-grained/fine-grained behavior sequences against other \"distractor\"\nbehavior sequences; Domain-related pretext task designs a classification\ntraining objective to correctly predict the domain-specific statistical results\nof anomalous behavior. We show that behavior representations from the\npre-trained InfoBehavior can be directly used or integrated with features from\nother side information to support a wide range of downstream tasks.\nExperimental results demonstrate that InfoBehavior significantly improves the\nperformance of Product Risk Management and Intellectual Property Protection.", "title": "InfoBehavior: Self-supervised Representation Learning for Ultra-long  Behavior Sequence via Hierarchical Grouping"}, {"link": "https://arxiv.org/abs/2106.06906", "abstract": "We address the problem of determining optimal sensor precisions for\nestimating the states of linear time-varying discrete-time stochastic dynamical\nsystems, with guaranteed bounds on the estimation errors. This is performed in\nthe Kalman filtering framework, where the sensor precisions are treated as\nvariables. They are determined by solving a constrained convex optimization\nproblem, which guarantees the specified upper bound on the posterior error\nvariance. Optimal sensor precisions are determined by minimizing the l1 norm,\nwhich promotes sparseness in the solution and indirectly addresses the sensor\nselection problem. The theory is applied to realistic flight mechanics and\nastrodynamics problems to highlight its engineering value. These examples\ndemonstrate the application of the presented theory to a) determine redundant\nsensing architectures for linear time invariant systems, b) accurately estimate\nstates with low-cost sensors, and c) optimally schedule sensors for linear\ntime-varying systems.", "title": "Optimal Sensor Precision for Multi-Rate Sensing for Bounded Estimation  Error"}, {"link": "https://arxiv.org/abs/2106.06907", "abstract": "Deceptive attacks exploiting the innate and the acquired vulnerabilities of\nhuman users have posed severe threats to information and infrastructure\nsecurity. This work proposes INADVERT, a systematic solution that generates\ninteractive visual aids in real-time to prevent users from inadvertence and\ncounter visual-deception attacks. Based on the eye-tracking outcomes and proper\ndata compression, the INADVERT platform automatically adapts the visual aids to\nthe user's varying attention status captured by the gaze location and duration.\nWe extract system-level metrics to evaluate the user's average attention level\nand characterize the magnitude and frequency of the user's mind-wandering\nbehaviors. These metrics contribute to an adaptive enhancement of the user's\nattention through reinforcement learning. To determine the optimal\nhyper-parameters in the attention enhancement mechanism, we develop an\nalgorithm based on Bayesian optimization to efficiently update the design of\nthe INADVERT platform and maximize the accuracy of the users' phishing\nrecognition.", "title": "INADVERT: An Interactive and Adaptive Counterdeception Platform for  Attention Enhancement and Phishing Prevention"}, {"link": "https://arxiv.org/abs/2106.06908", "abstract": "Medical imaging datasets usually exhibit domain shift due to the variations\nof scanner vendors, imaging protocols, etc. This raises the concern about the\ngeneralization capacity of machine learning models. Domain generalization (DG),\nwhich aims to learn a model from multiple source domains such that it can be\ndirectly generalized to unseen test domains, seems particularly promising to\nmedical imaging community. To address DG, recent model-agnostic meta-learning\n(MAML) has been introduced, which transfers the knowledge from previous\ntraining tasks to facilitate the learning of novel testing tasks. However, in\nclinical practice, there are usually only a few annotated source domains\navailable, which decreases the capacity of training task generation and thus\nincreases the risk of overfitting to training tasks in the paradigm. In this\npaper, we propose a novel DG scheme of episodic training with task augmentation\non medical imaging classification. Based on meta-learning, we develop the\nparadigm of episodic training to construct the knowledge transfer from episodic\ntraining-task simulation to the real testing task of DG. Motivated by the\nlimited number of source domains in real-world medical deployment, we consider\nthe unique task-level overfitting and we propose task augmentation to enhance\nthe variety during training task generation to alleviate it. With the\nestablished learning framework, we further exploit a novel meta-objective to\nregularize the deep embedding of training domains. To validate the\neffectiveness of the proposed method, we perform experiments on\nhistopathological images and abdominal CT images.", "title": "Domain Generalization on Medical Imaging Classification using Episodic  Training with Task Augmentation"}, {"link": "https://arxiv.org/abs/2106.06909", "abstract": "This paper introduces GigaSpeech, an evolving, multi-domain English speech\nrecognition corpus with 10,000 hours of high quality labeled audio suitable for\nsupervised training, and 40,000 hours of total audio suitable for\nsemi-supervised and unsupervised training. Around 40,000 hours of transcribed\naudio is first collected from audiobooks, podcasts and YouTube, covering both\nread and spontaneous speaking styles, and a variety of topics, such as arts,\nscience, sports, etc. A new forced alignment and segmentation pipeline is\nproposed to create sentence segments suitable for speech recognition training,\nand to filter out segments with low-quality transcription. For system training,\nGigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h,\nand 10000h. For our 10,000-hour XL training subset, we cap the word error rate\nat 4% during the filtering/validation stage, and for all our other smaller\ntraining subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the\nother hand, are re-processed by professional human transcribers to ensure high\ntranscription quality. Baseline systems are provided for popular speech\nrecognition toolkits, namely Athena, ESPnet, Kaldi and Pika.", "title": "GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of  Transcribed Audio"}, {"link": "https://arxiv.org/abs/2106.06910", "abstract": "As the Covid-19 outbreaks rapidly all over the world day by day and also\naffects the lives of million, a number of countries declared complete lock-down\nto check its intensity. During this lockdown period, social media plat-forms\nhave played an important role to spread information about this pandemic across\nthe world, as people used to express their feelings through the social\nnetworks. Considering this catastrophic situation, we developed an experimental\napproach to analyze the reactions of people on Twitter taking into ac-count the\npopular words either directly or indirectly based on this pandemic. This paper\nrepresents the sentiment analysis on collected large number of tweets on\nCoronavirus or Covid-19. At first, we analyze the trend of public sentiment on\nthe topics related to Covid-19 epidemic using an evolutionary classification\nfollowed by the n-gram analysis. Then we calculated the sentiment ratings on\ncollected tweet based on their class. Finally, we trained the long-short term\nnetwork using two types of rated tweets to predict sentiment on Covid-19 data\nand obtained an overall accuracy of 84.46%.", "title": "Sentiment Analysis of Covid-19 Tweets using Evolutionary  Classification-Based LSTM Model"}, {"link": "https://arxiv.org/abs/2106.06911", "abstract": "The field of Explainable Artificial Intelligence (XAI) aims to build\nexplainable and interpretable machine learning (or deep learning) methods\nwithout sacrificing prediction performance. Convolutional Neural Networks\n(CNNs) have been successful in making predictions, especially in image\nclassification. However, these famous deep learning models use tens of millions\nof parameters based on a large number of pre-trained filters which have been\nrepurposed from previous data sets. We propose a novel Interaction-based\nConvolutional Neural Network (ICNN) that does not make assumptions about the\nrelevance of local information. Instead, we use a model-free Influence Score\n(I-score) to directly extract the influential information from images to form\nimportant variable modules. We demonstrate that the proposed method produces\nstate-of-the-art prediction performance of 99.8% on a real-world data set\nclassifying COVID-19 Chest X-ray images without sacrificing the explanatory\npower of the model. This proposed design can efficiently screen COVID-19\npatients before human diagnosis, and will be the benchmark for addressing\nfuture XAI problems in large-scale data sets.", "title": "An Interaction-based Convolutional Neural Network (ICNN) Towards Better  Understanding of COVID-19 X-ray Images"}, {"link": "https://arxiv.org/abs/2106.06916", "abstract": "As Artificial Intelligence as a Service gains popularity, protecting\nwell-trained models as intellectual property is becoming increasingly\nimportant. Generally speaking, there are two common protection methods:\nownership verification and usage authorization. In this paper, we propose\nNon-Transferable Learning (NTL), a novel approach that captures the exclusive\ndata representation in the learned model and restricts the model generalization\nability to certain domains. This approach provides effective solutions to both\nmodel verification and authorization. For ownership verification, watermarking\ntechniques are commonly used but are often vulnerable to sophisticated\nwatermark removal methods. Our NTL-based model verification approach instead\nprovides robust resistance to state-of-the-art watermark removal methods, as\nshown in extensive experiments for four of such methods over the digits,\nCIFAR10 & STL10, and VisDA datasets. For usage authorization, prior solutions\nfocus on authorizing specific users to use the model, but authorized users can\nstill apply the model to any data without restriction. Our NTL-based\nauthorization approach instead provides data-centric usage protection by\nsignificantly degrading the performance of usage on unauthorized data. Its\neffectiveness is also shown through experiments on a variety of datasets.", "title": "Non-Transferable Learning: A New Approach for Model Verification and  Authorization"}, {"link": "https://arxiv.org/abs/2106.06917", "abstract": "In this paper, we explore the effect of architecture completeness on\nadversarial robustness. We train models with different architectures on\nCIFAR-10 and MNIST dataset. For each model, we vary different number of layers\nand different number of nodes in the layer. For every architecture candidate,\nwe use Fast Gradient Sign Method (FGSM) to generate untargeted adversarial\nattacks and use adversarial training to defend against those attacks. For each\narchitecture candidate, we report pre-attack, post-attack and post-defense\naccuracy for the model as well as the architecture parameters and the impact of\ncompleteness to the model accuracies.", "title": "ATRAS: Adversarially Trained Robust Architecture Search"}, {"link": "https://arxiv.org/abs/2106.06920", "abstract": "A multi-modal framework to generated user intention distributions when\noperating a mobile vehicle is proposed in this work. The model learns from past\nobserved trajectories and leverages traversability information derived from the\nvisual surroundings to produce a set of future trajectories, suitable to be\ndirectly embedded into a perception-action shared control strategy on a mobile\nagent, or as a safety layer to supervise the prudent operation of the vehicle.\nWe base our solution on a conditional Generative Adversarial Network with\nLong-Short Term Memory cells to capture trajectory distributions conditioned on\npast trajectories, further fused with traversability probabilities derived from\nvisual segmentation with a Convolutional Neural Network. The proposed\ndata-driven framework results in a significant reduction in error of the\npredicted trajectories (versus the ground truth) from comparable strategies in\nthe literature (e.g. Social-GAN) that fail to account for information other\nthan the agent's past history. Experiments were conducted on a dataset\ncollected with a custom wheelchair model built onto the open-source urban\ndriving simulator CARLA, proving also that the proposed framework can be used\nwith a small, un-annotated dataset.", "title": "Multi-modal Scene-compliant User Intention Estimation for Navigation"}, {"link": "https://arxiv.org/abs/2106.06921", "abstract": "Federated Learning~(FL) has emerged as a new paradigm of training machine\nlearning models without sacrificing data security and privacy. Learning models\nat edge devices such as cell phones is one of the most common use case of FL.\nHowever, the limited computing power and energy constraints of edge devices\nhinder the adoption of FL for both model training and deployment, especially\nfor the resource-hungry Deep Neural Networks~(DNNs). To this end, many model\ncompression methods have been proposed and network pruning is among the most\nwell-known. However, a pruning policy for a given model is highly\ndataset-dependent, which is not suitable for non-Independent and Identically\nDistributed~(Non-IID) FL edge devices. In this paper, we present an adaptive\npruning scheme for edge devices in an FL system, which applies dataset-aware\ndynamic pruning for inference acceleration on Non-IID datasets. Our evaluation\nshows that the proposed method accelerates inference by $2\\times$~($50\\%$ FLOPs\nreduction) while maintaining the model's quality on edge devices.", "title": "Adaptive Dynamic Pruning for Non-IID Federated Learning"}, {"link": "https://arxiv.org/abs/2106.06922", "abstract": "An important research direction in automatic speech recognition (ASR) has\ncentered around the development of effective methods to rerank the output\nhypotheses of an ASR system with more sophisticated language models (LMs) for\nfurther gains. A current mainstream school of thoughts for ASR N-best\nhypothesis reranking is to employ a recurrent neural network (RNN)-based LM or\nits variants, with performance superiority over the conventional n-gram LMs\nacross a range of ASR tasks. In real scenarios such as a long conversation, a\nsequence of consecutive sentences may jointly contain ample cues of\nconversation-level information such as topical coherence, lexical entrainment\nand adjacency pairs, which however remains to be underexplored. In view of\nthis, we first formulate ASR N-best reranking as a prediction problem, putting\nforward an effective cross-sentence neural LM approach that reranks the ASR\nN-best hypotheses of an upcoming sentence by taking into consideration the word\nusage in its precedent sentences. Furthermore, we also explore to extract\ntask-specific global topical information of the cross-sentence history in an\nunsupervised manner for better ASR performance. Extensive experiments conducted\non the AMI conversational benchmark corpus indicate the effectiveness and\nfeasibility of our methods in comparison to several state-of-the-art reranking\nmethods.", "title": "Cross-sentence Neural Language Models for Conversational Speech  Recognition"}, {"link": "https://arxiv.org/abs/2106.06924", "abstract": "Deep-learning\\textendash{centric} reversible steganography has emerged as a\npromising research paradigm. A direct way of applying deep learning to\nreversible steganography is to construct a pair of encoder and decoder, whose\nparameters are trained jointly, thereby learning the steganographic system as a\nwhole. This end-to-end framework, however, falls short of the reversibility\nrequirement because it is difficult for this kind of monolithic system, as a\nblack box, to create or duplicate intricate reversible mechanisms. In response\nto this issue, a recent approach is to carve up the steganographic system and\nwork on modules independently. In particular, neural networks are deployed in\nan analytics module to learn the data distribution, while an established\nmechanism is called upon to handle the remaining tasks. In this paper, we\ninvestigate the modular framework and deploy deep neural networks in a\nreversible steganographic scheme referred to as prediction-error modulation, in\nwhich an analytics module serves the purpose of pixel intensity prediction. The\nprimary focus of this study is on deep-learning\\textendash{based} context-aware\npixel intensity prediction. We address the unsolved issues reported in related\nliterature, including the impact of pixel initialisation on prediction accuracy\nand the influence of uncertainty propagation in dual-layer embedding.\nFurthermore, we establish a connection between context-aware pixel intensity\nprediction and low-level computer vision and analyse the performance of several\nadvanced neural networks.", "title": "Deep Learning for Reversible Steganography: Principles and Insights"}, {"link": "https://arxiv.org/abs/2106.06925", "abstract": "We study fairness in house allocation, where $m$ houses are to be allocated\namong $n$ agents so that every agent receives one house. We show that\nmaximizing the number of envy-free agents is hard to approximate to within a\nfactor of $n^{1-\\gamma}$ for any constant $\\gamma>0$, and that the exact\nversion is NP-hard even for binary utilities. Moreover, we prove that deciding\nwhether a proportional allocation exists is computationally hard, whereas the\ncorresponding problem for equitability can be solved efficiently.", "title": "On the Complexity of Fair House Allocation"}, {"link": "https://arxiv.org/abs/2106.06926", "abstract": "The use of pessimism, when reasoning about datasets lacking exhaustive\nexploration has recently gained prominence in offline reinforcement learning.\nDespite the robustness it adds to the algorithm, overly pessimistic reasoning\ncan be equally damaging in precluding the discovery of good policies, which is\nan issue for the popular bonus-based pessimism. In this paper, we introduce the\nnotion of Bellman-consistent pessimism for general function approximation:\ninstead of calculating a point-wise lower bound for the value function, we\nimplement pessimism at the initial state over the set of functions consistent\nwith the Bellman equations. Our theoretical guarantees only require Bellman\nclosedness as standard in the exploratory setting, in which case bonus-based\npessimism fails to provide guarantees. Even in the special case of linear MDPs\nwhere stronger function-approximation assumptions hold, our result improves\nupon a recent bonus-based approach by $\\mathcal{O}(d)$ in its sample complexity\nwhen the action space is finite. Remarkably, our algorithms automatically adapt\nto the best bias-variance tradeoff in the hindsight, whereas most prior\napproaches require tuning extra hyperparameters a priori.", "title": "Bellman-consistent Pessimism for Offline Reinforcement Learning"}, {"link": "https://arxiv.org/abs/2106.06927", "abstract": "Recent research in adversarially robust classifiers suggests their\nrepresentations tend to be aligned with human perception, which makes them\nattractive for image synthesis and restoration applications. Despite favorable\nempirical results on a few downstream tasks, their advantages are limited to\nslow and sensitive optimization-based techniques. Moreover, their use on\ngenerative models remains unexplored. This work proposes the use of robust\nrepresentations as a perceptual primitive for feature inversion models, and\nshow its benefits with respect to standard non-robust image features. We\nempirically show that adopting robust representations as an image prior\nsignificantly improves the reconstruction accuracy of CNN-based feature\ninversion models. Furthermore, it allows reconstructing images at multiple\nscales out-of-the-box. Following these findings, we propose an\nencoding-decoding network based on robust representations and show its\nadvantages for applications such as anomaly detection, style transfer and image\ndenoising.", "title": "Inverting Adversarially Robust Networks for Image Synthesis"}, {"link": "https://arxiv.org/abs/2106.06931", "abstract": "Formally verifying Deep Reinforcement Learning (DRL) systems is a challenging\ntask due to the dynamic continuity of system behaviors and the black-box\nfeature of embedded neural networks. In this paper, we propose a novel\nabstraction-based approach to train DRL systems on finite abstract domains\ninstead of concrete system states. It yields neural networks whose input states\nare finite, making hosting DRL systems directly verifiable using model checking\ntechniques. Our approach is orthogonal to existing DRL algorithms and\noff-the-shelf model checkers. We implement a resulting prototype training and\nverification framework and conduct extensive experiments on the\nstate-of-the-art benchmark. The results show that the systems trained in our\napproach can be verified more efficiently while they retain comparable\nperformance against those that are trained without abstraction.", "title": "Learning on Abstract Domains: A New Approach for Verifiable Guarantee in  Reinforcement Learning"}, {"link": "https://arxiv.org/abs/2106.06932", "abstract": "Actor-critic (AC) methods are ubiquitous in reinforcement learning. Although\nit is understood that AC methods are closely related to policy gradient (PG),\ntheir precise connection has not been fully characterized previously. In this\npaper, we explain the gap between AC and PG methods by identifying the exact\nadjustment to the AC objective/gradient that recovers the true policy gradient\nof the cumulative reward objective (PG). Furthermore, by viewing the AC method\nas a two-player Stackelberg game between the actor and critic, we show that the\nStackelberg policy gradient can be recovered as a special case of our more\ngeneral analysis. Based on these results, we develop practical algorithms,\nResidual Actor-Critic and Stackelberg Actor-Critic, for estimating the\ncorrection between AC and PG and use these to modify the standard AC algorithm.\nExperiments on popular tabular and continuous environments show the proposed\ncorrections can improve both the sample efficiency and final performance of\nexisting AC methods.", "title": "Characterizing the Gap Between Actor-Critic and Policy Gradient"}, {"link": "https://arxiv.org/abs/2106.06933", "abstract": "Network Traffic Classification (NTC) has become an important component in a\nwide variety of network management operations, e.g., Quality of Service (QoS)\nprovisioning and security purposes. Machine Learning (ML) algorithms as a\ncommon approach for NTC methods can achieve reasonable accuracy and handle\nencrypted traffic. However, ML-based NTC techniques suffer from the shortage of\nlabeled traffic data which is the case in many real-world applications. This\nstudy investigates the applicability of an active form of ML, called Active\nLearning (AL), which reduces the need for a high number of labeled examples by\nactively choosing the instances that should be labeled. The study first\nprovides an overview of NTC and its fundamental challenges along with surveying\nthe literature in the field of using ML techniques in NTC. Then, it introduces\nthe concepts of AL, discusses it in the context of NTC, and review the\nliterature in this field. Further, challenges and open issues in the use of AL\nfor NTC are discussed. Additionally, as a technical survey, some experiments\nare conducted to show the broad applicability of AL in NTC. The simulation\nresults show that AL can achieve high accuracy with a small amount of data.", "title": "Active Learning for Network Traffic Classification: A Technical Survey"}, {"link": "https://arxiv.org/abs/2106.06934", "abstract": "With the development of federated learning (FL), mobile devices (MDs) are\nable to train their local models with private data and sends them to a central\nserver for aggregation, thereby preventing sensitive raw data leakage. In this\npaper, we aim to improve the training performance of FL systems in the context\nof wireless channels and stochastic energy arrivals of MDs. To this purpose, we\ndynamically optimize MDs' transmission power and training task scheduling. We\nfirst model this dynamic programming problem as a constrained Markov decision\nprocess (CMDP). Due to high dimensions rooted from our CMDP problem, we propose\nonline stochastic learning methods to simplify the CMDP and design online\nalgorithms to obtain an efficient policy for all MDs. Since there are long-term\nconstraints in our CMDP, we utilize Lagrange multipliers approach to tackle\nthis issue. Furthermore, we prove the convergence of the proposed online\nstochastic learning algorithm. Numerical results indicate that the proposed\nalgorithms can achieve better performance than the benchmark algorithms.", "title": "Federated Learning Over Wireless Channels: Dynamic Resource Allocation  and Task Scheduling"}, {"link": "https://arxiv.org/abs/2106.06935", "abstract": "Link prediction is a very fundamental task on graphs. Inspired by traditional\npath-based methods, in this paper we propose a general and flexible\nrepresentation learning framework based on paths for link prediction.\nSpecifically, we define the representation of a pair of nodes as the\ngeneralized sum of all path representations, with each path representation as\nthe generalized product of the edge representations in the path. Motivated by\nthe Bellman-Ford algorithm for solving the shortest path problem, we show that\nthe proposed path formulation can be efficiently solved by the generalized\nBellman-Ford algorithm. To further improve the capacity of the path\nformulation, we propose the Neural Bellman-Ford Network (NBFNet), a general\ngraph neural network framework that solves the path formulation with learned\noperators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes\nthe generalized Bellman-Ford algorithm with 3 neural components, namely\nINDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary\ncondition, multiplication operator, and summation operator respectively. The\nNBFNet is very general, covers many traditional path-based methods, and can be\napplied to both homogeneous graphs and multi-relational graphs (e.g., knowledge\ngraphs) in both transductive and inductive settings. Experiments on both\nhomogeneous graphs and knowledge graphs show that the proposed NBFNet\noutperforms existing methods by a large margin in both transductive and\ninductive settings, achieving new state-of-the-art results.", "title": "Neural Bellman-Ford Networks: A General Graph Neural Network Framework  for Link Prediction"}, {"link": "https://arxiv.org/abs/2106.06937", "abstract": "Commonsense reasoning research has so far been limited to English. We aim to\nevaluate and improve popular multilingual language models (ML-LMs) to help\nadvance commonsense reasoning (CSR) beyond English. We collect the Mickey\nCorpus, consisting of 561k sentences in 11 different languages, which can be\nused for analyzing and improving ML-LMs. We propose Mickey Probe, a\nlanguage-agnostic probing task for fairly evaluating the common sense of\npopular ML-LMs across different languages. In addition, we also create two new\ndatasets, X-CSQA and X-CODAH, by translating their English versions to 15 other\nlanguages, so that we can evaluate popular ML-LMs for cross-lingual commonsense\nreasoning. To improve the performance beyond English, we propose a simple yet\neffective method -- multilingual contrastive pre-training (MCP). It\nsignificantly enhances sentence representations, yielding a large performance\ngain on both benchmarks.", "title": "Common Sense Beyond English: Evaluating and Improving Multilingual  Language Models for Commonsense Reasoning"}, {"link": "https://arxiv.org/abs/2106.06939", "abstract": "Cross-modal correlation provides an inherent supervision for video\nunsupervised representation learning. Existing methods focus on distinguishing\ndifferent video clips by visual and audio representations. We human visual\nperception could attend to regions where sounds are made, and our auditory\nperception could also ground their frequencies of sounding objects, which we\ncall bidirectional local correspondence. Such supervision is intuitive but not\nwell explored in the contrastive learning framework. This paper introduces a\npretext task, Cross-Modal Attention Consistency (CMAC), for exploring the\nbidirectional local correspondence property. The CMAC approach aims to align\nthe regional attention generated purely from the visual signal with the target\nattention generated under the guidance of acoustic signal, and do a similar\nalignment for frequency grounding on the acoustic attention. Accompanied by a\nremoulded cross-modal contrastive loss where we consider additional\nwithin-modal interactions, the CMAC approach works effectively for enforcing\nthe bidirectional alignment. Extensive experiments on six downstream benchmarks\ndemonstrate that CMAC can improve the state-of-the-art performance on both\nvisual and audio modalities.", "title": "Cross-Modal Attention Consistency for Video-Audio Unsupervised Learning"}, {"link": "https://arxiv.org/abs/2106.06942", "abstract": "This technical report analyzes an egocentric video action detection method we\nused in the 2021 EPIC-KITCHENS-100 competition hosted in CVPR2021 Workshop. The\ngoal of our task is to locate the start time and the end time of the action in\nthe long untrimmed video, and predict action category. We adopt sliding window\nstrategy to generate proposals, which can better adapt to short-duration\nactions. In addition, we show that classification and proposals are conflict in\nthe same network. The separation of the two tasks boost the detection\nperformance with high efficiency. By simply employing these strategy, we\nachieved 16.10\\% performance on the test set of EPIC-KITCHENS-100 Action\nDetection challenge using a single model, surpassing the baseline method by\n11.7\\% in terms of average mAP.", "title": "A Stronger Baseline for Ego-Centric Action Detection"}, {"link": "https://arxiv.org/abs/2106.06944", "abstract": "Subtext is a kind of deep semantics which can be acquired after one or more\nrounds of expression transformation. As a popular way of expressing one's\nintentions, it is well worth studying. In this paper, we try to make computers\nunderstand whether there is a subtext by means of machine learning. We build a\nChinese dataset whose source data comes from the popular social media (e.g.\nWeibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a\nbaseline model called SASICM to deal with subtext recognition. The F1 score of\nSASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97%\nhigher than that of BERT based model, 12.7% higher than that of traditional\nmethods on average, including support vector machine, logistic regression\nclassifier, maximum entropy classifier, naive bayes classifier and decision\ntree and 2.39% higher than that of the state-of-the-art, including MARIN and\nBTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%,\nwhich is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and\nSASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of\nother methods which are mentioned before.", "title": "SASICM A Multi-Task Benchmark For Subtext Recognition"}, {"link": "https://arxiv.org/abs/2106.06945", "abstract": "In the Internet of Things (IoT) networks, caching is a promising technique to\nalleviate energy consumption of sensors by responding to users' data requests\nwith the data packets cached in the edge caching node (ECN). However, without\nan efficient status update strategy, the information obtained by users may be\nstale, which in return would inevitably deteriorate the accuracy and\nreliability of derived decisions for real-time applications. In this paper, we\nfocus on striking the balance between the information freshness, in terms of\nage of information (AoI), experienced by users and energy consumed by sensors,\nby appropriately activating sensors to update their current status.\nParticularly, we first depict the evolutions of the AoI with each sensor from\ndifferent users' perspective with time steps of non-uniform duration, which are\ndetermined by both the users' data requests and the ECN's status update\ndecision. Then, we formulate a non-uniform time step based dynamic status\nupdate optimization problem to minimize the long-term average cost, jointly\nconsidering the average AoI and energy consumption. To this end, a Markov\nDecision Process is formulated and further, a dueling deep R-network based\ndynamic status update algorithm is devised by combining dueling deep Q-network\nand tabular R-learning, with which challenges from the curse of dimensionality\nand unknown of the environmental dynamics can be addressed. Finally, extensive\nsimulations are conducted to validate the effectiveness of our proposed\nalgorithm by comparing it with five baseline deep reinforcement learning\nalgorithms and policies.", "title": "Optimal Status Update for Caching Enabled IoT Networks: A Dueling Deep  R-Network Approach"}, {"link": "https://arxiv.org/abs/2106.06946", "abstract": "Randomized Smoothing (RS) is a promising method for obtaining robustness\ncertificates by evaluating a base model under noise. In this work we: (i)\ntheoretically motivate why ensembles are a particularly suitable choice as base\nmodels for RS, and (ii) empirically confirm this choice, obtaining state of the\nart results in multiple settings. The key insight of our work is that the\nreduced variance of ensembles over the perturbations introduced in RS leads to\nsignificantly more consistent classifications for a given input, in turn\nleading to substantially increased certifiable radii for difficult samples. We\nalso introduce key optimizations which enable an up to 50-fold decrease in\nsample complexity of RS, thus drastically reducing its computational overhead.\nExperimentally, we show that ensembles of only 3 to 10 classifiers consistently\nimprove on the strongest single model with respect to their average certified\nradius (ACR) by 5% to 21% on both CIFAR-10 and ImageNet. On the latter, we\nachieve a state-of-the-art ACR of 1.11. We release all code and models required\nto reproduce our results upon publication.", "title": "Boosting Randomized Smoothing with Variance Reduced Classifiers"}, {"link": "https://arxiv.org/abs/2106.06947", "abstract": "Given high-dimensional time series data (e.g., sensor data), how can we\ndetect anomalous events, such as system faults and attacks? More challengingly,\nhow can we do this in a way that captures complex inter-sensor relationships,\nand detects and explains anomalies which deviate from these relationships?\nRecently, deep learning approaches have enabled improvements in anomaly\ndetection in high-dimensional datasets; however, existing methods do not\nexplicitly learn the structure of existing relationships between variables, or\nuse them to predict the expected behavior of time series. Our approach combines\na structure learning approach with graph neural networks, additionally using\nattention weights to provide explainability for the detected anomalies.\nExperiments on two real-world sensor datasets with ground truth anomalies show\nthat our method detects anomalies more accurately than baseline approaches,\naccurately captures correlations between sensors, and allows users to deduce\nthe root cause of a detected anomaly.", "title": "Graph Neural Network-Based Anomaly Detection in Multivariate Time Series"}, {"link": "https://arxiv.org/abs/2106.06949", "abstract": "The sixth generation (6G), unlike any previous generations, is envisioned by\n2030 to connect everything. Moreover, in addition to the new use cases 6G is\nexpected to support, it will need to provide a superior performance over 5G.\nThe global connectivity, large-network dimensions, users heterogeneity,\nextremely-low power consumption, high-throughput, ultra-reliability,\nefficient-network operation and maintenance, and low-latency requirements to be\nmet by future networks inevitably necessitate the autonomy of 6G. Intelligence,\nfacilitated mainly by the advancement and innovation of the artificial\nintelligence (AI) technique, is a key to achieve autonomy. In this paper we\nprovide a bird's-eye view of future networks, vision, progress, and objectives.\nWe review some of the 6G technologies that would be mainly enabling the\nglobally-intelligent connected world. We, in addition to discussing the role of\nAI in future networks, unlike any other review papers provide our original\nresults that emphasize the necessity of deploying AI for 6G networks. We also\nvery importantly identify 6G implementation challenges and key innovative\ntechniques like quantum and blockchain to solve these challenges. This article\nserves as a starting point for learner to acquire more knowledge about 6G as it\ncombines some of the main contributions in the area and provide some references\nfor getting a more deeper knowledge, and also for researchers to contribute to\nthe field.", "title": "How Crucial is it for 6G Networks to be Autonomous?"}, {"link": "https://arxiv.org/abs/2106.06950", "abstract": "This paper describes the most efficient way to manage operations on groups of\nconsecutive elements, or \"blocks\" of elements, within an ordered set. This will\nbe done by introducing a new data structure called Wise Red-Black Trees, a\nvariation of classical Red-Black Trees. The goal is to improve time complexity,\nwhile also keeping spatial complexity to a minimum. The optimization will be\nvisible both at the asymptote and in terms of multiplicative constants,\naffecting not only the worst-case, but also the average one.", "title": "An efficient way to manage blocks of data with Wise Red-Black Trees"}, {"link": "https://arxiv.org/abs/2106.06951", "abstract": "Free-space optical (FSO) channel offers line-of-sight wireless communication\nwith high data rates and high secrecy utilizing unlicensed optical spectrum and\nalso paves the way to the solution of the last-mile access problem. Since\natmospheric turbulence is a hindrance to an enhanced secrecy performance, the\nmixed radio frequency (RF)-FSO system is gaining enormous research interest in\nrecent days. But conventional FSO models except for the double generalized\nGamma (DGG) model can not demonstrate secrecy performance for all ranges of\nturbulence severity. This reason has led us to propose a dual-hop eta-mu and\nunified DGG mixed RF-FSO network while considering eavesdropping at both RF and\nFSO hops. The security of these proposed scenarios is investigated in terms of\ntwo metrics, i.e., strictly positive secrecy capacity and secure outage\nprobability. Exploiting these expressions, we further investigate how the\nsecrecy performance is affected by various system parameters, i.e., fading,\nturbulence, and pointing errors. A demonstration is made between heterodyne\ndetection (HD) and intensity modulation and direct detection (IM/DD) techniques\nwhile exhibiting superior secrecy performance for HD technique over IM/DD\ntechnique. Finally, all analytical results are corroborated via Monte-Carlo\nsimulations.", "title": "Effects of Eavesdropper on the Performance of Mixed \u03b7-\u03bc and DGG  Cooperative Relaying System"}, {"link": "https://arxiv.org/abs/2106.06955", "abstract": "The lottery ticket hypothesis states that sparse subnetworks exist in\nrandomly initialized dense networks that can be trained to the same accuracy as\nthe dense network they reside in. However, the subsequent work has failed to\nreplicate this on large-scale models and required rewinding to an early stable\nstate instead of initialization. We show that by using a training method that\nis stable with respect to linear mode connectivity, large networks can also be\nentirely rewound to initialization. Our subsequent experiments on common vision\ntasks give strong credence to the hypothesis in Evci et al. (2020b) that\nlottery tickets simply retrain to the same regions (although not necessarily to\nthe same basin). These results imply that existing lottery tickets could not\nhave been found without the preceding dense training by iterative magnitude\npruning, raising doubts about the use of the lottery ticket hypothesis.", "title": "Towards Understanding Iterative Magnitude Pruning: Why Lottery Tickets  Win"}, {"link": "https://arxiv.org/abs/2106.06959", "abstract": "In this paper, we propose a method to find local-geometry-aware traversal\ndirections on the intermediate latent space of Generative Adversarial Networks\n(GANs). These directions are defined as an ordered basis of tangent space at a\nlatent code. Motivated by the intrinsic sparsity of the latent space, the basis\nis discovered by solving the low-rank approximation problem of the differential\nof the partial network. Moreover, the local traversal basis leads to a natural\niterative traversal on the latent space. Iterative Curve-Traversal shows stable\ntraversal on images, since the trajectory of latent code stays close to the\nlatent space even under the strong perturbations compared to the linear\ntraversal. This stability provides far more diverse variations of the given\nimage. Although the proposed method can be applied to various GAN models, we\nfocus on the W-space of the StyleGAN2, which is renowned for showing the better\ndisentanglement of the latent factors of variation. Our quantitative and\nqualitative analysis provides evidence showing that the W-space is still\nglobally warped while showing a certain degree of global consistency of\ninterpretable variation. In particular, we introduce some metrics on the\nGrassmannian manifolds to quantify the global warpage of the W-space and the\nsubspace traversal to test the stability of traversal directions.", "title": "Do Not Escape From the Manifold: Discovering the Local Coordinates on  the Latent Space of GANs"}, {"link": "https://arxiv.org/abs/2106.06960", "abstract": "Attention-based encoder-decoder framework is widely used in the scene text\nrecognition task. However, for the current state-of-the-art(SOTA) methods,\nthere is room for improvement in terms of the efficient usage of local visual\nand global context information of the input text image, as well as the robust\ncorrelation between the scene processing module(encoder) and the text\nprocessing module(decoder). In this paper, we propose a Representation and\nCorrelation Enhanced Encoder-Decoder Framework(RCEED) to address these\ndeficiencies and break performance bottleneck. In the encoder module, local\nvisual feature, global context feature, and position information are aligned\nand fused to generate a small-size comprehensive feature map. In the decoder\nmodule, two methods are utilized to enhance the correlation between scene and\ntext feature space. 1) The decoder initialization is guided by the holistic\nfeature and global glimpse vector exported from the encoder. 2) The feature\nenriched glimpse vector produced by the Multi-Head General Attention is used to\nassist the RNN iteration and the character prediction at each time step.\nMeanwhile, we also design a Layernorm-Dropout LSTM cell to improve model's\ngeneralization towards changeable texts. Extensive experiments on the\nbenchmarks demonstrate the advantageous performance of RCEED in scene text\nrecognition tasks, especially the irregular ones.", "title": "Representation and Correlation Enhanced Encoder-Decoder Framework for  Scene Text Recognition"}, {"link": "https://arxiv.org/abs/2106.06963", "abstract": "Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.", "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology  Report Generation"}, {"link": "https://arxiv.org/abs/2106.06964", "abstract": "Pre-trained word representations became a key component in many NLP tasks.\nHowever, the global geometry of the word embeddings remains poorly understood.\nIn this paper, we demonstrate that a typical word embeddings cloud is shaped as\na high-dimensional simplex with interpretable vertices and propose a simple yet\neffective method for enumeration of these vertices. We show that the proposed\nmethod can detect and describe vertices of the simplex for GloVe and fasttext\nspaces.", "title": "Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces"}, {"link": "https://arxiv.org/abs/2106.06965", "abstract": "Recently, chest X-ray report generation, which aims to automatically generate\ndescriptions of given chest X-ray images, has received growing research\ninterests. The key challenge of chest X-ray report generation is to accurately\ncapture and describe the abnormal regions. In most cases, the normal regions\ndominate the entire chest X-ray image, and the corresponding descriptions of\nthese normal regions dominate the final report. Due to such data bias,\nlearning-based models may fail to attend to abnormal regions. In this work, to\neffectively capture and describe abnormal regions, we propose the Contrastive\nAttention (CA) model. Instead of solely focusing on the current input image,\nthe CA model compares the current input image with normal images to distill the\ncontrastive information. The acquired contrastive information can better\nrepresent the visual features of abnormal regions. According to the experiments\non the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into\nseveral existing models can boost their performance across most metrics. In\naddition, according to the analysis, the CA model can help existing models\nbetter attend to the abnormal regions and provide more accurate descriptions\nwhich are crucial for an interpretable diagnosis. Specifically, we achieve the\nstate-of-the-art results on the two public datasets.", "title": "Contrastive Attention for Automatic Chest X-ray Report Generation"}, {"link": "https://arxiv.org/abs/2106.06966", "abstract": "Recently, convolutional neural network (CNN) based image super-resolution\n(SR) methods have achieved significant performance improvement. However, most\nCNN-based methods mainly focus on feed-forward architecture design and neglect\nto explore the feedback mechanism, which usually exists in the human visual\nsystem. In this paper, we propose feedback pyramid attention networks (FPAN) to\nfully exploit the mutual dependencies of features. Specifically, a novel\nfeedback connection structure is developed to enhance low-level feature\nexpression with high-level information. In our method, the output of each layer\nin the first stage is also used as the input of the corresponding layer in the\nnext state to re-update the previous low-level filters. Moreover, we introduce\na pyramid non-local structure to model global contextual information in\ndifferent scales and improve the discriminative representation of the network.\nExtensive experimental results on various datasets demonstrate the superiority\nof our FPAN in comparison with the state-of-the-art SR methods.", "title": "Feedback Pyramid Attention Networks for Single Image Super-Resolution"}, {"link": "https://arxiv.org/abs/2106.06969", "abstract": "We present a new framework SoundDet, which is an end-to-end trainable and\nlight-weight framework, for polyphonic moving sound event detection and\nlocalization. Prior methods typically approach this problem by preprocessing\nraw waveform into time-frequency representations, which is more amenable to\nprocess with well-established image processing pipelines. Prior methods also\ndetect in segment-wise manner, leading to incomplete and partial detections.\nSoundDet takes a novel approach and directly consumes the raw, multichannel\nwaveform and treats the spatio-temporal sound event as a complete\n``sound-object\" to be detected. Specifically, SoundDet consists of a backbone\nneural network and two parallel heads for temporal detection and spatial\nlocalization, respectively. Given the large sampling rate of raw waveform, the\nbackbone network first learns a set of phase-sensitive and frequency-selective\nbank of filters to explicitly retain direction-of-arrival information, whilst\nbeing highly computationally and parametrically efficient than standard 1D/2D\nconvolution. A dense sound event proposal map is then constructed to handle the\nchallenges of predicting events with large varying temporal duration.\nAccompanying the dense proposal map are a temporal overlapness map and a motion\nsmoothness map that measure a proposal's confidence to be an event from\ntemporal detection accuracy and movement consistency perspective. Involving the\ntwo maps guarantees SoundDet to be trained in a spatio-temporally unified\nmanner. Experimental results on the public DCASE dataset show the advantage of\nSoundDet on both segment-based and our newly proposed event-based evaluation\nsystem.", "title": "SoundDet: Polyphonic Sound Event Detection and Localization from Raw  Waveform"}, {"link": "https://arxiv.org/abs/2106.06971", "abstract": "Retinex model has been applied to low-light image enhancement in many\nexisting methods. More appropriate decomposition of a low-light image can help\nachieve better image enhancement. In this paper, we propose a new pixel-level\nnon-local Haar transform based illumination and reflectance decomposition\nmethod (NLHD). The unique low-frequency coefficient of Haar transform on each\nsimilar pixel group is used to reconstruct the illumination component, and the\nrest of all high-frequency coefficients are employed to reconstruct the\nreflectance component. The complete similarity of pixels in a matched similar\npixel group and the simple separable Haar transform help to obtain more\nappropriate image decomposition; thus, the image is hardly sharpened in the\nimage brightness enhancement procedure. The exponential transform and\nlogarithmic transform are respectively implemented on the illumination\ncomponent. Then a minimum fusion strategy on the results of these two\ntransforms is utilized to achieve more natural illumination component\nenhancement. It can alleviate the mosaic artifacts produced in the darker\nregions by the exponential transform with a gamma value less than 1 and reduce\ninformation loss caused by excessive enhancement of the brighter regions due to\nthe logarithmic transform. Finally, the Retinex model is applied to the\nenhanced illumination and reflectance to achieve image enhancement. We also\ndevelop a local noise level estimation based noise suppression method and a\nnon-local saturation reduction based color deviation correction method. These\ntwo methods can respectively attenuate noise or color deviation usually\npresented in the enhanced results of the extremely dark low-light images.\nExperiments on benchmark datasets show that the proposed method can achieve\nbetter low-light image enhancement results on subjective and objective\nevaluations than most existing methods.", "title": "NLHD: A Pixel-Level Non-Local Retinex Model for Low-Light Image  Enhancement"}, {"link": "https://arxiv.org/abs/2106.06972", "abstract": "Consistent alpha generation, i.e., maintaining an edge over the market,\nunderpins the ability of asset traders to reliably generate profits. Technical\nindicators and trading strategies are commonly used tools to determine when to\nbuy/hold/sell assets, yet these are limited by the fact that they operate on\nknown values. Over the past decades, multiple studies have investigated the\npotential of artificial intelligence in stock trading in conventional markets,\nwith some success. In this paper, we present RCURRENCY, an RNN-based trading\nengine to predict data in the highly volatile digital asset market which is\nable to successfully manage an asset portfolio in a live environment. By\ncombining asset value prediction and conventional trading tools, RCURRENCY\ndetermines whether to buy, hold or sell digital currencies at a given point in\ntime. Experimental results show that, given the data of an interval $t$, a\nprediction with an error of less than 0.5\\% of the data at the subsequent\ninterval $t+1$ can be obtained. Evaluation of the system through backtesting\nshows that RCURRENCY can be used to successfully not only maintain a stable\nportfolio of digital assets in a simulated live environment using real\nhistorical trading data but even increase the portfolio value over time.", "title": "RCURRENCY: Live Digital Asset Trading Using a Recurrent Neural  Network-based Forecasting System"}, {"link": "https://arxiv.org/abs/2106.06976", "abstract": "Generative Adversarial Network, as a promising research direction in the AI\ncommunity, recently attracts considerable attention due to its ability to\ngenerating high-quality realistic data. GANs are a competing game between two\nneural networks trained in an adversarial manner to reach a Nash equilibrium.\nDespite the improvement accomplished in GANs in the last years, there remain\nseveral issues to solve. In this way, how to tackle these issues and make\nadvances leads to rising research interests. This paper reviews literature that\nleverages the game theory in GANs and addresses how game models can relieve\nspecific generative models' challenges and improve the GAN's performance. In\nparticular, we firstly review some preliminaries, including the basic GAN model\nand some game theory backgrounds. After that, we present our taxonomy to\nsummarize the state-of-the-art solutions into three significant categories:\nmodified game model, modified architecture, and modified learning method. The\nclassification is based on the modifications made in the basic model by the\nproposed approaches from the game-theoretic perspective. We further classify\neach category into several subcategories. Following the proposed taxonomy, we\nexplore the main objective of each class and review the recent work in each\ngroup. Finally, we discuss the remaining challenges in this field and present\nthe potential future research topics.", "title": "Game of GANs: Game Theoretical Models for Generative Adversarial  Networks"}, {"link": "https://arxiv.org/abs/2106.06978", "abstract": "In this work, based on the hybrid generalized approximate message passing\n(HyGAMP) algorithm, we propose the message-scheduling GAMP (MSGAMP) algorithm\nin order to address the problem of joint active device detection and channel\nestimation in an uplink grant-free massive MIMO system scenario. In MSGAMP, we\napply three different scheduling techniques based on the Residual Belief\nPropagation (RBP) in which messages are generated using the latest available\ninformation. With a much lower computational cost than the state-of-the-art\nalgorithms, MSGAMP-type schemes exhibits good performance in terms of activity\nerror rate and normalized mean squared error, requiring a small number of\niterations for convergence. %", "title": "Study of Joint Activity Detection and Channel Estimation Based on  Message Passing with RBP Scheduling for MTC"}, {"link": "https://arxiv.org/abs/2106.06981", "abstract": "What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.", "title": "Thinking Like Transformers"}, {"link": "https://arxiv.org/abs/2106.06983", "abstract": "The problem of simultaneous column and row subset selection is addressed in\nthis paper. The column space and row space of a matrix are spanned by its left\nand right singular vectors, respectively. However, the singular vectors are not\nwithin actual columns/rows of the matrix. In this paper, an iterative approach\nis proposed to capture the most structural information of columns/rows via\nselecting a subset of actual columns/rows. This algorithm is referred to as\ntwo-way spectrum pursuit (TWSP) which provides us with an accurate solution for\nthe CUR matrix decomposition. TWSP is applicable in a wide range of\napplications since it enjoys a linear complexity w.r.t. number of original\ncolumns/rows. We demonstrated the application of TWSP for joint channel and\nsensor selection in cognitive radio networks, informative users and contents\ndetection, and efficient supervised data reduction.", "title": "Two-way Spectrum Pursuit for CUR Decomposition and Its Application in  Joint Column/Row Subset Selection"}, {"link": "https://arxiv.org/abs/2106.06984", "abstract": "Spiking Neural Network (SNN) has been recognized as one of the next\ngeneration of neural networks. Conventionally, SNN can be converted from a\npre-trained ANN by only replacing the ReLU activation to spike activation while\nkeeping the parameters intact. Perhaps surprisingly, in this work we show that\na proper way to calibrate the parameters during the conversion of ANN to SNN\ncan bring significant improvements. We introduce SNN Calibration, a cheap but\nextraordinarily effective method by leveraging the knowledge within a\npre-trained Artificial Neural Network (ANN). Starting by analyzing the\nconversion error and its propagation through layers theoretically, we propose\nthe calibration algorithm that can correct the error layer-by-layer. The\ncalibration only takes a handful number of training data and several minutes to\nfinish. Moreover, our calibration algorithm can produce SNN with\nstate-of-the-art architecture on the large-scale ImageNet dataset, including\nMobileNet and RegNet. Extensive experiments demonstrate the effectiveness and\nefficiency of our algorithm. For example, our advanced pipeline can increase up\nto 69% top-1 accuracy when converting MobileNet on ImageNet compared to\nbaselines. Codes are released at https://github.com/yhhhli/SNN_Calibration.", "title": "A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural  Networks Calibration"}, {"link": "https://arxiv.org/abs/2106.06988", "abstract": "Metric-based few-shot fine-grained image classification (FSFGIC) aims to\nlearn a transferable feature embedding network by estimating the similarities\nbetween query images and support classes from very few examples. In this work,\nwe propose, for the first time, to introduce the non-linear data projection\nconcept into the design of FSFGIC architecture in order to address the limited\nsample problem in few-shot learning and at the same time to increase the\ndiscriminability of the model for fine-grained image classification.\nSpecifically, we first design a feature re-abstraction embedding network that\nhas the ability to not only obtain the required semantic features for effective\nmetric learning but also re-enhance such features with finer details from input\nimages. Then the descriptors of the query images and the support classes are\nprojected into different non-linear spaces in our proposed similarity metric\nlearning network to learn discriminative projection factors. This design can\neffectively operate in the challenging and restricted condition of a FSFGIC\ntask for making the distance between the samples within the same class smaller\nand the distance between samples from different classes larger and for reducing\nthe coupling relationship between samples from different categories.\nFurthermore, a novel similarity measure based on the proposed non-linear data\nproject is presented for evaluating the relationships of feature information\nbetween a query image and a support set. It is worth to note that our proposed\narchitecture can be easily embedded into any episodic training mechanisms for\nend-to-end training from scratch. Extensive experiments on FSFGIC tasks\ndemonstrate the superiority of the proposed methods over the state-of-the-art\nbenchmarks.", "title": "NDPNet: A novel non-linear data projection network for few-shot  fine-gained image classification"}, {"link": "https://arxiv.org/abs/2106.06989", "abstract": "Order-agnostic autoregressive distribution estimation (OADE), i.e.,\nautoregressive distribution estimation where the features can occur in an\narbitrary order, is a challenging problem in generative machine learning. Prior\nwork on OADE has encoded feature identity (e.g., pixel location) by assigning\neach feature to a distinct fixed position in an input vector. As a result,\narchitectures built for these inputs must strategically mask either the input\nor model weights to learn the various conditional distributions necessary for\ninferring the full joint distribution of the dataset in an order-agnostic way.\nIn this paper, we propose an alternative approach for encoding feature\nidentities, where each feature's identity is included alongside its value in\nthe input. This feature identity encoding strategy allows neural architectures\ndesigned for sequential data to be applied to the OADE task without\nmodification. As a proof of concept, we show that a Transformer trained on this\ninput (which we refer to as \"the DEformer\", i.e., the distribution estimating\nTransformer) can effectively model binarized-MNIST, approaching the average\nnegative log-likelihood of fixed order autoregressive distribution estimating\nalgorithms while still being entirely order-agnostic.", "title": "The DEformer: An Order-Agnostic Distribution Estimating Transformer"}, {"link": "https://arxiv.org/abs/2106.06991", "abstract": "Recent works on Binary Neural Networks (BNNs) have made promising progress in\nnarrowing the accuracy gap of BNNs to their 32-bit counterparts. However, the\naccuracy gains are often based on specialized model designs using additional\n32-bit components. Furthermore, almost all previous BNNs use 32-bit for feature\nmaps and the shortcuts enclosing the corresponding binary convolution blocks,\nwhich helps to effectively maintain the accuracy, but is not friendly to\nhardware accelerators with limited memory, energy, and computing resources.\nThus, we raise the following question: How can accuracy and energy consumption\nbe balanced in a BNN network design? We extensively study this fundamental\nproblem in this work and propose a novel BNN architecture without most commonly\nused 32-bit components: \\textit{BoolNet}. Experimental results on ImageNet\ndemonstrate that BoolNet can achieve 4.6x energy reduction coupled with 1.2\\%\nhigher accuracy than the commonly used BNN architecture Bi-RealNet. Code and\ntrained models are available at: https://github.com/hpi-xnor/BoolNet.", "title": "BoolNet: Minimizing The Energy Consumption of Binary Neural Networks"}, {"link": "https://arxiv.org/abs/2106.06992", "abstract": "Being complex-valued and low in signal-to-noise ratios, magnitude-based\ndiffusion MRI is confounded by the noise-floor that falsely elevates signal\nmagnitude and incurs bias to the commonly used diffusion indices, such as\nfractional anisotropy (FA). To avoid noise-floor, most existing phase\ncorrection methods explore improving filters to estimate the noise-free\nbackground phase. In this work, after diving into the phase correction\nprocedures, we argue that even a perfect filter is insufficient for phase\ncorrection because the correction procedures are incapable of distinguishing\nsign-symbols of noise, resulting in artifacts (\\textit{i.e.}, arbitrary signal\nloss). With this insight, we generalize the definition of noise-floor to a\ncomplex polar coordinate system and propose a calibration procedure that could\nconveniently distinguish noise sign symbols. The calibration procedure is\nconceptually simple and easy to implement without relying on any external\ntechnique while keeping distinctly effective.", "title": "Is Perfect Filtering Enough Leading to Perfect Phase Correction for dMRI  data?"}, {"link": "https://arxiv.org/abs/2106.06996", "abstract": "Recently, deep convolutional neural network methods have achieved an\nexcellent performance in image superresolution (SR), but they can not be easily\napplied to embedded devices due to large memory cost. To solve this problem, we\npropose a pyramidal dense attention network (PDAN) for lightweight image\nsuper-resolution in this paper. In our method, the proposed pyramidal dense\nlearning can gradually increase the width of the densely connected layer inside\na pyramidal dense block to extract deep features efficiently. Meanwhile, the\nadaptive group convolution that the number of groups grows linearly with dense\nconvolutional layers is introduced to relieve the parameter explosion. Besides,\nwe also present a novel joint attention to capture cross-dimension interaction\nbetween the spatial dimensions and channel dimension in an efficient way for\nproviding rich discriminative feature representations. Extensive experimental\nresults show that our method achieves superior performance in comparison with\nthe state-of-the-art lightweight SR methods.", "title": "Pyramidal Dense Attention Networks for Lightweight Image  Super-Resolution"}, {"link": "https://arxiv.org/abs/2106.06997", "abstract": "Bayesian decision theory provides an elegant framework for acting optimally\nunder uncertainty when tractable posterior distributions are available. Modern\nBayesian models, however, typically involve intractable posteriors that are\napproximated with, potentially crude, surrogates. This difficulty has\nengendered loss-calibrated techniques that aim to learn posterior\napproximations that favor high-utility decisions. In this paper, focusing on\nBayesian neural networks, we develop methods for correcting approximate\nposterior predictive distributions encouraging them to prefer high-utility\ndecisions. In contrast to previous work, our approach is agnostic to the choice\nof the approximate inference algorithm, allows for efficient test time decision\nmaking through amortization, and empirically produces higher quality decisions.\nWe demonstrate the effectiveness of our approach through controlled experiments\nspanning a diversity of tasks and datasets.", "title": "Post-hoc loss-calibration for Bayesian neural networks"}, {"link": "https://arxiv.org/abs/2106.06998", "abstract": "Thanks to the combination of state-of-the-art accelerators and highly\noptimized open software frameworks, there has been tremendous progress in the\nperformance of deep neural networks. While these developments have been\nresponsible for many breakthroughs, progress towards solving large-scale\nproblems, such as video encoding and semantic segmentation in 3D, is hampered\nbecause access to on-premise memory is often limited. Instead of relying on\n(optimal) checkpointing or invertibility of the network layers -- to recover\nthe activations during backpropagation -- we propose to approximate the\ngradient of convolutional layers in neural networks with a multi-channel\nrandomized trace estimation technique. Compared to other methods, this approach\nis simple, amenable to analyses, and leads to a greatly reduced memory\nfootprint. Even though the randomized trace estimation introduces stochasticity\nduring training, we argue that this is of little consequence as long as the\ninduced errors are of the same order as errors in the gradient due to the use\nof stochastic gradient descent. We discuss the performance of networks trained\nwith stochastic backpropagation and how the error can be controlled while\nmaximizing memory usage and minimizing computational overhead.", "title": "Low-memory stochastic backpropagation with multi-channel randomized  trace estimation"}, {"link": "https://arxiv.org/abs/2106.07000", "abstract": "Service providers are considering the use of unmanned aerial vehicles (UAVs)\nto enhance wireless connectivity of cellular networks. To provide connectivity,\nUAVs have to be backhauled through terrestrial base stations (BSs) to the core\nnetwork. In particular, we consider millimeter-wave (mmWave) backhauling in the\ndownlink of a hybrid aerial-terrestrial network, where the backhaul links are\nsubject to beamforming misalignment errors. In the proposed model, the user\nequipment (UE) can connect to either a ground BS or a UAV, where we\ndifferentiate between two transmission schemes according to the backhaul\nstatus. In one scheme, the UEs are served by the UAVs regardless of whether the\nbackhaul links are good or not. In the other scheme, the UAVs are aware of the\nbackhaul links status, and hence, only the subset of successfully backhauled\nUAVs can serve the UEs. Using stochastic geometry, the performance of the\nproposed model is assessed in terms of coverage probability and validated\nagainst Monte-Carlo simulations. Several insights are provided for determining\nsome system parameters including the UAVs altitude and required number and the\nbeamforming misalignment error of the backhaul link. The obtained results\nhighlight the impact of the UAVs backhaul link on the UE experience.", "title": "Analysis of Large Scale Aerial Terrestrial Networks with mmWave  Backhauling"}, {"link": "https://arxiv.org/abs/2106.07003", "abstract": "Perception of the lane boundaries is crucial for the tasks related to\nautonomous trajectory control. In this paper, several methodologies for lane\ndetection are discussed with an experimental illustration: Hough\ntransformation, Blob analysis, and Bird's eye view. Following the abstraction\nof lane marks from the boundary, the next approach is applying a control law\nbased on the perception to control steering and speed control. In the\nfollowing, a comparative analysis is made between an open-loop response, PID\ncontrol, and a neural network control law through graphical statistics. To get\nthe perception of the surrounding a wireless streaming camera connected to\nRaspberry Pi is used. After pre-processing the signal received by the camera\nthe output is sent back to the Raspberry Pi that processes the input and\ncommunicates the control to the motors through Arduino via serial\ncommunication.", "title": "Experimental Analysis of Trajectory Control Using Computer Vision and  Artificial Intelligence for Autonomous Vehicles"}, {"link": "https://arxiv.org/abs/2106.07009", "abstract": "Recently, there has been extensive research interest in training deep\nnetworks to denoise images without clean reference. However, the representative\napproaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator\n(SURE), etc. seem to differ from one another and it is difficult to find the\ncoherent mathematical structure. To address this, here we present a novel\napproach, called Noise2Score, which reveals a missing link in order to unite\nthese seemingly different approaches. Specifically, we show that image\ndenoising problems without clean images can be addressed by finding the mode of\nthe posterior distribution and that the Tweedie's formula offers an explicit\nsolution through the score function (i.e. the gradient of log likelihood). Our\nmethod then uses the recent finding that the score function can be stably\nestimated from the noisy images using the amortized residual denoising\nautoencoder, the method of which is closely related to Noise2Noise or\nNose2Void. Our Noise2Score approach is so universal that the same network\ntraining can be used to remove noises from images that are corrupted by any\nexponential family distributions and noise parameters. Using extensive\nexperiments with Gaussian, Poisson, and Gamma noises, we show that Noise2Score\nsignificantly outperforms the state-of-the-art self-supervised denoising\nmethods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc.", "title": "Noise2Score: Tweedie's Approach to Self-Supervised Image Denoising  without Clean Images"}, {"link": "https://arxiv.org/abs/2106.07011", "abstract": "A new fluid-driven soft robot hand in this study uses the idea of the bionics\nand has the anthropomorphic form, which is oriented to the flexible grasp\nfunction. The soft robot hand is composed of a new kind of multi-freedom soft\nfinger and soft palm, which realizes the characteristic grasping function of\nforehand and backhand. Combined with the fine fluid control system, the soft\nhand can realize flexible grasping under high pressure, so as to realize\nflexible grasping operation for different types of target objects in the\nunderwater environment. The soft robot hand was controlled based on water\nhydraulic platform, Finally, the soft robot hand and the fine fluid control\nsystem were connected to form the underwater soft robot hand experiment\nplatform.", "title": "Underwater Soft Robotic Hand with Multi-Source Coupling Bio-Inspired  Soft Palm and Six Fingers Driven by Water Hydraulic"}, {"link": "https://arxiv.org/abs/2106.07012", "abstract": "The behaviour of information cascades (such as retweets) has been modelled\nextensively. While point process-based generative models have long been in use\nfor estimating cascade growths, deep learning has greatly enhanced diverse\nfeature integration. We observe two significant temporal signals in cascade\ndata that have not been emphasized or reported to our knowledge. First, the\npopularity of the cascade root is known to influence cascade size strongly; but\nthe effect falls off rapidly with time. Second, there is a measurable positive\ncorrelation between the novelty of the root content (with respect to a\nstreaming external corpus) and the relative size of the resulting cascade.\nResponding to these observations, we propose GammaCas, a new cascade growth\nmodel as a parametric function of time, which combines deep influence signals\nfrom content (e.g., tweet text), network features (e.g., followers of the root\nuser), and exogenous event sources (e.g., online news). Specifically, our model\nprocesses these signals through a customized recurrent network, whose states\nthen provide the parameters of the cascade rate function, which is integrated\nover time to predict the cascade size. The network parameters are trained\nend-to-end using observed cascades. GammaCas outperforms seven recent and\ndiverse baselines significantly on a large-scale dataset of retweet cascades\ncoupled with time-aligned online news -- it beats the best baseline with an\n18.98% increase in terms of Kendall's $\\tau$ correlation and $35.63$ reduction\nin Mean Absolute Percentage Error. Extensive ablation and case studies unearth\ninteresting insights regarding retweet cascade dynamics.", "title": "Incomplete Gamma Integrals for Deep Cascade Prediction using Content,  Network, and Exogenous Signals"}, {"link": "https://arxiv.org/abs/2106.07015", "abstract": "The device used in this work detects the objects over the surface of the\nwater using two thermal cameras which aid the users to detect and avoid the\nobjects in scenarios where the human eyes cannot (night, fog, etc.). To avoid\nthe obstacle collision autonomously, it is required to track the objects in\nreal-time and assign a specific identity to each object to determine its\ndynamics (trajectory, velocity, etc.) for making estimated collision\npredictions. In the following work, a Machine Learning (ML) approach for\nComputer Vision (CV) called Convolutional Neural Network (CNN) was used using\nTensorFlow as the high-level programming environment in Python. To validate the\nalgorithm a test set was generated using an annotation tool that was created\nduring the work for proper evaluation. Once validated, the algorithm was\ndeployed on the platform and tested with the sequence generated by the test\nboat.", "title": "Siamese Network Training Using Sampled Triplets and Image Transformation"}, {"link": "https://arxiv.org/abs/2106.07020", "abstract": "The near-infrared (NIR) spectral range (from 780 to 2500 nm) of the\nmultispectral remote sensing imagery provides vital information for the\nlandcover classification, especially concerning the vegetation assessment.\nDespite the usefulness of NIR, common RGB is not always accompanied by it.\nModern achievements in image processing via deep neural networks allow\ngenerating artificial spectral information, such as for the image colorization\nproblem. In this research, we aim to investigate whether this approach can\nproduce not only visually similar images but also an artificial spectral band\nthat can improve the performance of computer vision algorithms for solving\nremote sensing tasks. We study the generative adversarial network (GAN)\napproach in the task of the NIR band generation using just RGB channels of\nhigh-resolution satellite imagery. We evaluate the impact of a generated\nchannel on the model performance for solving the forest segmentation task. Our\nresults show an increase in model accuracy when using generated NIR comparing\nto the baseline model that uses only RGB (0.947 and 0.914 F1-score\naccordingly). Conducted study shows the advantages of generating the extra band\nand its implementation in applied challenges reducing the required amount of\nlabeled data.", "title": "Generation of the NIR spectral Band for Satellite Images with  Convolutional Neural Networks"}, {"link": "https://arxiv.org/abs/2106.07022", "abstract": "Improving wind turbine efficiency is essential for reducing the costs of\nenergy production. The highly nonlinear dynamics of the wind turbines and their\nuncertain operating conditions have posed many challenges for their control\nmethods. In this work, a robust control strategy based on sliding mode and\nadaptive fuzzy disturbance observer is proposed for speed tracking in a\nvariable speed wind turbine. First, the nonlinear mathematical model that\ndescribes the dynamics of the variable speed wind turbine is derived. This\nnonlinear model is then used to derive the control methodology and to find\nstability and robustness conditions. The control approach is designed to track\nthe optimal wind speed that causes maximum energy extraction. The stability\ncondition was verified using the Lyapunov stability theory. A simulation study\nwas conducted to verify the method, and a comparative analysis was used to\nmeasure its effectiveness. The results showed a high tracking ability and\nrobustness of the developed methodology. Moreover, higher power extraction was\nobserved when compared to a classical control method.", "title": "Robust Speed Control Methodology for Variable Speed Wind Turbines"}, {"link": "https://arxiv.org/abs/2106.07023", "abstract": "We propose Styleformer, which is a style-based generator for GAN\narchitecture, but a convolution-free transformer-based generator. In our paper,\nwe explain how a transformer can generate high-quality images, overcoming the\ndisadvantage that convolution operations are difficult to capture global\nfeatures in an image. Furthermore, we change the demodulation of StyleGAN2 and\nmodify the existing transformer structure (e.g., residual connection, layer\nnormalization) to create a strong style-based generator with a convolution-free\nstructure. We also make Styleformer lighter by applying Linformer, enabling\nStyleformer to generate higher resolution images and result in improvements in\nterms of speed and memory. We experiment with the low-resolution image dataset\nsuch as CIFAR-10, as well as the high-resolution image dataset like\nLSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark\ndataset, which is comparable performance to the current state-of-the-art and\noutperforms all GAN-based generative models, including StyleGAN2-ADA with fewer\nparameters on the unconditional setting. We also both achieve new\nstate-of-the-art with FID 20.11, IS 10.16, and FID 3.66, respectively on STL-10\nand CelebA. We release our code at\nhttps://github.com/Jeeseung-Park/Styleformer.", "title": "Styleformer: Transformer based Generative Adversarial Networks with  Style Vector"}, {"link": "https://arxiv.org/abs/2106.07024", "abstract": "A central problem in Binary Hypothesis Testing (BHT) is to determine the\noptimal tradeoff between the Type I error (referred to as false alarm) and Type\nII (referred to as miss) error. In this context, the exponential rate of\nconvergence of the optimal miss error probability -- as the sample size tends\nto infinity -- given some (positive) restrictions on the false alarm\nprobabilities is a fundamental question to address in theory. Considering the\nmore realistic context of a BHT with a finite number of observations, this\npaper presents a new non-asymptotic result for the scenario with monotonic\n(sub-exponential decreasing) restriction on the Type I error probability, which\nextends the result presented by Strassen in 2009. Building on the use of\nconcentration inequalities, we offer new upper and lower bounds to the optimal\nType II error probability for the case of finite observations. Finally, the\nderived bounds are evaluated and interpreted numerically (as a function of the\nnumber samples) for some vanishing Type I error restrictions.", "title": "Finite-Length Bounds on Hypothesis Testing Subject to Vanishing Type I  Error Restrictions"}, {"link": "https://arxiv.org/abs/2106.07026", "abstract": "This paper proposes a novel nonlinear activation mechanism typically for\nconvolutional neural network (CNN), named as reborn mechanism. In sharp\ncontrast to ReLU which cuts off the negative phase value, the reborn mechanism\nenjoys the capacity to reborn and reconstruct dead neurons. Compared to other\nimproved ReLU functions, reborn mechanism introduces a more proper way to\nutilize the negative phase information. Extensive experiments validate that\nthis activation mechanism is able to enhance the model representation ability\nmore significantly and make the better use of the input data information while\nmaintaining the advantages of the original ReLU function. Moreover, reborn\nmechanism enables a non-symmetry that is hardly achieved by traditional CNNs\nand can act as a channel compensation method, offering competitive or even\nbetter performance but with fewer learned parameters than traditional methods.\nReborn mechanism was tested on various benchmark datasets, all obtaining better\nperformance than previous nonlinear activation functions.", "title": "Reborn Mechanism: Rethinking the Negative Phase Information Flow in  Convolutional Neural Network"}, {"link": "https://arxiv.org/abs/2106.07029", "abstract": "Photo Response Non-Uniformity(PRNU) noise has proven to be very effective\ntool in camera based forensics. It helps to match a photo to the device that\nclicked it. In today's scenario, where millions and millions of images are\nuploaded every hour, it is very easy to compute this unique PRNU pattern from a\ncouple of shared images on social profiles. This endangers the privacy of the\ncamera owner and becomes a cause of major concern for the privacy-aware\nsociety. We propose SSS-PRNU scheme that facilitates the forensic investigators\nto carry out their crime investigation without breaching the privacy of the\npeople. Thus, maintaining a balance between the two. To preserve privacy,\nextraction of camera fingerprint and PRNU noise for a suspicious image is\ncomputed in a trusted execution environment such as ARM TrustZone. After\nextraction, the sensitive information of camera fingerprint and PRNU noise is\ndistributed into multiple obfuscated shares using Shamir secret sharing(SSS)\nscheme. These shares are information-theoretically secure and leak no\ninformation of underlying content. The encrypted information is distributed to\nmultiple third-part servers where correlation is computed on a share basis\nbetween the camera fingerprint and the PRNU noise. These partial correlation\nvalues are combined together to obtain the final correlation value that becomes\nthe basis for a match decision. Transforming the computation of the correlation\nvalue in the encrypted domain and making it well suited for a distributed\nenvironment is the main contribution of the paper. Experiment results validate\nthe feasibility of the proposed scheme that provides a secure framework for\nPRNU based source camera attribution. The security analysis and evaluation of\ncomputational and storage overheads are performed to analysis the practical\nfeasibility of the scheme.", "title": "SSS-PRNU: Privacy-Preserving PRNU Based Camera Attribution using Shamir  Secret Sharing"}, {"link": "https://arxiv.org/abs/2106.07030", "abstract": "The capabilities of natural neural systems have inspired new generations of\nmachine learning algorithms as well as neuromorphic very large-scale integrated\n(VLSI) circuits capable of fast, low-power information processing. However,\nmost modern machine learning algorithms are not neurophysiologically plausible\nand thus are not directly implementable in neuromorphic hardware. In\nparticular, the workhorse of modern deep learning, the backpropagation\nalgorithm, has proven difficult to translate to neuromorphic hardware. In this\nstudy, we present a neuromorphic, spiking backpropagation algorithm based on\npulse-gated dynamical information coordination and processing, implemented on\nIntel's Loihi neuromorphic research processor. We demonstrate a\nproof-of-principle three-layer circuit that learns to classify digits from the\nMNIST dataset. This implementation shows a path for using massively parallel,\nlow-power, low-latency neuromorphic processors in modern deep learning\napplications.", "title": "The Backpropagation Algorithm Implemented on Spiking Neuromorphic  Hardware"}, {"link": "https://arxiv.org/abs/2106.07032", "abstract": "Over the past two decades machine learning has permeated almost every realm\nof technology. At the same time, many researchers have begun using category\ntheory as a unifying language, facilitating communication between different\nscientific disciplines. It is therefore unsurprising that there is a burgeoning\ninterest in applying category theory to machine learning. We aim to document\nthe motivations, goals and common themes across these applications. We touch on\ngradient-based learning, probability, and equivariant learning.", "title": "Category Theory in Machine Learning"}, {"link": "https://arxiv.org/abs/2106.07033", "abstract": "Federated Learning (FL) is emerging as a promising paradigm of\nprivacy-preserving machine learning, which trains an algorithm across multiple\nclients without exchanging their data samples. Recent works highlighted several\nprivacy and robustness weaknesses in FL and addressed these concerns using\nlocal differential privacy (LDP) and some well-studied methods used in\nconventional ML, separately. However, it is still not clear how LDP affects\nadversarial robustness in FL. To fill this gap, this work attempts to develop a\ncomprehensive understanding of the effects of LDP on adversarial robustness in\nFL. Clarifying the interplay is significant since this is the first step\ntowards a principled design of private and robust FL systems. We certify that\nlocal differential privacy has both positive and negative effects on\nadversarial robustness using theoretical analysis and empirical verification.", "title": "Understanding the Interplay between Privacy and Robustness in Federated  Learning"}, {"link": "https://arxiv.org/abs/2106.07034", "abstract": "Due to the significant delay and cost associated with experimental tests, a\nmodel based evaluation of concrete compressive strength is of high value, both\nfor the purpose of strength prediction as well as the mixture optimization. In\nthis regard, several recent studies have employed state-of-the-art regression\nmodels in order to achieve a good prediction model, employing available\nexperimental data sets. Nevertheless, while each of the employed models can\nbetter adapt to a specific nature of the input data, the accuracy of each\nindividual model is limited due to the sensitivity to the choice of\nhyperparameters and the learning strategy. In the present work, we take a\nfurther step towards improving the accuracy of the prediction model via the\nweighted combination of multiple regression methods. Moreover, a (GA)-based\nmulti-objective mixture optimization is proposed, building on the obtained\nmulti-regression model. In particular, we present a data aided framework where\nthe regression methods based on artificial neural network, random forest\nregression, and polynomial regression are jointly implemented to predict the\ncompressive strength of concrete. The outcome of the individual regression\nmodels are then combined via a linear weighting strategy and optimized over the\ntraining data set as a quadratic convex optimization problem. It is worth\nmentioning that due to the convexity of the formulated problem, the globally\noptimum weighting strategy is obtained via standard numerical solvers.\nEmploying the proposed GA-based optimization, a Pareto front of the cost-CS\ntrade-of has been obtained employing the available data set. Moreover, the\nresulting accuracy of the proposed multi-model prediction method is shown to\noutperform the available single-model regression methods in the literature by a\nvaluable margin, via numerical simulations.", "title": "An Extended Multi-Model Regression Approach for Compressive Strength  Prediction and Optimization of a Concrete Mixture"}, {"link": "https://arxiv.org/abs/2106.07035", "abstract": "Lifelong Learning (LL) refers to the ability to continually learn and solve\nnew problems with incremental available information over time while retaining\nprevious knowledge. Much attention has been given lately to Supervised Lifelong\nLearning (SLL) with a stream of labelled data. In contrast, we focus on\nresolving challenges in Unsupervised Lifelong Learning (ULL) with streaming\nunlabelled data when the data distribution and the unknown class labels evolve\nover time. Bayesian framework is natural to incorporate past knowledge and\nsequentially update the belief with new data. We develop a fully Bayesian\ninference framework for ULL with a novel end-to-end Deep Bayesian Unsupervised\nLifelong Learning (DBULL) algorithm, which can progressively discover new\nclusters without forgetting the past with unlabelled data while learning latent\nrepresentations. To efficiently maintain past knowledge, we develop a novel\nknowledge preservation mechanism via sufficient statistics of the latent\nrepresentation for raw data. To detect the potential new clusters on the fly,\nwe develop an automatic cluster discovery and redundancy removal strategy in\nour inference inspired by Nonparametric Bayesian statistics techniques. We\ndemonstrate the effectiveness of our approach using image and text corpora\nbenchmark datasets in both LL and batch settings.", "title": "Deep Bayesian Unsupervised Lifelong Learning"}, {"link": "https://arxiv.org/abs/2106.07036", "abstract": "We propose a benchmark to study surrogate model accuracy for protein-ligand\ndocking. We share a dataset consisting of 200 million 3D complex structures and\n2D structure scores across a consistent set of 13 million ``in-stock''\nmolecules over 15 receptors, or binding sites, across the SARS-CoV-2 proteome.\nOur work shows surrogate docking models have six orders of magnitude more\nthroughput than standard docking protocols on the same supercomputer node\ntypes. We demonstrate the power of high-speed surrogate models by running each\ntarget against 1 billion molecules in under a day (50k predictions per GPU\nseconds). We showcase a workflow for docking utilizing surrogate ML models as a\npre-filter. Our workflow is ten times faster at screening a library of\ncompounds than the standard technique, with an error rate less than 0.01\\% of\ndetecting the underlying best scoring 0.1\\% of compounds. Our analysis of the\nspeedup explains that to screen more molecules under a docking paradigm,\nanother order of magnitude speedup must come from model accuracy rather than\ncomputing speed (which, if increased, will not anymore alter our throughput to\nscreen molecules). We believe this is strong evidence for the community to\nbegin focusing on improving the accuracy of surrogate models to improve the\nability to screen massive compound libraries 100x or even 1000x faster than\ncurrent techniques.", "title": "Protein-Ligand Docking Surrogate Models: A SARS-CoV-2 Benchmark for Deep  Learning Accelerated Virtual Screening"}, {"link": "https://arxiv.org/abs/2106.07037", "abstract": "Bloom filter is a compact memory-efficient probabilistic data structure\nsupporting membership testing, i.e., to check whether an element is in a given\nset. However, as Bloom filter maps each element with uniformly random hash\nfunctions, few flexibilities are provided even if the information of negative\nkeys (elements are not in the set) are available. The problem gets worse when\nthe misidentification of negative keys brings different costs. To address the\nabove problems, we propose a new Hash Adaptive Bloom Filter (HABF) that\nsupports the customization of hash functions for keys. The key idea of HABF is\nto customize the hash functions for positive keys (elements are in the set) to\navoid negative keys with high cost, and pack customized hash functions into a\nlightweight data structure named HashExpressor. Then, given an element at query\ntime, HABF follows a two-round pattern to check whether the element is in the\nset. Further, we theoretically analyze the performance of HABF and bound the\nexpected false positive rate. We conduct extensive experiments on\nrepresentative datasets, and the results show that HABF outperforms the\nstandard Bloom filter and its cutting-edge variants on the whole in terms of\naccuracy, construction time, query time, and memory space consumption (Note\nthat source codes are available in [1]).", "title": "Hash Adaptive Bloom Filter"}, {"link": "https://arxiv.org/abs/2106.07039", "abstract": "The influence maximization (IM) problem aims at finding a subset of seed\nnodes in a social network that maximize the spread of influence. In this study,\nwe focus on a sub-class of IM problems, where whether the nodes are willing to\nbe the seeds when being invited is uncertain, called contingency-aware IM. Such\ncontingency aware IM is critical for applications for non-profit organizations\nin low resource communities (e.g., spreading awareness of disease prevention).\nDespite the initial success, a major practical obstacle in promoting the\nsolutions to more communities is the tremendous runtime of the greedy\nalgorithms and the lack of high performance computing (HPC) for the non-profits\nin the field -- whenever there is a new social network, the non-profits usually\ndo not have the HPCs to recalculate the solutions. Motivated by this and\ninspired by the line of works that use reinforcement learning (RL) to address\ncombinatorial optimization on graphs, we formalize the problem as a Markov\nDecision Process (MDP), and use RL to learn an IM policy over historically seen\nnetworks, and generalize to unseen networks with negligible runtime at test\nphase. To fully exploit the properties of our targeted problem, we propose two\ntechnical innovations that improve the existing methods, including\nstate-abstraction and theoretically grounded reward shaping. Empirical results\nshow that our method achieves influence as high as the state-of-the-art methods\nfor contingency-aware IM, while having negligible runtime at test phase.", "title": "Contingency-Aware Influence Maximization: A Reinforcement Learning  Approach"}, {"link": "https://arxiv.org/abs/2106.07041", "abstract": "Link prediction methods are frequently applied in recommender systems, e.g.,\nto suggest citations for academic papers or friends in social networks.\nHowever, exposure bias can arise when users are systematically underexposed to\ncertain relevant items. For example, in citation networks, authors might be\nmore likely to encounter papers from their own field and thus cite them\npreferentially. This bias can propagate through naively trained link\npredictors, leading to both biased evaluation and high generalization error (as\nassessed by true relevance). Moreover, this bias can be exacerbated by feedback\nloops. We propose estimators that leverage known exposure probabilities to\nmitigate this bias and consequent feedback loops. Next, we provide a loss\nfunction for learning the exposure probabilities from data. Finally,\nexperiments on semi-synthetic data based on real-world citation networks, show\nthat our methods reliably identify (truly) relevant citations. Additionally,\nour methods lead to greater diversity in the recommended papers' fields of\nstudy. The code is available at\nhttps://github.com/shantanu95/exposure-bias-link-rec.", "title": "Correcting Exposure Bias for Link Recommendation"}, {"link": "https://arxiv.org/abs/2106.07045", "abstract": "Assertion checking is an invaluable programmer's tool for finding many\nclasses of errors or verifying their absence in dynamic languages such as\nProlog. For Prolog programmers this means being able to have relevant\nproperties such as modes, types, determinacy, non-failure, sharing,\nconstraints, cost, etc., checked and errors flagged without having to actually\nrun the program. Such global static analysis tools are arguably most useful the\nearlier they are used in the software development cycle, and fast response\ntimes are essential for interactive use. Triggering a full and precise semantic\nanalysis of a software project every time a change is made can be prohibitively\nexpensive. In our static analysis and verification framework this challenge is\naddressed through a combination of modular and incremental (context- and\npath-sensitive) analysis that is responsive to program edits, at different\nlevels of granularity. We describe how the combination of this framework within\nan integrated development environment (IDE) takes advantage of such\nincrementality to achieve a high level of reactivity when reflecting analysis\nand verification results back as colorings and tooltips directly on the program\ntext -- the tool's VeriFly mode. The concrete implementation that we describe\nis Emacs-based and reuses in part off-the-shelf \"on-the-fly\" syntax checking\nfacilities (flycheck). We believe that similar extensions are also reproducible\nwith low effort in other mature development environments. Our initial\nexperience with the tool shows quite promising results, with low latency times\nthat provide early, continuous, and precise assertion checking and other\nsemantic feedback to programmers during the development process. The tool\nsupports Prolog natively, as well as other languages by semantic transformation\ninto Horn clauses.", "title": "VeriFly: On-the-fly Assertion Checking via Incrementality"}, {"link": "https://arxiv.org/abs/2106.07046", "abstract": "We prove new upper and lower bounds for sample complexity of finding an\n$\\epsilon$-optimal policy of an infinite-horizon average-reward Markov decision\nprocess (MDP) given access to a generative model. When the mixing time of the\nprobability transition matrix of all policies is at most $t_\\mathrm{mix}$, we\nprovide an algorithm that solves the problem using\n$\\widetilde{O}(t_\\mathrm{mix} \\epsilon^{-3})$ (oblivious) samples per\nstate-action pair. Further, we provide a lower bound showing that a linear\ndependence on $t_\\mathrm{mix}$ is necessary in the worst case for any algorithm\nwhich computes oblivious samples. We obtain our results by establishing\nconnections between infinite-horizon average-reward MDPs and discounted MDPs of\npossible further utility.", "title": "Towards Tight Bounds on the Sample Complexity of Average-reward MDPs"}, {"link": "https://arxiv.org/abs/2106.07047", "abstract": "Despite significant improvements in natural language understanding models\nwith the advent of models like BERT and XLNet, these neural-network based\nclassifiers are vulnerable to blackbox adversarial attacks, where the attacker\nis only allowed to query the target model outputs. We add two more realistic\nrestrictions on the attack methods, namely limiting the number of queries\nallowed (query budget) and crafting attacks that easily transfer across\ndifferent pre-trained models (transferability), which render previous attack\nmodels impractical and ineffective. Here, we propose a target model agnostic\nadversarial attack method with a high degree of attack transferability across\nthe attacked models. Our empirical studies show that in comparison to baseline\nmethods, our method generates highly transferable adversarial sentences under\nthe restriction of limited query budgets.", "title": "Target Model Agnostic Adversarial Attacks with Query Budgets on Language  Understanding Models"}, {"link": "https://arxiv.org/abs/2106.07049", "abstract": "In the last few years, deep learning classifiers have shown promising results\nin image-based medical diagnosis. However, interpreting the outputs of these\nmodels remains a challenge. In cancer diagnosis, interpretability can be\nachieved by localizing the region of the input image responsible for the\noutput, i.e. the location of a lesion. Alternatively, segmentation or detection\nmodels can be trained with pixel-wise annotations indicating the locations of\nmalignant lesions. Unfortunately, acquiring such labels is labor-intensive and\nrequires medical expertise. To overcome this difficulty, weakly-supervised\nlocalization can be utilized. These methods allow neural network classifiers to\noutput saliency maps highlighting the regions of the input most relevant to the\nclassification task (e.g. malignant lesions in mammograms) using only\nimage-level labels (e.g. whether the patient has cancer or not) during\ntraining. When applied to high-resolution images, existing methods produce\nlow-resolution saliency maps. This is problematic in applications in which\nsuspicious lesions are small in relation to the image size. In this work, we\nintroduce a novel neural network architecture to perform weakly-supervised\nsegmentation of high-resolution images. The proposed model selects regions of\ninterest via coarse-level localization, and then performs fine-grained\nsegmentation of those regions. We apply this model to breast cancer diagnosis\nwith screening mammography, and validate it on a large clinically-realistic\ndataset. Measured by Dice similarity score, our approach outperforms existing\nmethods by a large margin in terms of localization performance of benign and\nmalignant lesions, relatively improving the performance by 39.6% and 20.0%,\nrespectively. Code and the weights of some of the models are available at\nhttps://github.com/nyukat/GLAM", "title": "Weakly-supervised High-resolution Segmentation of Mammography Images for  Breast Cancer Diagnosis"}, {"link": "https://arxiv.org/abs/2106.07051", "abstract": "Quality of Service (QoS) is now regarded as a requirement for all networks in\nmanaging resources like bandwidth and avoidance of network impairments like\npacket loss, jitter, and delay. Media transfer or streaming would be virtually\nimpossible if QoS parameters were not used even if the streaming protocols were\nperfectly designed. QoS Scheduling classes help in network traffic optimization\nand the priority management of packets. This paper presents an analysis of QoS\nscheduling classes using video traffic in a MANET. The main objective was to\nidentify a scheduling class that provides better QoS for video streaming. A\nsimulation was conducted using NetSim and results were analyzed according to\nthroughput, jitter, and delay. The overall results showed that extended\nreal-time Polling Service (ertPS) outperformed the other classes. ertPS has\nhybrid features of both real-time Polling Service (rtPS) and Unsolicited Grant\nService(UGS) hence the enhanced performance. It is recommended that ertPS\nscheduling class should be used in MANET where QoS consideration is utmost\nparticularly in multimedia streaming applications.", "title": "Comparative analysis of quality of service scheduling classes in mobile  ad-hoc networks"}, {"link": "https://arxiv.org/abs/2106.07052", "abstract": "Variational inference enables approximate posterior inference of the highly\nover-parameterized neural networks that are popular in modern machine learning.\nUnfortunately, such posteriors are known to exhibit various pathological\nbehaviors. We prove that as the number of hidden units in a single-layer\nBayesian neural network tends to infinity, the function-space posterior mean\nunder mean-field variational inference actually converges to zero, completely\nignoring the data. This is in contrast to the true posterior, which converges\nto a Gaussian process. Our work provides insight into the over-regularization\nof the KL divergence in variational inference.", "title": "Wide Mean-Field Variational Bayesian Neural Networks Ignore the Data"}, {"link": "https://arxiv.org/abs/2106.07055", "abstract": "In transfer learning, it is imperative to achieve strong alignment between a\npre-trained model and a downstream task. Prior work has done this by proposing\ntask-specific pre-training objectives, which sacrifices the inherent\nscalability of the transfer learning paradigm. We instead achieve strong\nalignment by simultaneously modifying both the pre-trained model and the\nformulation of the downstream task, which is more efficient and preserves the\nscalability of transfer learning. We present GenSF (Generative Slot Filling),\nwhich leverages a generative pre-trained open-domain dialog model for slot\nfilling. GenSF (1) adapts the pre-trained model by incorporating inductive\nbiases about the task and (2) adapts the downstream task by reformulating slot\nfilling to better leverage the pre-trained model's capabilities. GenSF achieves\nstate-of-the-art results on two slot filling datasets with strong gains in\nfew-shot and zero-shot settings. We achieve a 9 F1 score improvement in\nzero-shot slot filling. This highlights the value of strong alignment between\nthe pre-trained model and the downstream task.", "title": "GenSF: Simultaneous Adaptation of Generative Pre-trained Models and Slot  Filling"}, {"link": "https://arxiv.org/abs/2106.07056", "abstract": "Developing mechanisms that flexibly adapt dialog systems to unseen tasks and\ndomains is a major challenge in dialog research. Neural models implicitly\nmemorize task-specific dialog policies from the training data. We posit that\nthis implicit memorization has precluded zero-shot transfer learning. To this\nend, we leverage the schema-guided paradigm, wherein the task-specific dialog\npolicy is explicitly provided to the model. We introduce the Schema Attention\nModel (SAM) and improved schema representations for the STAR corpus. SAM\nobtains significant improvement in zero-shot settings, with a +22 F1 score\nimprovement over prior work. These results validate the feasibility of\nzero-shot generalizability in dialog. Ablation experiments are also presented\nto demonstrate the efficacy of SAM.", "title": "Schema-Guided Paradigm for Zero-Shot Dialog"}, {"link": "https://arxiv.org/abs/2106.07059", "abstract": "The scheduling literature has traditionally focused on a single type of\nresource (e.g., computing nodes). However, scientific applications in modern\nHigh-Performance Computing (HPC) systems process large amounts of data, hence\nhave diverse requirements on different types of resources (e.g., cores, cache,\nmemory, I/O). All of these resources could potentially be exploited by the\nruntime scheduler to improve the application performance. In this paper, we\nstudy multi-resource scheduling to minimize the makespan of computational\nworkflows comprised of parallel jobs subject to precedence constraints. The\njobs are assumed to be moldable, allowing the scheduler to flexibly select a\nvariable set of resources before execution. We propose a multi-resource,\nlist-based scheduling algorithm, and prove that, on a system with $d$ types of\nschedulable resources, our algorithm achieves an approximation ratio of\n$1.619d+2.545\\sqrt{d}+1$ for any $d$, and a ratio of $d+O(\\sqrt[3]{d^2})$ for\nlarge $d$. We also present improved results for independent jobs and for jobs\nwith special precedence constraints (e.g., series-parallel graphs and trees).\nFinally, we prove a lower bound of $d$ on the approximation ratio of any list\nscheduling scheme with local priority considerations. To the best of our\nknowledge, these are the first approximation results for moldable workflows\nwith multiple resource requirements.", "title": "Multi-Resource List Scheduling of Moldable Parallel Jobs under  Precedence Constraints"}, {"link": "https://arxiv.org/abs/2106.07062", "abstract": "We explore the use of a topological manifold, represented as a collection of\ncharts, as the target space of neural network based representation learning\ntasks. This is achieved by a simple adjustment to the output of an encoder's\nnetwork architecture plus the addition of a maximal mean discrepancy (MMD)\nbased loss function for regularization. Most algorithms in representation and\nmetric learning are easily adaptable to our framework and we demonstrate its\neffectiveness by adjusting SimCLR (for representation learning) and standard\ntriplet loss training (for metric learning) to have manifold encoding spaces.\nOur experiments show that we obtain a substantial performance boost over the\nbaseline for low dimensional encodings. In the case of triplet training, we\nalso find, independent of the manifold setup, that the MMD loss alone (i.e.\nkeeping a flat, euclidean target space but using an MMD loss to regularize it)\nincreases performance over the baseline in the typical, high-dimensional\nEuclidean target spaces. Code for reproducing experiments is provided at\nhttps://github.com/ekorman/neurve .", "title": "Atlas Based Representation and Metric Learning on Manifolds"}, {"link": "https://arxiv.org/abs/2106.07068", "abstract": "Advancement in digital pathology and artificial intelligence has enabled deep\nlearning-based computer vision techniques for automated disease diagnosis and\nprognosis. However, WSIs present unique computational and algorithmic\nchallenges. WSIs are gigapixel-sized, making them infeasible to be used\ndirectly for training deep neural networks. Hence, for modeling, a two-stage\napproach is adopted: Patch representations are extracted first, followed by the\naggregation for WSI prediction. These approaches require detailed pixel-level\nannotations for training the patch encoder. However, obtaining these\nannotations is time-consuming and tedious for medical experts. Transfer\nlearning is used to address this gap and deep learning architectures\npre-trained on ImageNet are used for generating patch-level representation.\nEven though ImageNet differs significantly from histopathology data,\npre-trained networks have been shown to perform impressively on histopathology\ndata. Also, progress in self-supervised and multi-task learning coupled with\nthe release of multiple histopathology data has led to the release of\nhistopathology-specific networks. In this work, we compare the performance of\nfeatures extracted from networks trained on ImageNet and histopathology data.\nWe use an attention pooling network over these extracted features for\nslide-level aggregation. We investigate if features learned using more complex\nnetworks lead to gain in performance. We use a simple top-k sampling approach\nfor fine-tuning framework and study the representation similarity between\nfrozen and fine-tuned networks using Centered Kernel Alignment. Further, to\nexamine if intermediate block representation is better suited for feature\nextraction and ImageNet architectures are unnecessarily large for\nhistopathology, we truncate the blocks of ResNet18 and DenseNet121 and examine\nthe performance.", "title": "HistoTransfer: Understanding Transfer Learning for Histopathology"}, {"link": "https://arxiv.org/abs/2106.07069", "abstract": "We investigate a specific finite element model to study the thermoelastic\nbehavior of an elastic body within the context of nonlinear strain-limiting\nconstitutive relation. As a special subclass of implicit relations, the\nthermoelastic response of our interest is such that stresses can be arbitrarily\nlarge, but strains remain small, especially in the neighborhood of crack-tips.\nThus, the proposed model can be inherently consistent with the assumption of\nthe small strain theory. In the present communication, we consider a\ntwo-dimensional coupled system-linear and quasilinear partial differential\nequations for temperature and displacements, respectively. Two distinct\ntemperature distributions of the Dirichlet type are considered for boundary\ncondition, and a standard finite element method of continuous Galerkin is\nemployed to obtain the numerical solutions for the field variables. For a\ndomain with an edge-crack, we find that the near-tip strain growth of our model\nis much slower than the growth of stress, which is the salient feature compared\nto the inconsistent results of the classical linearized description of the\nelastic body. Current study can provide a theoretical and computational\nframework to develop physically meaningful models and examine other coupled\nmulti-physics such as an evolution of complex network of cracks induced by\nthermal shocks.", "title": "A finite element model for a coupled thermo-mechanical system: nonlinear  strain-limiting thermoelastic body"}, {"link": "https://arxiv.org/abs/2106.07074", "abstract": "Radar systems are mainly used for tracking aircraft, missiles, satellites,\nand watercraft. In many cases, information regarding the objects detected by\nthe radar system is sent to, and used by, a peripheral consuming system, such\nas a missile system or a graphical user interface used by an operator. Those\nsystems process the data stream and make real-time, operational decisions based\non the data received. Given this, the reliability and availability of\ninformation provided by radar systems has grown in importance. Although the\nfield of cyber security has been continuously evolving, no prior research has\nfocused on anomaly detection in radar systems. In this paper, we present a deep\nlearning-based method for detecting anomalies in radar system data streams. We\npropose a novel technique which learns the correlation between numerical\nfeatures and an embedding representation of categorical features in an\nunsupervised manner. The proposed technique, which allows the detection of\nmalicious manipulation of critical fields in the data stream, is complemented\nby a timing-interval anomaly detection mechanism proposed for the detection of\nmessage dropping attempts. Real radar system data is used to evaluate the\nproposed method. Our experiments demonstrate the method's high detection\naccuracy on a variety of data stream manipulation attacks (average detection\nrate of 88% with 1.59% false alarms) and message dropping attacks (average\ndetection rate of 92% with 2.2% false alarms).", "title": "RadArnomaly: Protecting Radar Systems from Data Manipulation Attacks"}, {"link": "https://arxiv.org/abs/2106.07075", "abstract": "Semi-supervised learning is especially interesting in the dense prediction\ncontext due to high cost of pixel-level ground truth. Unfortunately, most such\napproaches are evaluated on outdated architectures which hamper research due to\nvery slow training and high requirements on GPU RAM. We address this concern by\npresenting a simple and effective baseline which works very well both on\nstandard and efficient architectures. Our baseline is based on one-way\nconsistency and non-linear geometric and photometric perturbations. We show\nadvantage of perturbing only the student branch and present a plausible\nexplanation of such behaviour. Experiments on Cityscapes and CIFAR-10\ndemonstrate competitive performance with respect to prior work.", "title": "A baseline for semi-supervised learning of efficient semantic  segmentation models"}, {"link": "https://arxiv.org/abs/2106.07080", "abstract": "We study the problem of {\\em crowdsourced PAC learning} of Boolean-valued\nfunctions through enriched queries, a problem that has attracted a surge of\nrecent research interests. In particular, we consider that the learner may\nquery the crowd to obtain a label of a given instance or a comparison tag of a\npair of instances. This is a challenging problem and only recently have\nbudget-efficient algorithms been established for the scenario where the\nmajority of the crowd are correct. In this work, we investigate the\nsignificantly more challenging case that the majority are incorrect which\nrenders learning impossible in general. We show that under the {semi-verified\nmodel} of Charikar~et~al.~(2017), where we have (limited) access to a trusted\noracle who always returns the correct annotation, it is possible to learn the\nunderlying function while the labeling cost is significantly mitigated by the\nenriched and more easily obtained queries.", "title": "Semi-verified Learning from the Crowd with Pairwise Comparisons"}, {"link": "https://arxiv.org/abs/2106.07084", "abstract": "The purpose of this document is to study the security properties of the\nSilver Bullet algorithm against worst-case RowHammer attacks. We mathematically\ndemonstrate that Silver Bullet, when properly configured and implemented in a\nDRAM chip, can securely prevent RowHammer attacks. The demonstration focuses on\nthe most representative implementation of Silver Bullet, the patent claiming\nmany implementation possibilities not covered in this demonstration. Our study\nconcludes that Silver Bullet is a promising RowHammer prevention mechanism that\ncan be configured to operate securely against RowHammer attacks at various\nefficiency-area tradeoff points, supporting relatively small hammer count\nvalues (e.g., 1000) and Silver Bullet table sizes (e.g., 1.06KB).", "title": "Security Analysis of the Silver Bullet Technique for RowHammer  Prevention"}, {"link": "https://arxiv.org/abs/2106.07085", "abstract": "Data augmentation has been widely used to improve deep nerual networks\nperformance. Numerous approaches are suggested, for example, dropout,\nregularization and image augmentation, to avoid over-ftting and enhancing\ngeneralization of neural networks. One of the sub-area within data augmentation\nis image mixing and deleting. This specific type of augmentation either mixes\ntwo images or delete image regions to hide or make certain characteristics of\nimages confusing for the network to force it to emphasize on overall structure\nof object in image. The model trained with this approach has shown to perform\nand generalize well as compared to one trained without imgage mixing or\ndeleting. Additional benefit achieved with this method of training is\nrobustness against image corruptions. Due to its low compute cost and success\nin recent past, many techniques of image mixing and deleting are proposed. This\npaper provides detailed review on these devised approaches, dividing\naugmentation strategies in three main categories cut and delete, cut and mix\nand mixup. The second part of paper emprically evaluates these approaches for\nimage classification, finegrained image recognition and object detection where\nit is shown that this category of data augmentation improves the overall\nperformance for deep neural networks.", "title": "Survey: Image Mixing and Deleting for Data Augmentation"}, {"link": "https://arxiv.org/abs/2106.07087", "abstract": "With the prevalence of deep learning (DL) in many applications, researchers\nare investigating different ways of optimizing FPGA architecture and CAD to\nachieve better quality-of-results (QoR) on DL-based workloads. In this\noptimization process, benchmark circuits are an essential component; the QoR\nachieved on a set of benchmarks is the main driver for architecture and CAD\ndesign choices. However, current academic benchmark suites are inadequate, as\nthey do not capture any designs from the DL domain. This work presents a new\nsuite of DL acceleration benchmark circuits for FPGA architecture and CAD\nresearch, called Koios. This suite of 19 circuits covers a wide variety of\naccelerated neural networks, design sizes, implementation styles, abstraction\nlevels, and numerical precisions. These designs are larger, more data parallel,\nmore heterogeneous, more deeply pipelined, and utilize more FPGA architectural\nfeatures compared to existing open-source benchmarks. This enables researchers\nto pin-point architectural inefficiencies for this class of workloads and\noptimize CAD tools on more realistic benchmarks that stress the CAD algorithms\nin different ways. In this paper, we describe the designs in our benchmark\nsuite, present results of running them through the Verilog-to-Routing (VTR)\nflow using a recent FPGA architecture model, and identify key insights from the\nresulting metrics. On average, our benchmarks have 3.7x more netlist\nprimitives, 1.8x and 4.7x higher DSP and BRAM densities, and 1.7x higher\nfrequency with 1.9x more near-critical paths compared to the widely-used VTR\nsuite. Finally, we present two example case studies showing how architectural\nexploration for DL-optimized FPGAs can be performed using our new benchmark\nsuite.", "title": "Koios: A Deep Learning Benchmark Suite for FPGA Architecture and CAD  Research"}, {"link": "https://arxiv.org/abs/2106.07088", "abstract": "This paper proposes a novel fuzzy action selection method to leverage human\nknowledge in reinforcement learning problems. Based on the estimates of the\nmost current action-state values, the proposed fuzzy nonlinear mapping as-signs\neach member of the action set to its probability of being chosen in the next\nstep. A user tunable parameter is introduced to control the action selection\npolicy, which determines the agent's greedy behavior throughout the learning\nprocess. This parameter resembles the role of the temperature parameter in the\nsoftmax action selection policy, but its tuning process can be more\nknowledge-oriented since this parameter reflects the human knowledge into the\nlearning agent by making modifications in the fuzzy rule base. Simulation\nresults indicate that including fuzzy logic within the reinforcement learning\nin the proposed manner improves the learning algorithm's convergence rate, and\nprovides superior performance.", "title": "A new soft computing method for integration of expert's knowledge in  reinforcement learn-ing problems"}, {"link": "https://arxiv.org/abs/2106.07091", "abstract": "Robustness to variations in lighting conditions is a key objective for any\ndeep vision system. To this end, our paper extends the receptive field of\nconvolutional neural networks with two residual components, ubiquitous in the\nvisual processing system of vertebrates: On-center and off-center pathways,\nwith excitatory center and inhibitory surround; OOCS for short. The on-center\npathway is excited by the presence of a light stimulus in its center but not in\nits surround, whereas the off-center one is excited by the absence of a light\nstimulus in its center but not in its surround. We design OOCS pathways via a\ndifference of Gaussians, with their variance computed analytically from the\nsize of the receptive fields. OOCS pathways complement each other in their\nresponse to light stimuli, ensuring this way a strong edge-detection\ncapability, and as a result, an accurate and robust inference under challenging\nlighting conditions. We provide extensive empirical evidence showing that\nnetworks supplied with the OOCS edge representation gain accuracy and\nillumination-robustness compared to standard deep models.", "title": "On-Off Center-Surround Receptive Fields for Accurate and Robust Image  Classification"}, {"link": "https://arxiv.org/abs/2106.07094", "abstract": "In this paper, we focus on facilitating differentially private quantized\ncommunication between the clients and server in federated learning (FL).\nTowards this end, we propose to have the clients send a \\textit{private\nquantized} version of only the \\textit{unit vector} along the change in their\nlocal parameters to the server, \\textit{completely throwing away the magnitude\ninformation}. We call this algorithm \\texttt{DP-NormFedAvg} and show that it\nhas the same order-wise convergence rate as \\texttt{FedAvg} on smooth\nquasar-convex functions (an important class of non-convex functions for\nmodeling optimization of deep neural networks), thereby establishing that\ndiscarding the magnitude information is not detrimental from an optimization\npoint of view. We also introduce QTDL, a new differentially private\nquantization mechanism for unit-norm vectors, which we use in\n\\texttt{DP-NormFedAvg}. QTDL employs \\textit{discrete} noise having a\nLaplacian-like distribution on a \\textit{finite support} to provide privacy. We\nshow that under a growth-condition assumption on the per-sample client losses,\nthe extra per-coordinate communication cost in each round incurred due to\nprivacy by our method is $\\mathcal{O}(1)$ with respect to the model dimension,\nwhich is an improvement over prior work. Finally, we show the efficacy of our\nproposed method with experiments on fully-connected neural networks trained on\nCIFAR-10 and Fashion-MNIST.", "title": "DP-NormFedAvg: Normalizing Client Updates for Privacy-Preserving  Federated Learning"}, {"link": "https://arxiv.org/abs/2106.07095", "abstract": "We propose a binary representation of categorical values using a linear map.\nThis linear representation preserves the neighborhood structure of categorical\nvalues. In the context of evolutionary algorithms, it means that every\ncategorical value can be reached in a single mutation. The linear\nrepresentation is embedded into standard metaheuristics, applied to the problem\nof Sudoku puzzles, and compared to the more traditional direct binary encoding.\nIt shows promising results in fixed-budget experiments and empirical cumulative\ndistribution functions with high dimension instances, and also in fixed-target\nexperiments with small dimension instances.", "title": "Linear representation of categorical values"}, {"link": "https://arxiv.org/abs/2106.07098", "abstract": "To enable safe and reliable decision-making, autonomous vehicles (AVs) feed\nsensor data to perception algorithms to understand the environment. Sensor\nfusion, and particularly semantic fusion, with multi-frame tracking is becoming\nincreasingly popular for detecting 3D objects. Recently, it was shown that\nLiDAR-based perception built on deep neural networks is vulnerable to LiDAR\nspoofing attacks. Thus, in this work, we perform the first analysis of\ncamera-LiDAR fusion under spoofing attacks and the first security analysis of\nsemantic fusion in any AV context. We find first that fusion is more successful\nthan existing defenses at guarding against naive spoofing. However, we then\ndefine the frustum attack as a new class of attacks on AVs and find that\nsemantic camera-LiDAR fusion exhibits widespread vulnerability to frustum\nattacks with between 70% and 90% success against target models. Importantly,\nthe attacker needs less than 20 random spoof points on average for successful\nattacks - an order of magnitude less than established maximum capability.\nFinally, we are the first to analyze the longitudinal impact of perception\nattacks by showing the impact of multi-frame attacks.", "title": "Security Analysis of Camera-LiDAR Semantic-Level Fusion Against  Black-Box Attacks on Autonomous Vehicles"}, {"link": "https://arxiv.org/abs/2106.07100", "abstract": "In the optional prisoner's dilemma (OPD), players can choose to cooperate and\ndefect as usual, but can also abstain as a third possible strategy. This\nstrategy models the players' participation in the game and is a relevant aspect\nin many settings, e.g. social networks or opinion dynamics where abstention is\nan option during an election. In this paper, we provide a formulation of the\nOPD where we consider irrational behaviours in the population inspired by\nprospect theory. Prospect theory has gained increasing popularity in recent\ntimes thanks to its ability to capture aspects such as reference dependence or\nloss aversion which are common in human behaviour. This element is original in\nour formulation of the game and is incorporated in our framework through\npairwise comparison dynamics. Recently, the impact of the environment has been\nstudied in the form of feedback on the population dynamics. Another element of\nnovelty in our work is the extension of the game-environment feedback to the\nOPD in two forms of dynamics, the replicator and the pairwise comparison. The\ncontribution of this paper is threefold. First, we propose a modelling\nframework where prospect theory is used to capture irrational behaviours in an\nevolutionary game with game-environment feedback. Second, we carry out the\nstability analysis of the system equilibria and discuss the oscillating\nbehaviours arising from the game-environment feedback. Finally, we extend our\nprevious results to the OPD and we discuss the main differences between the\nmodel resulting from the replicator dynamics and the one resulting from the\npairwise comparison dynamics.", "title": "The Impact of Irrational Behaviours in the Optional Prisoner's Dilemma  with Game-Environment Feedback"}, {"link": "https://arxiv.org/abs/2106.07102", "abstract": "Cloud deployments disaggregate storage from compute, providing more\nflexibility to both the storage and compute layers. In this paper, we explore\ndisaggregation by taking it one step further and applying it to memory (DRAM).\nDisaggregated memory uses network attached DRAM as a way to decouple memory\nfrom CPU. In the context of databases, such a design offers significant\nadvantages in terms of making a larger memory capacity available as a central\npool to a collection of smaller processing nodes. To explore these\npossibilities, we have implemented Farview, a disaggregated memory solution for\ndatabases, operating as a remote buffer cache with operator offloading\ncapabilities. Farview is implemented as an FPGA-based smart NIC making DRAM\navailable as a disaggregated, network attached memory module capable of\nperforming data processing at line rate over data streams to/from disaggregated\nmemory. Farview supports query offloading using operators such as selection,\nprojection, aggregation, regular expression matching and encryption. In this\npaper we focus on analytical queries and demonstrate the viability of the idea\nthrough an extensive experimental evaluation of Farview under different\nworkloads. Farview is competitive with a local buffer cache solution for all\nthe workloads and outperforms it in a number of cases, proving that a smart\ndisaggregated memory can be a viable alternative for databases deployed in\ncloud environments.", "title": "Farview: Disaggregated Memory with Operator Off-loading for Database  Engines"}, {"link": "https://arxiv.org/abs/2106.07105", "abstract": "Secret Unknown Ciphers (SUC) have been proposed recently as digital\nclone-resistant functions overcoming some of Physical(ly) Unclonable Functions\n(PUF) downsides, mainly their inconsistency because of PUFs analog nature. In\nthis paper, we propose a new practical mechanism for creating internally random\nciphers in modern volatile and non-volatile SoC FPGAs, coined as SRAM-SUC. Each\ncreated random cipher inside a SoC FPGA constitutes a robust digital PUF. This\nwork also presents a class of involutive SUCs, optimized for the targeted SoC\nFPGA architecture, as sample realization of the concept; it deploys a generated\nclass of involutive 8-bit S-Boxes, that are selected randomly from a defined\nlarge set through an internal process inside the SoC FPGA. Hardware and\nsoftware implementations show that the resulting SRAM-SUC has ultra-low latency\ncompared to well-known PUF-based authentication mechanisms. SRAM-SUC requires\nonly $2.88/0.72 \\mu s$ to generate a response for a challenge at 50/200 MHz\nrespectively. This makes SRAM-SUC a promising and appealing solution for\nUltra-Reliable Low Latency Communication (URLLC).", "title": "SRAM-SUC: Ultra-Low Latency Robust Digital PUF"}, {"link": "https://arxiv.org/abs/2106.07106", "abstract": "We present a novel approach to optimal transport between graphs from the\nperspective of stationary Markov chains. A weighted graph may be associated\nwith a stationary Markov chain by means of a random walk on the vertex set with\ntransition distributions depending on the edge weights of the graph. After\ndrawing this connection, we describe how optimal transport techniques for\nstationary Markov chains may be used in order to perform comparison and\nalignment of the graphs under study. In particular, we propose the graph\noptimal transition coupling problem, referred to as GraphOTC, in which the\nMarkov chains associated to two given graphs are optimally synchronized to\nminimize an expected cost. The joint synchronized chain yields an alignment of\nthe vertices and edges in the two graphs, and the expected cost of the\nsynchronized chain acts as a measure of distance or dissimilarity between the\ntwo graphs. We demonstrate that GraphOTC performs equal to or better than\nexisting state-of-the-art techniques in graph optimal transport for several\ntasks and datasets. Finally, we also describe a generalization of the GraphOTC\nproblem, called the FusedOTC problem, from which we recover the GraphOTC and OT\ncosts as special cases.", "title": "Graph Optimal Transport with Transition Couplings of Random Walks"}, {"link": "https://arxiv.org/abs/2106.07108", "abstract": "Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs) are\npopular tools for enforcing safety and stability of a controlled system,\nrespectively. They are commonly utilized to build constraints that can be\nincorporated in a min-norm quadratic program (CBF-CLF-QP) which solves for a\nsafety-critical control input. However, since these constraints rely on a model\nof the system, when this model is inaccurate the guarantees of safety and\nstability can be easily lost. In this paper, we present a Gaussian Process\n(GP)-based approach to tackle the problem of model uncertainty in\nsafety-critical controllers that use CBFs and CLFs. The considered model\nuncertainty is affected by both state and control input. We derive\nprobabilistic bounds on the effects that such model uncertainty has on the\ndynamics of the CBF and CLF. Then, we use these bounds to build safety and\nstability chance constraints that can be incorporated in a min-norm convex\noptimization program, called GP-CBF-CLF-SOCP. As the main theoretical result of\nthe paper, we present necessary and sufficient conditions for pointwise\nfeasibility of the proposed optimization problem. We believe that these\nconditions could serve as a starting point towards understanding what are the\nminimal requirements on the distribution of data collected from the real system\nin order to guarantee safety. Finally, we validate the proposed framework with\nnumerical simulations of an adaptive cruise controller for an automotive\nsystem.", "title": "Pointwise Feasibility of Gaussian Process-based Safety-Critical Control  under Model Uncertainty"}, {"link": "https://arxiv.org/abs/2106.07111", "abstract": "Traditional probabilistic methods for the simulation of advection-diffusion\nequations (ADEs) often overlook the entropic contribution of the\ndiscretization, e.g., the number of particles, within associated numerical\nmethods. Many times, the gain in accuracy of a highly discretized numerical\nmodel is outweighed by its associated computational costs or the noise within\nthe data. We address the question of how many particles are needed in a\nsimulation to best approximate and estimate parameters in one-dimensional\nadvective-diffusive transport. To do so, we use the well-known Akaike\nInformation Criterion (AIC) and a recently-developed correction called the\nComputational Information Criterion (COMIC) to guide the model selection\nprocess. Random-walk and mass-transfer particle tracking methods are employed\nto solve the model equations at various levels of discretization. Numerical\nresults demonstrate that the COMIC provides an optimal number of particles that\ncan describe a more efficient model in terms of parameter estimation and model\nprediction compared to the model selected by the AIC even when the data is\nsparse or noisy, the sampling volume is not uniform throughout the physical\ndomain, or the error distribution of the data is non-IID Gaussian.", "title": "A Computational Information Criterion for Particle-Tracking with Sparse  or Noisy Data"}, {"link": "https://arxiv.org/abs/2106.07112", "abstract": "Currently, there is a surge of interest in fair Artificial Intelligence (AI)\nand Machine Learning (ML) research which aims to mitigate discriminatory bias\nin AI algorithms, e.g. along lines of gender, age, and race. While most\nresearch in this domain focuses on developing fair AI algorithms, in this work,\nwe show that a fair AI algorithm on its own may be insufficient to achieve its\nintended results in the real world. Using career recommendation as a case\nstudy, we build a fair AI career recommender by employing gender debiasing\nmachine learning techniques. Our offline evaluation showed that the debiased\nrecommender makes fairer career recommendations without sacrificing its\naccuracy. Nevertheless, an online user study of more than 200 college students\nrevealed that participants on average prefer the original biased system over\nthe debiased system. Specifically, we found that perceived gender disparity is\na determining factor for the acceptance of a recommendation. In other words,\nour results demonstrate we cannot fully address the gender bias issue in AI\nrecommendations without addressing the gender bias in humans.", "title": "Bias: Friend or Foe? User Acceptance of Gender Stereotypes in Automated  Career Recommendations"}, {"link": "https://arxiv.org/abs/2106.07113", "abstract": "Due to the nature of their pathways, NASA Terra and NASA Aqua satellites\ncapture imagery containing swath gaps, which are areas of no data. Swath gaps\ncan overlap the region of interest (ROI) completely, often rendering the entire\nimagery unusable by Machine Learning (ML) models. This problem is further\nexacerbated when the ROI rarely occurs (e.g. a hurricane) and, on occurrence,\nis partially overlapped with a swath gap. With annotated data as supervision, a\nmodel can learn to differentiate between the area of focus and the swath gap.\nHowever, annotation is expensive and currently the vast majority of existing\ndata is unannotated. Hence, we propose an augmentation technique that\nconsiderably removes the existence of swath gaps in order to allow CNNs to\nfocus on the ROI, and thus successfully use data with swath gaps for training.\nWe experiment on the UC Merced Land Use Dataset, where we add swath gaps\nthrough empty polygons (up to 20 percent areas) and then apply augmentation\ntechniques to fill the swath gaps. We compare the model trained with our\naugmentation techniques on the swath gap-filled data with the model trained on\nthe original swath gap-less data and note highly augmented performance.\nAdditionally, we perform a qualitative analysis using activation maps that\nvisualizes the effectiveness of our trained network in not paying attention to\nthe swath gaps. We also evaluate our results with a human baseline and show\nthat, in certain cases, the filled swath gaps look so realistic that even a\nhuman evaluator did not distinguish between original satellite images and swath\ngap-filled images. Since this method is aimed at unlabeled data, it is widely\ngeneralizable and impactful for large scale unannotated datasets from various\nspace data domains.", "title": "Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models  for NASA MODIS Instruments"}, {"link": "https://arxiv.org/abs/2106.07114", "abstract": "This paper presents our research on leveraging social media Big Data and AI\nto support hurricane disaster emergency response. The current practice of\nhurricane emergency response for rescue highly relies on emergency call\ncentres. The more recent Hurricane Harvey event reveals the limitations of the\ncurrent systems. We use Hurricane Harvey and the associated Houston flooding as\nthe motivating scenario to conduct research and develop a prototype as a\nproof-of-concept of using an intelligent agent as a complementary role to\nsupport emergency centres in hurricane emergency response. This intelligent\nagent is used to collect real-time streaming tweets during a natural disaster\nevent, to identify tweets requesting rescue, to extract key information such as\naddress and associated geocode, and to visualize the extracted information in\nan interactive map in decision supports. Our experiment shows promising\noutcomes and the potential application of the research in support of hurricane\nemergency response.", "title": "Intelligent Agent for Hurricane Emergency Identification and Text  Information Extraction from Streaming Social Media Big Data"}, {"link": "https://arxiv.org/abs/2106.07115", "abstract": "Multiple views of data, both naturally acquired (e.g., image and audio) and\nartificially produced (e.g., via adding different noise to data samples), have\nproven useful in enhancing representation learning. Natural views are often\nhandled by multiview analysis tools, e.g., (deep) canonical correlation\nanalysis [(D)CCA], while the artificial ones are frequently used in\nself-supervised learning (SSL) paradigms, e.g., \\texttt{SimCLR} and\n\\texttt{Barlow Twins}. Both types of approaches often involve learning neural\nfeature extractors such that the embeddings of data exhibit high cross-view\ncorrelations. Although intuitive, the effectiveness of correlation-based neural\nembedding is only empirically validated. This work puts forth a theory-backed\nframework for unsupervised multiview learning. Our development starts with\nproposing a multiview model, where each view is a nonlinear mixture of shared\nand private components. Consequently, the learning problem boils down to\nshared/private component identification and disentanglement. Under this model,\nlatent correlation maximization is shown to guarantee the extraction of the\nshared components across views (up to certain ambiguities). In addition, the\nprivate information in each view can be provably disentangled from the shared\nusing proper regularization design. The method is tested on a series of tasks,\ne.g., downstream clustering, which all show promising performance. Our\ndevelopment also provides a unifying perspective for understanding various DCCA\nand SSL schemes.", "title": "Latent Correlation-Based Multiview Learning and Self-Supervision: A  Unifying Perspective"}, {"link": "https://arxiv.org/abs/2106.07116", "abstract": "Submodular optimization has numerous applications such as crowdsourcing and\nviral marketing. In this paper, we study the fundamental problem of\nnon-negative submodular function maximization subject to a $k$-system\nconstraint, which generalizes many other important constraints in submodular\noptimization such as cardinality constraint, matroid constraint, and\n$k$-extendible system constraint. The existing approaches for this problem\nachieve the best-known approximation ratio of $k+2\\sqrt{k+2}+3$ (for a general\nsubmodular function) based on deterministic algorithmic frameworks. We propose\nseveral randomized algorithms that improve upon the state-of-the-art algorithms\nin terms of approximation ratio and time complexity, both under the\nnon-adaptive setting and the adaptive setting. The empirical performance of our\nalgorithms is extensively evaluated in several applications related to data\nmining and social computing, and the experimental results demonstrate the\nsuperiorities of our algorithms in terms of both utility and efficiency.", "title": "The Power of Randomization: Efficient and Effective Algorithms for  Constrained Submodular Maximization"}, {"link": "https://arxiv.org/abs/2106.07117", "abstract": "Language understanding must identify the logical connections between events\nin a discourse, but core events are often unstated due to their commonsense\nnature. This paper fills in these missing events by generating precondition\nevents. Precondition generation can be framed as a sequence-to-sequence\nproblem: given a target event, generate a possible precondition. However, in\nmost real-world scenarios, an event can have several preconditions, requiring\ndiverse generation -- a challenge for standard seq2seq approaches. We propose\nDiP, a Diverse Precondition generation system that can generate unique and\ndiverse preconditions. DiP uses a generative process with three components --\nan event sampler, a candidate generator, and a post-processor. The event\nsampler provides control codes (precondition triggers) which the candidate\ngenerator uses to focus its generation. Unlike other conditional generation\nsystems, DiP automatically generates control codes without training on diverse\nexamples. Analysis against baselines reveals that DiP improves the diversity of\npreconditions significantly while also generating more preconditions.", "title": "Toward Diverse Precondition Generation"}, {"link": "https://arxiv.org/abs/2106.07125", "abstract": "Policy search reinforcement learning has been drawing much attention as a\nmethod of learning a robot control policy. In particular, policy search using\nsuch non-parametric policies as Gaussian process regression can learn optimal\nactions with high-dimensional and redundant sensors as input. However, previous\nmethods implicitly assume that the optimal action becomes unique for each\nstate. This assumption can severely limit such practical applications as robot\nmanipulations since designing a reward function that appears in only one\noptimal action for complex tasks is difficult. The previous methods might have\ncaused critical performance deterioration because the typical non-parametric\npolicies cannot capture the optimal actions due to their unimodality. We\npropose novel approaches in non-parametric policy searches with multiple\noptimal actions and offer two different algorithms commonly based on a sparse\nGaussian process prior and variational Bayesian inference. The following are\nthe key ideas: 1) multimodality for capturing multiple optimal actions and 2)\nmode-seeking for capturing one optimal action by ignoring the others. First, we\npropose a multimodal sparse Gaussian process policy search that uses multiple\noverlapped GPs as a prior. Second, we propose a mode-seeking sparse Gaussian\nprocess policy search that uses the student-t distribution for a likelihood\nfunction. The effectiveness of those algorithms is demonstrated through\napplications to object manipulation tasks with multiple optimal actions in\nsimulations.", "title": "Variational Policy Search using Sparse Gaussian Process Priors for  Learning Multimodal Optimal Actions"}, {"link": "https://arxiv.org/abs/2106.07127", "abstract": "In order to achieve autonomous vertical wall climbing, the transition phase\nfrom the ground to the wall requires extra consideration inevitably. This paper\nfocuses on the contact sequence planner to transition between flat terrain and\nvertical surfaces for multi-limbed climbing robots. To overcome the transition\nphase, it requires planning both multi-contact and contact wrenches\nsimultaneously which makes it difficult. Instead of using a predetermined\ncontact sequence, we consider various motions on different environment setups\nvia modeling contact constraints and limb switchability as complementarity\nconditions. Two safety factors for toe sliding and motor over-torque are the\nmain tuning parameters for different contact sequences. By solving as a\nnonlinear program (NLP), we can generate several feasible sequences of foot\nplacements and contact forces to avoid failure cases. We verified feasibility\nwith demonstrations on the hardware SiLVIA, a six-legged robot capable of\nvertically climbing between two walls by bracing itself in-between using only\nfriction.", "title": "Transition Motion Planning for Multi-Limbed Vertical Climbing Robots  Using Complementarity Constraints"}, {"link": "https://arxiv.org/abs/2106.07131", "abstract": "Operations in many essential industries including finance and banking are\noften characterized by the need to perform repetitive sequential tasks. Despite\ntheir criticality to the business, workflows are rarely fully automated or even\nformally specified, though there may exist a number of natural language\ndocuments describing these procedures for the employees of the company. Plan\nextraction methods provide us with the possibility of extracting structure\nplans from such natural language descriptions of the plans/workflows, which\ncould then be leveraged by an automated system. In this paper, we investigate\nthe utility of generalized language models in performing such extractions\ndirectly from such texts. Such models have already been shown to be quite\neffective in multiple translation tasks, and our initial results seem to point\nto their effectiveness also in the context of plan extractions. Particularly,\nwe show that GPT-3 is able to generate plan extraction results that are\ncomparable to many of the current state of the art plan extraction methods.", "title": "GPT3-to-plan: Extracting plans from text using GPT-3"}, {"link": "https://arxiv.org/abs/2106.07134", "abstract": "Attribution of paintings is a critical problem in art history. This study\nextends machine learning analysis to surface topography of painted works. A\ncontrolled study of positive attribution was designed with paintings produced\nby a class of art students. The paintings were scanned using a confocal optical\nprofilometer to produce surface data. The surface data were divided into\nvirtual patches and used to train an ensemble of convolutional neural networks\n(CNNs) for attribution. Over a range of patch sizes from 0.5 to 60 mm, the\nresulting attribution was found to be 60 to 96% accurate, and, when comparing\nregions of different color, was nearly twice as accurate as CNNs using color\nimages of the paintings. Remarkably, short length scales, as small as twice a\nbristle diameter, were the key to reliably distinguishing among artists. These\nresults show promise for real-world attribution, particularly in the case of\nworkshop practice.", "title": "Discerning the painter's hand: machine learning on surface topography"}, {"link": "https://arxiv.org/abs/2106.07135", "abstract": "Existing tensor completion formulation mostly relies on partial observations\nfrom a single tensor. However, tensors extracted from real-world data are often\nmore complex due to: (i) Partial observation: Only a small subset (e.g., 5%) of\ntensor elements are available. (ii) Coarse observation: Some tensor modes only\npresent coarse and aggregated patterns (e.g., monthly summary instead of daily\nreports). In this paper, we are given a subset of the tensor and some\naggregated/coarse observations (along one or more modes) and seek to recover\nthe original fine-granular tensor with low-rank factorization. We formulate a\ncoupled tensor completion problem and propose an efficient Multi-resolution\nTensor Completion model (MTC) to solve the problem. Our MTC model explores\ntensor mode properties and leverages the hierarchy of resolutions to\nrecursively initialize an optimization setup, and optimizes on the coupled\nsystem using alternating least squares. MTC ensures low computational and space\ncomplexity. We evaluate our model on two COVID-19 related spatio-temporal\ntensors. The experiments show that MTC could provide 65.20% and 75.79%\npercentage of fitness (PoF) in tensor completion with only 5% fine granular\nobservations, which is 27.96% relative improvement over the best baseline. To\nevaluate the learned low-rank factors, we also design a tensor prediction task\nfor daily and cumulative disease case predictions, where MTC achieves 50% in\nPoF and 30% relative improvements over the best baseline.", "title": "MTC: Multiresolution Tensor Completion from Partial and Coarse  Observations"}, {"link": "https://arxiv.org/abs/2106.07136", "abstract": "This paper reports a CPU-level real-time stereo matching method for surgical\nimages (10 Hz on 640 * 480 image with a single core of i5-9400). The proposed\nmethod is built on the fast ''dense inverse searching'' algorithm, which\nestimates the disparity of the stereo images. The overlapping image patches\n(arbitrary squared image segment) from the images at different scales are\naligned based on the photometric consistency presumption. We propose a Bayesian\nframework to evaluate the probability of the optimized patch disparity at\ndifferent scales. Moreover, we introduce a spatial Gaussian mixed probability\ndistribution to address the pixel-wise probability within the patch. In-vivo\nand synthetic experiments show that our method can handle ambiguities resulted\nfrom the textureless surfaces and the photometric inconsistency caused by the\nLambertian reflectance. Our Bayesian method correctly balances the probability\nof the patch for stereo images at different scales. Experiments indicate that\nthe estimated depth has higher accuracy and fewer outliers than the baseline\nmethods in the surgical scenario.", "title": "Bayesian dense inverse searching algorithm for real-time stereo matching  in minimally invasive surgery"}, {"link": "https://arxiv.org/abs/2106.07137", "abstract": "The huge size of the widely used BERT family models has led to recent efforts\nabout model distillation. The main goal of distillation is to create a\ntask-agnostic pre-trained model that can be fine-tuned on downstream tasks\nwithout fine-tuning its full-sized version. Despite the progress of\ndistillation, to what degree and for what reason a task-agnostic model can be\ncreated from distillation has not been well studied. Also, the mechanisms\nbehind transfer learning of those BERT models are not well investigated either.\nTherefore, this work focuses on analyzing the acceptable deduction when\ndistillation for guiding the future distillation procedure. Specifically, we\nfirst inspect the prunability of the Transformer heads in RoBERTa and ALBERT\nusing their head importance estimation proposed by Michel et al. (2019), and\nthen check the coherence of the important heads between the pre-trained task\nand downstream tasks. Hence, the acceptable deduction of performance on the\npre-trained task when distilling a model can be derived from the results, and\nwe further compare the behavior of the pruned model before and after\nfine-tuning. Our studies provide guidance for future directions about BERT\nfamily model distillation.", "title": "Why Can You Lay Off Heads? Investigating How BERT Heads Transfer"}, {"link": "https://arxiv.org/abs/2106.07139", "abstract": "Large-scale pre-trained models (PTMs) such as BERT and GPT have recently\nachieved great success and become a milestone in the field of artificial\nintelligence (AI). Owing to sophisticated pre-training objectives and huge\nmodel parameters, large-scale PTMs can effectively capture knowledge from\nmassive labeled and unlabeled data. By storing knowledge into huge parameters\nand fine-tuning on specific tasks, the rich knowledge implicitly encoded in\nhuge parameters can benefit a variety of downstream tasks, which has been\nextensively demonstrated via experimental verification and empirical analysis.\nIt is now the consensus of the AI community to adopt PTMs as backbone for\ndownstream tasks rather than learning models from scratch. In this paper, we\ntake a deep look into the history of pre-training, especially its special\nrelation with transfer learning and self-supervised learning, to reveal the\ncrucial position of PTMs in the AI development spectrum. Further, we\ncomprehensively review the latest breakthroughs of PTMs. These breakthroughs\nare driven by the surge of computational power and the increasing availability\nof data, towards four important directions: designing effective architectures,\nutilizing rich contexts, improving computational efficiency, and conducting\ninterpretation and theoretical analysis. Finally, we discuss a series of open\nproblems and research directions of PTMs, and hope our view can inspire and\nadvance the future study of PTMs.", "title": "Pre-Trained Models: Past, Present and Future"}, {"link": "https://arxiv.org/abs/2106.07140", "abstract": "We propose SinIR, an efficient reconstruction-based framework trained on a\nsingle natural image for general image manipulation, including\nsuper-resolution, editing, harmonization, paint-to-image, photo-realistic style\ntransfer, and artistic style transfer. We train our model on a single image\nwith cascaded multi-scale learning, where each network at each scale is\nresponsible for image reconstruction. This reconstruction objective greatly\nreduces the complexity and running time of training, compared to the GAN\nobjective. However, the reconstruction objective also exacerbates the output\nquality. Therefore, to solve this problem, we further utilize simple random\npixel shuffling, which also gives control over manipulation, inspired by the\nDenoising Autoencoder. With quantitative evaluation, we show that SinIR has\ncompetitive performance on various image manipulation tasks. Moreover, with a\nmuch simpler training objective (i.e., reconstruction), SinIR is trained 33.5\ntimes faster than SinGAN (for 500 X 500 images) that solves similar tasks. Our\ncode is publicly available at github.com/YooJiHyeong/SinIR.", "title": "SinIR: Efficient General Image Manipulation with Single Image  Reconstruction"}, {"link": "https://arxiv.org/abs/2106.07141", "abstract": "Although the adoption rate of deep neural networks (DNNs) has tremendously\nincreased in recent years, a solution for their vulnerability against\nadversarial examples has not yet been found. As a result, substantial research\nefforts are dedicated to fix this weakness, with many studies typically using a\nsubset of source images to generate adversarial examples, treating every image\nin this subset as equal. We demonstrate that, in fact, not every source image\nis equally suited for this kind of assessment. To do so, we devise a\nlarge-scale model-to-model transferability scenario for which we meticulously\nanalyze the properties of adversarial examples, generated from every suitable\nsource image in ImageNet by making use of two of the most frequently deployed\nattacks. In this transferability scenario, which involves seven distinct DNN\nmodels, including the recently proposed vision transformers, we reveal that it\nis possible to have a difference of up to $12.5\\%$ in model-to-model\ntransferability success, $1.01$ in average $L_2$ perturbation, and $0.03$\n($8/225$) in average $L_{\\infty}$ perturbation when $1,000$ source images are\nsampled randomly among all suitable candidates. We then take one of the first\nsteps in evaluating the robustness of images used to create adversarial\nexamples, proposing a number of simple but effective methods to identify\nunsuitable source images, thus making it possible to mitigate extreme cases in\nexperimentation and support high-quality benchmarking.", "title": "Selection of Source Images Heavily Influences the Effectiveness of  Adversarial Attacks"}, {"link": "https://arxiv.org/abs/2106.07152", "abstract": "A $k$-additive spanner of a graph is a subgraph that preserves the distance\nbetween any two nodes up to a total additive error of $+k$. Efficient\nalgorithms have been devised for constructing 2 [Aingworth et al. SIAM '99], 6\n[Baswana et al. ACM '10, Woodruff ICALP '13], and 8-additive spanners [Knudsen\n'17], but efficiency hasn't been studied for 4-additive spanner constructions.\nIn this paper we present a modification of Chechik's 4-additive spanner\nconstruction [Chechik SODA '13] that produces a 4-additive spanner on\n$\\widetilde{O}(n^{7/5})$ edges, with an improved runtime of\n$\\widetilde{O}(mn^{3/5})$ from $O(mn)$.", "title": "Fast Construction of 4-Additive Spanners"}, {"link": "https://arxiv.org/abs/2106.07153", "abstract": "We study private synthetic data generation for query release, where the goal\nis to construct a sanitized version of a sensitive dataset, subject to\ndifferential privacy, that approximately preserves the answers to a large\ncollection of statistical queries. We first present an algorithmic framework\nthat unifies a long line of iterative algorithms in the literature. Under this\nframework, we propose two new methods. The first method, private entropy\nprojection (PEP), can be viewed as an advanced variant of MWEM that adaptively\nreuses past query measurements to boost accuracy. Our second method, generative\nnetworks with the exponential mechanism (GEM), circumvents computational\nbottlenecks in algorithms such as MWEM and PEP by optimizing over generative\nmodels parameterized by neural networks, which capture a rich family of\ndistributions while enabling fast gradient-based optimization. We demonstrate\nthat PEP and GEM empirically outperform existing algorithms. Furthermore, we\nshow that GEM nicely incorporates prior information from public data while\novercoming limitations of PMW^Pub, the existing state-of-the-art method that\nalso leverages public data.", "title": "Iterative Methods for Private Synthetic Data: Unifying Framework and New  Methods"}, {"link": "https://arxiv.org/abs/2106.07154", "abstract": "We assess the performance of a set of local time-stepping schemes for the\nshallow water equations implemented in the global ocean model MPAS-Ocean. The\navailability of local time-stepping tools is of major relevance for ocean codes\nsuch as MPAS-Ocean, which rely on a multi-resolution approach to perform\nregional grid refinement, for instance in proximity of the coast. In presence\nof variable resolution, the size of the time-step of explicit numerical\nintegrators is bounded above by the size of the smallest cell on the grid,\naccording to the Courant-Friedrichs-Lewy (CFL) condition. This constraint means\nthat the time-step size used in low resolution regions must be the same as the\none used in high resolution regions, resulting in an unnecessary computational\neffort. Local time-stepping, on the other hand, allows one to select different\ntime-step sizes according to local, rather than global, CFL conditions,\nresulting in a more tailored integration process and reduced computational\ntimes. The present work is a preliminary but necessary effort aimed at paving\nthe way for a more comprehensive work on local time-stepping for the primitive\nequation set with realistic geography.", "title": "Local time stepping for the shallow water equations in MPAS-Ocean"}, {"link": "https://arxiv.org/abs/2106.07155", "abstract": "Federated learning (FL) is a prevailing distributed learning paradigm, where\na large number of workers jointly learn a model without sharing their training\ndata. However, high communication costs could arise in FL due to large-scale\n(deep) learning models and bandwidth-constrained connections. In this paper, we\nintroduce a communication-efficient algorithmic framework called CFedAvg for FL\nwith non-i.i.d. datasets, which works with general (biased or unbiased)\nSNR-constrained compressors. We analyze the convergence rate of CFedAvg for\nnon-convex functions with constant and decaying learning rates. The CFedAvg\nalgorithm can achieve an $\\mathcal{O}(1 / \\sqrt{mKT} + 1 / T)$ convergence rate\nwith a constant learning rate, implying a linear speedup for convergence as the\nnumber of workers increases, where $K$ is the number of local steps, $T$ is the\nnumber of total communication rounds, and $m$ is the total worker number. This\nmatches the convergence rate of distributed/federated learning without\ncompression, thus achieving high communication efficiency while not sacrificing\nlearning accuracy in FL. Furthermore, we extend CFedAvg to cases with\nheterogeneous local steps, which allows different workers to perform a\ndifferent number of local steps to better adapt to their own circumstances. The\ninteresting observation in general is that the noise/variance introduced by\ncompressors does not affect the overall convergence rate order for non-i.i.d.\nFL. We verify the effectiveness of our CFedAvg algorithm on three datasets with\ntwo gradient compression schemes of different compression ratios.", "title": "CFedAvg: Achieving Efficient Communication and Fast Convergence in  Non-IID Federated Learning"}, {"link": "https://arxiv.org/abs/2106.07156", "abstract": "High-dimensional observations are a major challenge in the application of\nmodel-based reinforcement learning (MBRL) to real-world environments. To handle\nhigh-dimensional sensory inputs, existing approaches use representation\nlearning to map high-dimensional observations into a lower-dimensional latent\nspace that is more amenable to dynamics estimation and planning. In this work,\nwe present an information-theoretic approach that employs temporal predictive\ncoding to encode elements in the environment that can be predicted across time.\nSince this approach focuses on encoding temporally-predictable information, we\nimplicitly prioritize the encoding of task-relevant components over nuisance\ninformation within the environment that are provably task-irrelevant. By\nlearning this representation in conjunction with a recurrent state space model,\nwe can then perform planning in latent space. We evaluate our model on a\nchallenging modification of standard DMControl tasks where the background is\nreplaced with natural videos that contain complex but irrelevant information to\nthe planning task. Our experiments show that our model is superior to existing\nmethods in the challenging complex-background setting while remaining\ncompetitive with current state-of-the-art models in the standard setting.", "title": "Temporal Predictive Coding For Model-Based Planning In Latent Space"}, {"link": "https://arxiv.org/abs/2106.07157", "abstract": "Rigid spherical microphone arrays (RSMAs) have been widely used in ambisonics\nsound field recording. While it is desired to combine the information captured\nby a grid of densely arranged RSMAs for expanding the area of accurate\nreconstruction, or sweet-spots, this is not trivial due to inter-array\ninterference. Here we propose multiple scattering ambisonics, a method for\nthree-dimensional ambisonics sound field recording using multiple acoustically\ninteracting RSMAs. Numerical experiments demonstrate the sweet-spot expansion\nrealized by the proposed method. The proposed method can be used with existing\nRSMAs as building blocks and opens possibilities including higher\ndegrees-of-freedom spatial audio.", "title": "Multiple scattering ambisonics: three-dimensional sound foeld estimation  using interacting spheres"}, {"link": "https://arxiv.org/abs/2106.07158", "abstract": "Anonymous access authentication schemes provide users with massive\napplication services while protecting the privacy of users' identities. The\nidentity protection schemes in 3G and 4G are not suitable for 5G anonymous\naccess authentication due to complex computation and pseudonym asynchrony. In\nthis paper, we consider mobile devices with limited resources in the 5G network\nand propose an anonymous access authentication scheme without the Public Key\nInfrastructure. The anonymous access authentication scheme provides users with\nvariable shard pseudonyms to protect users' identities asynchronously. With the\nvariable shared pseudonym, our scheme can ensure user anonymity and resist the\nmark attack, a novel attack aimed at the basic k-pseudonym scheme. Finally, we\nanalyze the scheme with BAN logic analysis and verify the user anonymity.", "title": "A Novel Variable K-Pseudonym Scheme Applied to 5G Anonymous Access  Authentication"}, {"link": "https://arxiv.org/abs/2106.07159", "abstract": "Instance segmentation is of great importance for many biological\napplications, such as study of neural cell interactions, plant phenotyping, and\nquantitatively measuring how cells react to drug treatment. In this paper, we\npropose a novel box-based instance segmentation method. Box-based instance\nsegmentation methods capture objects via bounding boxes and then perform\nindividual segmentation within each bounding box region. However, existing\nmethods can hardly differentiate the target from its neighboring objects within\nthe same bounding box region due to their similar textures and low-contrast\nboundaries. To deal with this problem, in this paper, we propose an\nobject-guided instance segmentation method. Our method first detects the center\npoints of the objects, from which the bounding box parameters are then\npredicted. To perform segmentation, an object-guided coarse-to-fine\nsegmentation branch is built along with the detection branch. The segmentation\nbranch reuses the object features as guidance to separate target object from\nthe neighboring ones within the same bounding box region. To further improve\nthe segmentation quality, we design an auxiliary feature refinement module that\ndensely samples and refines point-wise features in the boundary regions.\nExperimental results on three biological image datasets demonstrate the\nadvantages of our method. The code will be available at\nhttps://github.com/yijingru/ObjGuided-Instance-Segmentation.", "title": "Object-Guided Instance Segmentation With Auxiliary Feature Refinement  for Biological Images"}, {"link": "https://arxiv.org/abs/2106.07160", "abstract": "We study automated intrusion prevention using reinforcement learning. In a\nnovel approach, we formulate the problem of intrusion prevention as an optimal\nstopping problem. This formulation allows us insight into the structure of the\noptimal policies, which turn out to be threshold based. Since the computation\nof the optimal defender policy using dynamic programming is not feasible for\npractical cases, we approximate the optimal policy through reinforcement\nlearning in a simulation environment. To define the dynamics of the simulation,\nwe emulate the target infrastructure and collect measurements. Our evaluations\nshow that the learned policies are close to optimal and that they indeed can be\nexpressed using thresholds.", "title": "Learning Intrusion Prevention Policies through Optimal Stopping"}, {"link": "https://arxiv.org/abs/2106.07161", "abstract": "Simultaneous trajectory prediction for multiple heterogeneous traffic\nparticipants is essential for the safe and efficient operation of connected\nautomated vehicles under complex driving situations in the real world. The\nmulti-agent prediction task is challenging, as the motions of traffic\nparticipants are affected by many factors, including their individual dynamics,\ntheir interactions with surrounding agents, the traffic infrastructures, and\nthe number and modalities of the target agents. To further advance the\ntrajectory prediction techniques, in this work we propose a three-channel\nframework together with a novel Heterogeneous Edge-enhanced graph ATtention\nnetwork (HEAT), which is able to deal with the heterogeneity of the target\nagents and traffic participants involved. Specifically, the agent's dynamics\nare extracted from their historical states using type-specific encoders. The\ninter-agent interactions are represented with a directed edge-featured\nheterogeneous graph, and then interaction features are extracted using the\nproposed HEAT network. Besides, the map features are shared across all agents\nby introducing a selective gate mechanism. And finally, the trajectories of\nmulti-agent are executed simultaneously. Validations using both urban and\nhighway driving datasets show that the proposed model can realize simultaneous\ntrajectory predictions for multiple agents under complex traffic situations,\nand achieve state-of-the-art performance with respect to prediction accuracy,\ndemonstrating its feasibility and effectiveness.", "title": "Heterogeneous Edge-Enhanced Graph Attention Network For Multi-Agent  Trajectory Prediction"}, {"link": "https://arxiv.org/abs/2106.07162", "abstract": "Modern neural networks obtain information about the problem and calculate the\noutput solely from the input values. We argue that it is not always optimal,\nand the network's performance can be significantly improved by augmenting it\nwith a query mechanism that allows the network to make several solution trials\nat run time and get feedback on the loss value on each trial. To demonstrate\nthe capabilities of the query mechanism, we formulate an unsupervised (not\ndependant on labels) loss function for Boolean Satisfiability Problem (SAT) and\ntheoretically show that it allows the network to extract rich information about\nthe problem. We then propose a neural SAT solver with a query mechanism called\nQuerySAT and show that it outperforms the neural baseline on a wide range of\nSAT tasks and the classical baselines on SHA-1 preimage attack and 3-SAT task.", "title": "Goal-Aware Neural SAT Solver"}, {"link": "https://arxiv.org/abs/2106.07165", "abstract": "Deep models trained on large-scale RGB image datasets have shown tremendous\nsuccess. It is important to apply such deep models to real-world problems.\nHowever, these models suffer from a performance bottleneck under illumination\nchanges. Thermal IR cameras are more robust against such changes, and thus can\nbe very useful for the real-world problems. In order to investigate efficacy of\ncombining feature-rich visible spectrum and thermal image modalities, we\npropose an unsupervised domain adaptation method which does not require\nRGB-to-thermal image pairs. We employ large-scale RGB dataset MS-COCO as source\ndomain and thermal dataset FLIR ADAS as target domain to demonstrate results of\nour method. Although adversarial domain adaptation methods aim to align the\ndistributions of source and target domains, simply aligning the distributions\ncannot guarantee perfect generalization to the target domain. To this end, we\npropose a self-training guided adversarial domain adaptation method to promote\ngeneralization capabilities of adversarial domain adaptation methods. To\nperform self-training, pseudo labels are assigned to the samples on the target\nthermal domain to learn more generalized representations for the target domain.\nExtensive experimental analyses show that our proposed method achieves better\nresults than the state-of-the-art adversarial domain adaptation methods. The\ncode and models are publicly available.", "title": "Self-training Guided Adversarial Domain Adaptation For Thermal Imagery"}, {"link": "https://arxiv.org/abs/2106.07166", "abstract": "In this technical report, we present our solution to localize a\nspatio-temporal person in an untrimmed video based on a sentence. We achieve\nthe second vIOU(0.30025) in the HC-STVG track of the 3rd Person in Context(PIC)\nChallenge. Our solution contains three parts: 1) human attributes information\nis extracted from the sentence, it is helpful to filter out tube proposals in\nthe testing phase and supervise our classifier to learn appearance information\nin the training phase. 2) we detect humans with YoloV5 and track humans based\non the DeepSort framework but replace the original ReID network with FastReID.\n3) a visual transformer is used to extract cross-modal representations for\nlocalizing a spatio-temporal tube of the target person.", "title": "2rd Place Solutions in the HC-STVG track of Person in Context Challenge  2021"}, {"link": "https://arxiv.org/abs/2106.07167", "abstract": "We propose a new end-to-end neural diarization (EEND) system that is based on\nConformer, a recently proposed neural architecture that combines convolutional\nmappings and Transformer to model both local and global dependencies in speech.\nWe first show that data augmentation and convolutional subsampling layers\nenhance the original self-attentive EEND in the Transformer-based EEND, and\nthen Conformer gives an additional gain over the Transformer-based EEND.\nHowever, we notice that the Conformer-based EEND does not generalize as well\nfrom simulated to real conversation data as the Transformer-based model. This\nleads us to quantify the mismatch between simulated data and real speaker\nbehavior in terms of temporal statistics reflecting turn-taking between\nspeakers, and investigate its correlation with diarization error. By mixing\nsimulated and real data in EEND training, we mitigate the mismatch further,\nwith Conformer-based EEND achieving 24% error reduction over the baseline\nSA-EEND system, and 10% improvement over the best augmented Transformer-based\nsystem, on two-speaker CALLHOME data.", "title": "End-to-end Neural Diarization: From Transformer to Conformer"}, {"link": "https://arxiv.org/abs/2106.07171", "abstract": "A central goal of machine learning is to learn robust representations that\ncapture the causal relationship between inputs features and output labels.\nHowever, minimizing empirical risk over finite or biased datasets often results\nin models latching on to spurious correlations between the training\ninput/output pairs that are not fundamental to the problem at hand. In this\npaper, we define and analyze robust and spurious representations using the\ninformation-theoretic concept of minimal sufficient statistics. We prove that\neven when there is only bias of the input distribution (i.e. covariate shift),\nmodels can still pick up spurious features from their training data. Group\ndistributionally robust optimization (DRO) provides an effective tool to\nalleviate covariate shift by minimizing the worst-case training loss over a set\nof pre-defined groups. Inspired by our analysis, we demonstrate that group DRO\ncan fail when groups do not directly account for various spurious correlations\nthat occur in the data. To address this, we further propose to minimize the\nworst-case losses over a more flexible set of distributions that are defined on\nthe joint distribution of groups and instances, instead of treating each group\nas a whole at optimization time. Through extensive experiments on one image and\ntwo language tasks, we show that our model is significantly more robust than\ncomparable baselines under various partitions. Our code is available at\nhttps://github.com/violet-zct/group-conditional-DRO.", "title": "Examining and Combating Spurious Features under Distribution Shift"}, {"link": "https://arxiv.org/abs/2106.07172", "abstract": "Spiking neural networks (SNNs) have been gaining interest as energy-efficient\nalternatives of conventional artificial neural networks (ANNs) due to their\nevent-driven computation. Considering the future deployment of SNN models to\nconstrained neuromorphic devices, many studies have applied techniques\noriginally used for ANN model compression, such as network quantization,\npruning, and knowledge distillation, to SNNs. Among them, existing works on\nknowledge distillation reported accuracy improvements of student SNN model.\nHowever, analysis on energy efficiency, which is also an important feature of\nSNN, was absent. In this paper, we thoroughly analyze the performance of the\ndistilled SNN model in terms of accuracy and energy efficiency. In the process,\nwe observe a substantial increase in the number of spikes, leading to energy\ninefficiency, when using the conventional knowledge distillation methods. Based\non this analysis, to achieve energy efficiency, we propose a novel knowledge\ndistillation method with heterogeneous temperature parameters. We evaluate our\nmethod on two different datasets and show that the resulting SNN student\nsatisfies both accuracy improvement and reduction of the number of spikes. On\nMNIST dataset, our proposed student SNN achieves up to 0.09% higher accuracy\nand produces 65% less spikes compared to the student SNN trained with\nconventional knowledge distillation method. We also compare the results with\nother SNN compression techniques and training methods.", "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks"}, {"link": "https://arxiv.org/abs/2106.07174", "abstract": "Weakly supervised question answering usually has only the final answers as\nsupervision signals while the correct solutions to derive the answers are not\nprovided. This setting gives rise to the spurious solution problem: there may\nexist many spurious solutions that coincidentally derive the correct answer,\nbut training on such solutions can hurt model performance (e.g., producing\nwrong solutions or answers). For example, for discrete reasoning tasks as on\nDROP, there may exist many equations to derive a numeric answer, and typically\nonly one of them is correct. Previous learning methods mostly filter out\nspurious solutions with heuristics or using model confidence, but do not\nexplicitly exploit the semantic correlations between a question and its\nsolution. In this paper, to alleviate the spurious solution problem, we propose\nto explicitly exploit such semantic correlations by maximizing the mutual\ninformation between question-answer pairs and predicted solutions. Extensive\nexperiments on four question answering datasets show that our method\nsignificantly outperforms previous learning methods in terms of task\nperformance and is more effective in training models to produce correct\nsolutions.", "title": "A Mutual Information Maximization Approach for the Spurious Solution  Problem in Weakly Supervised Question Answering"}, {"link": "https://arxiv.org/abs/2106.07176", "abstract": "The core of a self-supervised learning method for pre-training language\nmodels includes the design of appropriate data augmentation and corresponding\npre-training task(s). Most data augmentations in language model pre-training\nare context-independent. The seminal contextualized augmentation recently\nproposed by the ELECTRA requires a separate generator, which leads to extra\ncomputation cost as well as the challenge in adjusting the capability of its\ngenerator relative to that of the other model component(s). We propose a\nself-augmented strategy (SAS) that uses a single forward pass through the model\nto augment the input data for model training in the next epoch. Essentially our\nstrategy eliminates a separate generator network and uses only one network to\ngenerate the data augmentation and undertake two pre-training tasks (the MLM\ntask and the RTD task) jointly, which naturally avoids the challenge in\nadjusting the generator's capability as well as reduces the computation cost.\nAdditionally, our SAS is a general strategy such that it can seamlessly\nincorporate many new techniques emerging recently or in the future, such as the\ndisentangled attention mechanism recently proposed by the DeBERTa model. Our\nexperiments show that our SAS is able to outperform the ELECTRA and other\nstate-of-the-art models in the GLUE tasks with the same or less computation\ncost.", "title": "SAS: Self-Augmented Strategy for Language Model Pre-training"}, {"link": "https://arxiv.org/abs/2106.07178", "abstract": "Anomalies represent rare observations (e.g., data records or events) that are\ndeviating significantly from others. Over the last forty years, researches on\nanomalies have received great interests because of their significance in many\ndisciplines (e.g., computer science, chemistry, and biology). Anomaly\ndetection, which aims to identify these rare observations, is among the most\nvital tasks and has shown its power in preventing detrimental events, such as\nfinancial fraud and network intrusion, from happening. The detection task is\ntypically solved by detecting outlying data points in the features space and\ninherently overlooks the structural information in real-world data. Graphs have\nbeen prevalently used to preserve the structural information, and this raises\nthe graph anomaly detection problem - identifying anomalous graph objects\n(i.e., nodes, edges and sub-graphs). However, conventional anomaly detection\ntechniques cannot well solve this problem because of the complexity of graph\ndata (e.g., irregular structures, non-independent and large-scale). For the\naptitudes of deep learning in breaking these limitations, graph anomaly\ndetection with deep learning has received intensified studies recently. In this\nsurvey, we aim to provide a systematic and comprehensive review of the\ncontemporary deep learning techniques for graph anomaly detection.\nSpecifically, our categorization follows a task-driven strategy and classifies\nexisting works according to the anomalous graph objects they can detect. We\nespecially focus on the motivations, key intuitions and technical details of\nexisting works. We also summarize open-sourced implementations, public\ndatasets, and commonly-used evaluation metrics for future studies. Finally, we\nhighlight twelve future research directions according to our survey results\ncovering emerging problems introduced by graph data, anomaly detection and real\napplications.", "title": "A Comprehensive Survey on Graph Anomaly Detection with Deep Learning"}, {"link": "https://arxiv.org/abs/2106.07182", "abstract": "The recent availability of commercial-off-the-shelf (COTS) legged robot\nplatforms have opened up new opportunities in deploying legged systems into\ndifferent scenarios. While the main advantage of legged robots is their ability\nto traverse unstructured terrain, there are still large gaps between what robot\nplatforms can achieve and their animal counterparts. Therefore, when deploying\nas part of a heterogeneous robot team of different platforms, it is beneficial\nto understand the different scenarios where a legged platform would perform\nbetter than a wheeled, tracked or aerial platform. Two COTS quadruped robots,\nGhost Robotics' Vision 60 and Boston Dynamics' Spot, were deployed into a\nheterogeneous team. A description of some of the challenges faced while\nintegrating the platforms, as well as some experiments in traversing different\nterrains are provided to give insight into the real-world deployment of legged\nrobots.", "title": "Deploying COTS Legged Robot Platforms into a Heterogeneous Robot Team"}, {"link": "https://arxiv.org/abs/2106.07185", "abstract": "In recent years, the brain and cognitive sciences have made great strides\ndeveloping a mechanistic understanding of object recognition in mature brains.\nDespite this progress, fundamental questions remain about the origins and\ncomputational foundations of object recognition. What learning algorithms\nunderlie object recognition in newborn brains? Since newborn animals learn\nlargely through unsupervised learning, we explored whether unsupervised\nlearning algorithms can be used to predict the view-invariant object\nrecognition behavior of newborn chicks. Specifically, we used feature\nrepresentations derived from unsupervised deep neural networks (DNNs) as inputs\nto cognitive models of categorization. We show that features derived from\nunsupervised DNNs make competitive predictions about chick behavior compared to\nsupervised features. More generally, we argue that linking controlled-rearing\nstudies to image-computable DNN models opens new experimental avenues for\nstudying the origins and computational basis of object recognition in newborn\nanimals.", "title": "Modeling Object Recognition in Newborn Chicks using Deep Neural Networks"}, {"link": "https://arxiv.org/abs/2106.07186", "abstract": "Commercial application of facial recognition demands robustness to a variety\nof challenges such as illumination, occlusion, spoofing, disguise, etc.\nDisguised face recognition is one of the emerging issues for access control\nsystems, such as security checkpoints at the borders. However, the lack of\navailability of face databases with a variety of disguise addons limits the\ndevelopment of academic research in the area. In this paper, we present a\nmultimodal disguised face dataset to facilitate the disguised face recognition\nresearch. The presented database contains 8 facial add-ons and 7 additional\ncombinations of these add-ons to create a variety of disguised face images.\nEach facial image is captured in visible, visible plus infrared, infrared, and\nthermal spectra. Specifically, the database contains 100 subjects divided into\nsubset-A (30 subjects, 1 image per modality) and subset-B (70 subjects, 5 plus\nimages per modality). We also present baseline face detection results performed\non the proposed database to provide reference results and compare the\nperformance in different modalities. Qualitative and quantitative analysis is\nperformed to evaluate the challenging nature of disguise addons. The dataset\nwill be publicly available with the acceptance of the research article. The\ndatabase is available at: https://github.com/usmancheema89/SejongFaceDatabase.", "title": "Sejong Face Database: A Multi-Modal Disguise Face Database"}, {"link": "https://arxiv.org/abs/2106.07189", "abstract": "This paper considers an arbitrarily-varying fading channel consisting of one\ntransmitter, one receiver and an arbitrarily varying adversary. The channel is\nassumed to have additive Gaussian noise and fast fading of the gain from the\nlegitimate user to the receiver. We study four variants of the problem\ndepending on whether the transmitter and/or adversary have access to the fading\ngains; we assume the receiver always knows the fading gains. In two variants\nthe adversary does not have access to the gains, we show that the capacity\ncorresponds to the capacity of a standard point-to-point fading channel with\nincreased noise variance. The capacity of the other two cases, in which the\nadversary has knowledge of the channel gains, are determined by the worst-case\nnoise variance as a function of the channel gain subject to the jammer's power\nconstraint; if the jammer has enough power, then it can imitate the legitimate\nuser's channel, causing the capacity to drop to zero. We also show that having\nthe channel gains causally or non-causally at the encoder and/or the adversary\ndoes not change the capacity, except for the case where all parties know the\nchannel gains. In this case, if the transmitter knows the gains non-causally,\nwhile the adversary knows the gains causally, then it is possible for the\nlegitimate users to keep a secret from the adversary. We show that in this case\nthe capacity is always positive.", "title": "Capacity of Gaussian Arbitrarily-Varying Fading Channels"}, {"link": "https://arxiv.org/abs/2106.07190", "abstract": "Video super-resolution (VSR) aims to estimate a high-resolution (HR) frame\nfrom a low-resolution (LR) frames. The key challenge for VSR lies in the\neffective exploitation of spatial correlation in an intra-frame and temporal\ndependency between consecutive frames. However, most of the previous methods\ntreat different types of the spatial features identically and extract spatial\nand temporal features from the separated modules. It leads to lack of obtaining\nmeaningful information and enhancing the fine details. In VSR, there are three\ntypes of temporal modeling frameworks: 2D convolutional neural networks (CNN),\n3D CNN, and recurrent neural networks (RNN). Among them, the RNN-based approach\nis suitable for sequential data. Thus the SR performance can be greatly\nimproved by using the hidden states of adjacent frames. However, at each of\ntime step in a recurrent structure, the RNN-based previous works utilize the\nneighboring features restrictively. Since the range of accessible motion per\ntime step is narrow, there are still limitations to restore the missing details\nfor dynamic or large motion. In this paper, we propose a group-based\nbi-directional recurrent wavelet neural networks (GBR-WNN) to exploit the\nsequential data and spatio-temporal information effectively for VSR. The\nproposed group-based bi-directional RNN (GBR) temporal modeling framework is\nbuilt on the well-structured process with the group of pictures (GOP). We\npropose a temporal wavelet attention (TWA) module, in which attention is\nadopted for both spatial and temporal features. Experimental results\ndemonstrate that the proposed method achieves superior performance compared\nwith state-of-the-art methods in both of quantitative and qualitative\nevaluations.", "title": "Group-based Bi-Directional Recurrent Wavelet Neural Networks for Video  Super-Resolution"}, {"link": "https://arxiv.org/abs/2106.07192", "abstract": "The advent of large pre-trained language models has made it possible to make\nhigh-quality predictions on how to add or change a sentence in a document.\nHowever, the high branching factor inherent to text generation impedes the\nability of even the strongest language models to offer useful editing\nsuggestions at a more global or document level. We introduce a new task,\ndocument sketching, which involves generating entire draft documents for the\nwriter to review and revise. These drafts are built from sets of documents that\noverlap in form - sharing large segments of potentially reusable text - while\ndiverging in content. To support this task, we introduce a Wikipedia-based\ndataset of analogous documents and investigate the application of weakly\nsupervised methods, including use of a transformer-based mixture of experts,\ntogether with reinforcement learning. We report experiments using automated and\nhuman evaluation methods and discuss relative merits of these models.", "title": "Automatic Document Sketching: Generating Drafts from Analogous Texts"}, {"link": "https://arxiv.org/abs/2106.07193", "abstract": "Unsupervised learning of the Dawid-Skene (D&S) model from noisy, incomplete\nand crowdsourced annotations has been a long-standing challenge, and is a\ncritical step towards reliably labeling massive data. A recent work takes a\ncoupled nonnegative matrix factorization (CNMF) perspective, and shows\nappealing features: It ensures the identifiability of the D\\&S model and enjoys\nlow sample complexity, as only the estimates of the co-occurrences of annotator\nlabels are involved. However, the identifiability holds only when certain\nsomewhat restrictive conditions are met in the context of crowdsourcing.\nOptimizing the CNMF criterion is also costly -- and convergence assurances are\nelusive. This work recasts the pairwise co-occurrence based D&S model learning\nproblem as a symmetric NMF (SymNMF) problem -- which offers enhanced\nidentifiability relative to CNMF. In practice, the SymNMF model is often\n(largely) incomplete, due to the lack of co-labeled items by some annotators.\nTwo lightweight algorithms are proposed for co-occurrence imputation. Then, a\nlow-complexity shifted rectified linear unit (ReLU)-empowered SymNMF algorithm\nis proposed to identify the D&S model. Various performance characterizations\n(e.g., missing co-occurrence recoverability, stability, and convergence) and\nevaluations are also presented.", "title": "Crowdsourcing via Annotator Co-occurrence Imputation and Provable  Symmetric Nonnegative Matrix Factorization"}, {"link": "https://arxiv.org/abs/2106.07197", "abstract": "Recently directed acyclic graph (DAG) structure learning is formulated as a\nconstrained continuous optimization problem with continuous acyclicity\nconstraints and was solved iteratively through subproblem optimization. To\nfurther improve efficiency, we propose a novel learning framework to model and\nlearn the weighted adjacency matrices in the DAG space directly. Specifically,\nwe first show that the set of weighted adjacency matrices of DAGs are\nequivalent to the set of weighted gradients of graph potential functions, and\none may perform structure learning by searching in this equivalent set of DAGs.\nTo instantiate this idea, we propose a new algorithm, DAG-NoCurl, which solves\nthe optimization problem efficiently with a two-step procedure: 1) first we\nfind an initial cyclic solution to the optimization problem, and 2) then we\nemploy the Hodge decomposition of graphs and learn an acyclic graph by\nprojecting the cyclic graph to the gradient of a potential function.\nExperimental studies on benchmark datasets demonstrate that our method provides\ncomparable accuracy but better efficiency than baseline DAG structure learning\nmethods on both linear and generalized structural equation models, often by\nmore than one order of magnitude.", "title": "DAGs with No Curl: An Efficient DAG Structure Learning Approach"}, {"link": "https://arxiv.org/abs/2106.07200", "abstract": "Traditionally, promoted by the internet companies, continuous delivery is\nmore and more appealing to industries which develop systems with\nsafety-critical functions. Since safety-critical systems must meet regulatory\nrequirements and require specific safety assessment processes in addition to\nthe normal development steps, enabling continuous delivery of software in\nsafety-critical systems requires the automation of the safety assessment\nprocess in the delivery pipeline. In this paper, we outline a continuous\ndelivery pipeline for realizing continuous safety assessment in\nsoftware-intensive safety-critical systems based on model-based safety\nassessment methods.", "title": "Towards Continuous Safety Assessment in Context of DevOps"}, {"link": "https://arxiv.org/abs/2106.07203", "abstract": "Designing provably efficient algorithms with general function approximation\nis an important open problem in reinforcement learning. Recently, Wang et\nal.~[2020c] establish a value-based algorithm with general function\napproximation that enjoys\n$\\widetilde{O}(\\mathrm{poly}(dH)\\sqrt{K})$\\footnote{Throughout the paper, we\nuse $\\widetilde{O}(\\cdot)$ to suppress logarithm factors. } regret bound, where\n$d$ depends on the complexity of the function class, $H$ is the planning\nhorizon, and $K$ is the total number of episodes. However, their algorithm\nrequires $\\Omega(K)$ computation time per round, rendering the algorithm\ninefficient for practical use. In this paper, by applying online sub-sampling\ntechniques, we develop an algorithm that takes\n$\\widetilde{O}(\\mathrm{poly}(dH))$ computation time per round on average, and\nenjoys nearly the same regret bound. Furthermore, the algorithm achieves low\nswitching cost, i.e., it changes the policy only\n$\\widetilde{O}(\\mathrm{poly}(dH))$ times during its execution, making it\nappealing to be implemented in real-life scenarios. Moreover, by using an\nupper-confidence based exploration-driven reward function, the algorithm\nprovably explores the environment in the reward-free setting. In particular,\nafter $\\widetilde{O}(\\mathrm{poly}(dH))/\\epsilon^2$ rounds of exploration, the\nalgorithm outputs an $\\epsilon$-optimal policy for any given reward function.", "title": "Online Sub-Sampling for Reinforcement Learning with General Function  Approximation"}, {"link": "https://arxiv.org/abs/2106.07204", "abstract": "Person re-identification (re-ID) has received great success with the\nsupervised learning methods. However, the task of unsupervised cross-domain\nre-ID is still challenging. In this paper, we propose a Hard Samples\nRectification (HSR) learning scheme which resolves the weakness of original\nclustering-based methods being vulnerable to the hard positive and negative\nsamples in the target unlabelled dataset. Our HSR contains two parts, an\ninter-camera mining method that helps recognize a person under different views\n(hard positive) and a part-based homogeneity technique that makes the model\ndiscriminate different persons but with similar appearance (hard negative). By\nrectifying those two hard cases, the re-ID model can learn effectively and\nachieve promising results on two large-scale benchmarks.", "title": "Hard Samples Rectification for Unsupervised Cross-domain Person  Re-identification"}, {"link": "https://arxiv.org/abs/2106.07207", "abstract": "Advanced large-scale neural language models have led to significant success\nin many language generation tasks. However, the most commonly used training\nobjective, Maximum Likelihood Estimation (MLE), has been shown problematic,\nwhere the trained model prefers using dull and repetitive phrases. In this\nwork, we introduce ScaleGrad, a modification straight to the gradient of the\nloss function, to remedy the degeneration issue of the standard MLE objective.\nBy directly maneuvering the gradient information, ScaleGrad makes the model\nlearn to use novel tokens. Empirical results show the effectiveness of our\nmethod not only in open-ended generation, but also in directed generation\ntasks. With the simplicity in architecture, our method can serve as a general\ntraining objective that is applicable to most of the neural text generation\ntasks.", "title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text  Generation"}, {"link": "https://arxiv.org/abs/2106.07211", "abstract": "This study aims at making the architecture search process more adaptive for\none-shot or online training. It is extended from the existing study on\ndifferentiable neural architecture search, and we made the backbone\narchitecture transformable rather than fixed during the training process. As is\nknown, differentiable neural architecture search (DARTS) requires a pre-defined\nover-parameterized backbone architecture, while its size is to be determined\nmanually. Also, in DARTS backbone, Hadamard product of two elements is not\nintroduced, which exists in both LSTM and GRU cells for recurrent nets. This\nstudy introduces a growing mechanism for differentiable neural architecture\nsearch based on network morphism. It enables growing of the cell structures\nfrom small size towards large size ones with one-shot training. Two modes can\nbe applied in integrating the growing and original pruning process. We also\nimplement a recently proposed two-input backbone architecture for recurrent\nneural networks. Initial experimental results indicate that our approach and\nthe two-input backbone structure can be quite effective compared with other\nbaseline architectures including LSTM, in a variety of learning tasks including\nmulti-variate time series forecasting and language modeling. On the other hand,\nwe find that dynamic network transformation is promising in improving the\nefficiency of differentiable architecture search.", "title": "Differentiable Neural Architecture Search with Morphism-based  Transformable Backbone Architectures"}, {"link": "https://arxiv.org/abs/2106.07213", "abstract": "Nowadays, researchers have moved to platforms like Twitter to spread\ninformation about their ideas and empirical evidence. Recent studies have shown\nthat social media affects the scientific impact of a paper. However, these\nstudies only utilize the tweet counts to represent Twitter activity. In this\npaper, we propose TweetPap, a large-scale dataset that introduces temporal\ninformation of citation/tweets and the metadata of the tweets to quantify and\nunderstand the discourse of scientific papers on social media. The dataset is\npublicly available at https://github.com/lingo-iitgn/TweetPap", "title": "TweetPap: A Dataset to Study the Social Media Discourse of Scientific  Papers"}, {"link": "https://arxiv.org/abs/2106.07214", "abstract": "Backdoor attacks inject poisoning samples during training, with the goal of\nenforcing a machine-learning model to output an attacker-chosen class when\npresented a specific trigger at test time. Although backdoor attacks have been\ndemonstrated in a variety of settings and against different models, the factors\naffecting their success are not yet well understood. In this work, we provide a\nunifying framework to study the process of backdoor learning under the lens of\nincremental learning and influence functions. We show that the success of\nbackdoor attacks inherently depends on (i) the complexity of the learning\nalgorithm, controlled by its hyperparameters, and (ii) the fraction of backdoor\nsamples injected into the training set. These factors affect how fast a\nmachine-learning model learns to correlate the presence of a backdoor trigger\nwith the target class. Interestingly, our analysis shows that there exists a\nregion in the hyperparameter space in which the accuracy on clean test samples\nis still high while backdoor attacks become ineffective, thereby suggesting\nnovel criteria to improve existing defenses.", "title": "Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence  Functions"}, {"link": "https://arxiv.org/abs/2106.07217", "abstract": "Due to the increasing need to handle the noisy label problem in a massive\ndataset, learning with noisy labels has received much attention in recent\nyears. As a promising approach, there have been recent studies to select clean\ntraining data by finding small-loss instances before a deep neural network\noverfits the noisy-label data. However, it is challenging to prevent\noverfitting. In this paper, we propose a novel noisy-label detection algorithm\nby employing the property of overfitting on individual data points. To this\nend, we present two novel criteria that statistically measure how much each\ntraining sample abnormally affects the model and clean validation data. Using\nthe criteria, our iterative algorithm removes noisy-label samples and retrains\nthe model alternately until no further performance improvement is made. In\nexperiments on multiple benchmark datasets, we demonstrate the validity of our\nalgorithm and show that our algorithm outperforms the state-of-the-art methods\nwhen the exact noise rates are not given. Furthermore, we show that our method\ncan not only be expanded to a real-world video dataset but also can be viewed\nas a regularization method to solve problems caused by overfitting.", "title": "Over-Fit: Noisy-Label Detection based on the Overfitted Model Property"}, {"link": "https://arxiv.org/abs/2106.07218", "abstract": "Background: Floods are the most common natural disaster in the world,\naffecting the lives of hundreds of millions. Flood forecasting is therefore a\nvitally important endeavor, typically achieved using physical water flow\nsimulations, which rely on accurate terrain elevation maps. However, such\nsimulations, based on solving partial differential equations, are\ncomputationally prohibitive on a large scale. This scalability issue is\ncommonly alleviated using a coarse grid representation of the elevation map,\nthough this representation may distort crucial terrain details, leading to\nsignificant inaccuracies in the simulation. Contributions: We train a deep\nneural network to perform physics-informed downsampling of the terrain map: we\noptimize the coarse grid representation of the terrain maps, so that the flood\nprediction will match the fine grid solution. For the learning process to\nsucceed, we configure a dataset specifically for this task. We demonstrate that\nwith this method, it is possible to achieve a significant reduction in\ncomputational cost, while maintaining an accurate solution. A reference\nimplementation accompanies the paper as well as documentation and code for\ndataset reproduction.", "title": "Physics-Aware Downsampling with Deep Learning for Scalable Flood  Modeling"}, {"link": "https://arxiv.org/abs/2106.07220", "abstract": "Recent advances in image inpainting have shown impressive results for\ngenerating plausible visual details on rather simple backgrounds. However, for\ncomplex scenes, it is still challenging to restore reasonable contents as the\ncontextual information within the missing regions tends to be ambiguous. To\ntackle this problem, we introduce pretext tasks that are semantically\nmeaningful to estimating the missing contents. In particular, we perform\nknowledge distillation on pretext models and adapt the features to image\ninpainting. The learned semantic priors ought to be partially invariant between\nthe high-level pretext task and low-level image inpainting, which not only help\nto understand the global context but also provide structural guidance for the\nrestoration of local textures. Based on the semantic priors, we further propose\na context-aware image inpainting model, which adaptively integrates global\nsemantics and local features in a unified image generator. The semantic learner\nand the image generator are trained in an end-to-end manner. We name the model\nSPL to highlight its ability to learn and leverage semantic priors. It achieves\nthe state of the art on Places2, CelebA, and Paris StreetView datasets.", "title": "Context-Aware Image Inpainting with Learned Semantic Priors"}, {"link": "https://arxiv.org/abs/2106.07221", "abstract": "Advances in machine learning (ML) open the way to innovating functions in the\navionic domain, such as navigation/surveillance assistance (e.g. vision-based\nnavigation, obstacle sensing, virtual sensing), speechto-text applications,\nautonomous flight, predictive maintenance or cockpit assistance. Current\ncertification standards and practices, which were defined and refined decades\nover decades with classical programming in mind, do not however support this\nnew development paradigm. This article provides an overview of the main\nchallenges raised by the use ML in the demonstration of compliance with\nregulation requirements, and a survey of literature relevant to these\nchallenges, with particular focus on the issues of robustness and\nexplainability of ML results.", "title": "Certification of embedded systems based on Machine Learning: A survey"}, {"link": "https://arxiv.org/abs/2106.07225", "abstract": "The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.", "title": "English to Bangla Machine Translation Using Recurrent Neural Network"}, {"link": "https://arxiv.org/abs/2106.07226", "abstract": "Deep fakes became extremely popular in the last years, also thanks to their\nincreasing realism. Therefore, there is the need to measures human's ability to\ndistinguish between real and synthetic face images when confronted with\ncutting-edge creation technologies. We describe the design and results of a\nperceptual experiment we have conducted, where a wide and diverse group of\nvolunteers has been exposed to synthetic face images produced by\nstate-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN,\nStyleGAN2). The experiment outcomes reveal how strongly we should call into\nquestion our human ability to discriminate real faces from synthetic ones\ngenerated through modern AI.", "title": "More Real than Real: A Study on Human Visual Perception of Synthetic  Faces"}, {"link": "https://arxiv.org/abs/2106.07228", "abstract": "Finding a parking space nowadays becomes an issue that is not to be\nneglected, it consumes time and energy. We have used computer vision techniques\nto infer the state of the parking lot given the data collected from the\nUniversity of The Witwatersrand. This paper presents an approach for a\nreal-time parking space classification based on Convolutional Neural Networks\n(CNN) using Caffe and Nvidia DiGITS framework. The training process has been\ndone using DiGITS and the output is a caffemodel used for predictions to detect\nvacant and occupied parking spots. The system checks a defined area whether a\nparking spot (bounding boxes defined at initialization of the system) is\ncontaining a car or not (occupied or vacant). Those bounding box coordinates\nare saved from a frame of the video of the parking lot in a JSON format, to be\nlater used by the system for sequential prediction on each parking spot. The\nsystem has been trained using the LeNet network with the Nesterov Accelerated\nGradient as solver and the AlexNet network with the Stochastic Gradient Descent\nas solver. We were able to get an accuracy on the validation set of 99\\% for\nboth networks. The accuracy on a foreign dataset(PKLot) returned as well 99\\%.\nThose are experimental results based on the training set shows how robust the\nsystem can be when the prediction has to take place in a different parking\nspace.", "title": "Automated Parking Space Detection Using Convolutional Neural Networks"}, {"link": "https://arxiv.org/abs/2106.07229", "abstract": "Fully homomorphic encryption (FHE) is one of the prospective tools for\nprivacypreserving machine learning (PPML), and several PPML models have been\nproposed based on various FHE schemes and approaches. Although the FHE schemes\nare known as suitable tools to implement PPML models, previous PPML models on\nFHE encrypted data are limited to only simple and non-standard types of machine\nlearning models. These non-standard machine learning models are not proven\nefficient and accurate with more practical and advanced datasets. Previous PPML\nschemes replace non-arithmetic activation functions with simple arithmetic\nfunctions instead of adopting approximation methods and do not use\nbootstrapping, which enables continuous homomorphic evaluations. Thus, they\ncould not use standard activation functions and could not employ a large number\nof layers. The maximum classification accuracy of the existing PPML model with\nthe FHE for the CIFAR-10 dataset was only 77% until now. In this work, we\nfirstly implement the standard ResNet-20 model with the RNS-CKKS FHE with\nbootstrapping and verify the implemented model with the CIFAR-10 dataset and\nthe plaintext model parameters. Instead of replacing the non-arithmetic\nfunctions with the simple arithmetic function, we use state-of-the-art\napproximation methods to evaluate these non-arithmetic functions, such as the\nReLU, with sufficient precision [1]. Further, for the first time, we use the\nbootstrapping technique of the RNS-CKKS scheme in the proposed model, which\nenables us to evaluate a deep learning model on the encrypted data. We\nnumerically verify that the proposed model with the CIFAR-10 dataset shows\n98.67% identical results to the original ResNet-20 model with non-encrypted\ndata. The classification accuracy of the proposed model is 90.67%, which is\npretty close to that of the original ResNet-20 CNN model...", "title": "Privacy-Preserving Machine Learning with Fully Homomorphic Encryption  for Deep Neural Network"}, {"link": "https://arxiv.org/abs/2106.07234", "abstract": "Recent developments in advanced driving assistance systems (ADAS) that rely\non some level of autonomy have led the automobile industry and research\ncommunity to investigate the impact they might have on driving performance.\nHowever, most of the research performed so far is based on simulated\nenvironments. In this study, we investigated the behavior of drivers in a\nvehicle with automated driving system (ADS) capabilities in a real-life driving\nscenario. We analyzed their response to a take over request (TOR) at two\ndifferent driving speeds while being engaged in non-driving-related tasks\n(NDRT). Results from the performed experiments showed that driver reaction time\nto a TOR, gaze behavior and self-reported trust in automation were affected by\nthe type of NDRT being concurrently performed and driver reaction time and gaze\nbehavior additionally depended on the driving or vehicle speed at the time of\nTOR.", "title": "Real-World Evaluation of the Impact of Automated Driving System  Technology on Driver Gaze Behavior, Reaction Time and Trust"}, {"link": "https://arxiv.org/abs/2106.07237", "abstract": "Recent progress in distributed semantic models (DSM) offers new ways to\nestimate personality traits of both fictive and real people. In this\nexploratory study we applied an extended version of the algorithm developed in\nJacobs (2019) to compute the likeability scores, emotional figure profiles and\nBIG5 personality traits for 100 historical persons from the arts, politics or\nscience domains whose names are rather unique (e.g., Einstein, Kahlo, Picasso).\nWe compared the results produced by static (word2vec) and dynamic (BERT)\nlanguage model representations in four studies. The results show both the\npotential and limitations of such DSM-based computations of personality\nprofiles and point ways to further develop this approach to become a useful\ntool in data science, psychology or computational and neurocognitive poetics\n(Jacobs, 2015).", "title": "Is Einstein more agreeable and less neurotic than Hitler? A  computational exploration of the emotional and personality profiles of  historical persons"}, {"link": "https://arxiv.org/abs/2106.07239", "abstract": "Clustering is a fundamental unsupervised learning problem where a dataset is\npartitioned into clusters that consist of nearby points in a metric space. A\nrecent variant, fair clustering, associates a color with each point\nrepresenting its group membership and requires that each color has\n(approximately) equal representation in each cluster to satisfy group fairness.\nIn this model, the cost of the clustering objective increases due to enforcing\nfairness in the algorithm. The relative increase in the cost, the ''price of\nfairness,'' can indeed be unbounded. Therefore, in this paper we propose to\ntreat an upper bound on the clustering objective as a constraint on the\nclustering problem, and to maximize equality of representation subject to it.\nWe consider two fairness objectives: the group utilitarian objective and the\ngroup egalitarian objective, as well as the group leximin objective which\ngeneralizes the group egalitarian objective. We derive fundamental lower bounds\non the approximation of the utilitarian and egalitarian objectives and\nintroduce algorithms with provable guarantees for them. For the leximin\nobjective we introduce an effective heuristic algorithm. We further derive\nimpossibility results for other natural fairness objectives. We conclude with\nexperimental results on real-world datasets that demonstrate the validity of\nour algorithms.", "title": "Fair Clustering Under a Bounded Cost"}, {"link": "https://arxiv.org/abs/2106.07240", "abstract": "Automatic detection of toxic language plays an essential role in protecting\nsocial media users, especially minority groups, from verbal abuse. However,\nbiases toward some attributes, including gender, race, and dialect, exist in\nmost training datasets for toxicity detection. The biases make the learned\nmodels unfair and can even exacerbate the marginalization of people.\nConsidering that current debiasing methods for general natural language\nunderstanding tasks cannot effectively mitigate the biases in the toxicity\ndetectors, we propose to use invariant rationalization (InvRat), a\ngame-theoretic framework consisting of a rationale generator and a predictor,\nto rule out the spurious correlation of certain syntactic patterns (e.g.,\nidentity mentions, dialect) to toxicity labels. We empirically show that our\nmethod yields lower false positive rate in both lexical and dialectal\nattributes than previous debiasing methods.", "title": "Mitigating Biases in Toxic Language Detection through Invariant  Rationalization"}, {"link": "https://arxiv.org/abs/2106.07241", "abstract": "We introduced the contemporary Amharic corpus, which is automatically tagged\nfor morpho-syntactic information. Texts are collected from 25,199 documents\nfrom different domains and about 24 million orthographic words are tokenized.\nSince it is partly a web corpus, we made some automatic spelling error\ncorrection. We have also modified the existing morphological analyzer,\nHornMorpho, to use it for the automatic tagging.", "title": "Contemporary Amharic Corpus: Automatically Morpho-Syntactically Tagged  Amharic Corpus"}, {"link": "https://arxiv.org/abs/2106.07242", "abstract": "A wide variety of biological phenomena can be modeled by the collective\nactivity of a population of individual units. A common strategy for simulating\nsuch a system, the population density approach, is to take the macroscopic\nlimit and update its population density function. However, in many cases, the\ncoupling between the units and noise gives rise to complex behaviors\nchallenging to existing population density approach methods. To address these\nchallenges, we develop the asymmetric particle population density (APPD) method\nthat efficiently and accurately simulates such populations consist of coupled\nelements. The APPD is well-suited for a parallel implementation. We compare the\nperformance of the method against direct Monte-Carlo simulation and verify its\naccuracy by applying it to the well-studied Hodgkin-Huxley model, with a range\nof challenging scenarios. We find that our method can accurately reproduce\ncomplex macroscopic behaviors such as inhibitory coupling-induced clustering\nand noise-induced firing while being faster than the direct simulation.", "title": "The asymmetric particle population density method for simulation of  coupled noisy oscillators"}, {"link": "https://arxiv.org/abs/2106.07247", "abstract": "Having timely and fresh knowledge about the current state of information\nsources is critical in a variety of applications. In particular, a status\nupdate may arrive at the destination later than its generation time due to\nprocessing and communication delays. The freshness of the status update at the\ndestination is captured by the notion of age of information. In this study, we\nanalyze a multiple sensing network with multiple sources, multiple servers, and\na monitor (destination). Each source corresponds to an independent piece of\ninformation and its age is measured individually. Given a particular source,\nthe servers independently sense the source of information and send the status\nupdate to the monitor. We assume that updates arrive at the servers according\nto Poisson random processes. Each server sends its updates to the monitor\nthrough a direct link, which is modeled as a queue. The service time to\ntransmit an update is considered to be an exponential random variable. We\nexamine both homogeneous and heterogeneous service and arrival rates for the\nsingle-source case, and only homogeneous arrival and service rates for the\nmultiple-source case. We derive a closed-form expression for the average age of\ninformation under a last-come-first-serve (LCFS) queue for a single source and\narbitrary number of homogeneous servers. Using a recursive method, we derive\nthe explicit average age of information for any number of sources and\nhomogeneous servers. We also investigate heterogeneous servers and a single\nsource, and present algorithms for finding the average age of information.", "title": "Age of Information for Multiple-Source Multiple-Server Networks"}, {"link": "https://arxiv.org/abs/2106.07250", "abstract": "In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.", "title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling:  With Case Study for Knowledge Graph Embedding"}, {"link": "https://arxiv.org/abs/2106.07252", "abstract": "In many clustering scenes, data samples' attribute values change over time.\nFor such data, we are often interested in obtaining a partition for each time\nstep and tracking the dynamic change of partitions. Normally, a smooth change\nis assumed for data to have a temporal smooth nature. Existing algorithms\nconsider the temporal smoothness as an a priori preference and bias the search\ntowards the preferred direction. This a priori manner leads to a risk of\nconverging to an unexpected region because it is not always the case that a\nreasonable preference can be elicited given the little prior knowledge about\nthe data. To address this issue, this paper proposes a new clustering framework\ncalled evolutionary robust clustering over time. One significant innovation of\nthe proposed framework is processing the temporal smoothness in an a posteriori\nmanner, which avoids unexpected convergence that occurs in existing algorithms.\nFurthermore, the proposed framework automatically tunes the weight of\nsmoothness without data's affinity matrix and predefined parameters, which\nholds better applicability and scalability. The effectiveness and efficiency of\nthe proposed framework are confirmed by comparing with state-of-the-art\nalgorithms on both synthetic and real datasets.", "title": "Evolutionary Robust Clustering Over Time for Temporal Data"}, {"link": "https://arxiv.org/abs/2106.07255", "abstract": "In this paper, we study the problem of recovering the community structure of\na network under federated myopic learning. Under this paradigm, we have several\nclients, each of them having a myopic view, i.e., observing a small subgraph of\nthe network. Each client sends a censored evidence graph to a central server.\nWe provide an efficient algorithm, which computes a consensus signed weighted\ngraph from clients evidence, and recovers the underlying network structure in\nthe central server. We analyze the topological structure conditions of the\nnetwork, as well as the signal and noise levels of the clients that allow for\nrecovery of the network structure. Our analysis shows that exact recovery is\npossible and can be achieved in polynomial time. We also provide\ninformation-theoretic limits for the central server to recover the network\nstructure from any single client evidence. Finally, as a byproduct of our\nanalysis, we provide a novel Cheeger-type inequality for general signed\nweighted graphs.", "title": "Federated Myopic Community Detection with One-shot Communication"}, {"link": "https://arxiv.org/abs/2106.07256", "abstract": "Accurate dense depth estimation is crucial for autonomous vehicles to analyze\ntheir environment. This paper presents a non-deep learning-based approach to\ndensify a sparse LiDAR-based depth map using a guidance RGB image. To achieve\nthis goal the RGB image is at first cleared from most of the camera-LiDAR\nmisalignment artifacts. Afterward, it is over segmented and a plane for each\nsuperpixel is approximated. In the case a superpixel is not well represented by\na plane, a plane is approximated for a convex hull of the most inlier. Finally,\nthe pinhole camera model is used for the interpolation process and the\nremaining areas are interpolated. The evaluation of this work is executed using\nthe KITTI depth completion benchmark, which validates the proposed work and\nshows that it outperforms the state-of-the-art non-deep learning-based methods,\nin addition to several deep learning-based methods.", "title": "Deterministic Guided LiDAR Depth Map Completion"}, {"link": "https://arxiv.org/abs/2106.07257", "abstract": "Conversational agents are a recent trend in human-computer interaction,\ndeployed in multidisciplinary applications to assist the users. In this paper,\nwe introduce \"Atreya\", an interactive bot for chemistry enthusiasts,\nresearchers, and students to study the ChEMBL database. Atreya is hosted by\nTelegram, a popular cloud-based instant messaging application. This\nuser-friendly bot queries the ChEMBL database, retrieves the drug details for a\nparticular disease, targets associated with that drug, etc. This paper explores\nthe potential of using a conversational agent to assist chemistry students and\nchemical scientist in complex information seeking process.", "title": "Communication is the universal solvent: atreya bot -- an interactive bot  for chemical scientists"}, {"link": "https://arxiv.org/abs/2106.07260", "abstract": "Planning provides a framework for optimizing sequential decisions in complex\nenvironments. Recent advances in efficient planning in deterministic or\nstochastic high-dimensional domains with continuous action spaces leverage\nbackpropagation through a model of the environment to directly optimize\nactions. However, existing methods typically not take risk into account when\noptimizing in stochastic domains, which can be incorporated efficiently in MDPs\nby optimizing the entropic utility of returns. We bridge this gap by\nintroducing Risk-Aware Planning using PyTorch (RAPTOR), a novel framework for\nrisk-sensitive planning through end-to-end optimization of the entropic utility\nobjective. A key technical difficulty of our approach lies in that direct\noptimization of the entropic utility by backpropagation is impossible due to\nthe presence of environment stochasticity. The novelty of RAPTOR lies in the\nreparameterization of the state distribution, which makes it possible to apply\nstochastic backpropagatation through sufficient statistics of the entropic\nutility computed from forward-sampled trajectories. The direct optimization of\nthis empirical objective in an end-to-end manner is called the risk-averse\nstraight-line plan, which commits to a sequence of actions in advance and can\nbe sub-optimal in highly stochastic domains. We address this shortcoming by\noptimizing for risk-aware Deep Reactive Policies (RaDRP) in our framework. We\nevaluate and compare these two forms of RAPTOR on three highly stochastic\ndo-mains, including nonlinear navigation, HVAC control, and linear reservoir\ncontrol, demonstrating the ability to manage risk in complex MDPs.", "title": "RAPTOR: End-to-end Risk-Aware MDP Planning and Policy Learning by  Backpropagation"}, {"link": "https://arxiv.org/abs/2106.07270", "abstract": "Despite their contributions to the financial efficiency and environmental\nsustainability of industrial processes, robotic assembly and disassembly have\nbeen understudied in the existing literature. This is in contradiction to their\nimportance in realizing the Fourth Industrial Revolution. More specifically,\nalthough most of the literature has extensively discussed how to optimally\nassemble or disassemble given products, the role of other factors has been\noverlooked. For example, the types of robots involved in implementing the\nsequence plans, which should ideally be taken into account throughout the whole\nchain consisting of design, assembly, disassembly and reassembly. Isolating the\nforegoing operations from the rest of the components of the relevant ecosystems\nmay lead to erroneous inferences toward both the necessity and efficiency of\nthe underlying procedures. In this paper we try to alleviate these shortcomings\nby comprehensively investigating the state-of-the-art in robotic assembly and\ndisassembly. We consider and review various aspects of manufacturing and\nremanufacturing frameworks while particularly focusing on their desirability\nfor supporting a circular economy.", "title": "Industry 4.0 and Prospects of Circular Economy: A Survey of Robotic  Assembly and Disassembly"}, {"link": "https://arxiv.org/abs/2106.07271", "abstract": "If devices are physically accessible optical fault injection attacks pose a\ngreat threat since the data processed as well as the operation flow can be\nmanipulated. Successful physical attacks may lead not only to leakage of secret\ninformation such as cryptographic private keys, but can also cause economic\ndamage especially if as a result of such a manipulation a critical\ninfrastructure is successfully attacked. Laser based attacks exploit the\nsensitivity of CMOS technologies to electromagnetic radiation in the visible or\nthe infrared spectrum. It can be expected that radiation-hard designs,\nspecially crafted for space applications, are more robust not only against\nhigh-energy particles and short electromagnetic waves but also against optical\nfault injection attacks. In this work we investigated the sensitivity of\nradiation-hard JICG shift registers to optical fault injection attacks. In our\nexperiments, we were able to repeatable trigger bit-set and bit-reset\noperations changing the data stored in single JICG flip-flops despite their\nhigh-radiation fault tolerance.", "title": "Optical Fault Injection Attacks against Radiation-Hard Registers"}, {"link": "https://arxiv.org/abs/2106.07273", "abstract": "A molecule is a complex of heterogeneous components, and the spatial\narrangements of these components determine the whole molecular properties and\ncharacteristics. With the advent of deep learning in computational chemistry,\nseveral studies have focused on how to predict molecular properties based on\nmolecular configurations. Message passing neural network provides an effective\nframework for capturing molecular geometric features with the perspective of a\nmolecule as a graph. However, most of these studies assumed that all\nheterogeneous molecular features, such as atomic charge, bond length, or other\ngeometric features always contribute equivalently to the target prediction,\nregardless of the task type. In this study, we propose a dual-branched neural\nnetwork for molecular property prediction based on message-passing framework.\nOur model learns heterogeneous molecular features with different scales, which\nare trained flexibly according to each prediction target. In addition, we\nintroduce a discrete branch to learn single atom features without local\naggregation, apart from message-passing steps. We verify that this novel\nstructure can improve the model performance with faster convergence in most\ntargets. The proposed model outperforms other recent models with sparser\nrepresentations. Our experimental results indicate that in the chemical\nproperty prediction tasks, the diverse chemical nature of targets should be\ncarefully considered for both model performance and generalizability.", "title": "Flexible dual-branched message passing neural network for quantum  mechanical property prediction with molecular conformation"}, {"link": "https://arxiv.org/abs/2106.07275", "abstract": "This paper summarizes our entries to both subtasks of the first DialDoc\nshared task which focuses on the agent response prediction task in\ngoal-oriented document-grounded dialogs. The task is split into two subtasks:\npredicting a span in a document that grounds an agent turn and generating an\nagent response based on a dialog and grounding document. In the first subtask,\nwe restrict the set of valid spans to the ones defined in the dataset, use a\nbiaffine classifier to model spans, and finally use an ensemble of different\nmodels. For the second subtask, we use a cascaded model which grounds the\nresponse prediction on the predicted span instead of the full document. With\nthese approaches, we obtain significant improvements in both subtasks compared\nto the baseline.", "title": "Cascaded Span Extraction and Response Generation for Document-Grounded  Dialog"}, {"link": "https://arxiv.org/abs/2106.07277", "abstract": "In the last decades the rapid development of technologies and methodologies\nin the field of digitization and 3D modelling has led to an increasing\nproliferation of 3D technologies in the Cultural Heritage domain. Despite the\ngreat potential of 3D digital heritage, the \"special effects\" of 3D may often\noverwhelm its importance in research. Projects and consortia of scholars have\ntried to put order in the different fields of application of these\ntechnologies, providing guidelines and proposing workflows. The use of computer\ngraphics as an effective methodology for CH research and communication\nhighlighted the need of transparent provenance data to properly document\ndigital assets and understand the degree of scientific quality and reliability\nof their outcomes. The building and release of provenance knowledge, consisting\nin the complete formal documentation of each phase of the process, is therefore\nof fundamental importance to ensure its repeatability and to guarantee the\nintegration and interoperability of the generated metadata on the Semantic Web.\nThis paper proposes a methodology for documenting the planning and creation of\n3D models used in archaeology and Cultural Heritage, by means of an application\nprofile based on the CIDOC CRM ecosystem and other international standards.", "title": "Ontological Entities for Planning and Describing Cultural Heritage 3D  Models Creation"}, {"link": "https://arxiv.org/abs/2106.07278", "abstract": "Mutual information maximization provides an appealing formalism for learning\nrepresentations of data. In the context of reinforcement learning (RL), such\nrepresentations can accelerate learning by discarding irrelevant and redundant\ninformation, while retaining the information necessary for control. Much of the\nprior work on these methods has addressed the practical difficulties of\nestimating mutual information from samples of high-dimensional observations,\nwhile comparatively less is understood about which mutual information\nobjectives yield representations that are sufficient for RL from a theoretical\nperspective. In this paper, we formalize the sufficiency of a state\nrepresentation for learning and representing the optimal policy, and study\nseveral popular mutual-information based objectives through this lens.\nSurprisingly, we find that two of these objectives can yield insufficient\nrepresentations given mild and common assumptions on the structure of the MDP.\nWe corroborate our theoretical results with empirical experiments on a\nsimulated game environment with visual observations.", "title": "Which Mutual-Information Representation Learning Objectives are  Sufficient for Control?"}, {"link": "https://arxiv.org/abs/2106.07283", "abstract": "While domain adaptation has been used to improve the performance of object\ndetectors when the training and test data follow different distributions,\nprevious work has mostly focused on two-stage detectors. This is because their\nuse of region proposals makes it possible to perform local adaptation, which\nhas been shown to significantly improve the adaptation effectiveness. Here, by\ncontrast, we target single-stage architectures, which are better suited to\nresource-constrained detection than two-stage ones but do not provide region\nproposals. To nonetheless benefit from the strength of local adaptation, we\nintroduce an attention mechanism that lets us identify the important regions on\nwhich adaptation should focus. Our approach is generic and can be integrated\ninto any single-stage detector. We demonstrate this on standard benchmark\ndatasets by applying it to both SSD and YOLO. Furthermore, for an equivalent\nsingle-stage architecture, our method outperforms the state-of-the-art domain\nadaptation technique even though it was designed specifically for this\nparticular detector.", "title": "Attention-based Domain Adaptation for Single Stage Detectors"}, {"link": "https://arxiv.org/abs/2106.07285", "abstract": "Pre-trained language models such as ClinicalBERT have achieved impressive\nresults on tasks such as medical Natural Language Inference. At first glance,\nthis may suggest that these models are able to perform medical reasoning tasks,\nsuch as mapping symptoms to diseases. However, we find that standard benchmarks\nsuch as MedNLI contain relatively few examples that require such forms of\nreasoning. To better understand the medical reasoning capabilities of existing\nlanguage models, in this paper we introduce DisKnE, a new benchmark for Disease\nKnowledge Evaluation. To construct this benchmark, we annotated each positive\nMedNLI example with the types of medical reasoning that are needed. We then\ncreated negative examples by corrupting these positive examples in an\nadversarial way. Furthermore, we define training-test splits per disease,\nensuring that no knowledge about test diseases can be learned from the training\ndata, and we canonicalize the formulation of the hypotheses to avoid the\npresence of artefacts. This leads to a number of binary classification\nproblems, one for each type of reasoning and each disease. When analysing\npre-trained models for the clinical/biomedical domain on the proposed\nbenchmark, we find that their performance drops considerably.", "title": "Probing Pre-Trained Language Models for Disease Knowledge"}, {"link": "https://arxiv.org/abs/2106.07286", "abstract": "State-of-the-art frame interpolation methods generate intermediate frames by\ninferring object motions in the image from consecutive key-frames. In the\nabsence of additional information, first-order approximations, i.e. optical\nflow, must be used, but this choice restricts the types of motions that can be\nmodeled, leading to errors in highly dynamic scenarios. Event cameras are novel\nsensors that address this limitation by providing auxiliary visual information\nin the blind-time between frames. They asynchronously measure per-pixel\nbrightness changes and do this with high temporal resolution and low latency.\nEvent-based frame interpolation methods typically adopt a synthesis-based\napproach, where predicted frame residuals are directly applied to the\nkey-frames. However, while these approaches can capture non-linear motions they\nsuffer from ghosting and perform poorly in low-texture regions with few events.\nThus, synthesis-based and flow-based approaches are complementary. In this\nwork, we introduce Time Lens, a novel indicates equal contribution method that\nleverages the advantages of both. We extensively evaluate our method on three\nsynthetic and two real benchmarks where we show an up to 5.21 dB improvement in\nterms of PSNR over state-of-the-art frame-based and event-based methods.\nFinally, we release a new large-scale dataset in highly dynamic scenarios,\naimed at pushing the limits of existing methods.", "title": "TimeLens: Event-based Video Frame Interpolation"}, {"link": "https://arxiv.org/abs/2106.07287", "abstract": "Data science has employed great research efforts in developing advanced\nanalytics, improving data models and cultivating new algorithms. However, not\nmany authors have come across the organizational and socio-technical challenges\nthat arise when executing a data science project: lack of vision and clear\nobjectives, a biased emphasis on technical issues, a low level of maturity for\nad-hoc projects and the ambiguity of roles in data science are among these\nchallenges. Few methodologies have been proposed on the literature that tackle\nthese type of challenges, some of them date back to the mid-1990, and\nconsequently they are not updated to the current paradigm and the latest\ndevelopments in big data and machine learning technologies. In addition, fewer\nmethodologies offer a complete guideline across team, project and data &\ninformation management. In this article we would like to explore the necessity\nof developing a more holistic approach for carrying out data science projects.\nWe first review methodologies that have been presented on the literature to\nwork on data science projects and classify them according to the their focus:\nproject, team, data and information management. Finally, we propose a\nconceptual framework containing general characteristics that a methodology for\nmanaging data science projects with a holistic point of view should have. This\nframework can be used by other researchers as a roadmap for the design of new\ndata science methodologies or the updating of existing ones.", "title": "Data Science Methodologies: Current Challenges and Future Approaches"}, {"link": "https://arxiv.org/abs/2106.07288", "abstract": "Computer systems such as storage systems normally require transparent\nwhite-box algorithms that are interpretable for human experts. In this work, we\npropose a learning-aided heuristic design method, which automatically generates\nhuman-readable strategies from Deep Reinforcement Learning (DRL) agents. This\nmethod benefits from the power of deep learning but avoids the shortcoming of\nits black-box property. Besides the white-box advantage, experiments in our\nstorage productions resource allocation scenario also show that this solution\noutperforms the systems default settings and the elaborately handcrafted\nstrategy by human experts.", "title": "Learning-Aided Heuristics Design for Storage System"}, {"link": "https://arxiv.org/abs/2106.07289", "abstract": "Personalized Federated Learning has recently seen tremendous progress,\nallowing the design of novel machine learning applications preserving privacy\nof the data used for training. Existing theoretical results in this field\nmainly focus on distributed optimization under minimization problems. This\npaper is the first to study PFL for saddle point problems, which cover a\nbroader class of optimization tasks and are thus of more relevance for\napplications than the minimization. In this work, we consider a recently\nproposed PFL setting with the mixing objective function, an approach combining\nthe learning of a global model together with local distributed learners. Unlike\nmost of the previous papers, which considered only the centralized setting, we\nwork in a more general and decentralized setup. This allows to design and to\nanalyze more practical and federated ways to connect devices to the network. We\npresent two new algorithms for our problem. A theoretical analysis of the\nmethods is presented for smooth (strongly-)convex-(strongly-)concave saddle\npoint problems. We also demonstrate the effectiveness of our problem\nformulation and the proposed algorithms on experiments with neural networks\nwith adversarial noise.", "title": "Decentralized Personalized Federated Min-Max Problems"}, {"link": "https://arxiv.org/abs/2106.07296", "abstract": "RRULES is presented as an improvement and optimization over RULES, a simple\ninductive learning algorithm for extracting IF-THEN rules from a set of\ntraining examples. RRULES optimizes the algorithm by implementing a more\neffective mechanism to detect irrelevant rules, at the same time that checks\nthe stopping conditions more often. This results in a more compact rule set\ncontaining more general rules which prevent overfitting the training set and\nobtain a higher test accuracy. Moreover, the results show that RRULES\noutperforms the original algorithm by reducing the coverage rate up to a factor\nof 7 while running twice or three times faster consistently over several\ndatasets.", "title": "RRULES: An improvement of the RULES rule-based classifier"}, {"link": "https://arxiv.org/abs/2106.07297", "abstract": "Node classification and link prediction are widely studied tasks in graph\nrepresentation learning. While both transductive node classification and link\nprediction operate over a single input graph, they are studied in isolation so\nfar, which leads to discrepancies. Node classification models take as input a\ngraph with node features and incomplete node labels, and implicitly assume that\nthe input graph is relationally complete, i.e., no edges are missing from the\ninput graph. This is in sharp contrast with link prediction models that are\nsolely motivated by the relational incompleteness of the input graph which does\nnot have any node features. We propose a unifying perspective and study the\nproblems of (i) transductive node classification over incomplete graphs and\n(ii) link prediction over graphs with node features. We propose an extension to\nan existing box embedding model, and show that this model is fully expressive,\nand can solve both of these tasks in an end-to-end fashion. To empirically\nevaluate our model, we construct a knowledge graph with node features, which is\nchallenging both for node classification and link prediction. Our model\nperforms very strongly when compared to the respective state-of-the-art models\nfor node classification and link prediction on this dataset and shows the\nimportance of a unified perspective for node classification and link prediction\non knowledge graphs.", "title": "Node Classification Meets Link Prediction on Knowledge Graphs"}, {"link": "https://arxiv.org/abs/2106.07299", "abstract": "Control performance of Unmanned Aerial Vehicles (UAVs) is directly affected\nby their ability to estimate their states accurately. With the increasing\npopularity of autonomous UAV solutions in real world applications, it is\nimperative to develop robust adaptive estimators that can ameliorate sensor\nnoises in low-cost UAVs. Utilizing the knowledge of UAV dynamics in estimation\ncan provide significant advantages, but remains challenging due to the complex\nand expensive pre-flight experiments required to obtain UAV dynamic parameters.\nIn this paper, we propose two decoupled dynamic model based Extended Kalman\nFilters for UAVs, that provide high rate estimates for position, and velocity\nof rotational and translational states, as well as filtered inertial\nacceleration. The dynamic model parameters are estimated online using the Deep\nNeural Network and Modified Relay Feedback Test (DNN-MRFT) framework, without\nrequiring any prior knowledge of the UAV physical parameters. The designed\nfilters with real-time identified process model parameters are tested\nexperimentally and showed two advantages. Firstly, smooth and lag-free\nestimates of the UAV rotational speed and inertial acceleration are obtained,\nand used to improve the closed loop system performance, reducing the controller\naction by over 6 %. Secondly, the proposed approach enabled the UAV to track\naggressive trajectories with low rate position measurements, a task usually\ninfeasible under those conditions. The experimental data shows that we achieved\nestimation performance matching other methods that requires full knowledge of\nthe UAV parameters.", "title": "Dynamic Based Estimator for UAVs with Real-time Identification Using DNN  and the Modified Relay Feedback Test"}, {"link": "https://arxiv.org/abs/2106.07300", "abstract": "We study the problem of fair allocation of a set of indivisible items among\nagents with additive valuations, under cardinality constraints. In this setting\nthe items are partitioned into categories, each with its own limit on the\nnumber of items it may contribute to any bundle. One example of such a problem\nis allocating seats in a multitrack conference. We consider the fairness\nmeasure known as the maximin share (MMS) guarantee, and propose a novel\npolynomial-time algorithm for finding $1/2$-approximate MMS allocations. We\nextend the notions and algorithms related to ordered and reduced instances to\nwork with cardinality constraints, and combine these with a bag filling style\nprocedure. Our algorithm improves on that of Biswas and Barman (IJCAI-18), with\nits approximation ratio of $1/3$. We also present an optimizing algorithm,\nwhich for each instance, instead of fixing $\\alpha = 1/2$, uses bisection to\nfind the largest $\\alpha$ for which our algorithm obtains a valid\n$\\alpha$-approximate MMS allocation. Numerical tests show that our algorithm\nfinds strictly better approximations than the guarantee of $1/2$ for most\ninstances, in many cases surpassing $3/5$. The optimizing version of the\nalgorithm produces MMS allocations in a comparable number of instances to that\nof Biswas and Barman's algorithm, on average achieving a better approximation\nwhen MMS is not obtained.", "title": "Guaranteeing Half-Maximin Shares Under Cardinality Constraints"}, {"link": "https://arxiv.org/abs/2106.07303", "abstract": "Boundary samples are special inputs to artificial neural networks crafted to\nidentify the execution environment used for inference by the resulting output\nlabel. The paper presents and evaluates algorithms to generate transparent\nboundary samples. Transparency refers to a small perceptual distortion of the\nhost signal (i.e., a natural input sample). For two established image\nclassifiers, ResNet on FMNIST and CIFAR10, we show that it is possible to\ngenerate sets of boundary samples which can identify any of four tested\nmicroarchitectures. These sets can be built to not contain any sample with a\nworse peak signal-to-noise ratio than 70dB. We analyze the relationship between\nsearch complexity and resulting transparency.", "title": "iNNformant: Boundary Samples as Telltale Watermarks"}, {"link": "https://arxiv.org/abs/2106.07306", "abstract": "In structured prediction, a major challenge for models is to represent the\ninterdependencies within their output structures. For the common case where\noutputs are structured as a sequence, linear-chain conditional random fields\n(CRFs) are a widely used model class which can learn local dependencies in\noutput sequences. However, the CRF's Markov assumption makes it impossible for\nthese models to capture nonlocal dependencies, and standard CRFs are unable to\nrespect nonlocal constraints of the data (such as global arity constraints on\noutput labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show using\nsynthetic data that it can be substantially better in practice. Additionally,\nwe demonstrate a practical benefit on downstream tasks by incorporating a\nRegCCRF into a deep neural model for semantic role labeling, exceeding\nstate-of-the-art results on a standard dataset.", "title": "Constraining Linear-chain CRFs to Regular Languages"}, {"link": "https://arxiv.org/abs/2106.07307", "abstract": "The Ubiquitous nature of smartphones has significantly increased the use of\nsocial media platforms, such as Facebook, Twitter, TikTok, and LinkedIn, etc.,\namong the public, government, and businesses. Facebook generated ~70 billion\nUSD in 2019 in advertisement revenues alone, a ~27% increase from the previous\nyear. Social media has also played a strong role in outbreaks of social\nprotests responsible for political changes in different countries. As we can\nsee from the above examples, social media plays a big role in business\nintelligence and international politics. In this paper, we present and discuss\na high-level functional intelligence model (recipe) of Social Media Analysis\n(SMA). This model synthesizes the input data and uses operational intelligence\nto provide actionable recommendations. In addition, it also matches the\nsynthesized function of the experiences and learning gained from the\nenvironment. The SMA model presented is independent of the application domain,\nand can be applied to different domains, such as Education, Healthcare and\nGovernment, etc. Finally, we also present some of the challenges faced by SMA\nand how the SMA model presented in this paper solves them.", "title": "A Recipe for Social Media Analysis"}, {"link": "https://arxiv.org/abs/2106.07310", "abstract": "The existing auto-encoder based face pose editing methods primarily focus on\nmodeling the identity preserving ability during pose synthesis, but are less\nable to preserve the image style properly, which refers to the color,\nbrightness, saturation, etc. In this paper, we take advantage of the well-known\nfrontal/profile optical illusion and present a novel two-stage approach to\nsolve the aforementioned dilemma, where the task of face pose manipulation is\ncast into face inpainting. By selectively sampling pixels from the input face\nand slightly adjust their relative locations with the proposed ``Pixel\nAttention Sampling\" module, the face editing result faithfully keeps the\nidentity information as well as the image style unchanged. By leveraging\nhigh-dimensional embedding at the inpainting stage, finer details are\ngenerated. Further, with the 3D facial landmarks as guidance, our method is\nable to manipulate face pose in three degrees of freedom, i.e., yaw, pitch, and\nroll, resulting in more flexible face pose editing than merely controlling the\nyaw angle as usually achieved by the current state-of-the-art. Both the\nqualitative and quantitative evaluations validate the superiority of the\nproposed approach.", "title": "Pixel Sampling for Style Preserving Face Pose Editing"}, {"link": "https://arxiv.org/abs/2106.07313", "abstract": "Computing the gradient of a function provides fundamental information about\nits behavior. This information is essential for several applications and\nalgorithms across various fields. One common application that require gradients\nare optimization techniques such as stochastic gradient descent, Newton's\nmethod and trust region methods. However, these methods usually requires a\nnumerical computation of the gradient at every iteration of the method which is\nprone to numerical errors. We propose a simple limited-memory technique for\nimproving the accuracy of a numerically computed gradient in this\ngradient-based optimization framework by exploiting (1) a coordinate\ntransformation of the gradient and (2) the history of previously taken descent\ndirections. The method is verified empirically by extensive experimentation on\nboth test functions and on real data applications. The proposed method is\nimplemented in the R package smartGrad and in C++.", "title": "Smart Gradient -- An Adaptive Technique for Improving Gradient  Estimation"}, {"link": "https://arxiv.org/abs/2106.07314", "abstract": "Increasing deployment of photovoltaics (PV) plants demands for cheap and fast\ninspection. A viable tool for this task is thermographic imaging by unmanned\naerial vehicles (UAV). In this work, we develop a computer vision tool for the\nsemi-automatic extraction of PV modules from thermographic UAV videos. We use\nit to curate a dataset containing 4.3 million IR images of 107842 PV modules\nfrom thermographic videos of seven different PV plants. To demonstrate its use\nfor automated PV plant inspection, we train a ResNet-50 to classify ten common\nmodule anomalies with more than 90 % test accuracy. Experiments show that our\ntool generalizes well to different PV plants. It successfully extracts PV\nmodules from 512 out of 561 plant rows. Failures are mostly due to an\ninappropriate UAV trajectory and erroneous module segmentation. Including all\nmanual steps our tool enables inspection of 3.5 MW p to 9 MW p of PV\ninstallations per day, potentially scaling to multi-gigawatt plants due to its\nparallel nature. While we present an effective method for automated PV plant\ninspection, we are also confident that our approach helps to meet the growing\ndemand for large thermographic datasets for machine learning tasks, such as\npower prediction or unsupervised defect identification.", "title": "Computer Vision Tool for Detection, Mapping and Fault Classification of  PV Modules in Aerial IR Videos"}, {"link": "https://arxiv.org/abs/2106.07316", "abstract": "Recently, pre-trained contextual models, such as BERT, have shown to perform\nwell in language related tasks. We revisit the design decisions that govern the\napplicability of these models for the passage re-ranking task in open-domain\nquestion answering. We find that common approaches in the literature rely on\nfine-tuning a pre-trained BERT model and using a single, global representation\nof the input, discarding useful fine-grained relevance signals in token- or\nsentence-level representations. We argue that these discarded tokens hold\nuseful information that can be leveraged. In this paper, we explicitly model\nthe sentence-level representations by using Dynamic Memory Networks (DMNs) and\nconduct empirical evaluation to show improvements in passage re-ranking over\nfine-tuned vanilla BERT models by memory-enhanced explicit sentence modelling\non a diverse set of open-domain QA datasets. We further show that freezing the\nBERT model and only training the DMN layer still comes close to the original\nperformance, while improving training efficiency drastically. This indicates\nthat the usual fine-tuning step mostly helps to aggregate the inherent\ninformation in a single output token, as opposed to adapting the whole model to\nthe new task, and only achieves rather small gains.", "title": "Exploiting Sentence-Level Representations for Passage Ranking"}, {"link": "https://arxiv.org/abs/2106.07317", "abstract": "Automated machine learning techniques benefited from tremendous research\nprogress in recently. These developments and the continuous-growing demand for\nmachine learning experts led to the development of numerous AutoML tools.\nHowever, these tools assume that the entire training dataset is available\nupfront and that the underlying distribution does not change over time. These\nassumptions do not hold in a data stream mining setting where an unbounded\nstream of data cannot be stored and is likely to manifest concept drift.\nIndustry applications of machine learning on streaming data become more popular\ndue to the increasing adoption of real-time streaming patterns in IoT,\nmicroservices architectures, web analytics, and other fields. The research\nsummarized in this paper surveys the state-of-the-art open-source AutoML tools,\napplies them to data collected from streams, and measures how their performance\nchanges over time. For comparative purposes, batch, batch incremental and\ninstance incremental estimators are applied and compared. Moreover, a\nmeta-learning technique for online algorithm selection based on meta-feature\nextraction is proposed and compared while model replacement and continual\nAutoML techniques are discussed. The results show that off-the-shelf AutoML\ntools can provide satisfactory results but in the presence of concept drift,\ndetection or adaptation techniques have to be applied to maintain the\npredictive accuracy over time.", "title": "Automated Machine Learning Techniques for Data Streams"}, {"link": "https://arxiv.org/abs/2106.07318", "abstract": "The source number identification is an essential step in direction-of-arrival\n(DOA) estimation. Existing methods may provide a wrong source number due to\ninferior statistical properties (in low SNR or limited snapshots) or modeling\nerrors (caused by relaxing sparse penalties), especially in impulsive noise. To\naddress this issue, we propose a novel idea of simultaneous source number\nidentification and DOA estimation. We formulate a multiobjective off-grid DOA\nestimation model to realize this idea, by which the source number can be\nautomatically identified together with DOA estimation. In particular, the\nsource number is properly exploited by the $l_0$ norm of impinging signals\nwithout relaxations, guaranteeing accuracy. Furthermore, we design a\nmultiobjective bilevel evolutionary algorithm to solve the proposed model. The\nsource number identification and sparse recovery are simultaneously optimized\nat the on-grid (lower) level. A forward search strategy is developed to further\nrefine the grid at the off-grid (upper) level. This strategy does not need\nlinear approximations and can eliminate the off-grid gap with low computational\ncomplexity. Simulation results demonstrate the outperformance of our method in\nterms of source number and root mean square error.", "title": "Multiobjective Bilevel Evolutionary Approach for Off-Grid  Direction-of-Arrival Estimation"}, {"link": "https://arxiv.org/abs/2106.07321", "abstract": "Microservice architecture advocates a number of technologies and practices\nsuch as lightweight container, container orchestration, and DevOps, with the\npromised benefits of faster delivery, improved scalability, and greater\nautonomy. However, microservice systems implemented in industry vary a lot in\nterms of adopted practices and achieved benefits, drastically different from\nwhat is advocated in the literature. In this article, we conduct an empirical\nstudy, including an online survey with 51 responses and 14 interviews for\nexperienced microservice experts to advance our understanding regarding to\nmicroservice practices in industry. As a part of our findings, the empirical\nstudy clearly revealed three levels of maturity of microservice systems (from\nbasic to advanced): independent development and deployment, high scalability\nand availability, and service ecosystem, categorized by the fulfilled benefits\nof microservices. We also identify 11 practical issues that constrain the\nmicroservice capabilities of organizations. For each issue, we summarize the\npractices that have been explored and adopted in industry, along with the\nremaining challenges. Our study can help practitioners better position their\nmicroservice systems and determine what infrastructures and capabilities are\nworth investing. Our study can also help researchers better understand\nindustrial microservice practices and identify useful research problems.", "title": "No Free Lunch: Microservice Practices Reconsidered in Industry"}, {"link": "https://arxiv.org/abs/2106.07324", "abstract": "We numerically study solitary waves in the coupled nonlinear Schr\\\"odinger\nequations. We detect pitchfork bifurcations of the fundamental solitary wave\nand compute eigenvalues and eigenfunctions of the corresponding eigenvalue\nproblems to determine the spectral stability of solitary waves born at the\npitchfork bifurcations. Our numerical results demonstrate the theoretical ones\nwhich the authors obtained recently. We also compute generalized eigenfunctions\nassociated with the zero eigenvalue for the bifurcated solitary wave exhibiting\na saddle-node bifurcation, and show that it does not change its stability type\nat the saddle-node bifurcation point.", "title": "Numerical Computations for Bifurcations and Spectral Stability of  Solitary Waves in Coupled Nonlinear Schr\u00f6dinger Equations"}, {"link": "https://arxiv.org/abs/2106.07327", "abstract": "Image classification is an important task in various machine learning\napplications. In recent years, a number of classification methods based on\nquantum machine learning and different quantum image encoding techniques have\nbeen proposed. In this paper, we study the effect of three different quantum\nimage encoding approaches on the performance of a convolution-inspired hybrid\nquantum-classical image classification algorithm called quanvolutional neural\nnetwork (QNN). We furthermore examine the effect of variational - i.e.\ntrainable - quantum circuits on the classification results. Our experiments\nindicate that some image encodings are better suited for variational circuits.\nHowever, our experiments show as well that there is not one best image\nencoding, but that the choice of the encoding depends on the specific\nconstraints of the application.", "title": "Variational Quanvolutional Neural Networks with enhanced image encoding"}, {"link": "https://arxiv.org/abs/2106.07329", "abstract": "We develop theory and algorithms for average-reward on-policy Reinforcement\nLearning (RL). We first consider bounding the difference of the long-term\naverage reward for two policies. We show that previous work based on the\ndiscounted return (Schulman et al., 2015; Achiam et al., 2017) results in a\nnon-meaningful bound in the average-reward setting. By addressing the\naverage-reward criterion directly, we then derive a novel bound which depends\non the average divergence between the two policies and Kemeny's constant. Based\non this bound, we develop an iterative procedure which produces a sequence of\nmonotonically improved policies for the average reward criterion. This\niterative procedure can then be combined with classic DRL (Deep Reinforcement\nLearning) methods, resulting in practical DRL algorithms that target the\nlong-run average reward criterion. In particular, we demonstrate that\nAverage-Reward TRPO (ATRPO), which adapts the on-policy TRPO algorithm to the\naverage-reward criterion, significantly outperforms TRPO in the most\nchallenging MuJuCo environments.", "title": "On-Policy Deep Reinforcement Learning for the Average-Reward Criterion"}, {"link": "https://arxiv.org/abs/2106.07333", "abstract": "Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in\nthe field of radiology to create images of the anatomical and physiological\nstructure of patients. MRI is the prevalent medical imaging practice to find\nabnormalities in soft tissues. Traditionally they are analyzed by a radiologist\nto detect abnormalities in soft tissues, especially the brain. The process of\ninterpreting a massive volume of patient's MRI is laborious. Hence, the use of\nMachine Learning methodologies can aid in detecting abnormalities in soft\ntissues with considerable accuracy. In this research, we have curated a novel\ndataset and developed a framework that uses Deep Transfer Learning to perform a\nmulti-classification of tumors in the brain MRI images. In this paper, we\nadopted the Deep Residual Convolutional Neural Network (ResNet50) architecture\nfor the experiments along with discriminative learning techniques to train the\nmodel. Using the novel dataset and two publicly available MRI brain datasets,\nthis proposed approach attained a classification accuracy of 86.40\\% on the\ncurated dataset, 93.80\\% on the Harvard Whole Brain Atlas dataset, and 97.05\\%\naccuracy on the School of Biomedical Engineering dataset. Results of our\nexperiments significantly demonstrate our proposed framework for transfer\nlearning is a potential and effective method for brain tumor\nmulti-classification tasks.", "title": "Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class  Classification"}, {"link": "https://arxiv.org/abs/2106.07336", "abstract": "This work considers new entropy-based proofs of some known, or otherwise\nrefined, combinatorial bounds for bipartite graphs. These include upper bounds\non the number of the independent sets, lower bounds on the minimal number of\ncolors in constrained edge coloring, and lower bounds on the number of walks of\na given length in bipartite graphs. The proofs of these combinatorial results\nrely on basic properties of the Shannon entropy.", "title": "Entropy-Based Proofs of Combinatorial Results on Bipartite Graphs"}, {"link": "https://arxiv.org/abs/2106.07338", "abstract": "This research paper proposes a novel Neighbourhood Rough Set based approach\nfor supervised Multi-document Text Summarization (MDTS) with analysis and\nimpact on the summarization results for MDTS. Here, Rough Set based LERS\nalgorithm is improved using Neighborhood Rough Set which is itself a novel\ncombination called Neighborhood-LERS to be experimented for evaluations of\nefficacy and efficiency. In this paper, we shall apply and evaluate the\nproposed Neighborhood-LERS for Multi-document Summarization which here is\nproved experimentally to be superior to the base LERS technique for MDTS.", "title": "Neighborhood Rough Set based Multi-document Summarization"}, {"link": "https://arxiv.org/abs/2106.07340", "abstract": "Due to the transfer learning nature of BERT model, researchers have achieved\nbetter performance than base BERT by further pre-training the original BERT on\na huge domain-specific corpus. Due to the special nature of mathematical texts\nwhich often contain math equations and symbols, the original BERT model\npre-trained on general English context will not fit Natural Language Processing\n(NLP) tasks in mathematical education well. Therefore, we propose MathBERT, a\nBERT pre-trained on large mathematical corpus including pre-k to graduate level\nmathematical content to tackle math-specific tasks. In addition, We generate a\ncustomized mathematical vocabulary to pre-train with MathBERT and compare the\nperformance to the MathBERT pre-trained with the original BERT vocabulary. We\nselect three important tasks in mathematical education such as knowledge\ncomponent, auto-grading, and knowledge tracing prediction to evaluate the\nperformance of MathBERT. Our experiments show that MathBERT outperforms the\nbase BERT by 2-9\\% margin. In some cases, MathBERT pre-trained with\nmathematical vocabulary is better than MathBERT trained with original\nvocabulary.To our best knowledge, MathBERT is the first pre-trained model for\ngeneral purpose mathematics education tasks.", "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in  Mathematics Education"}, {"link": "https://arxiv.org/abs/2106.07341", "abstract": "Although most logistics and freight forwarding organizations, in one way or\nanother, claim to have core values. The engagement of employees is a vast\nstructure that affects almost every part of the company's core environmental\nvalues. There is little theoretical knowledge about the relationship between\nfirms and the engagement of employees. Based on research literature, this paper\naims to provide a novel approach for insight around employee engagement in a\nlogistics organization by implementing deep natural language processing\nconcepts. The artificial intelligence-enabled solution named Intelligent Pulse\n(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and\nprovides the actionable insights and gist of employee feedback. I-Pulse allows\nthe stakeholders to think in new ways in their organization, helping them to\nhave a powerful influence on employee engagement, retention, and efficiency.\nThis study is of corresponding interest to researchers and practitioners.", "title": "i-Pulse: A NLP based novel approach for employee engagement in logistics  organization"}, {"link": "https://arxiv.org/abs/2106.07343", "abstract": "In this paper, we investigate few-shot joint learning for dialogue language\nunderstanding. Most existing few-shot models learn a single task each time with\nonly a few examples. However, dialogue language understanding contains two\nclosely related tasks, i.e., intent detection and slot filling, and often\nbenefits from jointly learning the two tasks. This calls for new few-shot\nlearning techniques that are able to capture task relations from only a few\nexamples and jointly learn multiple tasks. To achieve this, we propose a\nsimilarity-based few-shot learning scheme, named Contrastive Prototype Merging\nnetwork (ConProm), that learns to bridge metric spaces of intent and slot on\ndata-rich domains, and then adapt the bridged metric space to the specific\nfew-shot domain. Experiments on two public datasets, Snips and FewJoint, show\nthat our model significantly outperforms the strong baselines in one and five\nshots settings.", "title": "Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent  Detection and Slot Filling"}, {"link": "https://arxiv.org/abs/2106.07344", "abstract": "COVID-19 has affected the world economy and the daily life routine of almost\neveryone. It has been a hot topic on social media platforms such as Twitter,\nFacebook, etc. These social media platforms enable users to share information\nwith other users who can reshare this information, thus causing this\ninformation to spread. Twitter's retweet functionality allows users to share\nthe existing content with other users without altering the original content.\nAnalysis of social media platforms can help in detecting emergencies during\npandemics that lead to taking preventive measures. One such type of analysis is\npredicting the number of retweets for a given COVID-19 related tweet. Recently,\nCIKM organized a retweet prediction challenge for COVID-19 tweets focusing on\nusing numeric features only. However, our hypothesis is, tweet text may play a\nvital role in an accurate retweet prediction. In this paper, we combine numeric\nand text features for COVID-19 related retweet predictions. For this purpose,\nwe propose two CNN and RNN based models and evaluate the performance of these\nmodels on a publicly available TweetsCOV19 dataset using seven different\nevaluation metrics. Our evaluation results show that combining tweet text with\nnumeric features improves the performance of retweet prediction significantly.", "title": "Understanding Information Spreading Mechanisms During COVID-19 Pandemic  by Analyzing the Impact of Tweet Text and User Features for Retweet  Prediction"}, {"link": "https://arxiv.org/abs/2106.07345", "abstract": "Although BERT and its variants have reshaped the NLP landscape, it still\nremains unclear how best to derive sentence embeddings from such pre-trained\nTransformers. In this work, we propose a contrastive learning method that\nutilizes self-guidance for improving the quality of BERT sentence\nrepresentations. Our method fine-tunes BERT in a self-supervised fashion, does\nnot rely on data augmentation, and enables the usual [CLS] token embeddings to\nfunction as sentence vectors. Moreover, we redesign the contrastive learning\nobjective (NT-Xent) and apply it to sentence representation learning. We\ndemonstrate with extensive experiments that our approach is more effective than\ncompetitive baselines on diverse sentence-related tasks. We also show it is\nefficient at inference and robust to domain shifts.", "title": "Self-Guided Contrastive Learning for BERT Sentence Representations"}, {"link": "https://arxiv.org/abs/2106.07346", "abstract": "Topic modeling is an unsupervised method for revealing the hidden semantic\nstructure of a corpus. It has been increasingly widely adopted as a tool in the\nsocial sciences, including political science, digital humanities and\nsociological research in general. One desirable property of topic models is to\nallow users to find topics describing a specific aspect of the corpus. A\npossible solution is to incorporate domain-specific knowledge into topic\nmodeling, but this requires a specification from domain experts. We propose a\nnovel query-driven topic model that allows users to specify a simple query in\nwords or phrases and return query-related topics, thus avoiding tedious work\nfrom domain experts. Our proposed approach is particularly attractive when the\nuser-specified query has a low occurrence in a text corpus, making it difficult\nfor traditional topic models built on word cooccurrence patterns to identify\nrelevant topics. Experimental results demonstrate the effectiveness of our\nmodel in comparison with both classical topic models and neural topic models.", "title": "Query-Driven Topic Model"}, {"link": "https://arxiv.org/abs/2106.07347", "abstract": "Recommender system recommends interesting items to users based on users' past\ninformation history. Researchers have been paying attention to improvement of\nalgorithmic performance such as MAE and precision@K. Major techniques such as\nmatrix factorization and learning to rank are optimized based on such\nevaluation metrics. However, the intrinsic Matthew Effect problem poses great\nthreat to the fairness of the recommender system, and the unfairness problem\ncannot be resolved by optimization of traditional metrics. In this paper, we\npropose a novel algorithm that incorporates Matthew Effect reduction with the\nmatrix factorization framework. We demonstrate that our approach can boost the\nfairness of the algorithm and enhances performance evaluated by traditional\nmetrics.", "title": "Zipf Matrix Factorization : Matrix Factorization with Matthew Effect  Reduction"}, {"link": "https://arxiv.org/abs/2106.07348", "abstract": "In this era of digitisation, news reader tend to read news online. This is\nbecause, online media instantly provides access to a wide variety of content.\nThus, people don't have to wait for tomorrow's newspaper to know what's\nhappening today. Along with these virtues, online news have some vices as well.\nOne such vice is presence of social media posts (tweets) relating to news\narticles whose sole purpose is to draw attention of the users rather than\ndirecting them to read the actual content. Such posts are referred to as\nclickbaits. The objective of this project is to develop a system which would be\ncapable of predicting how likely are the social media posts (tweets) relating\nto new articles tend to be clickbait.", "title": "Is it a click bait? Let's predict using Machine Learning"}, {"link": "https://arxiv.org/abs/2106.07349", "abstract": "BERT has been a breakthrough in language understanding by leveraging the\nmulti-head self-attention mechanism in its architecture. To the best of our\nknowledge this work is the first to leverage Layer Integrated Gradients\nAttribution Scores (LIGAS) to explain the Linguistic Acceptability criteria\nthat are learnt by BERT on the Corpus of Linguistic Acceptability (CoLA)\nbenchmark dataset. Our experiments on 5 different categories of sentences lead\nto the following interesting findings: 1) LIGAS for Linguistically Acceptable\n(LA) sentences are significantly smaller in comparison to Linguistically\nUnacceptable (LUA) sentences, 2) There are specific subtrees of the\nConstituency Parse Tree (CPT) for LA and LUA sentences which contribute larger\nLIGAS, 3) Across the different categories of sentences we observed around 88%\nto 100% of the Correctly classified sentences had positive LIGAS, indicating a\nstrong positive relationship to the prediction confidence of the model, and 4)\nAround 57% of the Misclassified sentences had positive LIGAS, which we believe\ncan become correctly classified sentences if the LIGAS are parameterized in the\nloss function of the model.", "title": "Using Integrated Gradients to explain Linguistic Acceptability learnt by  BERT"}, {"link": "https://arxiv.org/abs/2106.07350", "abstract": "Transformer model architectures have become an indispensable staple in deep\nlearning lately for their effectiveness across a range of tasks. Recently, a\nsurge of \"X-former\" models have been proposed which improve upon the original\nTransformer architecture. However, most of these variants make changes only\naround the quadratic time and memory complexity of self-attention, i.e. the dot\nproduct between the query and the key. What's more, they are calculate solely\nin Euclidean space. In this work, we propose a novel Transformer with\nHyperbolic Geometry (THG) model, which take the advantage of both Euclidean\nspace and Hyperbolic space. THG makes improvements in linear transformations of\nself-attention, which are applied on the input sequence to get the query and\nthe key, with the proposed hyperbolic linear. Extensive experiments on sequence\nlabeling task, machine reading comprehension task and classification task\ndemonstrate the effectiveness and generalizability of our model. It also\ndemonstrates THG could alleviate overfitting.", "title": "THG: Transformer with Hyperbolic Geometry"}, {"link": "https://arxiv.org/abs/2106.07352", "abstract": "We present an instance-based nearest neighbor approach to entity linking. In\ncontrast to most prior entity retrieval systems which represent each entity\nwith a single vector, we build a contextualized mention-encoder that learns to\nplace similar mentions of the same entity closer in vector space than mentions\nof different entities. This approach allows all mentions of an entity to serve\nas \"class prototypes\" as inference involves retrieving from the full set of\nlabeled entity mentions in the training set and applying the nearest mention\nneighbor's entity label. Our model is trained on a large multilingual corpus of\nmention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor\ninference on an index of 700 million mentions. It is simpler to train, gives\nmore interpretable predictions, and outperforms all other systems on two\nmultilingual entity linking benchmarks.", "title": "MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation  Network"}, {"link": "https://arxiv.org/abs/2106.07353", "abstract": "Classifiers commonly make use of pre-annotated datasets, wherein a model is\nevaluated by pre-defined metrics on a held-out test set typically made of\nhuman-annotated labels. Metrics used in these evaluations are tied to the\navailability of well-defined ground truth labels, and these metrics typically\ndo not allow for inexact matches. These noisy ground truth labels and strict\nevaluation metrics may compromise the validity and realism of evaluation\nresults. In the present work, we discuss these concerns and conduct a\nsystematic posthoc verification experiment on the entity linking (EL) task.\nUnlike traditional methodologies, which asks annotators to provide free-form\nannotations, we ask annotators to verify the correctness of annotations after\nthe fact (i.e., posthoc). Compared to pre-annotation evaluation,\nstate-of-the-art EL models performed extremely well according to the posthoc\nevaluation methodology. Posthoc validation also permits the validation of the\nground truth dataset. Surprisingly, we find predictions from EL models had a\nsimilar or higher verification rate than the ground truth. We conclude with a\ndiscussion on these findings and recommendations for future evaluations.", "title": "Posthoc Verification and the Fallibility of the Ground Truth"}, {"link": "https://arxiv.org/abs/2106.07356", "abstract": "In many industrial applications like online advertising and recommendation\nsystems, diverse and accurate user profiles can greatly help improve\npersonalization. For building user profiles, deep learning is widely used to\nmine expressive tags to describe users' preferences from their historical\nactions. For example, tags mined from users' click-action history can represent\nthe categories of ads that users are interested in, and they are likely to\ncontinue being clicked in the future. Traditional solutions usually introduce\nmultiple independent Two-Tower models to mine tags from different actions,\ne.g., click, conversion. However, the models cannot learn complementarily and\nsupport effective training for data-sparse actions. Besides, limited by the\nlack of information fusion between the two towers, the model learning is\ninsufficient to represent users' preferences on various topics well. This paper\nintroduces a novel multi-task model called Mixture of Virtual-Kernel Experts\n(MVKE) to learn multiple topic-related user preferences based on different\nactions unitedly. In MVKE, we propose a concept of Virtual-Kernel Expert, which\nfocuses on modeling one particular facet of the user's preference, and all of\nthem learn coordinately. Besides, the gate-based structure used in MVKE builds\nan information fusion bridge between two towers, improving the model's\ncapability much and maintaining high efficiency. We apply the model in Tencent\nAdvertising System, where both online and offline evaluations show that our\nmethod has a significant improvement compared with the existing ones and brings\nabout an obvious lift to actual advertising revenue.", "title": "Mixture of Virtual-Kernel Experts for Multi-Objective User Profile  Modeling"}, {"link": "https://arxiv.org/abs/2106.07357", "abstract": "This paper deals with the numerical approximation of the biharmonic inverse\nsource problem in an abstract setting in which the measurement data is\nfinite-dimensional. This unified framework in particular covers the conforming\nand nonconforming finite element methods (FEMs). The inverse problem is\nanalysed through the forward problem. Error estimate for the forward solution\nis derived in an abstract set-up that applies to conforming and Morley\nnonconforming FEMs. Since the inverse problem is ill-posed, Tikhonov\nregularisation is considered to obtain a stable approximate solution. Error\nestimate is established for the regularised solution for different\nregularisation schemes. Numerical results that confirm the theoretical results\nare also presented.", "title": "Conforming and Nonconforming Finite Element Methods for Biharmonic  Inverse Source Problem"}, {"link": "https://arxiv.org/abs/2106.07359", "abstract": "Extracting metadata from scientific papers can be considered a solved problem\nin NLP due to the high accuracy of state-of-the-art methods. However, this does\nnot apply to German scientific publications, which have a variety of styles and\nlayouts. In contrast to most of the English scientific publications that follow\nstandard and simple layouts, the order, content, position and size of metadata\nin German publications vary greatly among publications. This variety makes\ntraditional NLP methods fail to accurately extract metadata from these\npublications. In this paper, we present a method that extracts metadata from\nPDF documents with different layouts and styles by viewing the document as an\nimage. We used Mask R-CNN that is trained on COCO dataset and finetuned with\nPubLayNet dataset that consists of ~200K PDF snapshots with five basic classes\n(e.g. text, figure, etc). We refine-tuned the model on our proposed synthetic\ndataset consisting of ~30K article snapshots to extract nine patterns (i.e.\nauthor, title, etc). Our synthetic dataset is generated using contents in both\nlanguages German and English and a finite set of challenging templates obtained\nfrom German publications. Our method achieved an average accuracy of around\n$90\\%$ which validates its capability to accurately extract metadata from a\nvariety of PDF documents with challenging templates.", "title": "MexPub: Deep Transfer Learning for Metadata Extraction from German  Publications"}, {"link": "https://arxiv.org/abs/2106.07360", "abstract": "In this work, we study the behavior of standard models for community\ndetection under spectral manipulations. Through various ablation experiments,\nwe evaluate the impact of bandpass filtering on the performance of a GCN: we\nempirically show that most of the necessary and used information for nodes\nclassification is contained in the low-frequency domain, and thus contrary to\nimages, high frequencies are less crucial to community detection. In\nparticular, it is sometimes possible to obtain accuracies at a state-of-the-art\nlevel with simple classifiers that rely only on a few low frequencies.", "title": "Low-Rank Projections of GCNs Laplacian"}, {"link": "https://arxiv.org/abs/2106.07363", "abstract": "Leveraging short-text contents to estimate the occupation of microblog\nauthors has significant gains in many applications. Yet challenges abound.\nFirstly brief textual contents come with excessive lexical noise that makes the\ninference problem challenging. Secondly, cognitive-semantics are not evident,\nand important linguistic features are latent in short-text contents. Thirdly,\nit is hard to measure the correlation between the cognitive short-text\nsemantics and the features pertaining various occupations. We argue that the\nmulti-aspect cognitive features are needed to correctly associate short-text\ncontents to a particular job and discover suitable people for the careers. To\nthis end, we devise a novel framework that on the one hand, can infer\nshort-text contents and exploit cognitive features, and on the other hand,\nfuses various adopted novel algorithms, such as curve fitting, support vector,\nand boosting modules to better predict the occupation of the authors. The final\nestimation module manufactures the $R^w$-tree via coherence weight to tune the\nbest outcome in the inferring process. We conduct comprehensive experiments on\nreal-life Twitter data. The experimental results show that compared to other\nrivals, our cognitive multi-aspect model can achieve a higher performance in\nthe career estimation procedure, where it is inevitable to neglect the\ncontextual semantics of users.", "title": "Cognitive-aware Short-text Understanding for Inferring Professions"}, {"link": "https://arxiv.org/abs/2106.07364", "abstract": "We exhibit that the implicit UCCA parser does not address numeric fused-heads\n(NFHs) consistently, which could result either from inconsistent annotation,\ninsufficient training data or a modelling limitation. and show which factors\nare involved. We consider this phenomenon important, as it is pervasive in text\nand critical for correct inference. Careful design and fine-grained annotation\nof NFHs in meaning representation frameworks would benefit downstream tasks\nsuch as machine translation, natural language inference and question answering,\nparticularly when they require numeric reasoning, as recovering and\ncategorizing them. We are investigating the treatment of this phenomenon by\nother meaning representations, such as AMR. We encourage researchers in meaning\nrepresentations, and computational linguistics in general, to address this\nphenomenon in future research.", "title": "Meaning Representation of Numeric Fused-Heads in UCCA"}, {"link": "https://arxiv.org/abs/2106.07369", "abstract": "Understanding how agents learn to generalize -- and, in particular, to\nextrapolate -- in high-dimensional, naturalistic environments remains a\nchallenge for both machine learning and the study of biological agents. One\napproach to this has been the use of function learning paradigms, which allow\npeoples' empirical patterns of generalization for smooth scalar functions to be\ndescribed precisely. However, to date, such work has not succeeded in\nidentifying mechanisms that acquire the kinds of general purpose\nrepresentations over which function learning can operate to exhibit the\npatterns of generalization observed in human empirical studies. Here, we\npresent a framework for how a learner may acquire such representations, that\nthen support generalization -- and extrapolation in particular -- in a few-shot\nfashion. Taking inspiration from a classic theory of visual processing, we\nconstruct a self-supervised encoder that implements the basic inductive bias of\ninvariance under topological distortions. We show the resulting representations\noutperform those from other models for unsupervised time series learning in\nseveral downstream function learning tasks, including extrapolation.", "title": "A Self-Supervised Framework for Function Learning and Extrapolation"}, {"link": "https://arxiv.org/abs/2106.07374", "abstract": "Since the emergence of the worldwide pandemic of COVID-19, relevant research\nhas been published at a dazzling pace, which makes it hard to follow the\nresearch in this area without dedicated efforts. It is practically impossible\nto implement this task manually due to the high volume of the relevant\nliterature. Text mining has been considered to be a powerful approach to\naddress this challenge, especially the topic modeling, a well-known\nunsupervised method that aims to reveal latent topics from the literature.\nHowever, in spite of its potential utility, the results generated from this\napproach are often investigated manually. Hence, its application to the\nCOVID-19 literature is not straightforward and expert knowledge is needed to\nmake meaningful interpretations. In order to address these challenges, we\npropose a novel analytical framework for effective visualization and mining of\ntopic modeling results. Here we assumed that topics constituting a paper can be\npositioned on an interaction map, which belongs to a high-dimensional Euclidean\nspace. Based on this assumption, after summarizing topics with their topic-word\ndistributions using the biterm topic model, we mapped these latent topics on\nnetworks to visualize relationships among the topics. Moreover, in the proposed\napproach, the change of relationships among topics can be traced using a\ntrajectory plot generated with different levels of word richness. These results\ntogether provide a deeply mined and intuitive representation of relationships\namong topics related to a specific research area. The application of this\nproposed framework to the PubMed literature shows that our approach facilitates\nunderstanding of the topics constituting the COVID-19 knowledge.", "title": "Graph-based Trajectory Visualization for Text Mining of COVID-19  Biomedical Literature"}, {"link": "https://arxiv.org/abs/2106.07378", "abstract": "We present the Multiple Criteria Decision Analysis Methods Selection Software\n(MCDA-MSS). This decision support system helps analysts answering a recurring\nquestion in decision science: Which is the most suitable Multiple Criteria\nDecision Analysis method (or a subset of MCDA methods) that should be used for\na given Decision-Making Problem (DMP)?. The MCDA-MSS includes guidance to lead\ndecision-making processes and choose among an extensive collection (over 200)\nof MCDA methods. These are assessed according to an original comprehensive set\nof problem characteristics. The accounted features concern problem formulation,\npreference elicitation and types of preference information, desired features of\na preference model, and construction of the decision recommendation. The\napplicability of the MCDA-MSS has been tested on several case studies. The\nMCDA-MSS includes the capabilities of (i) covering from very simple to very\ncomplex DMPs, (ii) offering recommendations for DMPs that do not match any\nmethod from the collection, (iii) helping analysts prioritize efforts for\nreducing gaps in the description of the DMPs, and (iv) unveiling methodological\nmistakes that occur in the selection of the methods. A community-wide\ninitiative involving experts in MCDA methodology, analysts using these methods,\nand decision-makers receiving decision recommendations will contribute to\nexpansion of the MCDA-MSS.", "title": "Recommending Multiple Criteria Decision Analysis Methods with A New  Taxonomy-based Decision Support System"}, {"link": "https://arxiv.org/abs/2106.07380", "abstract": "Social media creates crucial mass changes, as popular posts and opinions cast\na significant influence on users' decisions and thought processes. For example,\nthe recent Reddit uprising inspired by r/wallstreetbets which had remarkable\neconomic impact was started with a series of posts on the thread. The\nprediction of posts that may have a notable impact will allow for the\npreparation of possible following trends. This study aims to develop a machine\nlearning model capable of accurately predicting the popularity of a Reddit\npost. Specifically, the model is predicting the number of upvotes a post will\nreceive based on its textual content. I experimented with three different\nmodels: a baseline linear regression model, a random forest regression model,\nand a neural network. I collected Reddit post data from an online data set and\nanalyzed the model's performance when trained on a single subreddit and a\ncollection of subreddits. The results showed that the neural network model\nperformed the best when the loss of the models were compared. With the use of a\nmachine learning model to predict social trends through the reaction users have\nto post, a better picture of the near future can be envisioned.", "title": "Predicting the Popularity of Reddit Posts with AI"}, {"link": "https://arxiv.org/abs/2106.07381", "abstract": "In the area of customer support, understanding customers' intents is a\ncrucial step. Machine learning plays a vital role in this type of intent\nclassification. In reality, it is typical to collect confirmation from customer\nsupport representatives (CSRs) regarding the intent prediction, though it can\nunnecessarily incur prohibitive cost to ask CSRs to assign existing or new\nintents to the mis-classified cases. Apart from the confirmed cases with and\nwithout intent labels, there can be a number of cases with no human curation.\nThis data composition (Positives + Unlabeled + multiclass Negatives) creates\nunique challenges for model development. In response to that, we propose a\nsemi-supervised multi-task learning paradigm. In this manuscript, we share our\nexperience in building text-based intent classification models for a customer\nsupport service on an E-commerce website. We improve the performance\nsignificantly by evolving the model from multiclass classification to\nsemi-supervised multi-task learning by leveraging the negative cases, domain-\nand task-adaptively pretrained ALBERT on customer contact texts, and a number\nof un-curated data with no labels. In the evaluation, the final model boosts\nthe average AUC ROC by almost 20 points compared to the baseline finetuned\nmulticlass classification ALBERT model.", "title": "A Semi-supervised Multi-task Learning Approach to Classify Customer  Contact Intents"}, {"link": "https://arxiv.org/abs/2106.07384", "abstract": "Existing parking recommendation solutions mainly focus on finding and\nsuggesting parking spaces based on the unoccupied options only. However, there\nare other factors associated with parking spaces that can influence someone's\nchoice of parking such as fare, parking rule, walking distance to destination,\ntravel time, likelihood to be unoccupied at a given time. More importantly,\nthese factors may change over time and conflict with each other which makes the\nrecommendations produced by current parking recommender systems ineffective. In\nthis paper, we propose a novel problem called multi-objective parking\nrecommendation. We present a solution by designing a multi-objective parking\nrecommendation engine called MoParkeR that considers various conflicting\nfactors together. Specifically, we utilise a non-dominated sorting technique to\ncalculate a set of Pareto-optimal solutions, consisting of recommended\ntrade-off parking spots. We conduct extensive experiments using two real-world\ndatasets to show the applicability of our multi-objective recommendation\nmethodology.", "title": "MoParkeR : Multi-objective Parking Recommendation"}, {"link": "https://arxiv.org/abs/2106.07386", "abstract": "This study examines a basic assumption of peer review, namely, the idea that\nthere is a consensus on evaluation criteria among peers, which is a necessary\ncondition for the reliability of peer judgements. Empirical evidence indicating\nthat there is no consensus or more than one consensus would offer an\nexplanation for the disagreement effect, the low inter-rater reliability\nconsistently observed in peer review. To investigate this basic assumption, we\nhave surveyed all humanities scholars in Switzerland on 23 grant review\ncriteria. We have employed latent class tree modelling to identify subgroups in\nwhich scholars rated criteria similarly (i.e. latent classes) and to explore\ncovariates predicting class membership. We have identified two consensus\nclasses, two consensus-close classes, and a consensus-far class. The consensus\nclasses contain a core consensus (ten criteria related to knowledge gaps,\nfeasibility, rigour, comprehensibility and argumentation, and academic\nrelevance, as well as to the competence and experience of the applicant) and a\nbroad consensus that includes the core consensus plus eight\ncontribution-related criteria, such as originality. These results provide a\npossible explanation for the disagreement effect. Moreover, the results are\nconsistent with the notion of conservatism, which holds that original research\nis undervalued in peer review, while other aspects, such as methodology and\nfeasibility, are overweighted. The covariate analysis indicated that age and\nhaving tenure increases from the consensus-far to the consensus-close to the\nconsensus classes. This suggests that the more academic experience scholars\naccumulate, the more their understanding of review criteria conforms to the\nsocial norm.", "title": "Do peers share the same criteria for assessing grant applications?"}, {"link": "https://arxiv.org/abs/2106.07387", "abstract": "The Vehicle Routing Problem (VRP) is the combinatorial optimization problem\nof designing routes for vehicles to visit customers in such a fashion that a\ncost function, typically the number of vehicles, or the total travelled\ndistance is minimized. The problem finds applications in industrial scenarios,\nfor example where Automated Guided Vehicles run through the plant to deliver\ncomponents from the warehouse. This specific problem, henceforth called the\nElectric Conflict-Free Vehicle Routing Problem (CF-EVRP), involves constraints\nsuch as limited operating range of the vehicles, time windows on the delivery\nto the customers, and limited capacity on the number of vehicles the road\nsegments can accommodate at the same time. Such a complex system results in a\nlarge model that cannot easily be solved to optimality in reasonable time. We\ntherefore developed a compositional model that breaks down the problem into\nsmaller and simpler sub-problems and provides sub-optimal, feasible solutions\nto the original problem. The algorithm exploits the strengths of SMT solvers,\nwhich proved in our previous work to be an efficient approach to deal with\nscheduling problems. Compared to a monolithic model for the CF-EVRP, written in\nthe SMT standard language and solved using a state-of-the-art SMT solver the\ncompositional model was found to be significantly faster.", "title": "An SMT Based Compositional Model to Solve a Conflict-Free Electric  Vehicle Routing Problem"}, {"link": "https://arxiv.org/abs/2106.07395", "abstract": "Edges are a basic and fundamental feature in image processing, that are used\ndirectly or indirectly in huge amount of applications. Inspired by the\nexpansion of image resolution and processing power dilated convolution\ntechniques appeared. Dilated convolution have impressive results in machine\nlearning, we discuss here the idea of dilating the standard filters which are\nused in edge detection algorithms. In this work we try to put together all our\nprevious and current results by using instead of the classical convolution\nfilters a dilated one. We compare the results of the edge detection algorithms\nusing the proposed dilation filters with original filters or custom variants.\nExperimental results confirm our statement that dilation of filters have\npositive impact for edge detection algorithms form simple to rather complex\nalgorithms.", "title": "Dilated filters for edge detection algorithms"}, {"link": "https://arxiv.org/abs/2106.07400", "abstract": "Beam search is a go-to strategy for decoding neural sequence models. The\nalgorithm can naturally be viewed as a subset optimization problem, albeit one\nwhere the corresponding set function does not reflect interactions between\ncandidates. Empirically, this leads to sets often exhibiting high overlap,\ne.g., strings may differ by only a single word. Yet in use-cases that call for\nmultiple solutions, a diverse or representative set is often desired. To\naddress this issue, we propose a reformulation of beam search, which we call\ndeterminantal beam search. Determinantal beam search has a natural relationship\nto determinantal point processes (DPPs), models over sets that inherently\nencode intra-set interactions. By posing iterations in beam search as a series\nof subdeterminant maximization problems, we can turn the algorithm into a\ndiverse subset selection process. In a case study, we use the string\nsubsequence kernel to explicitly encourage n-gram coverage in text generated\nfrom a sequence model. We observe that our algorithm offers competitive\nperformance against other diverse set generation strategies in the context of\nlanguage generation, while providing a more general approach to optimizing for\ndiversity.", "title": "Determinantal Beam Search"}, {"link": "https://arxiv.org/abs/2106.07405", "abstract": "In this paper, we develop an adaptive high-order surface finite element\nmethod (FEM) to solve self-consistent field equations of polymers on general\ncurved surfaces. It is an improvement of the existing algorithm of [J. Comp.\nPhys. 387: 230-244 (2019)] in which a linear surface FEM was presented to\naddress this problem. The high-order surface FEM is obtained by the high-order\nsurface geometrical approximation and high-order function space approximation.\nIn order to describe the sharp interface in the strong segregation system more\naccurately, an adaptive FEM equipped with a novel Log marking strategy is\nproposed. Compared with the traditional strategy, this new marking strategy can\nnot only label the elements that need to be refined or coarsened, but also give\nthe refined or coarsened times, which can make full use of the information of a\nposterior error estimator and improve the efficiency of the adaptive algorithm.\nTo demonstrate the power of our approach, we investigate the self-assembled\npatterns of diblock copolymers on several distinct curved surfaces. Numerical\nresults illustrate the efficiency of the proposed method, especially for strong\nsegregation systems.", "title": "An adaptive high-order surface finite element method for the  self-consistent field theory on general curved surfaces"}, {"link": "https://arxiv.org/abs/2106.07408", "abstract": "This paper investigates applications of eye tracking in transport aircraft\ndesign evaluations. Piloted simulations were conducted for a complete flight\nprofile including take off, cruise and landing flight scenario using the\ntransport aircraft flight simulator at CSIR National Aerospace Laboratories.\nThirty-one simulation experiments were carried out with three pilots and\nengineers while recording the ocular parameters and the flight data.\nSimulations were repeated for high workload conditions like flying with\ndegraded visibility and during stall. Pilots visual scan behaviour and workload\nlevels were analysed using ocular parameters; while comparing with the\nstatistical deviations from the desired flight path. Conditions for fatigue\nwere also recreated through long duration simulations and signatures for the\nsame from the ocular parameters were assessed. Results from the study found\ncorrelation between the statistical inferences obtained from the ocular\nparameters with those obtained from the flight path deviations. The paper also\ndemonstrates an evaluators console that assists the designers or evaluators for\nbetter understanding of pilots attentional resource allocation.", "title": "Using Eye Tracker To Evaluate Cockpit Design -- A Flight Simulation  Study"}, {"link": "https://arxiv.org/abs/2106.07409", "abstract": "Short videos have many applications on fashion trends, hot spots, street\ninterviews, public education, and creative advertising. We propose an\nEdge-Aware Network(EANet) that uses edge information to refine the segmentation\nedge. And experiments show our proposed EANet boots up the facial parsing\nresults. We also use post-process like grab cut to refine and merge the parsing\nresults.", "title": "3rd Place Solution for Short-video Face Parsing Challenge"}, {"link": "https://arxiv.org/abs/2106.07410", "abstract": "Machine learning (ML) model explainability has received growing attention,\nespecially in the area related to model risk and regulations. In this paper, we\nreviewed and compared some popular ML model explainability methodologies,\nespecially those related to Natural Language Processing (NLP) models. We then\napplied one of the NLP explainability methods Layer-wise Relevance Propagation\n(LRP) to a NLP classification model. We used the LRP method to derive a\nrelevance score for each word in an instance, which is a local explainability.\nThe relevance scores are then aggregated together to achieve global variable\nimportance of the model. Through the case study, we also demonstrated how to\napply the local explainability method to false positive and false negative\ninstances to discover the weakness of a NLP model. These analysis can help us\nto understand NLP models better and reduce the risk due to the black-box nature\nof NLP models. We also identified some common issues due to the special natures\nof NLP models and discussed how explainability analysis can act as a control to\ndetect these issues after the model has been trained.", "title": "Model Explainability in Deep Learning Based Natural Language Processing"}, {"link": "https://arxiv.org/abs/2106.07411", "abstract": "A few years ago, the first CNN surpassed human performance on ImageNet.\nHowever, it soon became clear that machines lack robustness on more challenging\ntest cases, a major obstacle towards deploying machines \"in the wild\" and\ntowards obtaining better computational models of human visual perception. Here\nwe ask: Are we making progress in closing the gap between human and machine\nvision? To answer this question, we tested human observers on a broad range of\nout-of-distribution (OOD) datasets, adding the \"missing human baseline\" by\nrecording 85,120 psychophysical trials across 90 participants. We then\ninvestigated a range of promising machine learning developments that crucially\ndeviate from standard supervised CNNs along three axes: objective function\n(self-supervised, adversarially trained, CLIP language-image training),\narchitecture (e.g. vision transformers), and dataset size (ranging from 1M to\n1B). Our findings are threefold. (1.) The longstanding robustness gap between\nhumans and CNNs is closing, with the best models now matching or exceeding\nhuman performance on most OOD datasets. (2.) There is still a substantial\nimage-level consistency gap, meaning that humans make different errors than\nmodels. In contrast, most models systematically agree in their categorisation\nerrors, even substantially different ones like contrastive self-supervised vs.\nstandard supervised models. (3.) In many cases, human-to-model consistency\nimproves when training dataset size is increased by one to three orders of\nmagnitude. Our results give reason for cautious optimism: While there is still\nmuch room for improvement, the behavioural difference between human and machine\nvision is narrowing. In order to measure future progress, 17 OOD datasets with\nimage-level human behavioural data are provided as a benchmark here:\nhttps://github.com/bethgelab/model-vs-human/", "title": "Partial success in closing the gap between human and machine vision"}, {"link": "https://arxiv.org/abs/2106.07412", "abstract": "Computing sets of high quality solutions has gained increasing interest in\nrecent years. In this paper, we investigate how to obtain sets of optimal\nsolutions for the classical knapsack problem. We present an algorithm to count\nexactly the number of optima to a zero-one knapsack problem instance. In\naddition, we show how to efficiently sample uniformly at random from the set of\nall global optima. In our experimental study, we investigate how the number of\noptima develops for classical random benchmark instances dependent on their\ngenerator parameters. We find that the number of global optima can increase\nexponentially for practically relevant classes of instances with correlated\nweights and profits which poses a justification for the considered exact\ncounting problem.", "title": "Exact Counting and Sampling of Optima for the Knapsack Problem"}, {"link": "https://arxiv.org/abs/2106.07413", "abstract": "Numerous efforts have been invested in improving the effectiveness of bug\nlocalization techniques, whereas little attention is paid to making these tools\nrun more efficiently in continuously evolving software repositories. This paper\nfirst analyzes the information retrieval model behind a classic bug\nlocalization tool, BugLocator, and builds a mathematical foundation that the\nmodel can be updated incrementally when codebase or bug reports evolve. Then,\nwe present IncBL, a tool for Incremental Bug Localization in evolving software\nrepositories. IncBL is evaluated on the Bugzbook dataset, and the results show\nthat IncBL can significantly reduce the running time by 77.79% on average\ncompared with re-computing the model, while maintaining the same level of\naccuracy. We also implement IncBL as a Github App that can be easily integrated\ninto open-source projects on Github, and users can also deploy and use IncBL\nlocally. The demo video for IncBL can be viewed at\nhttps://youtu.be/G4gMuvlJSb0, and the source code can be found at\nhttps://github.com/soarsmu/IncBL", "title": "IncBL: Incremental Bug Localization"}, {"link": "https://arxiv.org/abs/2106.07417", "abstract": "The technology of network slicing, as the most characteristic feature of the\nfifth generation (5G) wireless networks, manages the resources and network\nfunctions in heterogeneous and logically isolated slices on the top of a shared\nphysical infrastructure, where every slice can be independently customized to\nfulfill the specific requirements of its devoted service type. It enables a new\nparadigm of multi-tenancy networking, where the network slices can be leased by\nthe mobile network operator (MNO) to tenants in form of public cloud computing\nservice, known as Slice-asa- Service (SlaaS). Similar to classical cloud\ncomputing scenarios, SlaaS benefits from overbooking its resources to numerous\ntenants, taking advantage of the resource elasticity and diversity, at a price\nof risking overloading network resources and violating the service-level\nagreements (SLAs), which stipulate the quality of service (QoS) that shall be\nguaranteed to the network slices. Thus, it becomes a critical challenge to the\nMNOs, accurately estimating the resource overload risk - especially under the\nsophisticated network dynamics - for monitoring and enhancing the reliability\nof SlaaS business.", "title": "Online Estimation of Resource Overload Risk in 5G Multi-Tenancy Network"}, {"link": "https://arxiv.org/abs/2106.07419", "abstract": "A low cost remote imaging platform for biological applications was developed.\nThe \"Picroscope\" is a device that allows the user to perform longitudinal\nimaging studies on multi-well cell culture plates. Here we present the network\narchitecture and software used to facilitate communication between modules\nwithin the device as well as external cloud services. A web based console was\ncreated to control the device and view experiment results. Post processing\ntools were developed to analyze captured data in the cloud. The result is a\nplatform for controlling biological experiments from outside the lab.", "title": "Low cost cloud based remote microscopy for biological sciences"}, {"link": "https://arxiv.org/abs/2106.07423", "abstract": "In this paper, we propose an Efficient Two-Level I/O Caching Architecture\n(ETICA) for virtualized platforms that can significantly improve I/O latency,\nendurance, and cost (in terms of cache size) while preserving the reliability\nof write-pending data blocks. As opposed to previous one-level I/O caching\nschemes in virtualized platforms, our proposed architecture 1) provides two\nlevels of cache by employing both Dynamic Random-Access Memory (DRAM) and SSD\nin the I/O caching layer of virtualized platforms and 2) effectively partitions\nthe cache space between running VMs to achieve maximum performance and minimum\ncache size. To manage the two-level cache, unlike the previous reuse distance\ncalculation schemes such as Useful Reuse Distance (URD), which only consider\nthe request type and neglect the impact of cache write policy, we propose a new\nmetric, Policy Optimized reuse Distance (POD). The key idea of POD is to\neffectively calculate the reuse distance and estimate the amount of two-level\nDRAM+SSD cache space to allocate by considering both 1) the request type and 2)\nthe cache write policy. Doing so results in enhanced performance and reduced\ncache size due to the allocation of cache blocks only for the requests that\nwould be served by the I/O cache. ETICA maintains the reliability of\nwrite-pending data blocks and improves performance by 1) assigning an effective\nand fixed write policy at each level of the I/O cache hierarchy and 2)\nemploying effective promotion and eviction methods between cache levels. Our\nextensive experiments conducted with a real implementation of the proposed\ntwo-level storage caching architecture show that ETICA provides 45% higher\nperformance, compared to the state-of-the-art caching schemes in virtualized\nplatforms, while improving both cache size and SSD endurance by 51.7% and\n33.8%, respectively.", "title": "ETICA: Efficient Two-Level I/O Caching Architecture for Virtualized  Platforms"}, {"link": "https://arxiv.org/abs/2106.07431", "abstract": "In this paper, we propose a novel score-base generative model for\nunconditional raw audio synthesis. Our proposal builds upon the latest\ndevelopments on diffusion process modeling with stochastic differential\nequations, which already demonstrated promising results on image generation. We\nmotivate novel heuristics for the choice of the diffusion processes better\nsuited for audio generation, and consider the use of a conditional U-Net to\napproximate the score function. While previous approaches on diffusion models\non audio were mainly designed as speech vocoders in medium resolution, our\nmethod termed CRASH (Controllable Raw Audio Synthesis with High-resolution)\nallows us to generate short percussive sounds in 44.1kHz in a controllable way.\nThrough extensive experiments, we showcase on a drum sound generation task the\nnumerous sampling schemes offered by our method (unconditional generation,\ndeterministic generation, inpainting, interpolation, variations,\nclass-conditional sampling) and propose the class-mixing sampling, a novel way\nto generate \"hybrid\" sounds. Our proposed method closes the gap with GAN-based\nmethods on raw audio, while offering more flexible generation capabilities with\nlighter and easier-to-train models.", "title": "CRASH: Raw Audio Score-based Generative Modeling for Controllable  High-resolution Drum Sound Synthesis"}, {"link": "https://arxiv.org/abs/2106.07432", "abstract": "When studying the evolution of complex systems one refers to model\nrepresentations comprising various descriptive parameters. There is hardly\nresearch where system evolution is described on the base of information flows\nin the system. The paper focuses on the link between the dynamics of\ninformation and system evolution. Information, exchanged between different\nsystem's parts, before being processed is first provided with meaning by the\nsystem. Meanings are generated from the perspective of hindsight, i.e. against\nthe arrow of time. The same information can be differently interpreted by\ndifferent system's parts (i,e,provided with different meanings) so that the\nnumber of options for possible system development is proliferated. Some options\neventually turn into observable system states. So that system evolutionary\ndynamics can be considered as due to information processing within the system.\nThis process is considered here in a model representation. The model under\nstudy is Triple Helix (TH) model, which was earlier used to describe\ninteractions between university, industry and government to foster innovations.\nIn TH model the system is comprised of three interacting parts where each part\nprocess information ina different way. The model is not limited to the sphere\nof innovation and can be used in a broader perspective. Here TH is\nconceptualized in the framework of three compertment model used to describe\ninfectious disease. The paper demonstrates how the dynamics of information and\nmeaning can be incorporated in the description of Covid-19 infectious\npropagation. The results show correspondence of model predictions with\nobservable infection dynamics.", "title": "Information exchange, meaning and redundancy generation in anticipatory  systems: self-organization of expectations -- the case of Covid-19"}, {"link": "https://arxiv.org/abs/2106.07435", "abstract": "There is evidence of misinformation in the online discourses and discussions\nabout the COVID-19 vaccines. Using a sample of 1.6 million geotagged English\ntweets and the data from the CDC COVID Data Tracker, we conduct a quantitative\nstudy to understand the influence of both misinformation and fact-based news on\nTwitter on the COVID-19 vaccine uptake in the U.S. from April 19 when U.S.\nadults were vaccine eligible to May 7, 2021, after controlling state-level\nfactors such as demographics, education, and the pandemic severity. We identify\nthe tweets related to either misinformation or fact-based news by analyzing the\nURLs. By analyzing the content of the most frequent tweets of these two groups,\nwe find that their structures are similar, making it difficult for Twitter\nusers to distinguish one from another by reading the text alone. The users who\nspread both fake news and fact-based news tend to show a negative attitude\ntowards the vaccines. We further conduct the Fama-MacBeth regression with the\nNewey-West adjustment to examine the effect of fake-news-related and\nfact-related tweets on the vaccination rate, and find marginally negative\ncorrelations.", "title": "Both Rates of Fake News and Fact-based News on Twitter Negatively  Correlate with the State-level COVID-19 Vaccine Uptake"}, {"link": "https://arxiv.org/abs/2106.07441", "abstract": "Relative radiometric normalization (RRN) mosaicking among multiple remote\nsensing images is crucial for the downstream tasks, including map-making, image\nrecognition, semantic segmentation, and change detection. However, there are\noften seam lines on the mosaic boundary and radiometric contrast left,\nespecially in complex scenarios, making the appearance of mosaic images\nunsightly and reducing the accuracy of the latter classification/recognition\nalgorithms. This paper renders a novel automatical approach to eliminate seam\nlines in complex RRN mosaicking scenarios. It utilizes the histogram matching\non the overlap area to alleviate radiometric contrast, Poisson editing to\nremove the seam lines, and merging procedure to determine the normalization\ntransfer order. Our method can handle the mosaicking seam lines with arbitrary\nshapes and images with extreme topological relationships (with a small\nintersection area). These conditions make the main feathering or blending\nmethods, e.g., linear weighted blending and Laplacian pyramid blending,\nunavailable. In the experiment, our approach visually surpasses the automatic\nmethods without Poisson editing and the manual blurring and feathering method\nusing GIMP software.", "title": "Automatically eliminating seam lines with Poisson editing in complex  relative radiometric normalization mosaicking scenarios"}, {"link": "https://arxiv.org/abs/2106.07442", "abstract": "Wireless applications that use high-reliability low-latency links depend\ncritically on the capability of the system to predict link quality. This\ndependence is especially acute at the high carrier frequencies used by mmWave\nand THz systems, where the links are susceptible to blockages. Predicting\nblockages with high reliability requires a large number of data samples to\ntrain effective machine learning modules. With the aim of mitigating data\nrequirements, we introduce a framework based on meta-learning, whereby data\nfrom distinct deployments are leveraged to optimize a shared initialization\nthat decreases the data set size necessary for any new deployment. Predictors\nof two different events are studied: (1) at least one blockage occurs in a time\nwindow, and (2) the link is blocked for the entire time window. The results\nshow that an RNN-based predictor trained using meta-learning is able to predict\nblockages after observing fewer samples than predictors trained using standard\nmethods.", "title": "Latency-Constrained Prediction of mmWave/THz Link Blockages through  Meta-Learning"}, {"link": "https://arxiv.org/abs/2106.07447", "abstract": "Self-supervised approaches for speech representation learning are challenged\nby three unique problems: (1) there are multiple sound units in each input\nutterance, (2) there is no lexicon of input sound units during the pre-training\nphase, and (3) sound units have variable lengths with no explicit segmentation.\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\napproach for self-supervised speech representation learning, which utilizes an\noffline clustering step to provide aligned target labels for a BERT-like\nprediction loss. A key ingredient of our approach is applying the prediction\nloss over the masked regions only, which forces the model to learn a combined\nacoustic and language model over the continuous inputs. HuBERT relies primarily\non the consistency of the unsupervised clustering step rather than the\nintrinsic quality of the assigned cluster labels. Starting with a simple\nk-means teacher of 100 clusters, and using two iterations of clustering, the\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\ndev-other and test-other evaluation subsets.", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked  Prediction of Hidden Units"}, {"link": "https://arxiv.org/abs/2106.07448", "abstract": "visual information can be converted into audio stream via sensory\nsubstitution devices in order to give visually impaired people the chance of\nperception of their surrounding easily and simultaneous to performing everyday\ntasks. In this study, visual environmental features namely, coordinate, type of\nobjects and their size are assigned to audio features related to music tones\nsuch as frequency, time duration and note permutations. Results demonstrated\nthat this new method has more training time efficiency in comparison with our\nprevious method named VBTones which sinusoidal tones were applied. Moreover,\nresults in blind object recognition for real objects was achieved 88.05 on\naverage.", "title": "A Novel mapping for visual to auditory sensory substitution"}, {"link": "https://arxiv.org/abs/2106.07449", "abstract": "We present a methodology for creating information flow specifications of\nhardware designs. Such specifications can help designers better understand\ntheir design and are necessary for security validation processes. By combining\ninformation flow tracking and specification mining, we are able to produce\ninformation flow properties of a design without prior knowledge of security\nagreements or specifications. We develop a tool, Isadora, to evaluate our\nmethodology. We demonstrate Isadora may define the information flows within an\naccess control module in isolation and within an SoC and over a RISC-V design.\nOver the access control module, Isadora mined output completely covers an\nassertion based security specification of the design provided by the designers.\nFor both the access control module and RISC-V, we sample Isadora output\nproperties and find 10 out of 10 and 8 out of 10 properties, respectively,\ndefine the design behavior to relevant to a Common Weakness Enumeration (CWE).\nWe find our methodology may independently mine security properties manually\ndeveloped by hardware designers, automatically generate properties describing\nCWEs over a design, and scale to SoC and CPU designs.", "title": "A Methodology For Creating Information Flow Specifications of Hardware  Designs"}, {"link": "https://arxiv.org/abs/2106.07451", "abstract": "Semi-supervised node classification, as a fundamental problem in graph\nlearning, leverages unlabeled nodes along with a small portion of labeled nodes\nfor training. Existing methods rely heavily on high-quality labels, which,\nhowever, are expensive to obtain in real-world applications since certain\nnoises are inevitably involved during the labeling process. It hence poses an\nunavoidable challenge for the learning algorithm to generalize well. In this\npaper, we propose a novel robust learning objective dubbed pairwise\ninteractions (PI) for the model, such as Graph Neural Network (GNN) to combat\nnoisy labels. Unlike classic robust training approaches that operate on the\npointwise interactions between node and class label pairs, PI explicitly forces\nthe embeddings for node pairs that hold a positive PI label to be close to each\nother, which can be applied to both labeled and unlabeled nodes. We design\nseveral instantiations for PI labels based on the graph structure and the node\nclass labels, and further propose a new uncertainty-aware training technique to\nmitigate the negative effect of the sub-optimal PI labels. Extensive\nexperiments on different datasets and GNN architectures demonstrate the\neffectiveness of PI, yielding a promising improvement over the state-of-the-art\nmethods.", "title": "PI-GNN: A Novel Perspective on Semi-Supervised Node Classification  against Noisy Labels"}, {"link": "https://arxiv.org/abs/2106.07453", "abstract": "Collaborative filtering (CF), as a fundamental approach for recommender\nsystems, is usually built on the latent factor model with learnable parameters\nto predict users' preferences towards items. However, designing a proper CF\nmodel for a given data is not easy, since the properties of datasets are highly\ndiverse. In this paper, motivated by the recent advances in automated machine\nlearning (AutoML), we propose to design a data-specific CF model by AutoML\ntechniques. The key here is a new framework that unifies state-of-the-art\n(SOTA) CF methods and splits them into disjoint stages of input encoding,\nembedding function, interaction function, and prediction function. We further\ndevelop an easy-to-use, robust, and efficient search strategy, which utilizes\nrandom search and a performance predictor for efficient searching within the\nabove framework. In this way, we can combinatorially generalize data-specific\nCF models, which have not been visited in the literature, from SOTA ones.\nExtensive experiments on five real-world datasets demonstrate that our method\ncan consistently outperform SOTA ones for various CF tasks. Further experiments\nverify the rationality of the proposed framework and the efficiency of the\nsearch strategy. The searched CF models can also provide insights for exploring\nmore effective methods in the future", "title": "Efficient Data-specific Model Search for Collaborative Filtering"}, {"link": "https://arxiv.org/abs/2106.07455", "abstract": "Optimal transport (OT) is a framework that can be used to guide the optimal\nallocation of a limited amount of resources. The classical OT paradigm does not\nconsider malicious attacks in its formulation and thus the designed transport\nplan lacks resiliency to an adversary. To address this concern, we establish an\nOT framework that explicitly accounts for the adversarial and stealthy\nmanipulation of participating nodes in the network during the transport\nstrategy design. Specifically, we propose a game-theoretic approach to capture\nthe strategic interactions between the transport planner and the deceptive\nattacker. We analyze the properties of the established two-person zero-sum game\nthoroughly. We further develop a fully distributed algorithm to compute the\noptimal resilient transport strategies, and show the convergence of the\nalgorithm to a saddle-point equilibrium. Finally, we demonstrate the\neffectiveness of the designed algorithm using case studies.", "title": "Resilient and Distributed Discrete Optimal Transport with Deceptive  Adversary: A Game-Theoretic Approach"}, {"link": "https://arxiv.org/abs/2106.07456", "abstract": "This paper presents a novel, non-standard set of vector instruction types for\nexploring custom SIMD instructions in a softcore. The new types allow\nsimultaneous access to a relatively high number of operands, reducing the\ninstruction count where applicable. Additionally, a high-performance\nopen-source RISC-V (RV32 IM) softcore is introduced, optimised for exploring\ncustom SIMD instructions and streaming performance. By providing instruction\ntemplates for instruction development in HDL/Verilog, efficient FPGA-based\ninstructions can be developed with few low-level lines of code. In order to\nimprove custom SIMD instruction performance, the softcore's cache hierarchy is\noptimised for bandwidth, such as with very wide blocks for the last-level\ncache. The approach is demonstrated on example memory-intensive applications on\nan FPGA. Although the exploration is based on the softcore, the goal is to\nprovide a means to experiment with advanced SIMD instructions which could be\nloaded in future CPUs that feature reconfigurable regions as custom\ninstructions. Finally, we provide some insights on the challenges and\neffectiveness of such future micro-architectures.", "title": "Extending the RISC-V ISA for exploring advanced reconfigurable SIMD  instructions"}, {"link": "https://arxiv.org/abs/2106.07459", "abstract": "For a given shape $S$ in the plane, one can ask what is the lowest possible\ndensity of a point set $P$ that pierces (``intersects'', ``hits'') all\ntranslates of $S$. This is equivalent to determining the covering density of\n$S$ and as such is well studied. Here we study the analogous question for\nfamilies of shapes where the connection to covering no longer exists. That is,\nwe require that a single point set $P$ simultaneously pierces each translate of\neach shape from some family $\\mathcal F$. We denote the lowest possible density\nof such an $\\mathcal F$-piercing point set by $\\pi_T(\\mathcal F)$.\nSpecifically, we focus on families $\\mathcal F$ consisting of axis-parallel\nrectangles. When $|\\mathcal F|=2$ we exactly solve the case when one rectangle\nis more squarish than $2\\times 1$, and give bounds (within $10\\,\\%$ of each\nother) for the remaining case when one rectangle is wide and the other one is\ntall. When $|\\mathcal F|\\ge 2$ we present a linear-time constant-factor\napproximation algorithm for computing $\\pi_T(\\mathcal F)$ (with ratio $1.895$).", "title": "Piercing All Translates of a Set of Axis-Parallel Rectangles"}, {"link": "https://arxiv.org/abs/2106.07464", "abstract": "In Meta-Interpretive Learning (MIL) the metarules, second-order datalog\nclauses acting as inductive bias, are manually defined by the user. In this\nwork we show that second-order metarules for MIL can be learned by MIL. We\ndefine a generality ordering of metarules by $\\theta$-subsumption and show that\nuser-defined sort metarules are derivable by specialisation of the most-general\nmatrix metarules in a language class; and that these matrix metarules are in\nturn derivable by specialisation of third-order punch metarules with variables\nthat range over the set of second-order literals and for which only an upper\nbound on their number of literals need be user-defined. We show that the\ncardinality of a metarule language is polynomial in the number of literals in\npunch metarules. We re-frame MIL as metarule specialisation by resolution. We\nmodify the MIL metarule specialisation operator to return new metarules rather\nthan first-order clauses and prove the correctness of the new operator. We\nimplement the new operator as TOIL, a sub-system of the MIL system Louise. Our\nexperiments show that as user-defined sort metarules are progressively replaced\nby sort metarules learned by TOIL, Louise's predictive accuracy is maintained\nat the cost of a small increase in training times. We conclude that\nautomatically derived metarules can replace user-defined metarules.", "title": "Meta-Interpretive Learning as Metarule Specialisation"}, {"link": "https://arxiv.org/abs/2106.07468", "abstract": "We study a class of spatial discretizations for the Vlasov-Poisson system\nwritten as an hyperbolic system using Hermite polynomials. In particular, we\nfocus on spectral methods and discontinuous Galerkin approximations. To obtain\nL 2 stability properties, we introduce a new L 2 weighted space, with a time\ndependent weight. For the Hermite spectral form of the Vlasov-Poisson system,\nwe prove conservation of mass, momentum and total energy, as well as global\nstability for the weighted L 2 norm. These properties are then discussed for\nseveral spatial discretizations. Finally, numerical simulations are performed\nwith the proposed DG/Hermite spectral method to highlight its stability and\nconservation features.", "title": "On the stability of conservative discontinuous Galerkin/Hermite spectral  methods for the Vlasov-Poisson system"}, {"link": "https://arxiv.org/abs/2106.07470", "abstract": "Vectors fields defined on surfaces constitute relevant and useful\nrepresentations but are rarely used. One reason might be that comparing vector\nfields across two surfaces of the same genus is not trivial: it requires to\ntransport the vector fields from the original surfaces onto a common domain. In\nthis paper, we propose a framework to achieve this task by mapping the vector\nfields onto a common space, using some notions of differential geometry. The\nproposed framework enables the computation of statistics on vector fields. We\ndemonstrate its interest in practice with an application on real data with a\nquantitative assessment of the reproducibility of curvature directions that\ndescribe the complex geometry of cortical folding patterns. The proposed\nframework is general and can be applied to different types of vector fields and\nsurfaces, allowing for a large number of high potential applications in medical\nimaging.", "title": "Comparing vector fields across surfaces: interest for characterizing the  orientations of cortical folds"}, {"link": "https://arxiv.org/abs/2106.07471", "abstract": "Higher-order networks have so far been considered primarily in the context of\nstudying the structure of complex systems, i.e., the higher-order or multi-way\nrelations connecting the constituent entities. More recently, a number of\nstudies have considered dynamical processes that explicitly ac- count for such\nhigher-order dependencies, e.g., in the context of epidemic spreading processes\nor opinion formation. In this chapter, we focus on a closely related, but\ndistinct third perspective: how can we use higher-order relationships to\nprocess signals and data supported on higher-order network structures. In\nparticular, we survey how ideas from signal processing of data supported on\nregular domains, such as time series or images, can be extended to graphs and\nsimplicial complexes. We discuss Fourier analysis, signal denois- ing, signal\ninterpolation, and nonlinear processing through neural networks based on\nsimplicial complexes. Key to our developments is the Hodge Laplacian matrix, a\nmulti-relational operator that leverages the special structure of simplicial\ncomplexes and generalizes desirable properties of the Laplacian matrix in graph\nsignal processing.", "title": "Signal processing on simplicial complexes"}, {"link": "https://arxiv.org/abs/2106.07472", "abstract": "Actor-critic methods integrating target networks have exhibited a stupendous\nempirical success in deep reinforcement learning. However, a theoretical\nunderstanding of the use of target networks in actor-critic methods is largely\nmissing in the literature. In this paper, we bridge this gap between theory and\npractice by proposing the first theoretical analysis of an online target-based\nactor-critic algorithm with linear function approximation in the discounted\nreward setting. Our algorithm uses three different timescales: one for the\nactor and two for the critic. Instead of using the standard single timescale\ntemporal difference (TD) learning algorithm as a critic, we use a two\ntimescales target-based version of TD learning closely inspired from practical\nactor-critic algorithms implementing target networks. First, we establish\nasymptotic convergence results for both the critic and the actor under\nMarkovian sampling. Then, we provide a finite-time analysis showing the impact\nof incorporating a target network into actor-critic methods.", "title": "Analysis of a Target-Based Actor-Critic Algorithm with Linear Function  Approximation"}, {"link": "https://arxiv.org/abs/2106.07474", "abstract": "This paper contributes to interpretable machine learning via visual knowledge\ndiscovery in parallel coordinates. The concepts of hypercubes and hyper-blocks\nare used as easily understandable by end-users in the visual form in parallel\ncoordinates. The Hyper algorithm for classification with mixed and pure\nhyper-blocks (HBs) is proposed to discover hyper-blocks interactively and\nautomatically in individual, multiple, overlapping, and non-overlapping\nsetting. The combination of hyper-blocks with linguistic description of visual\npatterns is presented too. It is shown that Hyper models generalize decision\ntrees. The Hyper algorithm was tested on the benchmark data from UCI ML\nrepository. It allowed discovering pure and mixed HBs with all data and then\nwith 10-fold cross validation. The links between hyper-blocks, dimension\nreduction and visualization are established. Major benefits of hyper-block\ntechnology and the Hyper algorithm are in their ability to discover and observe\nhyper-blocks by end-users including side by side visualizations making patterns\nvisible for all classes. Another advantage of sets of HBs relative to the\ndecision trees is the ability to avoid both data overgeneralization and\noverfitting.", "title": "Discovering Interpretable Machine Learning Models in Parallel  Coordinates"}, {"link": "https://arxiv.org/abs/2106.07475", "abstract": "Saliency maps have shown to be both useful and misleading for explaining\nmodel predictions especially in the context of images. In this paper, we\nperform sanity checks for text modality and show that the conclusions made for\nimage do not directly transfer to text. We also analyze the effects of the\ninput multiplier in certain saliency maps using similarity scores,\nmax-sensitivity and infidelity evaluation metrics. Our observations reveal that\nthe input multiplier carries input's structural patterns in explanation maps,\nthus leading to similar results regardless of the choice of model parameters.\nWe also show that the smoothness of a Neural Network (NN) function can affect\nthe quality of saliency-based explanations. Our investigations reveal that\nreplacing ReLUs with Softplus and MaxPool with smoother variants such as\nLogSumExp (LSE) can lead to explanations that are more reliable based on the\ninfidelity evaluation metric.", "title": "Investigating sanity checks for saliency maps with image and text  classification"}, {"link": "https://arxiv.org/abs/2106.07476", "abstract": "Deep graph neural networks (GNNs) have achieved excellent results on various\ntasks on increasingly large graph datasets with millions of nodes and edges.\nHowever, memory complexity has become a major obstacle when training deep GNNs\nfor practical applications due to the immense number of nodes, edges, and\nintermediate activations. To improve the scalability of GNNs, prior works\npropose smart graph sampling or partitioning strategies to train GNNs with a\nsmaller set of nodes or sub-graphs. In this work, we study reversible\nconnections, group convolutions, weight tying, and equilibrium models to\nadvance the memory and parameter efficiency of GNNs. We find that reversible\nconnections in combination with deep network architectures enable the training\nof overparameterized GNNs that significantly outperform existing methods on\nmultiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each)\nand RevGNN-Wide (448 layers with 224 channels each) were both trained on a\nsingle commodity GPU and achieve an ROC-AUC of $87.74 \\pm 0.13$ and $88.14 \\pm\n0.15$ on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep\nis the deepest GNN in the literature by one order of magnitude. Please visit\nour project website https://www.deepgcns.org/arch/gnn1000 for more information.", "title": "Training Graph Neural Networks with 1000 Layers"}, {"link": "https://arxiv.org/abs/2106.07477", "abstract": "Recently, visual Transformer (ViT) and its following works abandon the\nconvolution and exploit the self-attention operation, attaining a comparable or\neven higher accuracy than CNN. More recently, MLP-Mixer abandons both the\nconvolution and the self-attention operation, proposing an architecture\ncontaining only MLP layers. To achieve cross-patch communications, it devises\nan additional token-mixing MLP besides the channel-mixing MLP. It achieves\npromising results when training on an extremely large-scale dataset. But it\ncannot achieve as outstanding performance as its CNN and ViT counterparts when\ntraining on medium-scale datasets such as ImageNet1K and ImageNet21K. The\nperformance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We\ndiscover that token-mixing operation in MLP-Mixer is a variant of depthwise\nconvolution with a global reception field and spatial-specific configuration.\nBut the global reception field and the spatial-specific property make\ntoken-mixing MLP prone to over-fitting. In this paper, we propose a novel pure\nMLP architecture, spatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our\nS$^2$-MLP only contains channel-mixing MLP. We devise a spatial-shift operation\nfor achieving the communication between patches. It has a local reception field\nand is spatial-agnostic. Meanwhile, it is parameter-free and efficient for\ncomputation. The proposed S$^2$-MLP attains higher recognition accuracy than\nMLP-Mixer when training on ImageNet-1K dataset. Meanwhile, S$^2$-MLP\naccomplishes as excellent performance as ViT on ImageNet-1K dataset with\nconsiderably simpler architecture and fewer FLOPs and parameters.", "title": "S$^2$-MLP: Spatial-Shift MLP Architecture for Vision"}, {"link": "https://arxiv.org/abs/2106.07479", "abstract": "We present an efficient stochastic algorithm (RSG+) for canonical correlation\nanalysis (CCA) using a reparametrization of the projection matrices. We show\nhow this reparametrization (into structured matrices), simple in hindsight,\ndirectly presents an opportunity to repurpose/adjust mature techniques for\nnumerical optimization on Riemannian manifolds. Our developments nicely\ncomplement existing methods for this problem which either require $O(d^3)$ time\ncomplexity per iteration with $O(\\frac{1}{\\sqrt{t}})$ convergence rate (where\n$d$ is the dimensionality) or only extract the top $1$ component with\n$O(\\frac{1}{t})$ convergence rate. In contrast, our algorithm offers a strict\nimprovement for this classical problem: it achieves $O(d^2k)$ runtime\ncomplexity per iteration for extracting the top $k$ canonical components with\n$O(\\frac{1}{t})$ convergence rate. While the paper primarily focuses on the\nformulation and technical analysis of its properties, our experiments show that\nthe empirical behavior on common datasets is quite promising. We also explore a\npotential application in training fair models where the label of protected\nattribute is missing or otherwise unavailable.", "title": "An Online Riemannian PCA for Stochastic Canonical Correlation Analysis"}, {"link": "https://arxiv.org/abs/2106.07482", "abstract": "Recent years have witnessed tremendous interest in deep learning on\ngraph-structured data. Due to the high cost of collecting labeled\ngraph-structured data, domain adaptation is important to supervised graph\nlearning tasks with limited samples. However, current graph domain adaptation\nmethods are generally adopted from traditional domain adaptation tasks, and the\nproperties of graph-structured data are not well utilized. For example, the\nobserved social networks on different platforms are controlled not only by the\ndifferent crowd or communities but also by the domain-specific policies and the\nbackground noise. Based on these properties in graph-structured data, we first\nassume that the graph-structured data generation process is controlled by three\nindependent types of latent variables, i.e., the semantic latent variables, the\ndomain latent variables, and the random latent variables. Based on this\nassumption, we propose a disentanglement-based unsupervised domain adaptation\nmethod for the graph-structured data, which applies variational graph\nauto-encoders to recover these latent variables and disentangles them via three\nsupervised learning modules. Extensive experimental results on two real-world\ndatasets in the graph classification task reveal that our method not only\nsignificantly outperforms the traditional domain adaptation methods and the\ndisentangled-based domain adaptation methods but also outperforms the\nstate-of-the-art graph domain adaptation algorithms.", "title": "Graph Domain Adaptation: A Generative View"}, {"link": "https://arxiv.org/abs/2106.07483", "abstract": "Many ML models are opaque to humans, producing decisions too complex for\nhumans to easily understand. In response, explainable artificial intelligence\n(XAI) tools that analyze the inner workings of a model have been created.\nDespite these tools' strength in translating model behavior, critiques have\nraised concerns about the impact of XAI tools as a tool for `fairwashing` by\nmisleading users into trusting biased or incorrect models. In this paper, we\ncreated a framework for evaluating explainable AI tools with respect to their\ncapabilities for detecting and addressing issues of bias and fairness as well\nas their capacity to communicate these results to their users clearly. We found\nthat despite their capabilities in simplifying and explaining model behavior,\nmany prominent XAI tools lack features that could be critical in detecting\nbias. Developers can use our framework to suggest modifications needed in their\ntoolkits to reduce issues likes fairwashing.", "title": "Can Explainable AI Explain Unfairness? A Framework for Evaluating  Explainable AI"}, {"link": "https://arxiv.org/abs/2106.07484", "abstract": "We introduce conservative integrators for long term integration of piecewise\nsmooth systems with transversal dynamics and piecewise smooth conserved\nquantities. In essence, for a piecewise dynamical system with piecewise defined\nconserved quantities such that its trajectories cross transversally to its\ninterface, we combine Mannshardt's transition scheme and the Discrete\nMultiplier Method to obtain conservative integrators capable of preserving\nconserved quantities up to machine precision and accuracy order. We prove that\nthe order of accuracy of the integrators is preserved after crossing the\ndiscontinuity in the case of codimension one number of conserved quantities.\nNumerical examples illustrate the preservation of accuracy order.", "title": "Conservative Integrators for Piecewise Smooth Systems with Transversal  Dynamics"}, {"link": "https://arxiv.org/abs/2106.07485", "abstract": "Diagrammatically speaking, grammatical calculi such as pregroups provide\nwires between words in order to elucidate their interactions, and this enables\none to verify grammatical correctness of phrases and sentences. In this paper\nwe also provide wirings within words. This will enable us to identify\ngrammatical constructs that we expect to be either equal or closely related.\nHence, our work paves the way for a new theory of grammar, that provides novel\n`grammatical truths'. We give a nogo-theorem for the fact that our wirings for\nwords make no sense for preordered monoids, the form which grammatical calculi\nusually take. Instead, they require diagrams -- or equivalently, (free)\nmonoidal categories.", "title": "Grammar Equations"}, {"link": "https://arxiv.org/abs/2106.07487", "abstract": "Humans have the ability to seamlessly combine low-level visual input with\nhigh-level symbolic reasoning often in the form of recognising objects,\nlearning relations between them and applying rules. Neuro-symbolic systems aim\nto bring a unifying approach to connectionist and logic-based principles for\nvisual processing and abstract reasoning respectively. This paper presents a\ncomplete neuro-symbolic method for processing images into objects, learning\nrelations and logical rules in an end-to-end fashion. The main contribution is\na differentiable layer in a deep learning architecture from which symbolic\nrelations and rules can be extracted by pruning and thresholding. We evaluate\nour model using two datasets: subgraph isomorphism task for symbolic rule\nlearning and an image classification domain with compound relations for\nlearning objects, relations and rules. We demonstrate that our model scales\nbeyond state-of-the-art symbolic learners and outperforms deep relational\nneural network architectures.", "title": "pix2rule: End-to-end Neuro-symbolic Rule Learning"}, {"link": "https://arxiv.org/abs/2106.07488", "abstract": "Personalized image aesthetic assessment (PIAA) has recently become a hot\ntopic due to its usefulness in a wide variety of applications such as\nphotography, film and television, e-commerce, fashion design and so on. This\ntask is more seriously affected by subjective factors and samples provided by\nusers. In order to acquire precise personalized aesthetic distribution by small\namount of samples, we propose a novel user-guided personalized image aesthetic\nassessment framework. This framework leverages user interactions to retouch and\nrank images for aesthetic assessment based on deep reinforcement learning\n(DRL), and generates personalized aesthetic distribution that is more in line\nwith the aesthetic preferences of different users. It mainly consists of two\nstages. In the first stage, personalized aesthetic ranking is generated by\ninteractive image enhancement and manual ranking, meanwhile two policy networks\nwill be trained. The images will be pushed to the user for manual retouching\nand simultaneously to the enhancement policy network. The enhancement network\nutilizes the manual retouching results as the optimization goals of DRL. After\nthat, the ranking process performs the similar operations like the retouching\nmentioned before. These two networks will be trained iteratively and\nalternatively to help to complete the final personalized aesthetic assessment\nautomatically. In the second stage, these modified images are labeled with\naesthetic attributes by one style-specific classifier, and then the\npersonalized aesthetic distribution is generated based on the multiple\naesthetic attributes of these images, which conforms to the aesthetic\npreference of users better.", "title": "User-Guided Personalized Image Aesthetic Assessment based on Deep  Reinforcement Learning"}, {"link": "https://arxiv.org/abs/2106.07497", "abstract": "In this paper, I describe a recent practical experience where JUnit was used\nfor testing security bugs in addition to functional bugs. Perl scripts were\nalso used during the exploration phase. The application being tested was\nmature, but insecure.", "title": "Security testing using JUnit and Perl scripts"}, {"link": "https://arxiv.org/abs/2106.07499", "abstract": "NLP has achieved great progress in the past decade through the use of neural\nmodels and large labeled datasets. The dependence on abundant data prevents NLP\nmodels from being applied to low-resource settings or novel tasks where\nsignificant time, money, or expertise is required to label massive amounts of\ntextual data. Recently, data augmentation methods have been explored as a means\nof improving data efficiency in NLP. To date, there has been no systematic\nempirical overview of data augmentation for NLP in the limited labeled data\nsetting, making it difficult to understand which methods work in which\nsettings. In this paper, we provide an empirical survey of recent progress on\ndata augmentation for NLP in the limited labeled data setting, summarizing the\nlandscape of methods (including token-level augmentations, sentence-level\naugmentations, adversarial augmentations, and hidden-space augmentations) and\ncarrying out experiments on 11 datasets covering topics/news classification,\ninference tasks, paraphrasing tasks, and single-sentence tasks. Based on the\nresults, we draw several conclusions to help practitioners choose appropriate\naugmentations in different settings and discuss the current challenges and\nfuture directions for limited data learning in NLP.", "title": "An Empirical Survey of Data Augmentation for Limited Data Learning in  NLP"}, {"link": "https://arxiv.org/abs/2106.07501", "abstract": "We propose a balanced coarsening scheme for multilevel hypergraph\npartitioning. In addition, an initial partitioning algorithm is designed to\nimprove the quality of k-way hypergraph partitioning. By assigning vertex\nweights through the LPT algorithm, we generate a prior hypergraph under a\nrelaxed balance constraint. With the prior hypergraph, we have defined the\nWasserstein discrepancy to coordinate the optimal transport of coarsening\nprocess. And the optimal transport matrix is solved by Sinkhorn algorithm. Our\ncoarsening scheme fully takes into account the minimization of connectivity\nmetric (objective function). For the initial partitioning stage, we define a\nnormalized cut function induced by Fiedler vector, which is theoretically\nproved to be a concave function. Thereby, a three-point algorithm is designed\nto find the best cut under the balance constraint.", "title": "Balanced Coarsening for Multilevel Hypergraph Partitioning via  Wasserstein Discrepancy"}, {"link": "https://arxiv.org/abs/2106.07502", "abstract": "We introduce a framework for AI-based medical consultation system with\nknowledge graph embedding and reinforcement learning components and its\nimplement. Our implement of this framework leverages knowledge organized as a\ngraph to have diagnosis according to evidence collected from patients\nrecurrently and dynamically. According to experiment we designed for evaluating\nits performance, it archives a good result. More importantly, for getting\nbetter performance, researchers can implement it on this framework based on\ntheir innovative ideas, well designed experiments and even clinical trials.", "title": "Training like Playing: A Reinforcement Learning And Knowledge  Graph-based framework for building Automatic Consultation System in Medical  Field"}, {"link": "https://arxiv.org/abs/2106.07504", "abstract": "Fairwashing refers to the risk that an unfair black-box model can be\nexplained by a fairer model through post-hoc explanations' manipulation.\nHowever, to realize this, the post-hoc explanation model must produce different\npredictions than the original black-box on some inputs, leading to a decrease\nin the fidelity imposed by the difference in unfairness. In this paper, our\nmain objective is to characterize the risk of fairwashing attacks, in\nparticular by investigating the fidelity-unfairness trade-off. First, we\ndemonstrate through an in-depth empirical study on black-box models trained on\nseveral real-world datasets and for several statistical notions of fairness\nthat it is possible to build high-fidelity explanation models with low\nunfairness. For instance, we find that fairwashed explanation models can\nexhibit up to $99.20\\%$ fidelity to the black-box models they explain while\nbeing $50\\%$ less unfair. These results suggest that fidelity alone should not\nbe used as a proxy for the quality of black-box explanations. Second, we show\nthat fairwashed explanation models can generalize beyond the suing group\n(\\emph{i.e.}, data points that are being explained), which will only worsen as\nmore stable fairness methods get developed. Finally, we demonstrate that\nfairwashing attacks can transfer across black-box models, meaning that other\nblack-box models can perform fairwashing without explicitly using their\npredictions.", "title": "Characterizing the risk of fairwashing"}, {"link": "https://arxiv.org/abs/2106.07505", "abstract": "Hate speech and profanity detection suffer from data sparsity, especially for\nlanguages other than English, due to the subjective nature of the tasks and the\nresulting annotation incompatibility of existing corpora. In this study, we\nidentify profane subspaces in word and sentence representations and explore\ntheir generalization capability on a variety of similar and distant target\ntasks in a zero-shot setting. This is done monolingually (German) and\ncross-lingually to closely-related (English), distantly-related (French) and\nnon-related (Arabic) tasks. We observe that, on both similar and distant target\ntasks and across all languages, the subspace-based representations transfer\nmore effectively than standard BERT representations in the zero-shot setting,\nwith improvements between F1 +10.9 and F1 +42.9 over the baselines across all\ntested monolingual and cross-lingual scenarios.", "title": "Modeling Profanity and Hate Speech in Social Media with Semantic  Subspaces"}, {"link": "https://arxiv.org/abs/2106.07510", "abstract": "In this paper, we give a new characterization of the cut locus of a point on\na compact Riemannian manifold as the zero set of the optimal transport density\nsolution of the Monge-Kantorovich equations, a PDE formulation of the optimal\ntransport problem with cost equal to the geodesic distance. Combining this\nresult with an optimal transport numerical solver based on the so-called\ndynamical Monge-Kantorovich approach, we propose a novel framework for the\nnumerical approximation of the cut locus of a point in a manifold. We show the\napplicability of the proposed method on a few examples settled on 2d-surfaces\nembedded in $R^{3}$ and discuss advantages and limitations.", "title": "Computing the Cut Locus of a Riemannian Manifold via Optimal Transport"}, {"link": "https://arxiv.org/abs/2106.07513", "abstract": "The appropriate use of design patterns in code is a vital measurement of good\nsoftware quality in object-oriented software applications. There exist tools to\ndetect design pattern usage in Java source files, where their detection\nmechanisms have been honed through the use of supervised machine learning\ntechniques that require large datasets of labelled files. However, manually\nlabelling these files leads to issues such as tediousness if the team of\nlabellers is small, and conflicting opinions between labellers, if large. Thus,\nwe present CodeLabeller, a web-based tool which aims to provide a more\nefficient approach in handling the process of labelling Java source files at\nscale by improving the data collection process throughout, and improving the\ndegree of reliability of responses by requiring each labeller to attach a\nconfidence rating to each of their responses. We test CodeLabeller by\nconstructing a corpus of over a thousand source files obtained from a large\ncollection of open-source Java projects, and labelling each Java source file\nwith their respective design patterns (if any), and summaries. This paper\ndiscusses the motivation behind thecreation of CodeLabeller, a demonstration of\nthe tool and its UI, its implementation, benefits and lastly, some ideas for\nfuture improvements. A demo version of CodeLabeller can be found at:\nhttps://codelabeller.org.", "title": "CodeLabeller: A Web-based Code Annotation Tool for Java Design Patterns  and Summaries"}, {"link": "https://arxiv.org/abs/2106.07520", "abstract": "Researchers and practitioners have designed and implemented various automated\ntest case generators to support effective software testing. Such generators\nexist for various languages (e.g., Java, C#, or Python) and for various\nplatforms (e.g., desktop, web, or mobile applications). Such generators exhibit\nvarying effectiveness and efficiency, depending on the testing goals they aim\nto satisfy (e.g., unit-testing of libraries vs. system-testing of entire\napplications) and the underlying techniques they implement. In this context,\npractitioners need to be able to compare different generators to identify the\nmost suited one for their requirements, while researchers seek to identify\nfuture research directions. This can be achieved through the systematic\nexecution of large-scale evaluations of different generators. However, the\nexecution of such empirical evaluations is not trivial and requires a\nsubstantial effort to collect benchmarks, setup the evaluation infrastructure,\nand collect and analyse the results. In this paper, we present our JUnit\nGeneration benchmarking infrastructure (JUGE) supporting generators (e.g.,\nsearch-based, random-based, symbolic execution, etc.) seeking to automate the\nproduction of unit tests for various purposes (e.g., validation, regression\ntesting, fault localization, etc.). The primary goal is to reduce the overall\neffort, ease the comparison of several generators, and enhance the knowledge\ntransfer between academia and industry by standardizing the evaluation and\ncomparison process. Since 2013, eight editions of a unit testing tool\ncompetition, co-located with the Search-Based Software Testing Workshop, have\ntaken place and used and updated JUGE. As a result, an increasing amount of\ntools (over ten) from both academia and industry have been evaluated on JUGE,\nmatured over the years, and allowed the identification of future research\ndirections.", "title": "JUGE: An Infrastructure for Benchmarking Java Unit Test Generators"}, {"link": "https://arxiv.org/abs/2106.07527", "abstract": "The structure of an RNA molecule plays a significant role in its biological\nfunction. Predicting structure given a one dimensional sequence of RNA\nnucleotide bases is a difficult and important problem. Many computer programs\n(known as in silico) are available for predicting 2-dimensional (secondary)\nstructures however 3-dimensional (tertiary) structure prediction is much more\ndifficult mainly due to the far greater number of feasible solutions and fewer\nexperimental data on the thermodynamic energies of 3D structures. It is also\nchallenging to verify the most likely three dimensional structure even with the\navailability of sophisticated x-ray crystallography and nuclear magnetic\nresonance imaging technologies. In this paper we develop three dimensional RNA\nfolding predictions by adding penalty and reward parameters to a previous two\ndimensional approach based on Quadratic Unconstrained Binary Optimization\n(QUBO) models. These parameters provide flexibility in the amount of three\ndimensional folding allowed. We address the problem of multiple near-optimal\nstructures via a new weighted similarity structure measure and illustrate\nfolding pathways via progressively improving local optimal solutions. The\nproblems are solved via a new commercial QUBO solver AlphaQUBO (Meta-Analytics,\n2020) that solves problems having hundreds of thousands of binary variables.", "title": "Predicting 3D RNA Folding Patterns via Quadratic Binary Optimization"}, {"link": "https://arxiv.org/abs/2106.07528", "abstract": "With the fast development of digital technologies, we are running into a\ndigital world. The relationship among people and the connections among things\nbecome more and more complex, and new challenges arise. To tackle these\nchallenges, trust-a soft security mechanism-is considered as a promising\ntechnology. Thus, in this survey, we do a comprehensive study on the trust and\ntrust modelling for the future digital world. We revisit the definitions and\nproperties of trust, and analysis the trust theories and discuss their impact\non digital trust modelling. We analyze the digital world and its corresponding\nenvironment where people, things, and infrastructure connect with each other.\nWe detail the challenges that require trust in these digital scenarios. Under\nour analysis of trust and the digital world, we define different types of trust\nrelationships and find out the factors that are needed to ensure a fully\nrepresentative model. Next, to meet the challenges of digital trust modelling,\ncomprehensive trust model evaluation criteria are proposed, and potential\nsecurities and privacy issues of trust modelling are analyzed. Finally, we\nprovide a wide-ranging analysis of different methodologies, mathematical\ntheories, and how they can be applied to trust modelling.", "title": "On the Trust and Trust Modelling for the Future Fully-Connected Digital  World: A Comprehensive Study"}, {"link": "https://arxiv.org/abs/2106.07534", "abstract": "With the advent of big data and the birth of the data markets that sell\npersonal information, individuals' privacy is of utmost importance. The\nclassical response is anonymization, i.e., sanitizing the information that can\ndirectly or indirectly allow users' re-identification. The most popular\nsolution in the literature is the k-anonymity. However, it is hard to achieve\nk-anonymity on a continuous stream of data, as well as when the number of\ndimensions becomes high.In this paper, we propose a novel anonymization\nproperty called z-anonymity. Differently from k-anonymity, it can be achieved\nwith zero-delay on data streams and it is well suited for high dimensional\ndata. The idea at the base of z-anonymity is to release an attribute (an atomic\ninformation) about a user only if at least z - 1 other users have presented the\nsame attribute in a past time window. z-anonymity is weaker than k-anonymity\nsince it does not work on the combinations of attributes, but treats them\nindividually. In this paper, we present a probabilistic framework to map the\nz-anonymity into the k-anonymity property. Our results show that a proper\nchoice of the z-anonymity parameters allows the data curator to likely obtain a\nk-anonymized dataset, with a precisely measurable probability. We also evaluate\na real use case, in which we consider the website visits of a population of\nusers and show that z-anonymity can work in practice for obtaining the\nk-anonymity too.", "title": "z-anonymity: Zero-Delay Anonymization for Data Streams"}, {"link": "https://arxiv.org/abs/2106.07536", "abstract": "Flexible optical network is a promising technology to accommodate\nhigh-capacity demands in next-generation networks. To ensure uninterrupted\ncommunication, existing lightpath provisioning schemes are mainly done with the\nassumption of worst-case resource under-provisioning and fixed channel spacing,\nwhich preserves an excessive signal-to-noise ratio (SNR) margin. However, under\na resource over-provisioning scenario, the excessive SNR margin restricts the\ntransmission bit-rate, leading to physical layer resource waste and stranded\ntransmission capacity. To tackle this challenging problem, we leverage an\niterative feedback tuning algorithm to provide a just-enough SNR margin, so as\nto maximize the network throughput. Specifically, the proposed algorithm is\nimplemented in three steps. First, starting from the high SNR margin setup, we\nestablish an integer linear programming model as well as a heuristic algorithm\nto maximize the network throughput by solving the problem of routing,\nmodulation format, forward error correction, baud-rate selection, and spectrum\nassignment. Second, we optimize the channel spacing of the lightpaths obtained\nfrom the previous step, thereby increasing the available physical layer\nresources. Finally, we iteratively reduce the SNR margin of each lightpath\nuntil the network throughput cannot be increased. Through numerical\nsimulations, we confirm the throughput improvement in different networks and\nwith different baud-rates. In particular, we find that our algorithm enables\nover 20\\% relative gain when network resource is over-provisioned, compared to\nthe traditional method preserving an excessive SNR margin.", "title": "Throughput Maximization Leveraging Just-Enough SNR Margin and Channel  Spacing Optimization"}, {"link": "https://arxiv.org/abs/2106.07539", "abstract": "Numerical solutions to high-dimensional partial differential equations (PDEs)\nbased on neural networks have seen exciting developments. This paper derives\ncomplexity estimates of the solutions of $d$-dimensional second-order elliptic\nPDEs in the Barron space, that is a set of functions admitting the integral of\ncertain parametric ridge function against a probability measure on the\nparameters. We prove under some appropriate assumptions that if the\ncoefficients and the source term of the elliptic PDE lie in Barron spaces, then\nthe solution of the PDE is $\\epsilon$-close with respect to the $H^1$ norm to a\nBarron function. Moreover, we prove dimension-explicit bounds for the Barron\nnorm of this approximate solution, depending at most polynomially on the\ndimension $d$ of the PDE. As a direct consequence of the complexity estimates,\nthe solution of the PDE can be approximated on any bounded domain by a\ntwo-layer neural network with respect to the $H^1$ norm with a\ndimension-explicit convergence rate.", "title": "On the Representation of Solutions to Elliptic PDEs in Barron Spaces"}, {"link": "https://arxiv.org/abs/2106.07540", "abstract": "The first step in any NLP pipeline is learning word vector representations.\nHowever, given a large text corpus, representing all the words is not\nefficient. In the literature, many tokenization algorithms have emerged to\ntackle this problem by creating subwords which in turn limits the vocabulary\nsize in any text corpus. However such algorithms are mostly language-agnostic\nand lack a proper way of capturing meaningful tokens. Not to mention the\ndifficulty of evaluating such techniques in practice. In this paper, we\nintroduce three new tokenization algorithms for Arabic and compare them to\nthree other baselines using unsupervised evaluations. In addition to that, we\ncompare all the six algorithms by evaluating them on three tasks which are\nsentiment analysis, news classification and poetry classification. Our\nexperiments show that the performance of such tokenization algorithms depends\non the size of the dataset, type of the task, and the amount of morphology that\nexists in the dataset.", "title": "Evaluating Various Tokenizers for Arabic Text Classification"}, {"link": "https://arxiv.org/abs/2106.07542", "abstract": "This paper presents a model for predicting a driver's stress level up to one\nminute in advance. Successfully predicting future stress would allow stress\nmitigation to begin before the subject becomes stressed, reducing or possibly\navoiding the performance penalties of stress. The proposed model takes features\nextracted from Galvanic Skin Response (GSR) signals on the foot and hand and\nRespiration and Electrocardiogram (ECG) signals from the chest of the driver.\nThe data used to train the model was retrieved from an existing database and\nthen processed to create statistical and frequency features. A total of 42\nfeatures were extracted from the data and then expanded into a total of 252\nfeatures by grouping the data and taking six statistical measurements of each\ngroup for each feature. A Random Forest Classifier was trained and evaluated\nusing a leave-one-subject-out testing approach. The model achieved 94% average\naccuracy on the test data. Results indicate that the model performs well and\ncould be used as part of a vehicle stress prevention system.", "title": "Machine Learning Based Prediction of Future Stress Events in a Driving  Scenario"}, {"link": "https://arxiv.org/abs/2106.07544", "abstract": "The digital media, identified as computational propaganda provides a pathway\nfor propaganda to expand its reach without limit. State-backed propaganda aims\nto shape the audiences' cognition toward entities in favor of a certain\npolitical party or authority. Furthermore, it has become part of modern\ninformation warfare used in order to gain an advantage over opponents. Most of\nthe current studies focus on using machine learning, quantitative, and\nqualitative methods to distinguish if a certain piece of information on social\nmedia is propaganda. Mainly conducted on English content, but very little\nresearch addresses Chinese Mandarin content. From propaganda detection, we want\nto go one step further to provide more fine-grained information on propaganda\ntechniques that are applied. In this research, we aim to bridge the information\ngap by providing a multi-labeled propaganda techniques dataset in Mandarin\nbased on a state-backed information operation dataset provided by Twitter. In\naddition to presenting the dataset, we apply a multi-label text classification\nusing fine-tuned BERT. Potentially this could help future research in detecting\nstate-backed propaganda online especially in a cross-lingual context and cross\nplatforms identity consolidation.", "title": "Dataset of Propaganda Techniques of the State-Sponsored Information  Operation of the People's Republic of China"}, {"link": "https://arxiv.org/abs/2106.07545", "abstract": "Recent works recognized lidars as an inherently streaming data source and\nshowed that the end-to-end latency of lidar perception models can be reduced\nsignificantly by operating on wedge-shaped point cloud sectors rather then the\nfull point cloud. However, due to use of cartesian coordinate systems these\nmethods represent the sectors as rectangular regions, wasting memory and\ncompute. In this work we propose using a polar coordinate system and make two\nkey improvements on this design. First, we increase the spatial context by\nusing multi-scale padding from neighboring sectors: preceding sector from the\ncurrent scan and/or the following sector from the past scan. Second, we improve\nthe core polar convolutional architecture by introducing feature undistortion\nand range stratified convolutions. Experimental results on the nuScenes dataset\nshow significant improvements over other streaming based methods. We also\nachieve comparable results to existing non-streaming methods but with lower\nlatencies.", "title": "PolarStream: Streaming Lidar Object Detection and Segmentation with  Polar Pillars"}, {"link": "https://arxiv.org/abs/2106.07548", "abstract": "Identification methods for dynamic networks typically require prior knowledge\nof the network and disturbance topology, and often rely on solving poorly\nscalable non-convex optimization problems. While methods for estimating network\ntopology are available in the literature, less attention has been paid to\nestimating the disturbance topology, i.e., the (spatial) noise correlation\nstructure and the noise rank. In this work we present an identification method\nfor dynamic networks, in which an estimation of the disturbance topology\nprecedes the identification of the full dynamic network with known network\ntopology. To this end we extend the multi-step Sequential Linear Regression and\nWeighted Null Space Fitting methods to deal with reduced rank noise, and use\nthese methods to estimate the disturbance topology and the network dynamics. As\na result, we provide a multi-step least squares algorithm with parallel\ncomputation capabilities and that rely only on explicit analytical solutions,\nthereby avoiding the usual non-convex optimizations involved. Consequently we\nconsistently estimate dynamic networks of Box Jenkins model structure, while\nkeeping the computational burden low. We provide a consistency proof that\nincludes path-based data informativity conditions for allocation of excitation\nsignals in the experimental design. Numerical simulations performed on a\ndynamic network with reduced rank noise clearly illustrate the potential of\nthis method.", "title": "A scalable multi-step least squares method for network identification  with unknown disturbance topology"}, {"link": "https://arxiv.org/abs/2106.07549", "abstract": "Discriminating the matched named entity pairs or identifying the entities'\ncanonical forms are critical in text mining tasks. More precise named entity\nnormalization in text mining will benefit other subsequent text analytic\napplications. We built the named entity normalization model with a novel Edge\nWeight Updating Neural Network. Our proposed model when tested on four\ndifferent datasets achieved state-of-the-art results. We, next, verify our\nmodel's performance on NCBI Disease, BC5CDR Disease, and BC5CDR Chemical\ndatabases, which are widely used named entity normalization datasets in the\nbioinformatics field. We also tested our model with our own financial named\nentity normalization dataset to validate the efficacy for more general\napplications. Using the constructed dataset, we differentiate named entity\npairs. Our model achieved the highest named entity normalization performances\nin terms of various evaluation metrics.", "title": "Named Entity Normalization Model Using Edge Weight Updating Neural  Network: Assimilation Between Knowledge-Driven Graph and Data-Driven Graph"}, {"link": "https://arxiv.org/abs/2106.07550", "abstract": "With the advent of state of the art nature-inspired pure attention based\nmodels i.e. transformers, and their success in natural language processing\n(NLP), their extension to machine vision (MV) tasks was inevitable and much\nfelt. Subsequently, vision transformers (ViTs) were introduced which are giving\nquite a challenge to the established deep learning based machine vision\ntechniques. However, pure attention based models/architectures like\ntransformers require huge data, large training times and large computational\nresources. Some recent works suggest that combinations of these two varied\nfields can prove to build systems which have the advantages of both these\nfields. Accordingly, this state of the art survey paper is introduced which\nhopefully will help readers get useful information about this interesting and\npotential research area. A gentle introduction to attention mechanisms is\ngiven, followed by a discussion of the popular attention based deep\narchitectures. Subsequently, the major categories of the intersection of\nattention mechanisms and deep learning for machine vision (MV) based are\ndiscussed. Afterwards, the major algorithms, issues and trends within the scope\nof the paper are discussed.", "title": "Attention mechanisms and deep learning for machine vision: A survey of  the state of the art"}, {"link": "https://arxiv.org/abs/2106.07551", "abstract": "Population-based multi-agent reinforcement learning (PB-MARL) refers to the\nseries of methods nested with reinforcement learning (RL) algorithms, which\nproduces a self-generated sequence of tasks arising from the coupled population\ndynamics. By leveraging auto-curricula to induce a population of distinct\nemergent strategies, PB-MARL has achieved impressive success in tackling\nmulti-agent tasks. Despite remarkable prior arts of distributed RL frameworks,\nPB-MARL poses new challenges for parallelizing the training frameworks due to\nthe additional complexity of multiple nested workloads between sampling,\ntraining and evaluation involved with heterogeneous policy interactions. To\nsolve these problems, we present MALib, a scalable and efficient computing\nframework for PB-MARL. Our framework is comprised of three key components: (1)\na centralized task dispatching model, which supports the self-generated tasks\nand scalable training with heterogeneous policy combinations; (2) a programming\narchitecture named Actor-Evaluator-Learner, which achieves high parallelism for\nboth training and sampling, and meets the evaluation requirement of\nauto-curriculum learning; (3) a higher-level abstraction of MARL training\nparadigms, which enables efficient code reuse and flexible deployments on\ndifferent distributed computing paradigms. Experiments on a series of complex\ntasks such as multi-agent Atari Games show that MALib achieves throughput\nhigher than 40K FPS on a single machine with $32$ CPU cores; 5x speedup than\nRLlib and at least 3x speedup than OpenSpiel in multi-agent training tasks.\nMALib is publicly available at https://github.com/sjtu-marl/malib.", "title": "MALib: A Parallel Framework for Population-based Multi-agent  Reinforcement Learning"}, {"link": "https://arxiv.org/abs/2106.07552", "abstract": "In recent times, the scope of LIDAR (Light Detection and Ranging)\nsensor-based technology has spread across numerous fields. It is popularly used\nto map terrain and navigation information into reliable 3D point cloud data,\npotentially revolutionizing the autonomous vehicles and assistive robotic\nindustry. A point cloud is a dense compilation of spatial data in 3D\ncoordinates. It plays a vital role in modeling complex real-world scenes since\nit preserves structural information and avoids perspective distortion, unlike\nimage data, which is the projection of a 3D structure on a 2D plane. In order\nto leverage the intrinsic capabilities of the LIDAR data, we propose a\nPointNet-based approach for 3D Multi-Object Tracking (MOT).", "title": "PC-DAN: Point Cloud based Deep Affinity Network for 3D Multi-Object  Tracking (Accepted as an extended abstract in JRDB-ACT Workshop at CVPR21)"}, {"link": "https://arxiv.org/abs/2106.07553", "abstract": "This paper reviews literature in cognitive science, human-computer\ninteraction (HCI) and natural-language processing (NLP) to consider how\nanalogical reasoning (AR) could help inform the design of communication and\nlearning technologies, as well as online communities and digital platforms.\nFirst, analogical reasoning (AR) is defined, and use-cases of AR in the\ncomputing sciences are presented. The concept of schema is introduced, along\nwith use-cases in computing. Finally, recommendations are offered for future\nwork on using analogical reasoning and schema methods in the computing\nsciences.", "title": "A Cognitive Science perspective for learning how to design meaningful  user experiences and human-centered technology"}, {"link": "https://arxiv.org/abs/2106.07554", "abstract": "In recent years many different deep neural networks were developed, but due\nto a large number of layers in deep networks, their training requires a long\ntime and a large number of datasets. Today is popular to use trained deep\nneural networks for various tasks, even for simple ones in which such deep\nnetworks are not required. The well-known deep networks such as YoloV3, SSD,\netc. are intended for tracking and monitoring various objects, therefore their\nweights are heavy and the overall accuracy for a specific task is low.\nEye-tracking tasks need to detect only one object - an iris in a given area.\nTherefore, it is logical to use a neural network only for this task. But the\nproblem is the lack of suitable datasets for training the model. In the\nmanuscript, we presented a dataset that is suitable for training custom models\nof convolutional neural networks for eye-tracking tasks. Using data set data,\neach user can independently pre-train the convolutional neural network models\nfor eye-tracking tasks. This dataset contains annotated 10,000 eye images in an\nextension of 416 by 416 pixels. The table with annotation information shows the\ncoordinates and radius of the eye for each image. This manuscript can be\nconsidered as a guide for the preparation of datasets for eye-tracking devices", "title": "Dataset for eye-tracking tasks"}, {"link": "https://arxiv.org/abs/2106.07555", "abstract": "While there is evidence that user-adaptive support can greatly enhance the\neffectiveness of educational systems, designing such support for exploratory\nlearning environments (e.g., simulations) is still challenging due to the\nopen-ended nature of their interaction. In particular, there is little a priori\nknowledge of which student's behaviors can be detrimental to learning in such\nenvironments. To address this problem, we focus on a data-driven user-modeling\nframework that uses logged interaction data to learn which behavioral or\nactivity patterns should trigger help during interaction with a specific\nlearning environment. This framework has been successfully used to provide\nadaptive support in interactive learning simulations. Here we present a novel\napplication of this framework we are working on, namely to Massive Open Online\nCourses (MOOCs), a form of exploratory environment that could greatly benefit\nfrom adaptive support due to the large diversity of their users, but typically\nlack of such adaptation. We describe an experiment aimed at investigating the\nvalue of our framework to identify student's behaviors that can justify\nadapting to, and report some preliminary results.", "title": "A Framework to Counteract Suboptimal User-Behaviors in Exploratory  Learning Environments: an Application to MOOCs"}, {"link": "https://arxiv.org/abs/2106.07556", "abstract": "Human activity recognition in videos is a challenging problem that has drawn\na lot of interest, particularly when the goal requires the analysis of a large\nvideo database. AOLME project provides a collaborative learning environment for\nmiddle school students to explore mathematics, computer science, and\nengineering by processing digital images and videos. As part of this project,\naround 2200 hours of video data was collected for analysis. Because of the size\nof the dataset, it is hard to analyze all the videos of the dataset manually.\nThus, there is a huge need for reliable computer-based methods that can detect\nactivities of interest. My thesis is focused on the development of accurate\nmethods for detecting and tracking objects in long videos. All the models are\nvalidated on videos from 7 different sessions, ranging from 45 minutes to 90\nminutes. The keyboard detector achieved a very high average precision (AP) of\n92% at 0.5 intersection over union (IoU). Furthermore, a combined system of the\ndetector with a fast tracker KCF (159fps) was developed so that the algorithm\nruns significantly faster without sacrificing accuracy. For a video of 23\nminutes having resolution 858X480 @ 30 fps, the detection alone runs at 4.7Xthe\nreal-time, and the combined algorithm runs at 21Xthe real-time for an average\nIoU of 0.84 and 0.82, respectively. The hand detector achieved average\nprecision (AP) of 72% at 0.5 IoU. The detection results were improved to 81%\nusing optimal data augmentation parameters. The hand detector runs at 4.7Xthe\nreal-time with AP of 81% at 0.5 IoU. The hand detection method was integrated\nwith projections and clustering for accurate proposal generation. This approach\nreduced the number of false-positive hand detections by 80%. The overall hand\ndetection system runs at 4Xthe real-time, capturing all the activity regions of\nthe current collaborative group.", "title": "Long Term Object Detection and Tracking in Collaborative Learning  Environments"}, {"link": "https://arxiv.org/abs/2106.07557", "abstract": "Corneal endothelial cell segmentation plays a vital role inquantifying\nclinical indicators such as cell density, coefficient of variation,and\nhexagonality. However, the corneal endothelium's uneven reflectionand the\nsubject's tremor and movement cause blurred cell edges in theimage, which is\ndifficult to segment, and need more details and contextinformation to release\nthis problem. Due to the limited receptive field oflocal convolution and\ncontinuous downsampling, the existing deep learn-ing segmentation methods\ncannot make full use of global context andmiss many details. This paper\nproposes a Multi-Branch hybrid Trans-former Network (MBT-Net) based on the\ntransformer and body-edgebranch. Firstly, We use the convolutional block to\nfocus on local tex-ture feature extraction and establish long-range\ndependencies over space,channel, and layer by the transformer and residual\nconnection. Besides,We use the body-edge branch to promote local consistency\nand to provideedge position information. On the self-collected dataset\nTM-EM3000 andpublic Alisarine dataset, compared with other State-Of-The-Art\n(SOTA)methods, the proposed method achieves an improvement.", "title": "A Multi-Branch Hybrid Transformer Networkfor Corneal Endothelial Cell  Segmentation"}, {"link": "https://arxiv.org/abs/2106.07558", "abstract": "Recent advancements in computational power and algorithms have enabled\nunabridged data (e.g., raw images or audio) to be used as input in some models\n(e.g., deep learning). However, the black box nature of such models reduces\ntheir likelihood of adoption by marketing scholars. Our paradigm of analysis,\nthe Transparent Model of Unabridged Data (TMUD), enables researchers to\ninvestigate the inner workings of such black box models by incorporating an ex\nante filtration module and an ex post experimentation module. We empirically\ndemonstrate the TMUD by investigating the role of facial components and sexual\ndimorphism in face perceptions, which have implications for four marketing\ncontexts: advertisement (perceptions of approachability, trustworthiness, and\ncompetence), brand (perceptions of whether a face represents a brand's typical\ncustomer), category (perceptions of whether a face represents a category's\ntypical customer), and customer persona (perceptions of whether a face\nrepresents the persona of a brand's customer segment). Our results reveal new\nand useful findings that enrich the existing literature on face perception,\nmost of which is based on abridged attributes (e.g., width of mouth). The TMUD\nhas great potential to be a useful paradigm for generating theoretical insights\nand may encourage more marketing researchers and practitioners to use\nunabridged data.", "title": "Transparent Model of Unabridged Data (TMUD)"}, {"link": "https://arxiv.org/abs/2106.07559", "abstract": "Machine learning has achieved much success on supervised learning tasks with\nlarge sets of well-annotated training samples. However, in many practical\nsituations, such strong and high-quality supervision provided by training data\nis unavailable due to the expensive and labor-intensive labeling process.\nAutomatically identifying and recognizing object categories in a large volume\nof unlabeled images with weak supervision remains an important, yet unsolved\nchallenge in computer vision. In this paper, we propose a novel machine\nlearning framework, artificial perceptual learning (APL), to tackle the problem\nof weakly supervised image categorization. The proposed APL framework is\nconstructed using state-of-the-art machine learning algorithms as building\nblocks to mimic the cognitive development process known as infant\ncategorization. We develop and illustrate the proposed framework by\nimplementing a wide-field fine-grain ecological survey of tree species over an\n8,000-hectare area of the El Yunque rainforest in Puerto Rico. It is based on\nunlabeled high-resolution aerial images of the tree canopy. Misplaced\nground-based labels were available for less than 1% of these images, which\nserve as the only weak supervision for this learning framework. We validate the\nproposed framework using a small set of images with high quality human\nannotations and show that the proposed framework attains human-level cognitive\neconomy.", "title": "Artificial Perceptual Learning: Image Categorization with Weak  Supervision"}, {"link": "https://arxiv.org/abs/2106.07560", "abstract": "We study the problem of allocating bailouts (stimulus, subsidy allocations)\nto people participating in a financial network subject to income shocks. We\nbuild on the financial clearing framework of Eisenberg and Noe that allows the\nincorporation of a bailout policy that is based on discrete bailouts motivated\nby the types of stimulus checks people receive around the world as part of\nCOVID-19 economical relief plans. We show that optimally allocating such\nbailouts on a financial network in order to maximize a variety of social\nwelfare objectives of this form is a computationally intractable problem. We\ndevelop approximation algorithms to optimize these objectives and establish\nguarantees for their approximation rations. Then, we incorporate multiple\nfairness constraints in the optimization problems and establish relative bounds\non the solutions with versus without these constraints. Finally, we apply our\nmethodology to a variety of data, both in the context of a system of large\nfinancial institutions with real-world data, as well as in a realistic societal\ncontext with financial interactions between people and businesses for which we\nuse semi-artificial data derived from mobility patterns. Our results suggest\nthat the algorithms we develop and study have reasonable results in practice\nand outperform other network-based heuristics. We argue that the presented\nproblem through the societal-level lens could assist policymakers in making\ninformed decisions on issuing subsidies.", "title": "Allocating Stimulus Checks in Times of Crisis"}, {"link": "https://arxiv.org/abs/2106.07561", "abstract": "This work demonstrates direct visual sensory-motor control using high-speed\nCNN inference via a SCAMP-5 Pixel Processor Array (PPA). We demonstrate how\nPPAs are able to efficiently bridge the gap between perception and action. A\nbinary Convolutional Neural Network (CNN) is used for a classic rock, paper,\nscissors classification problem at over 8000 FPS. Control instructions are\ndirectly sent to a servo motor from the PPA according to the CNN's\nclassification result without any other intermediate hardware.", "title": "Direct Servo Control from In-Sensor CNN Inference with A Pixel Processor  Array"}, {"link": "https://arxiv.org/abs/2106.07562", "abstract": "Recent work has shown that the activation function of the convolutional\nneural network can meet the Lipschitz condition, then the corresponding\nconvolutional neural network structure can be constructed according to the\nscale of the data set, and the data set can be trained more deeply, more\naccurately and more effectively. In this article, we have accepted the\nexperimental results and introduced the core block N-Gauss, N-Gauss, and Swish\n(Conv1, Conv2, FC1) neural network structure design to train MNIST, CIFAR10,\nand CIFAR100 respectively. Experiments show that N-Gauss gives full play to the\nmain role of nonlinear modeling of activation functions, so that deep\nconvolutional neural networks have hierarchical nonlinear mapping learning\ncapabilities. At the same time, the training ability of N-Gauss on simple\none-dimensional channel small data sets is equivalent to the performance of\nReLU and Swish.", "title": "Neural Network Structure Design based on N-Gauss Activation Function"}, {"link": "https://arxiv.org/abs/2106.07563", "abstract": "The flow-based generative model is a deep learning generative model, which\nobtains the ability to generate data by explicitly learning the data\ndistribution. Theoretically its ability to restore data is stronger than other\ngenerative models. However, its implementation has many limitations, including\nlimited model design, too many model parameters and tedious calculation. In\nthis paper, a bi-parallel linear flow model for facial emotion generation from\nemotion set images is constructed, and a series of improvements have been made\nin terms of the expression ability of the model and the convergence speed in\ntraining. The model is mainly composed of several coupling layers superimposed\nto form a multi-scale structure, in which each coupling layer contains 1*1\nreversible convolution and linear operation modules. Furthermore, this paper\nsorted out the current public data set of facial emotion images, made a new\nemotion data, and verified the model through this data set. The experimental\nresults show that, under the traditional convolutional neural network, the\n3-layer 3*3 convolution kernel is more conducive to extracte the features of\nthe face images. The introduction of principal component decomposition can\nimprove the convergence speed of the model.", "title": "BPLF: A Bi-Parallel Linear Flow Model for Facial Expression Generation  from Emotion Set Images"}, {"link": "https://arxiv.org/abs/2106.07564", "abstract": "To overcome the limitations of convolutional neural network in the process of\nfacial expression recognition, a facial expression recognition model\nCapsule-LSTM based on video frame sequence is proposed. This model is composed\nof three networks includingcapsule encoders, capsule decoders and LSTM network.\nThe capsule encoder extracts the spatial information of facial expressions in\nvideo frames. Capsule decoder reconstructs the images to optimize the network.\nLSTM extracts the temporal information between video frames and analyzes the\ndifferences in expression changes between frames. The experimental results from\nthe MMI dataset show that the Capsule-LSTM model proposed in this paper can\neffectively improve the accuracy of video expression recognition.", "title": "An optimized Capsule-LSTM model for facial expression recognition with  video sequences"}, {"link": "https://arxiv.org/abs/2106.07565", "abstract": "Inpatient falls are a serious safety issue in hospitals and healthcare\nfacilities. Recent advances in video analytics for patient monitoring provide a\nnon-intrusive avenue to reduce this risk through continuous activity\nmonitoring. However, in-bed fall risk assessment systems have received less\nattention in the literature. The majority of prior studies have focused on fall\nevent detection, and do not consider the circumstances that may indicate an\nimminent inpatient fall. Here, we propose a video-based system that can monitor\nthe risk of a patient falling, and alert staff of unsafe behaviour to help\nprevent falls before they occur. We propose an approach that leverages recent\nadvances in human localisation and skeleton pose estimation to extract spatial\nfeatures from video frames recorded in a simulated environment. We demonstrate\nthat body positions can be effectively recognised and provide useful evidence\nfor fall risk assessment. This work highlights the benefits of video-based\nmodels for analysing behaviours of interest, and demonstrates how such a system\ncould enable sufficient lead time for healthcare professionals to respond and\naddress patient needs, which is necessary for the development of fall\nintervention programs.", "title": "Video-Based Inpatient Fall Risk Assessment: A Case Study"}, {"link": "https://arxiv.org/abs/2106.07568", "abstract": "This paper proposed a new methodology for machine learning in 2-dimensional\nspace (2-D ML) in inline coordinates. It is a full machine learning approach\nthat does not require to deal with n-dimensional data in n-dimensional space.\nIt allows discovering n-D patterns in 2-D space without loss of n-D information\nusing graph representations of n-D data in 2-D. Specifically, it can be done\nwith the inline based coordinates in different modifications, including static\nand dynamic ones. The classification and regression algorithms based on these\ninline coordinates were introduced. A successful case study based on a\nbenchmark data demonstrated the feasibility of the approach. This approach\nhelps to consolidate further a whole new area of full 2-D machine learning as a\npromising ML methodology. It has advantages of abilities to involve actively\nthe end-users into the discovering of models and their justification. Another\nadvantage is providing interpretable ML models.", "title": "Full interpretable machine learning in 2D with inline coordinates"}, {"link": "https://arxiv.org/abs/2106.07575", "abstract": "While the advances in synchrotron light sources, together with the\ndevelopment of focusing optics and detectors, allow nanoscale ptychographic\nimaging of materials and biological specimens, the corresponding experiments\ncan yield terabyte-scale large volumes of data that can impose a heavy burden\non the computing platform. While Graphical Processing Units (GPUs) provide high\nperformance for such large-scale ptychography datasets, a single GPU is\ntypically insufficient for analysis and reconstruction. Several existing works\nhave considered leveraging multiple GPUs to accelerate the ptychographic\nreconstruction. However, they utilize only Message Passing Interface (MPI) to\nhandle the communications between GPUs. It poses inefficiency for the\nconfiguration that has multiple GPUs in a single node, especially while\nprocessing a single large projection, since it provides no optimizations to\nhandle the heterogeneous GPU interconnections containing both low-speed links,\ne.g., PCIe, and high-speed links, e.g., NVLink. In this paper, we provide a\nmulti-GPU implementation that can effectively solve large-scale ptychographic\nreconstruction problem with optimized performance on intra-node multi-GPU. We\nfocus on the conventional maximum-likelihood reconstruction problem using\nconjugate-gradient (CG) for the solution and propose a novel hybrid\nparallelization model to address the performance bottlenecks in CG solver.\nAccordingly, we develop a tool called PtyGer (Ptychographic GPU(multiple)-based\nreconstruction), implementing our hybrid parallelization model design. The\ncomprehensive evaluation verifies that PtyGer can fully preserve the original\nalgorithm's accuracy while achieving outstanding intra-node GPU scalability.", "title": "Scalable and accurate multi-GPU based image reconstruction of  large-scale ptychography data"}, {"link": "https://arxiv.org/abs/2106.07577", "abstract": "With the increasing demand for audio communication and online conference,\nensuring the robustness of Acoustic Echo Cancellation (AEC) under the\ncomplicated acoustic scenario including noise, reverberation and nonlinear\ndistortion has become a top issue. Although there have been some traditional\nmethods that consider nonlinear distortion, they are still inefficient for echo\nsuppression and the performance will be attenuated when noise is present. In\nthis paper, we present a real-time AEC approach using complex neural network to\nbetter modeling the important phase information and frequency-time-LSTMs\n(F-T-LSTM), which scan both frequency and time axis, for better temporal\nmodeling. Moreover, we utilize modified SI-SNR as cost function to make the\nmodel to have better echo cancellation and noise suppression (NS) performance.\nWith only 1.4M parameters, the proposed approach outperforms the AEC-challenge\nbaseline by 0.27 in terms of Mean Opinion Score (MOS).", "title": "F-T-LSTM based Complex Network for Joint Acoustic Echo Cancellation and  Speech Enhancement"}, {"link": "https://arxiv.org/abs/2106.07578", "abstract": "In this paper, a new learning algorithm for Federated Learning (FL) is\nintroduced. The proposed scheme is based on a weighted gradient aggregation\nusing two-step optimization to offer a flexible training pipeline. Herein, two\ndifferent flavors of the aggregation method are presented, leading to an order\nof magnitude improvement in convergence speed compared to other distributed or\nFL training algorithms like BMUF and FedAvg. Further, the aggregation algorithm\nacts as a regularizer of the gradient quality. We investigate the effect of our\nFL algorithm in supervised and unsupervised Speech Recognition (SR) scenarios.\nThe experimental validation is performed based on three tasks: first, the\nLibriSpeech task showing a speed-up of 7x and 6% word error rate reduction\n(WERR) compared to the baseline results. The second task is based on session\nadaptation providing 20% WERR over a powerful LAS model. Finally, our\nunsupervised pipeline is applied to the conversational SR task. The proposed FL\nsystem outperforms the baseline systems in both convergence speed and overall\nmodel performance.", "title": "Dynamic Gradient Aggregation for Federated Domain Adaptation"}, {"link": "https://arxiv.org/abs/2106.07582", "abstract": "Generative diffusion processes are an emerging and effective tool for image\nand speech generation. In the existing methods, the underline noise\ndistribution of the diffusion process is Gaussian noise. However, fitting\ndistributions with more degrees of freedom, could help the performance of such\ngenerative models. In this work, we investigate other types of noise\ndistribution for the diffusion process. Specifically, we show that noise from\nGamma distribution provides improved results for image and speech generation.\nMoreover, we show that using a mixture of Gaussian noise variables in the\ndiffusion process improves the performance over a diffusion process that is\nbased on a single distribution. Our approach preserves the ability to\nefficiently sample state in the training diffusion process while using Gamma\nnoise and a mixture of noise.", "title": "Non Gaussian Denoising Diffusion Models"}, {"link": "https://arxiv.org/abs/2106.07583", "abstract": "We introduce BioCoM, a contrastive learning framework for biomedical entity\nlinking that uses only two resources: a small-sized dictionary and a large\nnumber of raw biomedical articles. Specifically, we build the training\ninstances from raw PubMed articles by dictionary matching and use them to train\na context-aware entity linking model with contrastive learning. We predict the\nnormalized biomedical entity at inference time through a nearest-neighbor\nsearch. Results found that BioCoM substantially outperforms state-of-the-art\nmodels, especially in low-resource settings, by effectively using the context\nof the entities.", "title": "Biomedical Entity Linking via Contrastive Context Matching"}, {"link": "https://arxiv.org/abs/2106.07588", "abstract": "India is expected to witness rapid growth in electricity use over the next\ntwo decades. Here, we introduce a custom regression model to project\nelectricity consumption in India over the coming decades, which includes a\nbottom-up estimate of electricity consumption for two major growth drivers, air\nconditioning, and vehicle electrification. The model projections are available\nat a customizable level of spatial aggregation at an hourly temporal\nresolution, which makes them useful as inputs to long-term electricity\ninfrastructure planning studies. The approach is used to develop electricity\nconsumption data sets spanning various technology adoption and growth scenarios\nup to the year 2050 in five-year increments. The aim of the data is to provide\na range of scenarios for India's demand growth given new technology adoption.\nWith long-term hourly demand projections serving as an essential input for\nelectricity infrastructure modeling, this data publication enables further work\non energy efficiency, generation, and transmission expansion planning for a\nfast-growing and increasingly important region from a global climate mitigation\nperspective.", "title": "Scenarios of future Indian electricity demand accounting for space  cooling and electric vehicle adoption"}, {"link": "https://arxiv.org/abs/2106.07590", "abstract": "The growing demand for electricity in emerging markets and developing\neconomies such as India is causing loading and congestion problems on\ndistribution networks, particularly in urban locations. Electric utilities in\nthese regions face unique constraints regarding raising capital required to\nupgrade their congested networks. Battery storage has emerged as a non-wire\nalternative to feeder-level upgrades. This article presents a valuation\nframework by optimally sizing and placing battery storage on the distribution\nnetwork. We evaluate the value of storage using a real options analysis through\na Markov Chain Monte Carlo to identify the least-cost network upgrade strategy,\ngiven demand growth uncertainty. When applied to urban distribution network\nfeeders typical of those found in congested cities in India, the approach\nhighlights the economic value of network investment deferrals by making use of\nbattery storage. We find that storage costs below 261 USD/kWh justify\ninvestments in distribution level storage and storage as a non-wire alternative\nonly makes sense on moderately loaded feeders where storage charging is still\nfeasible without violating network thermal capacity limits.", "title": "Distribution level battery storage valuation framework"}, {"link": "https://arxiv.org/abs/2106.07594", "abstract": "Self-supervised learning on graph-structured data has drawn recent interest\nfor learning generalizable, transferable and robust representations from\nunlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged\nwith promising representation learning performance. Unfortunately, unlike its\ncounterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data\naugmentations, which have to be manually picked per dataset, by either rules of\nthumb or trial-and-errors, owing to the diverse nature of graph data. That\nsignificantly limits the more general applicability of GraphCL. Aiming to fill\nin this crucial gap, this paper proposes a unified bi-level optimization\nframework to automatically, adaptively and dynamically select data\naugmentations when performing GraphCL on specific graph data. The general\nframework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as\nmin-max optimization. The selections of augmentations made by JOAO are shown to\nbe in general aligned with previous \"best practices\" observed from handcrafted\ntuning: yet now being automated, more flexible and versatile. Moreover, we\npropose a new augmentation-aware projection head mechanism, which will route\noutput features through different projection heads corresponding to different\naugmentations chosen at each training step. Extensive experiments demonstrate\nthat JOAO performs on par with or sometimes better than the state-of-the-art\ncompetitors including GraphCL, on multiple graph datasets of various scales and\ntypes, yet without resorting to any laborious dataset-specific tuning on\naugmentation selection. We release the code at\nhttps://github.com/Shen-Lab/GraphCL_Automated.", "title": "Graph Contrastive Learning Automated"}, {"link": "https://arxiv.org/abs/2106.07596", "abstract": "Flexible optical networks (FONs) are being adopted to accommodate the\nincreasingly heterogeneous traffic in today's Internet. However, in presence of\nhigh traffic load, not all offered traffic can be satisfied at all time. As\ncarried traffic load brings revenues to operators, traffic blocking due to\nlimited spectrum resource leads to revenue losses. In this study, given a set\nof traffic requests to be provisioned, we consider the problem of maximizing\noperator's revenue, subject to limited spectrum resource and physical layer\nimpairments (PLIs), namely amplified spontaneous emission noise (ASE),\nself-channel interference (SCI), cross-channel interference (XCI), and node\ncrosstalk. In FONs, adaptive modulation, multiple FEC, and the tuning of power\nspectrum density (PSD) can be effectively employed to mitigate the impact of\nPLIs. Hence, in our study, we propose a universal bandwidth-related impairment\nevaluation model based on channel bandwidth, which allows a performance\nanalysis for different PSD, FEC and modulations. Leveraging this PLI model and\na piecewise linear fitting function, we succeed to formulate the revenue\nmaximization problem as a mixed integer linear program. Then, to solve the\nproblem on larger network instances, a fast two-phase heuristic algorithm is\nalso proposed, which is shown to be near-optimal for revenue maximization.\nThrough simulations, we demonstrate that using adaptive modulation enables to\nsignificantly increase revenues in the scenario of high signal-to-noise ratio\n(SNR), where the revenue can even be doubled for high traffic load, while using\nmultiple FECs is more profitable for scenarios with low SNR.", "title": "Maximizing Revenue with Adaptive Modulation and Multiple FECs in  Flexible Optical Networks"}, {"link": "https://arxiv.org/abs/2106.07597", "abstract": "Advancements in ultra-low-power tiny machine learning (TinyML) systems\npromise to unlock an entirely new class of smart applications. However,\ncontinued progress is limited by the lack of a widely accepted and easily\nreproducible benchmark for these systems. To meet this need, we present MLPerf\nTiny, the first industry-standard benchmark suite for ultra-low-power tiny\nmachine learning systems. The benchmark suite is the collaborative effort of\nmore than 50 organizations from industry and academia and reflects the needs of\nthe community. MLPerf Tiny measures the accuracy, latency, and energy of\nmachine learning inference to properly evaluate the tradeoffs between systems.\nAdditionally, MLPerf Tiny implements a modular design that enables benchmark\nsubmitters to show the benefits of their product, regardless of where it falls\non the ML deployment stack, in a fair and reproducible manner. The suite\nfeatures four benchmarks: keyword spotting, visual wake words, image\nclassification, and anomaly detection.", "title": "MLPerf Tiny Benchmark"}, {"link": "https://arxiv.org/abs/2106.07603", "abstract": "Dimensionality of parameters and variables is a fundamental issue in physics\nbut mostly ignored from a mathematical point of view. Diffculties arising from\ndimensional inconsistence are overcome by scaling analysis and, often, both\nconcepts, dimensionality and scaling, are confused. In the particular case of\niterative methods for solving non-linear equations, dimensionality and scaling\naffects their robutness: while some classical methods, such as Newton, are\nadimensional and scale independent, some other iterations as Steffensen's are\nnot; their convergence depends on the scaling, and their evaluation needs a\ndimensional congruence. In this paper we introduce the concept of adimensional\nform of a function in order to study the behavior of iterative methods, thus\ncorrecting, if possible, some pathological features. From this adimensional\nform we will devise an adimensional and scale invariant method based on\nSteffensen's which we will call ASIS method.", "title": "On the Adimensional Scale Invariant Steffensen (ASIS) Method"}, {"link": "https://arxiv.org/abs/2106.07606", "abstract": "A physics informed neural network (PINN) incorporates the physics of a system\nby satisfying its boundary value problem through a neural network's loss\nfunction. The PINN approach has shown great success in approximating the map\nbetween the solution of a partial differential equation (PDE) and its\nspatio-temporal input. However, for strongly non-linear and higher order\npartial differential equations PINN's accuracy reduces significantly. To\nresolve this problem, we propose a novel PINN scheme that solves the PDE\nsequentially over successive time segments using a single neural network. The\nkey idea is to re-train the same neural network for solving the PDE over\nsuccessive time segments while satisfying the already obtained solution for all\nprevious time segments. Thus it is named as backward compatible PINN (bc-PINN).\nTo illustrate the advantages of bc-PINN, we have used the Cahn Hilliard and\nAllen Cahn equations, which are widely used to describe phase separation and\nreaction diffusion systems. Our results show significant improvement in\naccuracy over the PINN method while using a smaller number of collocation\npoints. Additionally, we have shown that using the phase space technique for a\nhigher order PDE could further improve the accuracy and efficiency of the\nbc-PINN scheme.", "title": "A Physics Informed Neural Network for Time-Dependent Nonlinear and  Higher Order Partial Differential Equations"}, {"link": "https://arxiv.org/abs/2106.07609", "abstract": "A wave view of the universe is proposed in which each natural phenomenon is\nequipped with its own unique natural viewing lens. A self-sameness modeling\nprinciple and its systematic application in Fourier-Laplace transform space is\nproposed as a novel, universal discrete modeling paradigm for\nadvection-diffusion-reaction equations (ADREs) across non-integer derivatives,\ntime scales, and wave spectral signatures. Its implementation is a novel exact\nspectral derivative discretization finite difference method (ESDDFD), a way for\ncrafting wave viewing lenses by obtaining discrete wave models from ADRE\nmodels. The template for building these lenses come in the form of natural\nderivative representations obtained from the wave signature probability\ndistribution function and its harmonic oscillation in FL transform space; use\nof the ESDDFD method in the discrete numerical modeling of wave equations\nrequires no a-priori theory of any mathematical derivative. A major\nmathematical consequence of this viewpoint is that all notions of the\nmathematical integer or non-integer derivatives have representation as limits\nof such natural derivative representations; this and other consequences are\ndiscussed and a discretization of a simple integer derivative\ndiffusion-reaction equation is presented to illustrate the method. The\nresulting view lenses, in the form of ESDDFD models, work well in detecting\nboth local and non-local Debye or Kohlrausch-Williams-Watts exponential\npatterns; only Brownian motion and sub-diffusion are discussed in the present\narticle.", "title": "The Exact Spectral Derivative Discretization Finite Difference (ESDDFD)  Method for Wave Models: A Universal Wave View Through Natural Fractional /  Fractal Derivative Representations (or View Lens Shops for The Exponential  Wave Universe)"}, {"link": "https://arxiv.org/abs/2106.07611", "abstract": "Mixed-precision quantization is a powerful tool to enable memory and compute\nsavings of neural network workloads by deploying different sets of bit-width\nprecisions on separate compute operations. Recent research has shown\nsignificant progress in applying mixed-precision quantization techniques to\nreduce the memory footprint of various workloads, while also preserving task\nperformance. Prior work, however, has often ignored additional objectives, such\nas bit-operations, that are important for deployment of workloads on hardware.\nHere we present a flexible and scalable framework for automated mixed-precision\nquantization that optimizes multiple objectives. Our framework relies on\nNeuroevolution-Enhanced Multi-Objective Optimization (NEMO), a novel search\nmethod, to find Pareto optimal mixed-precision configurations for memory and\nbit-operations objectives. Within NEMO, a population is divided into\nstructurally distinct sub-populations (species) which jointly form the Pareto\nfrontier of solutions for the multi-objective problem. At each generation,\nspecies are re-sized in proportion to the goodness of their contribution to the\nPareto frontier. This allows NEMO to leverage established search techniques and\nneuroevolution methods to continually improve the goodness of the Pareto\nfrontier. In our experiments we apply a graph-based representation to describe\nthe underlying workload, enabling us to deploy graph neural networks trained by\nNEMO to find Pareto optimal configurations for various workloads trained on\nImageNet. Compared to the state-of-the-art, we achieve competitive results on\nmemory compression and superior results for compute compression for\nMobileNet-V2, ResNet50 and ResNeXt-101-32x8d. A deeper analysis of the results\nobtained by NEMO also shows that both the graph representation and the\nspecies-based approach are critical in finding effective configurations for all\nworkloads.", "title": "Neuroevolution-Enhanced Multi-Objective Optimization for Mixed-Precision  Quantization"}, {"link": "https://arxiv.org/abs/2106.07613", "abstract": "We propose a novel approach to dimensionality reduction combining techniques\nof metric geometry and distributed persistent homology, in the form of a\ngradient-descent based method called DIPOLE. DIPOLE is a\ndimensionality-reduction post-processing step that corrects an initial\nembedding by minimizing a loss functional with both a local, metric term and a\nglobal, topological term. By fixing an initial embedding method (we use\nIsomap), DIPOLE can also be viewed as a full dimensionality-reduction pipeline.\nThis framework is based on the strong theoretical and computational properties\nof distributed persistent homology and comes with the guarantee of almost sure\nconvergence. We observe that DIPOLE outperforms popular methods like UMAP,\nt-SNE, and Isomap on a number of popular datasets, both visually and in terms\nof precise quantitative metrics.", "title": "Improving Metric Dimensionality Reduction with Distributed Topology"}, {"link": "https://arxiv.org/abs/2106.07615", "abstract": "We present Magic Layouts; a method for parsing screenshots or hand-drawn\nsketches of user interface (UI) layouts. Our core contribution is to extend\nexisting detectors to exploit a learned structural prior for UI designs,\nenabling robust detection of UI components; buttons, text boxes and similar.\nSpecifically we learn a prior over mobile UI layouts, encoding common spatial\nco-occurrence relationships between different UI components. Conditioning\nregion proposals using this prior leads to performance gains on UI layout\nparsing for both hand-drawn UIs and app screenshots, which we demonstrate\nwithin the context an interactive application for rapidly acquiring digital\nprototypes of user experience (UX) designs.", "title": "Magic Layouts: Structural Prior for Component Detection in User  Interface Designs"}, {"link": "https://arxiv.org/abs/2106.07616", "abstract": "We present an exponentially convergent semi-implicit meshless algorithm for\nthe solution of Navier-Stokes equations in complex domains. The algorithm\ndiscretizes partial derivatives at scattered points using radial basis\nfunctions as interpolants. Higher-order polynomials are appended to\npolyharmonic splines (PHS-RBF) and a collocation method is used to derive the\ninterpolation coefficients. The interpolating kernels are then differentiated\nand the partial-differential equations are satisfied by collocation at the\nscattered points. The PHS-RBF interpolation is shown to be exponentially\nconvergent with discretization errors decreasing as a high power of a\nrepresentative distance between points. We present here a semi-implicit\nalgorithm for time-dependent and steady state fluid flows in complex domains.\nAt each time step, several iterations are performed to converge the momentum\nand continuity equations. A Poisson equation for pressure corrections is\nformulated by imposing divergence free condition on the iterated velocity\nfield. At each time step, the momentum and pressure correction equations are\nrepeatedly solved until the velocities and pressure converge to a pre-specified\ntolerance. We have demonstrated the convergence and discretization accuracy of\nthe algorithm for two model problems and simulated three other complex\nproblems. In all cases, the algorithm is stable for Courant numbers in excess\nof ten. The algorithm has the potential to accurately and efficiently solve\nmany fluid flow and heat transfer problems in complex domains. An open source\ncode Meshless Multi-Physics Software (MeMPhyS) is available for interested\nusers of the algorithm.", "title": "A Semi-Implicit Meshless Method for Incompressible Flows in Complex  Geometries"}, {"link": "https://arxiv.org/abs/2106.07617", "abstract": "Recently, Vision Transformers (ViTs) have achieved impressive results on\nvarious vision tasks. Yet, their generalization ability under different\ndistribution shifts is rarely understood. In this work, we provide a\ncomprehensive study on the out-of-distribution generalization of ViTs. To\nsupport a systematic investigation, we first present a taxonomy of distribution\nshifts by categorizing them into five conceptual groups: corruption shift,\nbackground shift, texture shift, destruction shift, and style shift. Then we\nperform extensive evaluations of ViT variants under different groups of\ndistribution shifts and compare their generalization ability with CNNs. Several\nimportant observations are obtained: 1) ViTs generalize better than CNNs under\nmultiple distribution shifts. With the same or fewer parameters, ViTs are ahead\nof corresponding CNNs by more than 5% in top-1 accuracy under most distribution\nshifts. 2) Larger ViTs gradually narrow the in-distribution and\nout-of-distribution performance gap. To further improve the generalization of\nViTs, we design the Generalization-Enhanced ViTs by integrating adversarial\nlearning, information theory, and self-supervised learning. By investigating\nthree types of generalization-enhanced ViTs, we observe their\ngradient-sensitivity and design a smoother learning strategy to achieve a\nstable training process. With modified training schemes, we achieve\nimprovements on performance towards out-of-distribution data by 4% from vanilla\nViTs. We comprehensively compare three generalization-enhanced ViTs with their\ncorresponding CNNs, and observe that: 1) For the enhanced model, larger ViTs\nstill benefit more for the out-of-distribution generalization. 2)\ngeneralization-enhanced ViTs are more sensitive to the hyper-parameters than\ncorresponding CNNs. We hope our comprehensive study could shed light on the\ndesign of more generalizable learning architectures.", "title": "Delving Deep into the Generalization of Vision Transformers under  Distribution Shifts"}, {"link": "https://arxiv.org/abs/2106.07620", "abstract": "In this paper, explicit stable integrators based on symplectic and contact\ngeometries are proposed for a non-autonomous ordinarily differential equation\n(ODE) found in improving convergence rate of Nesterov's accelerated gradient\nmethod. Symplectic geometry is known to be suitable for describing Hamiltonian\nmechanics, and contact geometry is known as an odd-dimensional counterpart of\nsymplectic geometry. Moreover, a procedure, called symplectization, is a known\nway to construct a symplectic manifold from a contact manifold, yielding\nHamiltonian systems from contact ones. It is found in this paper that a\npreviously investigated non-autonomous ODE can be written as a contact\nHamiltonian system. Then, by symplectization of a non-autonomous contact\nHamiltonian vector field expressing the non-autonomous ODE, novel symplectic\nintegrators are derived. Because the proposed symplectic integrators preserve\nhidden symplectic and contact structures in the ODE, they should be more stable\nthan the Runge-Kutta method. Numerical experiments demonstrate that, as\nexpected, the second-order symplectic integrator is stable and high convergence\nrates are achieved.", "title": "Fast symplectic integrator for Nesterov-type acceleration method"}, {"link": "https://arxiv.org/abs/2106.07621", "abstract": "Summation formulas, such as the Euler-Maclaurin expansion or Gregory's\nquadrature, have found many applications in mathematics, ranging from\naccelerating series, to evaluating fractional sums and analyzing asymptotics,\namong others. We show that these summation formulas actually arise as\nparticular instances of a single series expansion, including Euler's method for\nalternating series. This new summation formula gives rise to a family of\npolynomials, which contain both the Bernoulli and Gregory numbers in their\ncoefficients. We prove some properties of those polynomials, such as recurrence\nidentities and symmetries. Lastly, we present one case study, which illustrates\none potential application of the new expansion for finite impulse response\n(FIR) filters.", "title": "A Generalization of Classical Formulas in Numerical Integration and  Series Convergence Acceleration"}, {"link": "https://arxiv.org/abs/2106.07625", "abstract": "The Landau-Lifshitz-Gilbert equation yields a mathematical model to describe\nthe evolution of the magnetization of a magnetic material, particularly in\nresponse to an external applied magnetic field. It allows one to take into\naccount various physical effects, such as the exchange within the magnetic\nmaterial itself. In particular, the Landau-Lifshitz-Gilbert equation encodes\nrelaxation effects, i.e., it describes the time-delayed alignment of the\nmagnetization field with an external magnetic field. These relaxation effects\nare an important aspect in magnetic particle imaging, particularly in the\ncalibration process. In this article, we address the data-driven modeling of\nthe system function in magnetic particle imaging, where the\nLandau-Lifshitz-Gilbert equation serves as the basic tool to include relaxation\neffects in the model. We formulate the respective parameter identification\nproblem both in the all-at-once and the reduced setting, present reconstruction\nalgorithms that yield a regularized solution and discuss numerical experiments.\nApart from that, we propose a practical numerical solver to the nonlinear\nLandau-Lifshitz-Gilbert equation, not via the classical finite element method,\nbut through solving only linear PDEs in an inverse problem framework.", "title": "On numerical aspects of parameter identification for the  Landau-Lifshitz-Gilbert equation in Magnetic Particle Imaging"}, {"link": "https://arxiv.org/abs/2106.07627", "abstract": "This paper explores the challenge of teaching a machine how to\nreverse-engineer the grid-marked surfaces used to represent data in 3D surface\nplots of two-variable functions. These are common in scientific and economic\npublications; and humans can often interpret them with ease, quickly gleaning\ngeneral shape and curvature information from the simple collection of curves.\nWhile machines have no such visual intuition, they do have the potential to\naccurately extract the more detailed quantitative data that guided the\nsurface's construction. We approach this problem by synthesizing a new dataset\nof 3D grid-marked surfaces (SurfaceGrid) and training a deep neural net to\nestimate their shape. Our algorithm successfully recovers shape information\nfrom synthetic 3D surface plots that have had axes and shading information\nremoved, been rendered with a variety of grid types, and viewed from a range of\nviewpoints.", "title": "Toward Automatic Interpretation of 3D Plots"}, {"link": "https://arxiv.org/abs/2106.07628", "abstract": "The multiscale complexity of modern problems in computational science and\nengineering can prohibit the use of traditional numerical methods in\nmulti-dimensional simulations. Therefore, novel algorithms are required in\nthese situations to solve partial differential equations (PDEs) with features\nevolving on a wide range of spatial and temporal scales. To meet these\nchallenges, we present a multiresolution wavelet algorithm to solve PDEs with\nsignificant data compression and explicit error control. We discretize in space\nby projecting fields and spatial derivative operators onto wavelet basis\nfunctions. We provide error estimates for the wavelet representation of fields\nand their derivatives. Then, our estimates are used to construct a sparse\nmultiresolution discretization which guarantees the prescribed accuracy.\nAdditionally, we embed a predictor-corrector procedure within the temporal\nintegration to dynamically adapt the computational grid and maintain the\naccuracy of the solution of the PDE as it evolves. We present examples to\nhighlight the accuracy and adaptivity of our approach.", "title": "A multiresolution adaptive wavelet method for nonlinear partial  differential equations"}, {"link": "https://arxiv.org/abs/2106.07630", "abstract": "Hierarchical forecasting is a key problem in many practical multivariate\nforecasting applications - the goal is to simultaneously predict a large number\nof correlated time series that are arranged in a pre-specified aggregation\nhierarchy. The challenge is to exploit the hierarchical correlations to\nsimultaneously obtain good prediction accuracy for time series at different\nlevels of the hierarchy. In this paper, we propose a new approach for\nhierarchical forecasting based on decomposing the time series along a global\nset of basis time series and modeling hierarchical constraints using the\ncoefficients of the basis decomposition for each time series. Unlike past\nmethods, our approach is scalable at inference-time (forecasting for a specific\ntime series only needs access to its own data) while (approximately) preserving\ncoherence among the time series forecasts. We experiment on several publicly\navailable datasets and demonstrate significantly improved overall performance\non forecasts at different levels of the hierarchy, compared to existing\nstate-of-the-art hierarchical reconciliation methods.", "title": "Hierarchically Regularized Deep Forecasting"}, {"link": "https://arxiv.org/abs/2106.07631", "abstract": "Attention-based models, exemplified by the Transformer, can effectively model\nlong range dependency, but suffer from the quadratic complexity of\nself-attention operation, making them difficult to be adopted for\nhigh-resolution image generation based on Generative Adversarial Networks\n(GANs). In this paper, we introduce two key ingredients to Transformer to\naddress this challenge. First, in low-resolution stages of the generative\nprocess, standard global self-attention is replaced with the proposed\nmulti-axis blocked self-attention which allows efficient mixing of local and\nglobal attention. Second, in high-resolution stages, we drop self-attention\nwhile only keeping multi-layer perceptrons reminiscent of the implicit neural\nfunction. To further improve the performance, we introduce an additional\nself-modulation component based on cross-attention. The resulting model,\ndenoted as HiT, has a linear computational complexity with respect to the image\nsize and thus directly scales to synthesizing high definition images. We show\nin the experiments that the proposed HiT achieves state-of-the-art FID scores\nof 31.87 and 2.95 on unconditional ImageNet $128 \\times 128$ and FFHQ $256\n\\times 256$, respectively, with a reasonable throughput. We believe the\nproposed HiT is an important milestone for generators in GANs which are\ncompletely free of convolutions.", "title": "Improved Transformer for High-Resolution GANs"}, {"link": "https://arxiv.org/abs/2106.07635", "abstract": "Learning the causal structure that underlies data is a crucial step towards\nrobust real-world decision making. The majority of existing work in causal\ninference focuses on determining a single directed acyclic graph (DAG) or a\nMarkov equivalence class thereof. However, a crucial aspect to acting\nintelligently upon the knowledge about causal structure which has been inferred\nfrom finite data demands reasoning about its uncertainty. For instance,\nplanning interventions to find out more about the causal mechanisms that govern\nour data requires quantifying epistemic uncertainty over DAGs. While Bayesian\ncausal inference allows to do so, the posterior over DAGs becomes intractable\neven for a small number of variables. Aiming to overcome this issue, we propose\na form of variational inference over the graphs of Structural Causal Models\n(SCMs). To this end, we introduce a parametric variational family modelled by\nan autoregressive distribution over the space of discrete DAGs. Its number of\nparameters does not grow exponentially with the number of variables and can be\ntractably learned by maximising an Evidence Lower Bound (ELBO). In our\nexperiments, we demonstrate that the proposed variational posterior is able to\nprovide a good approximation of the true posterior.", "title": "Variational Causal Networks: Approximate Bayesian Inference over Causal  Structures"}, {"link": "https://arxiv.org/abs/2106.07643", "abstract": "Learning sensorimotor control policies from high-dimensional images crucially\nrelies on the quality of the underlying visual representations. Prior works\nshow that structured latent space such as visual keypoints often outperforms\nunstructured representations for robotic control. However, most of these\nrepresentations, whether structured or unstructured are learned in a 2D space\neven though the control tasks are usually performed in a 3D environment. In\nthis work, we propose a framework to learn such a 3D geometric structure\ndirectly from images in an end-to-end unsupervised manner. The input images are\nembedded into latent 3D keypoints via a differentiable encoder which is trained\nto optimize both a multi-view consistency loss and downstream task objective.\nThese discovered 3D keypoints tend to meaningfully capture robot joints as well\nas object movements in a consistent manner across both time and 3D space. The\nproposed approach outperforms prior state-of-art methods across a variety of\nreinforcement learning benchmarks. Code and videos at\nhttps://buoyancy99.github.io/unsup-3d-keypoints/", "title": "Unsupervised Learning of Visual 3D Keypoints for Control"}]