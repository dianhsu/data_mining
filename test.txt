<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2/css/arXiv.css?v=20200611"" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>

<!-- Pendo -->
<script>
 (function(apiKey){
     (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=[];
         v=['initialize','identify','updateOptions','pageLoad'];for(w=0,x=v.length;w<x;++w)(function(m){
             o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
         y=e.createElement(n);y.async=!0;y.src='https://content.analytics.arxiv.org/agent/static/'+apiKey+'/pendo.js';
         z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

     // Call this whenever information about your visitors becomes available
     // Please use Strings, Numbers, or Bools for value types.
     pendo.initialize({
         visitor: {
             id:              'VISITOR-UNIQUE-ID'   // Required if user is logged in
             // email:        // Recommended if using Pendo Feedback, or NPS Email
             // full_name:    // Recommended if using Pendo Feedback
             // role:         // Optional

             // You can add any additional visitor level key-values here,
             // as long as it's not one of the above reserved names.
         },

         account: {
             id:           'ACCOUNT-UNIQUE-ID' // Highly recommended
             // name:         // Optional
             // is_paying:    // Recommended if using Pendo Feedback
             // monthly_value:// Recommended if using Pendo Feedback
             // planLevel:    // Optional
             // planPrice:    // Optional
             // creationDate: // Optional

             // You can add any additional account level key-values here,
             // as long as it's not one of the above reserved names.
         }
     });
 })('d6494389-b427-4103-7c76-03182ecc8e60');
</script>
<!-- End Pendo -->


</head>
<body class="with-cu-identity">

 
<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1><a href="/">arXiv.org</a> &gt; <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri 11 Jun 21  to  Mon 14 Jun 21, announced Tue, 15 Jun 21</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item525">Cross-lists</a></li>
<li><a href="#item585">Replacements</a></li>
</ul>
<small>[ total of 951 entries:  <b>1-951</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue, 15 Jun 21</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06555" title="Abstract">arXiv:2106.06555</a> [<a href="/pdf/2106.06555" title="Download PDF">pdf</a>, <a href="/format/2106.06555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Knowledge Graph Completion with Stacked Convolutions and a  Student Re-Ranking Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lovelace%2C+J">Justin Lovelace</a>, 
<a href="/search/cs?searchtype=author&query=Newman-Griffis%2C+D">Denis Newman-Griffis</a>, 
<a href="/search/cs?searchtype=author&query=Vashishth%2C+S">Shikhar Vashishth</a>, 
<a href="/search/cs?searchtype=author&query=Lehman%2C+J+F">Jill Fain Lehman</a>, 
<a href="/search/cs?searchtype=author&query=Ros%C3%A9%2C+C+P">Carolyn Penstein Ros&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Knowledge Graph (KG) completion research usually focuses on densely connected
benchmark datasets that are not representative of real KGs. We curate two KG
datasets that include biomedical and encyclopedic knowledge and use an existing
commonsense KG dataset to explore KG completion in the more realistic setting
where dense connectivity is not guaranteed. We develop a deep convolutional
network that utilizes textual entity representations and demonstrate that our
model outperforms recent KG completion methods in this challenging setting. We
find that our model's performance improvements stem primarily from its
robustness to sparsity. We then distill the knowledge from the convolutional
network into a student network that re-ranks promising candidate entities. This
re-ranking stage leads to further improvements in performance and demonstrates
the effectiveness of entity re-ranking for KG completion.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06560" title="Abstract">arXiv:2106.06560</a> [<a href="/pdf/2106.06560" title="Download PDF">pdf</a>, <a href="/format/2106.06560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HR-NAS: Searching Efficient High-Resolution Neural Architectures with  Lightweight Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+X">Xiaochen Lian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaojie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiwu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2021 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">High-resolution representations (HR) are essential for dense prediction tasks
such as segmentation, detection, and pose estimation. Learning HR
representations is typically ignored in previous Neural Architecture Search
(NAS) methods that focus on image classification. This work proposes a novel
NAS method, called HR-NAS, which is able to find efficient and accurate
networks for different tasks, by effectively encoding multiscale contextual
information while maintaining high-resolution representations. In HR-NAS, we
renovate the NAS search space as well as its searching strategy. To better
encode multiscale image contexts in the search space of HR-NAS, we first
carefully design a lightweight transformer, whose computational complexity can
be dynamically changed with respect to different objective functions and
computation budgets. To maintain high-resolution representations of the learned
networks, HR-NAS adopts a multi-branch architecture that provides convolutional
encoding of multiple feature resolutions, inspired by HRNet. Last, we proposed
an efficient fine-grained search strategy to train HR-NAS, which effectively
explores the search space, and finds optimal architectures given various tasks
and computation resources. HR-NAS is capable of achieving state-of-the-art
trade-offs between performance and FLOPs for three dense prediction tasks and
an image classification task, given only small computational budgets. For
example, HR-NAS surpasses SqueezeNAS that is specially designed for semantic
segmentation while improving efficiency by 45.9%. Code is available at
https://github.com/dingmyu/HR-NAS
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06561" title="Abstract">arXiv:2106.06561</a> [<a href="/pdf/2106.06561" title="Download PDF">pdf</a>, <a href="/format/2106.06561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GANs N&#x27; Roses: Stable, Controllable, Diverse Image to Image Translation  (works for videos too!)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chong%2C+M+J">Min Jin Chong</a>, 
<a href="/search/cs?searchtype=author&query=Forsyth%2C+D">David Forsyth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> code is here <a href="https://github.com/mchong6/GANsNRoses">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We show how to learn a map that takes a content code, derived from a face
image, and a randomly chosen style code to an anime image. We derive an
adversarial loss from our simple and effective definitions of style and
content. This adversarial loss guarantees the map is diverse -- a very wide
range of anime can be produced from a single content code. Under plausible
assumptions, the map is not just diverse, but also correctly represents the
probability of an anime, conditioned on an input face. In contrast, current
multimodal generation procedures cannot capture the complex styles that appear
in anime. Extensive quantitative experiments support the idea the map is
correct. Extensive qualitative results show that the method can generate a much
more diverse range of styles than SOTA comparisons. Finally, we show that our
formalization of content and style allows us to perform video to video
translation without ever training on videos.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06566" title="Abstract">arXiv:2106.06566</a> [<a href="/pdf/2106.06566" title="Download PDF">pdf</a>, <a href="/format/2106.06566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-efficient Linguistic Generalizations through Program Synthesis:  Experiments with Phonology Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaduguru%2C+S">Saujas Vaduguru</a>, 
<a href="/search/cs?searchtype=author&query=Sathe%2C+A">Aalok Sathe</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+M">Monojit Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+D+M">Dipti Misra Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGMORPHON 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Neural models excel at extracting statistical patterns from large amounts of
data, but struggle to learn patterns or reason about language from only a few
examples. In this paper, we ask: Can we learn explicit rules that generalize
well from only a few examples? We explore this question using program
synthesis. We develop a synthesis model to learn phonology rules as programs in
a domain-specific language. We test the ability of our models to generalize
from few training examples using our new dataset of problems from the
Linguistics Olympiad, a challenging set of tasks that require strong linguistic
reasoning ability. In addition to being highly sample-efficient, our approach
generates human-readable programs, and allows control over the generalizability
of the learnt programs.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06575" title="Abstract">arXiv:2106.06575</a> [<a href="/pdf/2106.06575" title="Download PDF">pdf</a>, <a href="/format/2106.06575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-NBA: Efficient and Effective Search Over the Joint Space of  Networks, Bitwidths, and Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yonggan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cox%2C+D">David Cox</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yingyan Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While maximizing deep neural networks' (DNNs') acceleration efficiency
requires a joint search/design of three different yet highly coupled aspects,
including the networks, bitwidths, and accelerators, the challenges associated
with such a joint search have not yet been fully understood and addressed. The
key challenges include (1) the dilemma of whether to explode the memory
consumption due to the huge joint space or achieve sub-optimal designs, (2) the
discrete nature of the accelerator design space that is coupled yet different
from that of the networks and bitwidths, and (3) the chicken and egg problem
associated with network-accelerator co-search, i.e., co-search requires
operation-wise hardware cost, which is lacking during search as the optimal
accelerator depending on the whole network is still unknown during search. To
tackle these daunting challenges towards optimal and fast development of DNN
accelerators, we propose a framework dubbed Auto-NBA to enable jointly
searching for the Networks, Bitwidths, and Accelerators, by efficiently
localizing the optimal design within the huge joint design space for each
target dataset and acceleration specification. Our Auto-NBA integrates a
heterogeneous sampling strategy to achieve unbiased search with constant memory
consumption, and a novel joint-search pipeline equipped with a generic
differentiable accelerator search engine. Extensive experiments and ablation
studies validate that both Auto-NBA generated networks and accelerators
consistently outperform state-of-the-art designs (including
co-search/exploration techniques, hardware-aware NAS methods, and DNN
accelerators), in terms of search time, task accuracy, and accelerator
efficiency. Our codes are available at: https://github.com/RICE-EIC/Auto-NBA.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06577" title="Abstract">arXiv:2106.06577</a> [<a href="/pdf/2106.06577" title="Download PDF">pdf</a>, <a href="/format/2106.06577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A3C-S: Automated Agent Accelerator Co-Search towards Efficient Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yonggan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chaojian Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhongzhi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yingyan Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at DAC 2021. arXiv admin note: text overlap with <a href="/abs/2012.13091">arXiv:2012.13091</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Driven by the explosive interest in applying deep reinforcement learning
(DRL) agents to numerous real-time control and decision-making applications,
there has been a growing demand to deploy DRL agents to empower daily-life
intelligent devices, while the prohibitive complexity of DRL stands at odds
with limited on-device resources. In this work, we propose an Automated Agent
Accelerator Co-Search (A3C-S) framework, which to our best knowledge is the
first to automatically co-search the optimally matched DRL agents and
accelerators that maximize both test scores and hardware efficiency. Extensive
experiments consistently validate the superiority of our A3C-S over
state-of-the-art techniques.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06579" title="Abstract">arXiv:2106.06579</a> [<a href="/pdf/2106.06579" title="Download PDF">pdf</a>, <a href="/format/2106.06579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venkatesha%2C+Y">Yeshwanth Venkatesha</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youngeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Tassiulas%2C+L">Leandros Tassiulas</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+P">Priyadarshini Panda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">As neural networks get widespread adoption in resource-constrained embedded
devices, there is a growing need for low-power neural systems. Spiking Neural
Networks (SNNs)are emerging to be an energy-efficient alternative to the
traditional Artificial Neural Networks (ANNs) which are known to be
computationally intensive. From an application perspective, as federated
learning involves multiple energy-constrained devices, there is a huge scope to
leverage energy efficiency provided by SNNs. Despite its importance, there has
been little attention on training SNNs on a large-scale distributed system like
federated learning. In this paper, we bring SNNs to a more realistic federated
learning scenario. Specifically, we propose a federated learning framework for
decentralized and privacy-preserving training of SNNs. To validate the proposed
federated learning framework, we experimentally evaluate the advantages of SNNs
on various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks.
We observe that SNNs outperform ANNs in terms of overall accuracy by over 15%
when the data is distributed across a large number of clients in the federation
while providing up to5.3x energy efficiency. In addition to efficiency, we also
analyze the sensitivity of the proposed federated SNN framework to data
distribution among the clients, stragglers, and gradient noise and perform a
comprehensive comparison with ANNs.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06580" title="Abstract">arXiv:2106.06580</a> [<a href="/pdf/2106.06580" title="Download PDF">pdf</a>, <a href="/format/2106.06580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revealing the canalizing structure of Boolean functions: Algorithms and  applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dimitrova%2C+E">Elena Dimitrova</a>, 
<a href="/search/cs?searchtype=author&query=Stigler%2C+B">Brandilyn Stigler</a>, 
<a href="/search/cs?searchtype=author&query=Kadelka%2C+C">Claus Kadelka</a>, 
<a href="/search/cs?searchtype=author&query=Murrugarra%2C+D">David Murrugarra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Boolean functions can be represented in many ways including logical forms,
truth tables, and polynomials. Additionally, Boolean functions have different
canonical representations such as minimal disjunctive normal forms. Other
canonical representation is based on the polynomial representation of Boolean
functions where they can be written as a nested product of canalizing layers
and a polynomial that contains the noncanalizing variables. In this paper we
study the problem of identifying the canalizing layers format of Boolean
functions. First, we show that the problem of finding the canalizing layers is
NP-hard. Second, we present several algorithms for finding the canalizing
layers of a Boolean function, discuss their complexities, and compare their
performances. Third, we show applications where the computation of canalizing
layers can be used for finding a disjunctive normal form of a nested canalizing
function. Another application deals with the reverse engineering of Boolean
networks with a prescribed layering format. Finally, implementations of our
algorithms in Python and in the computer algebra system Macaulay2 are available
at https://github.com/ckadelka/BooleanCanalization.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06583" title="Abstract">arXiv:2106.06583</a> [<a href="/pdf/2106.06583" title="Download PDF">pdf</a>, <a href="/format/2106.06583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deception Detection and Remote Physiological Monitoring: A Dataset and  Baseline Experimental Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Speth%2C+J">Jeremy Speth</a>, 
<a href="/search/cs?searchtype=author&query=Vance%2C+N">Nathan Vance</a>, 
<a href="/search/cs?searchtype=author&query=Czajka%2C+A">Adam Czajka</a>, 
<a href="/search/cs?searchtype=author&query=Bowyer%2C+K+W">Kevin W. Bowyer</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+D">Diane Wright</a>, 
<a href="/search/cs?searchtype=author&query=Flynn%2C+P">Patrick Flynn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The dataset will be available for download at <a href="https://cvrl.nd.edu/projects/data/#deception-detection-and-physiological-monitoringddpm">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present the Deception Detection and Physiological Monitoring (DDPM)
dataset and initial baseline results on this dataset. Our application context
is an interview scenario in which the interviewee attempts to deceive the
interviewer on selected responses. The interviewee is recorded in RGB,
near-infrared, and long-wave infrared, along with cardiac pulse, blood
oxygenation, and audio. After collection, data were annotated for
interviewer/interviewee, curated, ground-truthed, and organized into train /
test parts for a set of canonical deception detection experiments. Baseline
experiments found random accuracy for micro-expressions as an indicator of
deception, but that saccades can give a statistically significant response. We
also estimated subject heart rates from face videos (remotely) with a mean
absolute error as low as 3.16 bpm. The database contains almost 13 hours of
recordings of 70 subjects, and over 8 million visible-light, near-infrared, and
thermal video frames, along with appropriate meta, audio and pulse oximeter
data. To our knowledge, this is the only collection offering recordings of five
modalities in an interview scenario that can be used in both deception
detection and remote photoplethysmography research.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06585" title="Abstract">arXiv:2106.06585</a> [<a href="/pdf/2106.06585" title="Download PDF">pdf</a>, <a href="/format/2106.06585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the numerical accuracy in finite-volume methods to accurately capture  turbulence in compressible flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Motheau%2C+E">Emmanuel Motheau</a>, 
<a href="/search/math?searchtype=author&query=Wakefield%2C+J">John Wakefield</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1902.06665">arXiv:1902.06665</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The goal of the present paper is to understand the impact of numerical
schemes for the reconstruction of data at cell faces in finite-volume methods,
and to assess their interaction with the quadrature rule used to compute the
average over the cell volume. Here, third-, fifth- and seventh-order WENO-Z
schemes are investigated. On a problem with a smooth solution, the theoretical
order of convergence rate for each method is retrieved, and changing the order
of the reconstruction at cell faces does not impact the results, whereas for a
shock-driven problem all the methods collapse to first-order. Study of the
decay of compressible homogeneous isotropic turbulence reveals that using a
high-order quadrature rule to compute the average over a finite volume cell
does not improve the spectral accuracy and that all methods present a
second-order convergence rate. However the choice of the numerical method to
reconstruct data at cell faces is found to be critical to correctly capture
turbulent spectra. In the context of simulations with finite-volume methods of
practical flows encountered in engineering applications, it becomes apparent
that an efficient strategy is to perform the average integration with a
low-order quadrature rule on a fine mesh resolution, whereas high-order schemes
should be used to reconstruct data at cell faces.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06586" title="Abstract">arXiv:2106.06586</a> [<a href="/pdf/2106.06586" title="Download PDF">pdf</a>, <a href="/format/2106.06586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Limit of Graph Neural Networks by Improving the  Assortativity of Graphs with Local Mixing Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suresh%2C+S">Susheel Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Budde%2C+V">Vinith Budde</a>, 
<a href="/search/cs?searchtype=author&query=Neville%2C+J">Jennifer Neville</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianzhu Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published in KDD 2021; 11 pages;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graph neural networks (GNNs) have achieved tremendous success on multiple
graph-based learning tasks by fusing network structure and node features.
Modern GNN models are built upon iterative aggregation of neighbor's/proximity
features by message passing. Its prediction performance has been shown to be
strongly bounded by assortative mixing in the graph, a key property wherein
nodes with similar attributes mix/connect with each other. We observe that real
world networks exhibit heterogeneous or diverse mixing patterns and the
conventional global measurement of assortativity, such as global assortativity
coefficient, may not be a representative statistic in quantifying this mixing.
We adopt a generalized concept, node-level assortativity, one that is based at
the node level to better represent the diverse patterns and accurately quantify
the learnability of GNNs. We find that the prediction performance of a wide
range of GNN models is highly correlated with the node level assortativity. To
break this limit, in this work, we focus on transforming the input graph into a
computation graph which contains both proximity and structural information as
distinct type of edges. The resulted multi-relational graph has an enhanced
level of assortativity and, more importantly, preserves rich information from
the original graph. We then propose to run GNNs on this computation graph and
show that adaptively choosing between structure and proximity leads to improved
performance under diverse mixing. Empirically, we show the benefits of adopting
our transformation framework for semi-supervised node classification task on a
variety of real world graph learning benchmarks.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06588" title="Abstract">arXiv:2106.06588</a> [<a href="/pdf/2106.06588" title="Download PDF">pdf</a>, <a href="/format/2106.06588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visualization Techniques to Enhance Automated Event Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henn%2C+S">Sophia Henn</a>, 
<a href="/search/cs?searchtype=author&query=Sticha%2C+A">Abigail Sticha</a>, 
<a href="/search/cs?searchtype=author&query=Burley%2C+T">Timothy Burley</a>, 
<a href="/search/cs?searchtype=author&query=Verdeja%2C+E">Ernesto Verdeja</a>, 
<a href="/search/cs?searchtype=author&query=Brenner%2C+P">Paul Brenner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Robust visualization of complex data is critical for the effective use of NLP
for event classification, as the volume of data is large and the
high-dimensional structure of text makes data challenging to summarize
succinctly. In event extraction tasks in particular, visualization can aid in
understanding and illustrating the textual relationships from which machine
learning tools produce insights. Through our case study which seeks to identify
potential triggers of state-led mass killings from news articles using NLP, we
demonstrate how visualizations can aid in each stage, from exploratory analysis
of raw data, to machine learning training analysis, and finally post-inference
validation.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06592" title="Abstract">arXiv:2106.06592</a> [<a href="/pdf/2106.06592" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dise&#xf1;o y desarrollo de aplicaci&#xf3;n m&#xf3;vil para la clasificaci&#xf3;n de  flora nativa chilena utilizando redes neuronales convolucionales
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz%2C+I">Ignacio Mu&#xf1;oz</a>, 
<a href="/search/cs?searchtype=author&query=Bolt%2C+A">Alfredo Bolt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in Spanish
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Introduction: Mobile apps, through artificial vision, are capable of
recognizing vegetable species in real time. However, the existing species
recognition apps do not take in consideration the wide variety of endemic and
native (Chilean) species, which leads to wrong species predictions. This study
introduces the development of a chilean species dataset and an optimized
classification model implemented to a mobile app. Method: the data set was
built by putting together pictures of several species captured on the field and
by selecting some pictures available from other datasets available online.
Convolutional neural networks were used in order to develop the images
prediction models. The networks were trained by performing a sensitivity
analysis, validating with k-fold cross validation and performing tests with
different hyper-parameters, optimizers, convolutional layers, and learning
rates in order to identify and choose the best models and then put them
together in one classification model. Results: The final data set was
compounded by 46 species, including native species, endemic and exotic from
Chile, with 6120 training pictures and 655 testing pictures. The best models
were implemented on a mobile app, obtaining a 95% correct prediction rate with
respect to the set of tests. Conclusion: The app developed in this study is
capable of classifying species with a high level of accuracy, depending on the
state of the art of the artificial vision and it can also show relevant
information related to the classified species.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06593" title="Abstract">arXiv:2106.06593</a> [<a href="/pdf/2106.06593" title="Download PDF">pdf</a>, <a href="/format/2106.06593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Accurate and Realistic Outfits Visualization with Attention to  Details
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kedan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chong%2C+M+j">Min jin Chong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jeffrey Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingen Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR2021. Live demo here <a href="https://revery.ai/demo.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Virtual try-on methods aim to generate images of fashion models wearing
arbitrary combinations of garments. This is a challenging task because the
generated image must appear realistic and accurately display the interaction
between garments. Prior works produce images that are filled with artifacts and
fail to capture important visual details necessary for commercial applications.
We propose Outfit Visualization Net (OVNet) to capture these important details
(e.g. buttons, shading, textures, realistic hemlines, and interactions between
garments) and produce high quality multiple-garment virtual try-on images.
OVNet consists of 1) a semantic layout generator and 2) an image generation
pipeline using multiple coordinated warps. We train the warper to output
multiple warps using a cascade loss, which refines each successive warp to
focus on poorly generated regions of a previous warp and yields consistent
improvements in detail. In addition, we introduce a method for matching outfits
with the most suitable model and produce significant improvements for both our
and other previous try-on methods. Through quantitative and qualitative
analysis, we demonstrate our method generates substantially higher-quality
studio images compared to prior works for multi-garment outfits. An interactive
interface powered by this method has been deployed on fashion e-commerce
websites and received overwhelmingly positive feedback.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06595" title="Abstract">arXiv:2106.06595</a> [<a href="/pdf/2106.06595" title="Download PDF">pdf</a>, <a href="/format/2106.06595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bucket-brigade inspired power line network protocol for sensed quantity  profile acquisition with smart sensors deployed as a queue in harsh  environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+E+J+P">Edval J. P. Santos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 13 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Pressure and temperature profile are key data for safe production in oil and
gas wells. In this paper, a bucket-brigade inspired sensor network protocol is
proposed which can be used to extract sensed data profile from the nanoscale up
to kilometer long structures. The PHY/MAC layers are discussed. This protocol
is best suited for low data rate exchanges in small fixed-size packets, named
buckets, transmitted as time-domain bursts among high-precision smart sensors
deployed as a queue. There is only one coordinator, which is not directly
accessible by most of the sensor nodes. The coordinator is responsible for
collecting the measurement profile and send it to a supervisory node. There is
no need for complex routing mechanism, as the network topology is determined
during deployment. There are many applications which require sensors to be
deployed as a long queue and sensed data could be transmitted at low data
rates. Examples of such monitoring applications are: neural connected
artificial skin, oil/gas/water pipeline integrity, power transmission line
tower integrity, (rail)road/highway lighting and integrity, individualized
monitoring in vineyard or re-foresting or plantation, underwater
telecommunications cable integrity, oil/gas riser integrity, oil/gas well
temperature and pressure profile, among others. For robustness and reduced
electromagnetic interference, wired network is preferred. Besides in some harsh
environment wireless is not feasible. To reduce wiring, communications can be
carried out in the same cable used to supply electrical power.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06596" title="Abstract">arXiv:2106.06596</a> [<a href="/pdf/2106.06596" title="Download PDF">pdf</a>, <a href="/format/2106.06596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling the Roles of Curation, Data-Augmentation and the Prior in  the Cold Posterior Effect
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noci%2C+L">Lorenzo Noci</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+K">Kevin Roth</a>, 
<a href="/search/cs?searchtype=author&query=Bachmann%2C+G">Gregor Bachmann</a>, 
<a href="/search/cs?searchtype=author&query=Nowozin%2C+S">Sebastian Nowozin</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The "cold posterior effect" (CPE) in Bayesian deep learning describes the
uncomforting observation that the predictive performance of Bayesian neural
networks can be significantly improved if the Bayes posterior is artificially
sharpened using a temperature parameter T&lt;1. The CPE is problematic in theory
and practice and since the effect was identified many researchers have proposed
hypotheses to explain the phenomenon. However, despite this intensive research
effort the effect remains poorly understood. In this work we provide novel and
nuanced evidence relevant to existing explanations for the cold posterior
effect, disentangling three hypotheses: 1. The dataset curation hypothesis of
Aitchison (2020): we show empirically that the CPE does not arise in a real
curated data set but can be produced in a controlled experiment with varying
curation strength. 2. The data augmentation hypothesis of Izmailov et al.
(2021) and Fortuin et al. (2021): we show empirically that data augmentation is
sufficient but not necessary for the CPE to be present. 3. The bad prior
hypothesis of Wenzel et al. (2020): we use a simple experiment evaluating the
relative importance of the prior and the likelihood, strongly linking the CPE
to the prior. Our results demonstrate how the CPE can arise in isolation from
synthetic curation, data augmentation, and bad priors. Cold posteriors observed
"in the wild" are therefore unlikely to arise from a single simple cause; as a
result, we do not expect a simple "fix" for cold posteriors.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06598" title="Abstract">arXiv:2106.06598</a> [<a href="/pdf/2106.06598" title="Download PDF">pdf</a>, <a href="/format/2106.06598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Pre-trained Language Model for Speech Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shon%2C+S">Suwon Shon</a>, 
<a href="/search/cs?searchtype=author&query=Brusco%2C+P">Pablo Brusco</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jing Pan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K+J">Kyu J. Han</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we explore the use of pre-trained language models to learn
sentiment information of written texts for speech sentiment analysis. First, we
investigate how useful a pre-trained language model would be in a 2-step
pipeline approach employing Automatic Speech Recognition (ASR) and
transcripts-based sentiment analysis separately. Second, we propose a pseudo
label-based semi-supervised training strategy using a language model on an
end-to-end speech sentiment approach to take advantage of a large, but
unlabeled speech dataset for training. Although spoken and written texts have
different linguistic characteristics, they can complement each other in
understanding sentiment. Therefore, the proposed system can not only model
acoustic characteristics to bear sentiment-specific information in speech
signals, but learn latent information to carry sentiments in the text
representation. In these experiments, we demonstrate the proposed approaches
improve F1 scores consistently compared to systems without a language model.
Moreover, we also show that the proposed framework can reduce 65% of human
supervision by leveraging a large amount of data without human sentiment
annotation and boost performance in a low-resource condition where the human
sentiment annotation is not available enough.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06600" title="Abstract">arXiv:2106.06600</a> [<a href="/pdf/2106.06600" title="Download PDF">pdf</a>, <a href="/format/2106.06600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Break-It-Fix-It: Unsupervised Learning for Program Repair
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yasunaga%2C+M">Michihiro Yasunaga</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021. Code &amp; data available at <a href="https://github.com/michiyasunaga/bifi">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Software Engineering (cs.SE)

</div>
<p class="mathjax">We consider repair tasks: given a critic (e.g., compiler) that assesses the
quality of an input, the goal is to train a fixer that converts a bad example
(e.g., code with syntax errors) into a good one (e.g., code with no errors).
Existing works create training data consisting of (bad, good) pairs by
corrupting good examples using heuristics (e.g., dropping tokens). However,
fixers trained on this synthetically-generated data do not extrapolate well to
the real distribution of bad inputs. To bridge this gap, we propose a new
training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use
the critic to check a fixer's output on real bad inputs and add good (fixed)
outputs to the training data, and (ii) we train a breaker to generate realistic
bad code from good code. Based on these ideas, we iteratively update the
breaker and the fixer while using them in conjunction to generate more paired
data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new
dataset we introduce where the goal is to repair Python code with AST parse
errors; and DeepFix, where the goal is to repair C code with compiler errors.
BIFI outperforms existing methods, obtaining 90.5% repair accuracy on
GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not
require any labeled data; we hope it will be a strong starting point for
unsupervised learning of various repair tasks.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06601" title="Abstract">arXiv:2106.06601</a> [<a href="/pdf/2106.06601" title="Download PDF">pdf</a>, <a href="/format/2106.06601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COSTA: Communication-Optimal Shuffle and Transpose Algorithm with  Process Relabeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kabi%C4%87%2C+M">Marko Kabi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Pintarelli%2C+S">Simon Pintarelli</a>, 
<a href="/search/cs?searchtype=author&query=Kozhevnikov%2C+A">Anton Kozhevnikov</a>, 
<a href="/search/cs?searchtype=author&query=VandeVondele%2C+J">Joost VandeVondele</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the proceedings of the 36th International Conference on High Performance Computing, ISC High Performance 2021. The implementation of the algorithm is available at: <a href="https://github.com/eth-cscs/COSTA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Communication-avoiding algorithms for Linear Algebra have become increasingly
popular, in particular for distributed memory architectures. In practice, these
algorithms assume that the data is already distributed in a specific way, thus
making data reshuffling a key to use them. For performance reasons, a
straightforward all-to-all exchange must be avoided.
<br />Here, we show that process relabeling (i.e. permuting processes in the final
layout) can be used to obtain communication optimality for data reshuffling,
and that it can be efficiently found by solving a Linear Assignment Problem
(Maximum Weight Bipartite Perfect Matching). Based on this, we have developed a
Communication-Optimal Shuffle and Transpose Algorithm (COSTA): this
highly-optimised algorithm implements $A=\alpha\cdot \operatorname{op}(B) +
\beta \cdot A,\ \operatorname{op} \in \{\operatorname{transpose},
\operatorname{conjugate-transpose}, \operatorname{identity}\}$ on distributed
systems, where $A, B$ are matrices with potentially different (distributed)
layouts and $\alpha, \beta$ are scalars. COSTA can take advantage of the
communication-optimal process relabeling even for heterogeneous network
topologies, where latency and bandwidth differ among nodes. The implementation
not only outperforms the best available ScaLAPACK redistribute and transpose
routines multiple times, but is also able to deal with more general matrix
layouts, in particular it is not limited to block-cyclic layouts. Finally, we
use COSTA to integrate a communication-optimal matrix multiplication algorithm
into the CP2K quantum chemistry simulation package. This way, we show that
COSTA can be used to unlock the full potential of recent Linear Algebra
algorithms in applications by facilitating interoperability between algorithms
with a wide range of data layouts, in addition to bringing significant
redistribution speedups.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06603" title="Abstract">arXiv:2106.06603</a> [<a href="/pdf/2106.06603" title="Download PDF">pdf</a>, <a href="/format/2106.06603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Shuffling Framework for Local Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meehan%2C+C">Casey Meehan</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+A+R">Amrita Roy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+K">Kamalika Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">ldp deployments are vulnerable to inference attacks as an adversary can link
the noisy responses to their identity and subsequently, auxiliary information
using the order of the data. An alternative model, shuffle DP, prevents this by
shuffling the noisy responses uniformly at random. However, this limits the
data learnability -- only symmetric functions (input order agnostic) can be
learned. In this paper, we strike a balance and propose a generalized shuffling
framework that interpolates between the two deployment models. We show that
systematic shuffling of the noisy responses can thwart specific inference
attacks while retaining some meaningful data learnability. To this end, we
propose a novel privacy guarantee, d-sigma privacy, that captures the privacy
of the order of a data sequence. d-sigma privacy allows tuning the granularity
at which the ordinal information is maintained, which formalizes the degree the
resistance to inference attacks trading it off with data learnability.
Additionally, we propose a novel shuffling mechanism that can achieve d-sigma
privacy and demonstrate the practicality of our mechanism via evaluation on
real-world datasets.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06604" title="Abstract">arXiv:2106.06604</a> [<a href="/pdf/2106.06604" title="Download PDF">pdf</a>, <a href="/format/2106.06604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verified Synthesis of Optimal Safety Controllers for Human-Robot  Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gleirscher%2C+M">Mario Gleirscher</a>, 
<a href="/search/cs?searchtype=author&query=Calinescu%2C+R">Radu Calinescu</a>, 
<a href="/search/cs?searchtype=author&query=Douthwaite%2C+J">James Douthwaite</a>, 
<a href="/search/cs?searchtype=author&query=Lesage%2C+B">Benjamin Lesage</a>, 
<a href="/search/cs?searchtype=author&query=Paterson%2C+C">Colin Paterson</a>, 
<a href="/search/cs?searchtype=author&query=Aitken%2C+J">Jonathan Aitken</a>, 
<a href="/search/cs?searchtype=author&query=Alexander%2C+R">Rob Alexander</a>, 
<a href="/search/cs?searchtype=author&query=Law%2C+J">James Law</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 31 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC); Software Engineering (cs.SE); Systems and Control (eess.SY)

</div>
<p class="mathjax">We present a tool-supported approach for the synthesis, verification and
validation of the control software responsible for the safety of the
human-robot interaction in manufacturing processes that use collaborative
robots. In human-robot collaboration, software-based safety controllers are
used to improve operational safety, e.g., by triggering shutdown mechanisms or
emergency stops to avoid accidents. Complex robotic tasks and increasingly
close human-robot interaction pose new challenges to controller developers and
certification authorities. Key among these challenges is the need to assure the
correctness of safety controllers under explicit (and preferably weak)
assumptions. Our controller synthesis, verification and validation approach is
informed by the process, risk analysis, and relevant safety regulations for the
target application. Controllers are selected from a design space of feasible
controllers according to a set of optimality criteria, are formally verified
against correctness criteria, and are translated into executable code and
validated in a digital twin. The resulting controller can detect the occurrence
of hazards, move the process into a safe state, and, in certain circumstances,
return the process to an operational state from which it can resume its
original task. We show the effectiveness of our software engineering approach
through a case study involving the development of a safety controller for a
manufacturing work cell equipped with a collaborative robot.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06605" title="Abstract">arXiv:2106.06605</a> [<a href="/pdf/2106.06605" title="Download PDF">pdf</a>, <a href="/format/2106.06605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Language Usage and Listener Engagement in Podcasts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Sravana Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Lazarova%2C+M">Marina Lazarova</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yongze Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+R">Rosie Jones</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While there is an abundance of popular writing targeted to podcast creators
on how to speak in ways that engage their listeners, there has been little
data-driven analysis of podcasts that relates linguistic style with listener
engagement. In this paper, we investigate how various factors -- vocabulary
diversity, distinctiveness, emotion, and syntax, among others -- correlate with
engagement, based on analysis of the creators' written descriptions and
transcripts of the audio. We build models with different textual
representations, and show that the identified features are highly predictive of
engagement. Our analysis tests popular wisdom about stylistic elements in
high-engagement podcasts, corroborating some aspects, and adding new
perspectives on others.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06607" title="Abstract">arXiv:2106.06607</a> [<a href="/pdf/2106.06607" title="Download PDF">pdf</a>, <a href="/format/2106.06607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariance Principle Meets Information Bottleneck for  Out-of-Distribution Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+K">Kartik Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Caballero%2C+E">Ethan Caballero</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Mitliagkas%2C+I">Ioannis Mitliagkas</a>, 
<a href="/search/cs?searchtype=author&query=Rish%2C+I">Irina Rish</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The invariance principle from causality is at the heart of notable approaches
such as invariant risk minimization (IRM) that seek to address
out-of-distribution (OOD) generalization failures. Despite the promising
theory, invariance principle-based approaches fail in common classification
tasks, where invariant (causal) features capture all the information about the
label. Are these failures due to the methods failing to capture the invariance?
Or is the invariance principle itself insufficient? To answer these questions,
we revisit the fundamental assumptions in linear regression tasks, where
invariance-based approaches were shown to provably generalize OOD. In contrast
to the linear regression tasks, we show that for linear classification tasks we
need much stronger restrictions on the distribution shifts, or otherwise OOD
generalization is impossible. Furthermore, even with appropriate restrictions
on distribution shifts in place, we show that the invariance principle alone is
insufficient. We prove that a form of the information bottleneck constraint
along with invariance helps address key failures when invariant features
capture all the information about the label and also retains the existing
success when they do not. We propose an approach that incorporates both of
these principles and demonstrate its effectiveness in several experiments.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06610" title="Abstract">arXiv:2106.06610</a> [<a href="/pdf/2106.06610" title="Download PDF">pdf</a>, <a href="/ps/2106.06610" title="Download PostScript">ps</a>, <a href="/format/2106.06610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalars are universal: Gauge-equivariant machine learning, structured  like classical physics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Villar%2C+S">Soledad Villar</a> (JHU), 
<a href="/search/cs?searchtype=author&query=Hogg%2C+D+W">David W.Hogg</a> (Flatiron, NYU), 
<a href="/search/cs?searchtype=author&query=Storey-Fisher%2C+K">Kate Storey-Fisher</a> (NYU), 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Weichi Yao</a> (NYU), 
<a href="/search/cs?searchtype=author&query=Blum-Smith%2C+B">Ben Blum-Smith</a> (NYU)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Mathematical Physics (math-ph); Machine Learning (stat.ML)

</div>
<p class="mathjax">There has been enormous progress in the last few years in designing
conceivable (though not always practical) neural networks that respect the
gauge symmetries -- or coordinate freedom -- of physical law. Some of these
frameworks make use of irreducible representations, some make use of higher
order tensor objects, and some apply symmetry-enforcing constraints. Different
physical laws obey different combinations of fundamental symmetries, but a
large fraction (possibly all) of classical physics is equivariant to
translation, rotation, reflection (parity), boost (relativity), and
permutations. Here we show that it is simple to parameterize universally
approximating polynomial functions that are equivariant under these symmetries,
or under the Euclidean, Lorentz, and Poincar\'e groups, at any dimensionality
$d$. The key observation is that nonlinear O($d$)-equivariant (and
related-group-equivariant) functions can be expressed in terms of a lightweight
collection of scalars -- scalar products and scalar contractions of the scalar,
vector, and tensor inputs. These results demonstrate theoretically that
gauge-invariant deep learning models for classical physics with good scaling
for large problems are feasible right now.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06613" title="Abstract">arXiv:2106.06613</a> [<a href="/pdf/2106.06613" title="Download PDF">pdf</a>, <a href="/format/2106.06613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Formalism, Method and Open Issues for Zero-Shot Coordination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Treutlein%2C+J">Johannes Treutlein</a>, 
<a href="/search/cs?searchtype=author&query=Dennis%2C+M">Michael Dennis</a>, 
<a href="/search/cs?searchtype=author&query=Oesterheld%2C+C">Caspar Oesterheld</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In many coordination problems, independently reasoning humans are able to
discover mutually compatible policies. In contrast, independently trained
self-play policies are often mutually incompatible. Zero-shot coordination
(ZSC) has recently been proposed as a new frontier in multi-agent reinforcement
learning to address this fundamental issue. Prior work approaches the ZSC
problem by assuming players can agree on a shared learning algorithm but not on
labels for actions and observations, and proposes other-play as an optimal
solution. However, until now, this "label-free" problem has only been
informally defined. We formalize this setting as the label-free coordination
(LFC) problem by defining the label-free coordination game. We show that
other-play is not an optimal solution to the LFC problem as it fails to
consistently break ties between incompatible maximizers of the other-play
objective. We introduce an extension of the algorithm, other-play with
tie-breaking, and prove that it is optimal in the LFC problem and an
equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the
ZSC setting aims to prevent, we conclude that the LFC problem does not reflect
the aims of ZSC. To address this, we introduce an alternative informal
operationalization of ZSC as a starting point for future work.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06614" title="Abstract">arXiv:2106.06614</a> [<a href="/pdf/2106.06614" title="Download PDF">pdf</a>, <a href="/format/2106.06614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nawrotzki&#x27;s Algorithm for the Countable Splitting Lemma, Constructively
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sokolova%2C+A">Ana Sokolova</a>, 
<a href="/search/cs?searchtype=author&query=Woracek%2C+H">Harald Woracek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Probability (math.PR)

</div>
<p class="mathjax">We reprove the countable splitting lemma by adapting Nawrotzki's algorithm
which produces a sequence that converges to a solution. Our algorithm combines
Nawrotzki's approach with taking finite cuts. It is constructive in the sense
that each term of the iteratively built approximating sequence as well as the
error between the approximants and the solution is computable with finitely
many algebraic operations.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06615" title="Abstract">arXiv:2106.06615</a> [<a href="/pdf/2106.06615" title="Download PDF">pdf</a>, <a href="/format/2106.06615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise characterization of the prior predictive distribution of deep  ReLU networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noci%2C+L">Lorenzo Noci</a>, 
<a href="/search/cs?searchtype=author&query=Bachmann%2C+G">Gregor Bachmann</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+K">Kevin Roth</a>, 
<a href="/search/cs?searchtype=author&query=Nowozin%2C+S">Sebastian Nowozin</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent works on Bayesian neural networks (BNNs) have highlighted the need to
better understand the implications of using Gaussian priors in combination with
the compositional structure of the network architecture. Similar in spirit to
the kind of analysis that has been developed to devise better initialization
schemes for neural networks (cf. He- or Xavier initialization), we derive a
precise characterization of the prior predictive distribution of finite-width
ReLU networks with Gaussian weights. While theoretical results have been
obtained for their heavy-tailedness, the full characterization of the prior
predictive distribution (i.e. its density, CDF and moments), remained unknown
prior to this work. Our analysis, based on the Meijer-G function, allows us to
quantify the influence of architectural choices such as the width or depth of
the network on the resulting shape of the prior predictive distribution. We
also formally connect our results to previous work in the infinite width
setting, demonstrating that the moments of the distribution converge to those
of a normal log-normal mixture in the infinite depth limit. Finally, our
results provide valuable guidance on prior design: for instance, controlling
the predictive variance with depth- and width-informed priors on the weights of
the network.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06616" title="Abstract">arXiv:2106.06616</a> [<a href="/pdf/2106.06616" title="Download PDF">pdf</a>, <a href="/format/2106.06616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Learning of Competitive Equilibria in Exchange Economies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenshuo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Kandasamy%2C+K">Kirthevasan Kandasamy</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+E">Joseph E Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Stoica%2C+I">Ion Stoica</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The sharing of scarce resources among multiple rational agents is one of the
classical problems in economics. In exchange economies, which are used to model
such situations, agents begin with an initial endowment of resources and
exchange them in a way that is mutually beneficial until they reach a
competitive equilibrium (CE). CE allocations are Pareto efficient and fair.
Consequently, they are used widely in designing mechanisms for fair division.
However, computing CEs requires the knowledge of agent preferences which are
unknown in several applications of interest. In this work, we explore a new
online learning mechanism, which, on each round, allocates resources to the
agents and collects stochastic feedback on their experience in using that
allocation. Its goal is to learn the agent utilities via this feedback and
imitate the allocations at a CE in the long run. We quantify CE behavior via
two losses and propose a randomized algorithm which achieves
$\bigOtilde(\sqrt{T})$ loss after $T$ rounds under both criteria. Empirically,
we demonstrate the effectiveness of this mechanism through numerical
simulations.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06617" title="Abstract">arXiv:2106.06617</a> [<a href="/pdf/2106.06617" title="Download PDF">pdf</a>, <a href="/format/2106.06617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inexact Loops in Robotics Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nelson%2C+E">Erik Nelson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Robotics: Science and Systems (RSS) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Loops are pervasive in robotics problems, appearing in mapping and
localization, where one is interested in finding loop closure constraints to
better approximate robot poses or other estimated quantities, as well as
planning and prediction, where one is interested in the homotopy classes of the
space through which a robot is moving. We generalize the standard topological
definition of a loop to cases where a trajectory passes close to itself, but
doesn't necessarily touch, giving a definition that is more practical for real
robotics problems. This relaxation leads to new and useful properties of
inexact loops, such as their ability to be partitioned into topologically
connected sets closely matching the concept of a "loop closure", and the
existence of simple and nonsimple loops. Building from these ideas, we
introduce several ways to measure properties and quantities of inexact loops on
a trajectory, such as the trajectory's "loop area" and "loop density", and use
them to compare strategies for sampling representative inexact loops to build
constraints in mapping and localization problems.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06620" title="Abstract">arXiv:2106.06620</a> [<a href="/pdf/2106.06620" title="Download PDF">pdf</a>, <a href="/format/2106.06620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Representation Learning via Perceptual Similarity Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taghanaki%2C+S+A">Saeid Asgari Taghanaki</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+K">Kristy Choi</a>, 
<a href="/search/cs?searchtype=author&query=Khasahmadi%2C+A">Amir Khasahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+A">Anirudh Goyal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">A fundamental challenge in artificial intelligence is learning useful
representations of data that yield good performance on a downstream task,
without overfitting to spurious input features. Extracting such task-relevant
predictive information is particularly difficult for real-world datasets. In
this work, we propose Contrastive Input Morphing (CIM), a representation
learning framework that learns input-space transformations of the data to
mitigate the effect of irrelevant input features on downstream performance. Our
method leverages a perceptual similarity metric via a triplet loss to ensure
that the transformation preserves task-relevant information.Empirically, we
demonstrate the efficacy of our approach on tasks which typically suffer from
the presence of spurious correlations: classification with nuisance
information, out-of-distribution generalization, and preservation of subgroup
accuracies. We additionally show that CIM is complementary to other mutual
information-based representation learning techniques, and demonstrate that it
improves the performance of variational information bottleneck (VIB) when used
together.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06621" title="Abstract">arXiv:2106.06621</a> [<a href="/pdf/2106.06621" title="Download PDF">pdf</a>, <a href="/format/2106.06621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Piecewise-constant Neural ODEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Greydanus%2C+S">Sam Greydanus</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Stefan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Fern%2C+A">Alan Fern</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures (not counting appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neural networks are a popular tool for modeling sequential data but they
generally do not treat time as a continuous variable. Neural ODEs represent an
important exception: they parameterize the time derivative of a hidden state
with a neural network and then integrate over arbitrary amounts of time. But
these parameterizations, which have arbitrary curvature, can be hard to
integrate and thus train and evaluate. In this paper, we propose making a
piecewise-constant approximation to Neural ODEs to mitigate these issues. Our
model can be integrated exactly via Euler integration and can generate
autoregressive samples in 3-20 times fewer steps than comparable RNN and
ODE-RNN models. We evaluate our model on several synthetic physics tasks and a
planning task inspired by the game of billiards. We find that it matches the
performance of baseline approaches while requiring less time to train and
evaluate.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06623" title="Abstract">arXiv:2106.06623</a> [<a href="/pdf/2106.06623" title="Download PDF">pdf</a>, <a href="/format/2106.06623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pay Attention with Focus: A Novel Learning Scheme for Classification of  Whole Slide Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalra%2C+S">Shivam Kalra</a>, 
<a href="/search/cs?searchtype=author&query=Adnan%2C+M">Mohammed Adnan</a>, 
<a href="/search/cs?searchtype=author&query=Hemati%2C+S">Sobhan Hemati</a>, 
<a href="/search/cs?searchtype=author&query=Dehkharghanian%2C+T">Taher Dehkharghanian</a>, 
<a href="/search/cs?searchtype=author&query=Rahnamayan%2C+S">Shahryar Rahnamayan</a>, 
<a href="/search/cs?searchtype=author&query=Tizhoosh%2C+H">Hamid Tizhoosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in MICCAI, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep learning methods such as convolutional neural networks (CNNs) are
difficult to directly utilize to analyze whole slide images (WSIs) due to the
large image dimensions. We overcome this limitation by proposing a novel
two-stage approach. First, we extract a set of representative patches (called
mosaic) from a WSI. Each patch of a mosaic is encoded to a feature vector using
a deep network. The feature extractor model is fine-tuned using hierarchical
target labels of WSIs, i.e., anatomic site and primary diagnosis. In the second
stage, a set of encoded patch-level features from a WSI is used to compute the
primary diagnosis probability through the proposed Pay Attention with Focus
scheme, an attention-weighted averaging of predicted probabilities for all
patches of a mosaic modulated by a trainable focal factor. Experimental results
show that the proposed model can be robust, and effective for the
classification of WSIs.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06624" title="Abstract">arXiv:2106.06624</a> [<a href="/pdf/2106.06624" title="Download PDF">pdf</a>, <a href="/format/2106.06624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxing Local Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leino%2C+K">Klas Leino</a>, 
<a href="/search/cs?searchtype=author&query=Fredrikson%2C+M">Matt Fredrikson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Certifiable local robustness, which rigorously precludes small-norm
adversarial examples, has received significant attention as a means of
addressing security concerns in deep learning. However, for some classification
problems, local robustness is not a natural objective, even in the presence of
adversaries; for example, if an image contains two classes of subjects, the
correct label for the image may be considered arbitrary between the two, and
thus enforcing strict separation between them is unnecessary. In this work, we
introduce two relaxed safety properties for classifiers that address this
observation: (1) relaxed top-k robustness, which serves as the analogue of
top-k accuracy; and (2) affinity robustness, which specifies which sets of
labels must be separated by a robustness margin, and which can be
$\epsilon$-close in $\ell_p$ space. We show how to construct models that can be
efficiently certified against each relaxed robustness property, and trained
with very little overhead relative to standard gradient descent. Finally, we
demonstrate experimentally that these relaxed variants of robustness are
well-suited to several significant classification problems, leading to lower
rejection rates and higher certified accuracies than can be obtained when
certifying "standard" local robustness.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06627" title="Abstract">arXiv:2106.06627</a> [<a href="/pdf/2106.06627" title="Download PDF">pdf</a>, <a href="/format/2106.06627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Less Centralized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+L">Li Chou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zichang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhuang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+A">Anshumali Shrivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">With the rapid growth in mobile computing, massive amounts of data and
computing resources are now located at the edge. To this end, Federated
learning (FL) is becoming a widely adopted distributed machine learning (ML)
paradigm, which aims to harness this expanding skewed data locally in order to
develop rich and informative models. In centralized FL, a collection of devices
collaboratively solve a ML task under the coordination of a central server.
However, existing FL frameworks make an over-simplistic assumption about
network connectivity and ignore the communication bandwidth of the different
links in the network. In this paper, we present and study a novel FL algorithm,
in which devices mostly collaborate with other devices in a pairwise manner.
Our nonparametric approach is able to exploit network topology to reduce
communication bottlenecks. We evaluate our approach on various FL benchmarks
and demonstrate that our method achieves 10X better communication efficiency
and around 8% increase in accuracy compared to the centralized approach.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06629" title="Abstract">arXiv:2106.06629</a> [<a href="/pdf/2106.06629" title="Download PDF">pdf</a>, <a href="/format/2106.06629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mirror3D: Depth Refinement for Mirror Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jiaqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weijie Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+A+X">Angel X. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Savva%2C+M">Manolis Savva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper presented at CVPR 2021. For code, data and pretrained models, see <a href="https://3dlg-hcvc.github.io/mirror3d/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite recent progress in depth sensing and 3D reconstruction, mirror
surfaces are a significant source of errors. To address this problem, we create
the Mirror3D dataset: a 3D mirror plane dataset based on three RGBD datasets
(Matterport3D, NYUv2 and ScanNet) containing 7,011 mirror instance masks and 3D
planes. We then develop Mirror3DNet: a module that refines raw sensor depth or
estimated depth to correct errors on mirror surfaces. Our key idea is to
estimate the 3D mirror plane based on RGB input and surrounding depth context,
and use this estimate to directly regress mirror surface depth. Our experiments
show that Mirror3DNet significantly mitigates errors from a variety of input
depth data, including raw sensor depth and depth estimation or completion
methods.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06630" title="Abstract">arXiv:2106.06630</a> [<a href="/pdf/2106.06630" title="Download PDF">pdf</a>, <a href="/format/2106.06630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corruption-Robust Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuezhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiding Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jerry Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the adversarial robustness in offline reinforcement learning. Given
a batch dataset consisting of tuples $(s, a, r, s')$, an adversary is allowed
to arbitrarily modify $\epsilon$ fraction of the tuples. From the corrupted
dataset the learner aims to robustly identify a near-optimal policy. We first
show that a worst-case $\Omega(d\epsilon)$ optimality gap is unavoidable in
linear MDP of dimension $d$, even if the adversary only corrupts the reward
element in a tuple. This contrasts with dimension-free results in robust
supervised learning and best-known lower-bound in the online RL setting with
corruption. Next, we propose robust variants of the Least-Square Value
Iteration (LSVI) algorithm utilizing robust supervised learning oracles, which
achieve near-matching performances in cases both with and without full data
coverage. The algorithm requires the knowledge of $\epsilon$ to design the
pessimism bonus in the no-coverage case. Surprisingly, in this case, the
knowledge of $\epsilon$ is necessary, as we show that being adaptive to unknown
$\epsilon$ is impossible.This again contrasts with recent results on
corruption-robust online RL and implies that robust offline RL is a strictly
harder problem.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06631" title="Abstract">arXiv:2106.06631</a> [<a href="/pdf/2106.06631" title="Download PDF">pdf</a>, <a href="/format/2106.06631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Counterfactual Explanations in Tree Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parmentier%2C+A">Axel Parmentier</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+T">Thibaut Vidal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the Proceedings of the 38th International Conference on Machine Learning, PMLR 139, 2021. Open source code available at <a href="https://github.com/vidalt/OCEAN">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Counterfactual explanations are usually generated through heuristics that are
sensitive to the search's initial conditions. The absence of guarantees of
performance and robustness hinders trustworthiness. In this paper, we take a
disciplined approach towards counterfactual explanations for tree ensembles. We
advocate for a model-based search aiming at "optimal" explanations and propose
efficient mixed-integer programming approaches. We show that isolation forests
can be modeled within our framework to focus the search on plausible
explanations with a low outlier score. We provide comprehensive coverage of
additional constraints that model important objectives, heterogeneous data
types, structural constraints on the feature space, along with resource and
actionability restrictions. Our experimental analyses demonstrate that the
proposed search approach requires a computational effort that is orders of
magnitude smaller than previous mathematical programming algorithms. It scales
up to large data sets and tree ensembles, where it provides, within seconds,
systematic explanations grounded on well-defined models solved to optimality.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06633" title="Abstract">arXiv:2106.06633</a> [<a href="/pdf/2106.06633" title="Download PDF">pdf</a>, <a href="/ps/2106.06633" title="Download PostScript">ps</a>, <a href="/format/2106.06633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on confluence in typed probabilistic lambda calculi
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Romero%2C+R">Rafael Romero</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz-Caro%2C+A">Alejandro D&#xed;az-Caro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at LSFA 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">On the topic of probabilistic rewriting, there are several works studying
both termination and confluence of different systems. While working with a
lambda calculus modelling quantum computation, we found a system with
probabilistic rewriting rules and strongly normalizing terms. We examine the
effect of small modifications in probabilistic rewriting, affine variables, and
strategies on the overall confluence in this strongly normalizing probabilistic
calculus.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06635" title="Abstract">arXiv:2106.06635</a> [<a href="/pdf/2106.06635" title="Download PDF">pdf</a>, <a href="/ps/2106.06635" title="Download PostScript">ps</a>, <a href="/format/2106.06635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On D2D Caching with Uncoded Cache Placement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yapar%2C+%C3%87">&#xc7;a&#x11f;kan Yapar</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Schaefer%2C+R+F">Rafael F. Schaefer</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was presented in the IEEE International Symposium on Information Theory (ISIT 2019) in Paris, France. Longer version available at <a href="/abs/1901.05921">arXiv:1901.05921</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We consider a cache-aided wireless device-to-device (D2D) network under the
constraint of one-shot delivery, where the placement phase is orchestrated by a
central server. We assume that the devices' caches are filled with uncoded
data, and the whole file database at the server is made available in the
collection of caches. Following this phase, the files requested by the users
are serviced by inter-device multicast communication. For such a system
setting, we provide the exact characterization of load-memory trade-off, by
deriving both the minimum average and the minimum peak sum-loads of links
between devices, for a given individual memory size at disposal of each user.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06636" title="Abstract">arXiv:2106.06636</a> [<a href="/pdf/2106.06636" title="Download PDF">pdf</a>, <a href="/format/2106.06636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized  Streaming ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junkun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Mingbo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Renjie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liang Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by Findings of ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Simultaneous speech-to-text translation is widely useful in many scenarios.
The conventional cascaded approach uses a pipeline of streaming ASR followed by
simultaneous MT, but suffers from error propagation and extra latency. To
alleviate these issues, recent efforts attempt to directly translate the source
speech into target text simultaneously, but this is much harder due to the
combination of two separate tasks. We instead propose a new paradigm with the
advantages of both cascaded and end-to-end approaches. The key idea is to use
two separate, but synchronized, decoders on streaming ASR and direct
speech-to-text translation (ST), respectively, and the intermediate results of
ASR guide the decoding policy of (but is not fed as input to) ST. During
training time, we use multitask learning to jointly learn these two tasks with
a shared encoder. En-to-De and En-to-Es experiments on the MuSTC dataset
demonstrate that our proposed technique achieves substantially better
translation quality at similar levels of latency.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06637" title="Abstract">arXiv:2106.06637</a> [<a href="/pdf/2106.06637" title="Download PDF">pdf</a>, <a href="/format/2106.06637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAR-Net: Unsupervised Co-Attention Guided Registration Network for Joint  Registration and Structure Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+N">Nishant Ravikumar</a>, 
<a href="/search/cs?searchtype=author&query=Frangi%2C+A+F">Alejandro F Frangi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image registration is a fundamental building block for various applications
in medical image analysis. To better explore the correlation between the fixed
and moving images and improve registration performance, we propose a novel deep
learning network, Co-Attention guided Registration Network (CAR-Net). CAR-Net
employs a co-attention block to learn a new representation of the inputs, which
drives the registration of the fixed and moving images. Experiments on UK
Biobank cardiac cine-magnetic resonance image data demonstrate that CAR-Net
obtains higher registration accuracy and smoother deformation fields than
state-of-the-art unsupervised registration methods, while achieving comparable
or better registration performance than corresponding weakly-supervised
variants. In addition, our approach can provide critical structural information
of the input fixed and moving images simultaneously in a completely
unsupervised manner.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06639" title="Abstract">arXiv:2106.06639</a> [<a href="/pdf/2106.06639" title="Download PDF">pdf</a>, <a href="/format/2106.06639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with Buffered Asynchronous Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+J">John Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+K">Kshitiz Malik</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Hongyuan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Yousefpour%2C+A">Ashkan Yousefpour</a>, 
<a href="/search/cs?searchtype=author&query=Rabbat%2C+M">Michael Rabbat</a>, 
<a href="/search/cs?searchtype=author&query=Esmaeili%2C+M+M">Mani Malek Esmaeili</a>, 
<a href="/search/cs?searchtype=author&query=Huba%2C+D">Dzmitry Huba</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated Learning (FL) trains a shared model across distributed devices
while keeping the training data on the devices. Most FL schemes are
synchronous: they perform a synchronized aggregation of model updates from
individual devices. Synchronous training can be slow because of late-arriving
devices (stragglers). On the other hand, completely asynchronous training makes
FL less private because of incompatibility with secure aggregation. In this
work, we propose a model aggregation scheme, FedBuff, that combines the best
properties of synchronous and asynchronous FL. Similar to synchronous FL,
FedBuff is compatible with secure aggregation. Similar to asynchronous FL,
FedBuff is robust to stragglers. In FedBuff, clients trains asynchronously and
send updates to the server. The server aggregates client updates in a private
buffer until updates have been received, at which point a server model update
is immediately performed. We provide theoretical convergence guarantees for
FedBuff in a non-convex setting. Empirically, FedBuff converges up to 3.8x
faster than previous proposals for synchronous FL (e.g., FedAvgM), and up to
2.5x faster than previous proposals for asynchronous FL (e.g., FedAsync). We
show that FedBuff is robust to different staleness distributions and is more
scalable than synchronous FL techniques.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06641" title="Abstract">arXiv:2106.06641</a> [<a href="/pdf/2106.06641" title="Download PDF">pdf</a>, <a href="/format/2106.06641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conservative Integrators for Many-body Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wan%2C+A+T+S">Andy T. S. Wan</a>, 
<a href="/search/math?searchtype=author&query=Bihlo%2C+A">Alexander Bihlo</a>, 
<a href="/search/math?searchtype=author&query=Nave%2C+J">Jean-Christophe Nave</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Conservative symmetric second-order one-step schemes are derived for
dynamical systems describing various many-body systems using the Discrete
Multiplier Method. This includes conservative schemes for the $n$-species
Lotka-Volterra system, the $n$-body problem with radially symmetric potential
and the $n$-point vortex models in the plane and on the sphere. In particular,
we recover Greenspan-Labudde's conservative schemes for the $n$-body problem.
Numerical experiments are shown verifying the conservative property of the
schemes and second-order accuracy.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06646" title="Abstract">arXiv:2106.06646</a> [<a href="/pdf/2106.06646" title="Download PDF">pdf</a>, <a href="/format/2106.06646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatially Scalable Lossy Coded Caching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bayat%2C+M">Mozhgan Bayat</a>, 
<a href="/search/cs?searchtype=author&query=Yapar%2C+%C3%87">&#xc7;a&#x11f;kan Yapar</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was presented in the IEEE International Symposium on Wireless Communication Systems (ISWCS 2018) in Lisbon, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI); Systems and Control (eess.SY)

</div>
<p class="mathjax">We apply the coded caching scheme proposed by Maddah-Ali and Niesen to a
multipoint multicasting video paradigm. Partially caching the video files on
the wireless devices provides an opportunity to decrease data traffic load in
peak hours via sending multicast coded messages to users. In this paper, we
propose a two-hop wireless network for video multicasting, where the common
coded multicast message is transmitted through different single antenna Edge
Nodes (ENs) to multiple antenna users. Each user can decide to decode any EN by
using a zero forcing receiver. Motivated by Scalable Video Coding (SVC), we
consider successive refinement source coding in order to provide a ``softer''
tradeoff between the number of decoded ENs and the source distortion at each
user receiver. The resulting coding scheme can be seen as the concatenation of
Maddah-Ali and Niesen coded caching for each source-coded layer, and multiple
description coding. Using stochastic geometry, we investigate the tradeoff
between delivery time and per-user average source distortion. The proposed
system is spatially scalable in the sense that, for given users' and ENs'
spatial density, the achieved distortion-delivery time performance is
independent of the coverage area (for in the limit of large area).
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06649" title="Abstract">arXiv:2106.06649</a> [<a href="/pdf/2106.06649" title="Download PDF">pdf</a>, <a href="/format/2106.06649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 1st Place Solution for YouTubeVOS Challenge 2021:Video Instance  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+C">Thuy C. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+T+N">Tuan N. Tang</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+N+L">Nam LH. Phan</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C+H">Chuong H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Yamazaki%2C+M">Masayuki Yamazaki</a>, 
<a href="/search/cs?searchtype=author&query=Yamanaka%2C+M">Masao Yamanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CPVR 2021 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video Instance Segmentation (VIS) is a multi-task problem performing
detection, segmentation, and tracking simultaneously. Extended from image set
applications, video data additionally induces the temporal information, which,
if handled appropriately, is very useful to identify and predict object
motions. In this work, we design a unified model to mutually learn these tasks.
Specifically, we propose two modules, named Temporally Correlated Instance
Segmentation (TCIS) and Bidirectional Tracking (BiTrack), to take the benefit
of the temporal correlation between the object's instance masks across adjacent
frames. On the other hand, video data is often redundant due to the frame's
overlap. Our analysis shows that this problem is particularly severe for the
YoutubeVOS-VIS2021 data. Therefore, we propose a Multi-Source Data (MSD)
training mechanism to compensate for the data deficiency. By combining these
techniques with a bag of tricks, the network performance is significantly
boosted compared to the baseline, and outperforms other methods by a
considerable margin on the YoutubeVOS-VIS 2019 and 2021 datasets.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06650" title="Abstract">arXiv:2106.06650</a> [<a href="/pdf/2106.06650" title="Download PDF">pdf</a>, <a href="/format/2106.06650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Scale Unsupervised Object Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vo%2C+H+V">Huy V. Vo</a>, 
<a href="/search/cs?searchtype=author&query=Sizikova%2C+E">Elena Sizikova</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+C">Cordelia Schmid</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+P">Patrick P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Ponce%2C+J">Jean Ponce</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing approaches to unsupervised object discovery (UOD) do not scale up to
large datasets without approximations which compromise their performance. We
propose a novel formulation of UOD as a ranking problem, amenable to the
arsenal of distributed methods available for eigenvalue problems and link
analysis. Extensive experiments with COCO and OpenImages demonstrate that, in
the single-object discovery setting where a single prominent object is sought
in each image, the proposed LOD (Large-scale Object Discovery) approach is on
par with, or better than the state of the art for medium-scale datasets (up to
120K images), and over 37% better than the only other algorithms capable of
scaling up to 1.7M images. In the multi-object discovery setting where multiple
objects are sought in each image, the proposed LOD is over 14% better in
average precision (AP) than all other methods for datasets ranging from 20K to
1.7M images.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06652" title="Abstract">arXiv:2106.06652</a> [<a href="/pdf/2106.06652" title="Download PDF">pdf</a>, <a href="/format/2106.06652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lessons learned from hyper-parameter tuning for microservice candidate  identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yedida%2C+R">Rahul Yedida</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Rahul Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Kalia%2C+A">Anup Kalia</a>, 
<a href="/search/cs?searchtype=author&query=Menzies%2C+T">Tim Menzies</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jin Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Vukovic%2C+M">Maja Vukovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ASE 2021 Industry Track, short paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">When optimizing software for the cloud, monolithic applications need to be
partitioned into many smaller *microservices*. While many tools have been
proposed for this task, we warn that the evaluation of those approaches has
been incomplete; e.g. minimal prior exploration of hyperparameter optimization.
Using a set of open source Java EE applications, we show here that (a) such
optimization can significantly improve microservice partitioning; and that (b)
an open issue for future work is how to find which optimizer works best for
different problems. To facilitate that future work, see
[https://github.com/yrahul3910/ase-tuned-mono2micro](https://github.com/yrahul3910/ase-tuned-mono2micro)
for a reproduction package for this research.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06654" title="Abstract">arXiv:2106.06654</a> [<a href="/pdf/2106.06654" title="Download PDF">pdf</a>, <a href="/format/2106.06654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disrupting Model Training with Adversarial Shortcuts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Evtimov%2C+I">Ivan Evtimov</a>, 
<a href="/search/cs?searchtype=author&query=Covert%2C+I">Ian Covert</a>, 
<a href="/search/cs?searchtype=author&query=Kusupati%2C+A">Aditya Kusupati</a>, 
<a href="/search/cs?searchtype=author&query=Kohno%2C+T">Tadayoshi Kohno</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">When data is publicly released for human consumption, it is unclear how to
prevent its unauthorized usage for machine learning purposes. Successful model
training may be preventable with carefully designed dataset modifications, and
we present a proof-of-concept approach for the image classification setting. We
propose methods based on the notion of adversarial shortcuts, which encourage
models to rely on non-robust signals rather than semantic features, and our
experiments demonstrate that these measures successfully prevent deep learning
models from achieving high accuracy on real, unmodified data examples.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06655" title="Abstract">arXiv:2106.06655</a> [<a href="/pdf/2106.06655" title="Download PDF">pdf</a>, <a href="/format/2106.06655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metrics for 3D Object Pointing and Manipulation in Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Triantafyllidis%2C+E">Eleftherios Triantafyllidis</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenbin Hu</a>, 
<a href="/search/cs?searchtype=author&query=McGreavy%2C+C">Christopher McGreavy</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhibin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and to appear at the IEEE Robotics &amp; Automation Magazine. The manuscript has 14 pages, 6 figures, 5 tables, 10 equations and 22 references in total
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Assessing the performance of human movements during teleoperation and virtual
reality is a challenging problem, particularly in 3D space due to complex
spatial settings. Despite the presence of a multitude of metrics, a compelling
standardized 3D metric is yet missing, aggravating inter-study comparability
between different studies. Hence, evaluating human performance in virtual
environments is a long-standing research goal, and a performance metric that
combines two or more metrics under one formulation remains largely unexplored,
particularly in higher dimensions. The absence of such a metric is primarily
attributed to the discrepancies between pointing and manipulation, the complex
spatial variables in 3D, and the combination of translational and rotational
movements altogether. In this work, four experiments were designed and
conducted with progressively higher spatial complexity to study and compare
existing metrics thoroughly. The research goal was to quantify the difficulty
of these 3D tasks and model human performance sufficiently in full 3D
peripersonal space. Consequently, a new model extension has been proposed and
its applicability has been validated across all the experimental results,
showing improved modelling and representation of human performance in combined
movements of 3D object pointing and manipulation tasks than existing work.
Lastly, the implications on 3D interaction, teleoperation and object task
design in virtual reality are discussed.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06657" title="Abstract">arXiv:2106.06657</a> [<a href="/pdf/2106.06657" title="Download PDF">pdf</a>, <a href="/ps/2106.06657" title="Download PostScript">ps</a>, <a href="/format/2106.06657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Adaptation across Multiway Domains via Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhili Feng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shaobo Han</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon S. Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper studies zero-shot domain adaptation where each domain is indexed
on a multi-dimensional array, and we only have data from a small subset of
domains. Our goal is to produce predictors that perform well on \emph{unseen}
domains. We propose a model which consists of a domain-invariant latent
representation layer and a domain-specific linear prediction layer with a
low-rank tensor structure. Theoretically, we present explicit sample complexity
bounds to characterize the prediction error on unseen domains in terms of the
number of domains with training data and the number of data per domain. To our
knowledge, this is the first finite-sample guarantee for zero-shot domain
adaptation. In addition, we provide experiments on two-way MNIST and four-way
fiber sensing datasets to demonstrate the effectiveness of our proposed model.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06658" title="Abstract">arXiv:2106.06658</a> [<a href="/pdf/2106.06658" title="Download PDF">pdf</a>, <a href="/ps/2106.06658" title="Download PostScript">ps</a>, <a href="/format/2106.06658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polymorphic Context-free Session Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almeida%2C+B">Bernardo Almeida</a>, 
<a href="/search/cs?searchtype=author&query=Mordido%2C+A">Andreia Mordido</a>, 
<a href="/search/cs?searchtype=author&query=Thiemann%2C+P">Peter Thiemann</a>, 
<a href="/search/cs?searchtype=author&query=Vasconcelos%2C+V+T">Vasco T. Vasconcelos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Context-free session types provide a typing discipline for recursive
structured communication protocols on bidirectional channels. They overcome the
restriction of regular session type systems to tail recursive protocols. This
extension enables us to implement serialisation and deserialisation of tree
structures in a fully type-safe manner.
<br />We present the theory underlying the language FreeST 2, which features
context-free session types in an extension of System F with linear types and a
kind system to distinguish message types and channel types. The system presents
some metatheoretical challenges, which we address, contractivity in the
presence of polymorphism, a non-trivial equational theory on types, and
decidability of type equivalence. We also establish standard results on type
preservation, progress, and a characterisation of erroneous processes.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06662" title="Abstract">arXiv:2106.06662</a> [<a href="/pdf/2106.06662" title="Download PDF">pdf</a>, <a href="/format/2106.06662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Networks for Pixelized Spheres
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shakerinava%2C+M">Mehran Shakerinava</a>, 
<a href="/search/cs?searchtype=author&query=Ravanbakhsh%2C+S">Siamak Ravanbakhsh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Pixelizations of Platonic solids such as the cube and icosahedron have been
widely used to represent spherical data, from climate records to Cosmic
Microwave Background maps. Platonic solids have well-known global symmetries.
Once we pixelize each face of the solid, each face also possesses its own local
symmetries in the form of Euclidean isometries. One way to combine these
symmetries is through a hierarchy. However, this approach does not adequately
model the interplay between the two levels of symmetry transformations. We show
how to model this interplay using ideas from group theory, identify the
equivariant linear maps, and introduce equivariant padding that respects these
symmetries. Deep networks that use these maps as their building blocks
generalize gauge equivariant CNNs on pixelized spheres. These deep networks
achieve state-of-the-art results on semantic segmentation for climate data and
omnidirectional image processing. Code is available at https://git.io/JGiZA.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06663" title="Abstract">arXiv:2106.06663</a> [<a href="/pdf/2106.06663" title="Download PDF">pdf</a>, <a href="/format/2106.06663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TDGIA:Effective Injection Attacks on Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+X">Xu Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qinkai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+X">Xinyu Guan</a>, 
<a href="/search/cs?searchtype=author&query=Kharlamov%2C+E">Evgeny Kharlamov</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jialiang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> KDD 2021 research track paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have achieved promising performance in various
real-world applications. However, recent studies have shown that GNNs are
vulnerable to adversarial attacks. In this paper, we study a
recently-introduced realistic attack scenario on graphs -- graph injection
attack (GIA). In the GIA scenario, the adversary is not able to modify the
existing link structure and node attributes of the input graph, instead the
attack is performed by injecting adversarial nodes into it. We present an
analysis on the topological vulnerability of GNNs under GIA setting, based on
which we propose the Topological Defective Graph Injection Attack (TDGIA) for
effective injection attacks. TDGIA first introduces the topological defective
edge selection strategy to choose the original nodes for connecting with the
injected ones. It then designs the smooth feature optimization objective to
generate the features for the injected nodes. Extensive experiments on
large-scale datasets show that TDGIA can consistently and significantly
outperform various attack baselines in attacking dozens of defense GNN models.
Notably, the performance drop on target GNNs resultant from TDGIA is more than
double the damage brought by the best attack solution among hundreds of
submissions on KDD-CUP 2020.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06666" title="Abstract">arXiv:2106.06666</a> [<a href="/pdf/2106.06666" title="Download PDF">pdf</a>, <a href="/ps/2106.06666" title="Download PostScript">ps</a>, <a href="/format/2106.06666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learnable Hypergraph Laplacian for Hypergraph Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuzhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Runiu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shu-Tao Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their
potential in modeling high-order relations preserved in graph structured data.
However, most existing convolution filters are localized and determined by the
pre-defined initial hypergraph topology, neglecting to explore implicit and
long-ange relations in real-world data. In this paper, we propose the first
learning-based method tailored for constructing adaptive hypergraph structure,
termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic
plug-in-play module for improving the representational power of HGCNNs.
Specifically, HERALD adaptively optimizes the adjacency relationship between
hypernodes and hyperedges in an end-to-end manner and thus the task-aware
hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism
to capture the non-local paired-nodes relation. Extensive experiments on
various popular hypergraph datasets for node classification and graph
classification tasks demonstrate that our approach obtains consistent and
considerable performance enhancement, proving its effectiveness and
generalization ability.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06667" title="Abstract">arXiv:2106.06667</a> [<a href="/pdf/2106.06667" title="Download PDF">pdf</a>, <a href="/format/2106.06667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARTL: Cooperative Adversarially-Robust Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hongxin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinli Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Transfer learning eases the burden of training a well-performed model from
scratch, especially when training data is scarce and computation power is
limited. In deep learning, a typical strategy for transfer learning is to
freeze the early layers of a pre-trained model and fine-tune the rest of its
layers on the target domain. Previous work focuses on the accuracy of the
transferred model but neglects the transfer of adversarial robustness. In this
work, we first show that transfer learning improves the accuracy on the target
domain but degrades the inherited robustness of the target model. To address
such a problem, we propose a novel cooperative adversarially-robust transfer
learning (CARTL) by pre-training the model via feature distance minimization
and fine-tuning the pre-trained model with non-expansive fine-tuning for target
domain tasks. Empirical results show that CARTL improves the inherited
robustness by about 28% at most compared with the baseline with the same degree
of accuracy. Furthermore, we study the relationship between the batch
normalization (BN) layers and the robustness in the context of transfer
learning, and we reveal that freezing BN layers can further boost the
robustness transfer.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06672" title="Abstract">arXiv:2106.06672</a> [<a href="/pdf/2106.06672" title="Download PDF">pdf</a>, <a href="/format/2106.06672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Regularized Attention for Deformable Object Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2020 Workshop on Object Representations for Learning and Reasoning; code is available at <a href="https://github.com/shenao-zhang/StRA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Capturing contextual dependencies has proven useful to improve the
representational power of deep neural networks. Recent approaches that focus on
modeling global context, such as self-attention and non-local operation,
achieve this goal by enabling unconstrained pairwise interactions between
elements. In this work, we consider learning representations for deformable
objects which can benefit from context exploitation by modeling the structural
dependencies that the data intrinsically possesses. To this end, we provide a
novel structure-regularized attention mechanism, which formalizes feature
interaction as structural factorization through the use of a pair of
light-weight operations. The instantiated building blocks can be directly
incorporated into modern convolutional neural networks, to boost the
representational power in an efficient manner. Comprehensive studies on
multiple tasks and empirical comparisons with modern attention mechanisms
demonstrate the gains brought by our method in terms of both performance and
model complexity. We further investigate its effect on feature representations,
showing that our trained models can capture diversified representations
characterizing object parts without resorting to extra supervision.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06673" title="Abstract">arXiv:2106.06673</a> [<a href="/pdf/2106.06673" title="Download PDF">pdf</a>, <a href="/format/2106.06673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Study of sampling methods in sentiment analysis of imbalanced data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayyed%2C+Z+A">Zeeshan Ali Sayyed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This work investigates the application of sampling methods for sentiment
analysis on two different highly imbalanced datasets. One dataset contains
online user reviews from the cooking platform Epicurious and the other contains
comments given to the Planned Parenthood organization. In both these datasets,
the classes of interest are rare. Word n-grams were used as features from these
datasets. A feature selection technique based on information gain is first
applied to reduce the number of features to a manageable space. A number of
different sampling methods were then applied to mitigate the class imbalance
problem which are then analyzed.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06676" title="Abstract">arXiv:2106.06676</a> [<a href="/pdf/2106.06676" title="Download PDF">pdf</a>, <a href="/format/2106.06676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised Active Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Devvrit%2C+F">Fnu Devvrit</a>, 
<a href="/search/cs?searchtype=author&query=Rajaraman%2C+N">Nived Rajaraman</a>, 
<a href="/search/cs?searchtype=author&query=Awasthi%2C+P">Pranjal Awasthi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Labelled data often comes at a high cost as it may require recruiting human
labelers or running costly experiments. At the same time, in many practical
scenarios, one already has access to a partially labelled, potentially biased
dataset that can help with the learning task at hand. Motivated by such
settings, we formally initiate a study of $semi-supervised$ $active$ $learning$
through the frame of linear regression. In this setting, the learner has access
to a dataset $X \in \mathbb{R}^{(n_1+n_2) \times d}$ which is composed of $n_1$
unlabelled examples that an algorithm can actively query, and $n_2$ examples
labelled a-priori. Concretely, denoting the true labels by $Y \in
\mathbb{R}^{n_1 + n_2}$, the learner's objective is to find $\widehat{\beta}
\in \mathbb{R}^d$ such that, \begin{equation}
<br />\| X \widehat{\beta} - Y \|_2^2 \le (1 + \epsilon) \min_{\beta \in
\mathbb{R}^d} \| X \beta - Y \|_2^2 \end{equation} while making as few
additional label queries as possible. In order to bound the label queries, we
introduce an instance dependent parameter called the reduced rank, denoted by
$R_X$, and propose an efficient algorithm with query complexity
$O(R_X/\epsilon)$. This result directly implies improved upper bounds for two
important special cases: (i) active ridge regression, and (ii) active kernel
ridge regression, where the reduced-rank equates to the statistical dimension,
$sd_\lambda$ and effective dimension, $d_\lambda$ of the problem respectively,
where $\lambda \ge 0$ denotes the regularization parameter. For active ridge
regression we also prove a matching lower bound of $O(sd_\lambda / \epsilon)$
on the query complexity of any algorithm. This subsumes prior work that only
considered the unregularized case, i.e., $\lambda = 0$.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06677" title="Abstract">arXiv:2106.06677</a> [<a href="/pdf/2106.06677" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Passenger Vehicle Miles Traveled and Carbon Emissions in the  Boston Metropolitan Area
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aslanyan%2C+T">Tigran Aslanyan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shan Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Applications (stat.AP)

</div>
<p class="mathjax">With spatial analytic, econometric, and visualization tools, this book
chapter investigates greenhouse gas emissions for the on-road passenger vehicle
transport sector in the Boston metropolitan area in 2014. It compares
greenhouse gas emission estimations from both the production-based and
consumption-based perspectives with two large-scale administrative datasets:
the vehicle odometer readings from individual vehicle annual inspection, and
the road inventory data containing road segment level geospatial and traffic
information. Based on spatial econometric models that examine socioeconomic and
built environment factors contributing to the vehicle miles traveled at the
census tract level, it offers insights to help cities reduce VMT and carbon
footprint for passenger vehicle travel. Finally, it recommends a pathway for
cities and towns in the Boston metropolitan area to curb VMT and mitigate
carbon emissions to achieve climate goals of carbon neutrality.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06678" title="Abstract">arXiv:2106.06678</a> [<a href="/pdf/2106.06678" title="Download PDF">pdf</a>, <a href="/format/2106.06678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iThing: Designing Next-Generation Things with Battery Health  Self-Monitoring Capabilities for Sustainable IoT in Smart Cities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Aparna Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+D">Debanjan Das</a>, 
<a href="/search/cs?searchtype=author&query=Udutalapally%2C+V">Venkanna Udutalapally</a>, 
<a href="/search/cs?searchtype=author&query=Selvarajan%2C+M+K">Mukil Kumar Selvarajan</a>, 
<a href="/search/cs?searchtype=author&query=Mohanty%2C+S+P">Saraju P. Mohanty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">An accurate and reliable technique for predicting Remaining Useful Life (RUL)
for battery cells proves helpful in battery-operated IoT devices, especially in
remotely operated sensor nodes. Data-driven methods have proved to be the most
effective methods until now. These IoT devices have low computational
capabilities to save costs, but Data-Driven battery health techniques often
require a comparatively large amount of computational power to predict SOH and
RUL due to most methods being feature-heavy. This issue calls for ways to
predict RUL with the least amount of calculations and memory. This paper
proposes an effective and novel peak extraction method to reduce computation
and memory needs and provide accurate prediction methods using the least number
of features while performing all calculations on-board. The model can
self-sustain, requires minimal external interference, and hence operate
remotely much longer. Experimental results prove the accuracy and reliability
of this method. The Absolute Error (AE), Relative error (RE), and Root Mean
Square Error (RMSE) are calculated to compare effectiveness. The training of
the GPR model takes less than 2 seconds, and the correlation between SOH from
peak extraction and RUL is 0.97.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06680" title="Abstract">arXiv:2106.06680</a> [<a href="/pdf/2106.06680" title="Download PDF">pdf</a>, <a href="/format/2106.06680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Markov Decision Processes with Long-Term Average Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+M">Mridul Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Q">Qinbo Bai</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">We consider the problem of constrained Markov Decision Process (CMDP) where
an agent interacts with a unichain Markov Decision Process. At every
interaction, the agent obtains a reward. Further, there are $K$ cost functions.
The agent aims to maximize the long-term average reward while simultaneously
keeping the $K$ long-term average costs lower than a certain threshold. In this
paper, we propose CMDP-PSRL, a posterior sampling based algorithm using which
the agent can learn optimal policies to interact with the CMDP. Further, for
MDP with $S$ states, $A$ actions, and diameter $D$, we prove that following
CMDP-PSRL algorithm, the agent can bound the regret of not accumulating rewards
from optimal policy by $\Tilde{O}(poly(DSA)\sqrt{T})$. Further, we show that
the violations for any of the $K$ constraints is also bounded by
$\Tilde{O}(poly(DSA)\sqrt{T})$. To the best of our knowledge, this is the first
work which obtains a $\Tilde{O}(\sqrt{T})$ regret bounds for ergodic MDPs with
long-term average constraints.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06682" title="Abstract">arXiv:2106.06682</a> [<a href="/pdf/2106.06682" title="Download PDF">pdf</a>, <a href="/format/2106.06682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving PDEs on Unknown Manifolds with Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+S">Senwei Liang</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+S+W">Shixiao W. Jiang</a>, 
<a href="/search/math?searchtype=author&query=Harlim%2C+J">John Harlim</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+H">Haizhao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper proposes a mesh-free computational framework and machine learning
theory for solving elliptic PDEs on unknown manifolds, identified with point
clouds, based on diffusion maps (DM) and deep learning. The PDE solver is
formulated as a supervised learning task to solve a least-squares regression
problem that imposes an algebraic equation approximating a PDE (and boundary
conditions if applicable). This algebraic equation involves a graph-Laplacian
type matrix obtained via DM asymptotic expansion, which is a consistent
estimator of second-order elliptic differential operators. The resulting
numerical method is to solve a highly non-convex empirical risk minimization
problem subjected to a solution from a hypothesis space of neural-network type
functions. In a well-posed elliptic PDE setting, when the hypothesis space
consists of feedforward neural networks with either infinite width or depth, we
show that the global minimizer of the empirical loss function is a consistent
solution in the limit of large training data. When the hypothesis space is a
two-layer neural network, we show that for a sufficiently large width, the
gradient descent method can identify a global minimizer of the empirical loss
function. Supporting numerical examples demonstrate the convergence of the
solutions and the effectiveness of the proposed solver in avoiding numerical
issues that hampers the traditional approach when a large data set becomes
available, e.g., large matrix inversion.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06683" title="Abstract">arXiv:2106.06683</a> [<a href="/pdf/2106.06683" title="Download PDF">pdf</a>, <a href="/format/2106.06683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Multilingual Fairness in Pre-trained Multimodal  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jialu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+E">Xin Eric Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently pre-trained multimodal models, such as CLIP, have received a surge
of attention for their exceptional capabilities towards connecting images and
natural language. The textual representations in English can be desirably
transferred to multilingualism and support promising downstream multimodal
tasks for different languages. Nevertheless, previous fairness discourse in
vision-and-language learning mainly focuses on monolingual representational
biases, and rarely scrutinizes the principles of multilingual fairness in this
multimodal setting, where one language is equated to a group of individuals and
images provide the universal grounding for bridging different languages.
<br />In this paper, we provide a nuanced understanding of individual fairness and
group fairness by viewing language as the recipient of fairness notions. We
define new fairness notions within multilingual context and analytically
articulate that, pre-trained vision-and-language representations are
individually fair across languages but not guaranteed to group fairness.
Furthermore, we conduct extensive experiments to explore the prevalent group
disparity across languages and protected groups including race, gender and age.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06684" title="Abstract">arXiv:2106.06684</a> [<a href="/pdf/2106.06684" title="Download PDF">pdf</a>, <a href="/format/2106.06684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multistream ValidNet: Improving 6D Object Pose Estimation by Automatic  Multistream Validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazumder%2C+J">Joy Mazumder</a>, 
<a href="/search/cs?searchtype=author&query=Zand%2C+M">Mohsen Zand</a>, 
<a href="/search/cs?searchtype=author&query=Greenspan%2C+M">Michael Greenspan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, 2 tables. To appear in the proceedings of the 28th IEEE International Conference on Image Processing (IEEE - ICIP), September 19-22, 2021, Anchorage, Alaska, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This work presents a novel approach to improve the results of pose estimation
by detecting and distinguishing between the occurrence of True and False
Positive results. It achieves this by training a binary classifier on the
output of an arbitrary pose estimation algorithm, and returns a binary label
indicating the validity of the result. We demonstrate that our approach
improves upon a state-of-the-art pose estimation result on the Sil\'eane
dataset, outperforming a variation of the alternative CullNet method by 4.15%
in average class accuracy and 0.73% in overall accuracy at validation. Applying
our method can also improve the pose estimation average precision results of
Op-Net by 6.06% on average.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06685" title="Abstract">arXiv:2106.06685</a> [<a href="/pdf/2106.06685" title="Download PDF">pdf</a>, <a href="/format/2106.06685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Robustness via Fisher-Rao Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Picot%2C+M">Marine Picot</a>, 
<a href="/search/cs?searchtype=author&query=Messina%2C+F">Francisco Messina</a>, 
<a href="/search/cs?searchtype=author&query=Boudiaf%2C+M">Malik Boudiaf</a>, 
<a href="/search/cs?searchtype=author&query=Labeau%2C+F">Fabrice Labeau</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+I+B">Ismail Ben Ayed</a>, 
<a href="/search/cs?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Adversarial robustness has become a topic of growing interest in machine
learning since it was observed that neural networks tend to be brittle. We
propose an information-geometric formulation of adversarial defense and
introduce FIRE, a new Fisher-Rao regularization for the categorical
cross-entropy loss, which is based on the geodesic distance between natural and
perturbed input features. Based on the information-geometric properties of the
class of softmax distributions, we derive an explicit characterization of the
Fisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some
interesting properties as well as connections with standard regularization
metrics. Furthermore, for a simple linear and Gaussian model, we show that all
Pareto-optimal points in the accuracy-robustness region can be reached by FIRE
while other state-of-the-art methods fail. Empirically, we evaluate the
performance of various classifiers trained with the proposed loss on standard
datasets, showing up to 2\% of improvements in terms of robustness while
reducing the training time by 20\% over the best-performing methods.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06688" title="Abstract">arXiv:2106.06688</a> [<a href="/pdf/2106.06688" title="Download PDF">pdf</a>, <a href="/format/2106.06688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BRAIN2DEPTH: Lightweight CNN Model for Classification of Cognitive  States from EEG Recordings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandey%2C+P">Pankaj Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Miyapuram%2C+K+P">Krishna Prasad Miyapuram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures, 6 tables, To be published in 25th Conference on Medical Image Understanding and Analysis (MIUA), 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Several Convolutional Deep Learning models have been proposed to classify the
cognitive states utilizing several neuro-imaging domains. These models have
achieved significant results, but they are heavily designed with millions of
parameters, which increases train and test time, making the model complex and
less suitable for real-time analysis. This paper proposes a simple, lightweight
CNN model to classify cognitive states from Electroencephalograph (EEG)
recordings. We develop a novel pipeline to learn distinct cognitive
representation consisting of two stages. The first stage is to generate the 2D
spectral images from neural time series signals in a particular frequency band.
Images are generated to preserve the relationship between the neighboring
electrodes and the spectral property of the cognitive events. The second is to
develop a time-efficient, computationally less loaded, and high-performing
model. We design a network containing 4 blocks and major components include
standard and depth-wise convolution for increasing the performance and followed
by separable convolution to decrease the number of parameters which maintains
the tradeoff between time and performance. We experiment on open access EEG
meditation dataset comprising expert, nonexpert meditative, and control states.
We compare performance with six commonly used machine learning classifiers and
four state of the art deep learning models. We attain comparable performance
utilizing less than 4\% of the parameters of other models. This model can be
employed in a real-time computation environment such as neurofeedback.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06689" title="Abstract">arXiv:2106.06689</a> [<a href="/pdf/2106.06689" title="Download PDF">pdf</a>, <a href="/format/2106.06689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Combinatory Constituency Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhousi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Longtu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Imankulova%2C+A">Aizhan Imankulova</a>, 
<a href="/search/cs?searchtype=author&query=Komachi%2C+M">Mamoru Komachi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of ACL 2021; 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose two fast neural combinatory models for constituency parsing:
binary and multi-branching. Our models decompose the bottom-up parsing process
into 1) classification of tags, labels, and binary orientations or chunks and
2) vector composition based on the computed orientations or chunks. These
models have theoretical sub-quadratic complexity and empirical linear
complexity. The binary model achieves an F1 score of 92.54 on Penn Treebank,
speeding at 1327.2 sents/sec. Both the models with XLNet provide near
state-of-the-art accuracies for English. Syntactic branching tendency and
headedness of a language are observed during the training and inference
processes for Penn Treebank, Chinese Treebank, and Keyaki Treebank (Japanese).
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06692" title="Abstract">arXiv:2106.06692</a> [<a href="/pdf/2106.06692" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delay Analysis of Base Station Flow Table in SDN-enabled Radio Access  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rastegar%2C+S+H">Seyed Hamed Rastegar</a>, 
<a href="/search/cs?searchtype=author&query=Abbasfar%2C+A">Aliazam Abbasfar</a>, 
<a href="/search/cs?searchtype=author&query=Shah-Mansouri%2C+V">Vahid Shah-Mansouri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Future generation wireless networks are designed with extremely low delay
requirements which makes even small contributed delays important. On the other
hand, software defined networking (SDN) has been introduced as a key enabler of
future wireless and cellular networks in order to make them more flexible. In
SDN, a central controller manages all network equipments by setting the
match-action pairs in flow tables of the devices. However, these flow tables
have limited capacity and thus are not capable of storing the rules of all the
users. In this paper, we consider an SDN-enabled base station (SD-BS) in a cell
equipped with a limited capacity flow table. We analyze the expected delay
incurred in processing of the incoming packets to the SD-BS and present a
mathematical expression for it in terms of density of the users and cell area.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06694" title="Abstract">arXiv:2106.06694</a> [<a href="/pdf/2106.06694" title="Download PDF">pdf</a>, <a href="/format/2106.06694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reverse-engineer the Distributional Structure of Infant Egocentric Views  for Training Generalizable Image Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsutsui%2C+S">Satoshi Tsutsui</a>, 
<a href="/search/cs?searchtype=author&query=Crandall%2C+D">David Crandall</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chen Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2021 CVPR Workshop on Egocentric Perception, Interaction and Computing (EPIC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We analyze egocentric views of attended objects from infants. This paper
shows 1) empirical evidence that children's egocentric views have more diverse
distributions compared to adults' views, 2) we can computationally simulate the
infants' distribution, and 3) the distribution is beneficial for training more
generalized image classifiers not only for infant egocentric vision but for
third-person computer vision.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06695" title="Abstract">arXiv:2106.06695</a> [<a href="/pdf/2106.06695" title="Download PDF">pdf</a>, <a href="/format/2106.06695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SKIing on Simplices: Kernel Interpolation on the Permutohedral Lattice  for Scalable Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+S">Sanyam Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Finzi%2C+M">Marc Finzi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K+A">Ke Alexander Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+A+G">Andrew Gordon Wilson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Machine Learning (ICML), 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">State-of-the-art methods for scalable Gaussian processes use iterative
algorithms, requiring fast matrix vector multiplies (MVMs) with the covariance
kernel. The Structured Kernel Interpolation (SKI) framework accelerates these
MVMs by performing efficient MVMs on a grid and interpolating back to the
original space. In this work, we develop a connection between SKI and the
permutohedral lattice used for high-dimensional fast bilateral filtering. Using
a sparse simplicial grid instead of a dense rectangular one, we can perform GP
inference exponentially faster in the dimension than SKI. Our approach,
Simplex-GP, enables scaling SKI to high dimensions, while maintaining strong
predictive performance. We additionally provide a CUDA implementation of
Simplex-GP, which enables significant GPU acceleration of MVM based inference.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06697" title="Abstract">arXiv:2106.06697</a> [<a href="/pdf/2106.06697" title="Download PDF">pdf</a>, <a href="/format/2106.06697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining the Deep Natural Language Processing by Mining Textual  Interpretable Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ventura%2C+F">Francesco Ventura</a>, 
<a href="/search/cs?searchtype=author&query=Greco%2C+S">Salvatore Greco</a>, 
<a href="/search/cs?searchtype=author&query=Apiletti%2C+D">Daniele Apiletti</a>, 
<a href="/search/cs?searchtype=author&query=Cerquitelli%2C+T">Tania Cerquitelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the high accuracy offered by state-of-the-art deep natural-language
models (e.g. LSTM, BERT), their application in real-life settings is still
widely limited, as they behave like a black-box to the end-user. Hence,
explainability is rapidly becoming a fundamental requirement of
future-generation data-driven systems based on deep-learning approaches.
Several attempts to fulfill the existing gap between accuracy and
interpretability have been done. However, robust and specialized xAI
(Explainable Artificial Intelligence) solutions tailored to deep
natural-language models are still missing. We propose a new framework, named
T-EBAnO, which provides innovative prediction-local and class-based
model-global explanation strategies tailored to black-box deep natural-language
models. Given a deep NLP model and the textual input data, T-EBAnO provides an
objective, human-readable, domain-specific assessment of the reasons behind the
automatic decision-making process. Specifically, the framework extracts sets of
interpretable features mining the inner knowledge of the model. Then, it
quantifies the influence of each feature during the prediction process by
exploiting the novel normalized Perturbation Influence Relation index at the
local level and the novel Global Absolute Influence and Global Relative
Influence indexes at the global level. The effectiveness and the quality of the
local and global explanations obtained with T-EBAnO are proved on (i) a
sentiment analysis task performed by a fine-tuned BERT model, and (ii) a toxic
comment classification task performed by an LSTM model.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06703" title="Abstract">arXiv:2106.06703</a> [<a href="/pdf/2106.06703" title="Download PDF">pdf</a>, <a href="/format/2106.06703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Place Recognition with Deep Embedding Learning over Radar  Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gadd%2C+M">Matthew Gadd</a>, 
<a href="/search/cs?searchtype=author&query=De+Martini%2C+D">Daniele De Martini</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+P">Paul Newman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be presented at the Workshop on Radar Perception for All-Weather Autonomy at the IEEE International Conference on Robotics and Automation (ICRA) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">We learn, in an unsupervised way, an embedding from sequences of radar images
that is suitable for solving place recognition problem using complex radar
data. We experiment on 280 km of data and show performance exceeding
state-of-the-art supervised approaches, localising correctly 98.38% of the time
when using just the nearest database candidate.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06706" title="Abstract">arXiv:2106.06706</a> [<a href="/pdf/2106.06706" title="Download PDF">pdf</a>, <a href="/format/2106.06706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Matching in a Probabilistic Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeloudar%2C+M+Y">Mobin Y. Jeloudar</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+I">Irene Lo</a>, 
<a href="/search/cs?searchtype=author&query=Pollner%2C+T">Tristan Pollner</a>, 
<a href="/search/cs?searchtype=author&query=Saberi%2C+A">Amin Saberi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We consider a model for repeated stochastic matching where compatibility is
probabilistic, is realized the first time agents are matched, and persists in
the future. Such a model has applications in the gig economy, kidney exchange,
and mentorship matching.
<br />We ask whether a $decentralized$ matching process can approximate the optimal
online algorithm. In particular, we consider a decentralized $stable$
$matching$ process where agents match with the most compatible partner who does
not prefer matching with someone else, and known compatible pairs continue
matching in all future rounds. We demonstrate that the above process provides a
0.316-approximation to the optimal online algorithm for matching on general
graphs. We also provide a $\frac{1}{7}$-approximation for many-to-one bipartite
matching, a $\frac{1}{11}$-approximation for capacitated matching on general
graphs, and a $\frac{1}{2k}$-approximation for forming teams of up to $k$
agents. Our results rely on a novel coupling argument that decomposes the
successful edges of the optimal online algorithm in terms of their
round-by-round comparison with stable matching.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06707" title="Abstract">arXiv:2106.06707</a> [<a href="/pdf/2106.06707" title="Download PDF">pdf</a>, <a href="/format/2106.06707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks with Local Graph Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barcel%C3%B3%2C+P">Pablo Barcel&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Geerts%2C+F">Floris Geerts</a>, 
<a href="/search/cs?searchtype=author&query=Reutter%2C+J">Juan Reutter</a>, 
<a href="/search/cs?searchtype=author&query=Ryschkov%2C+M">Maksimilian Ryschkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Various recent proposals increase the distinguishing power of Graph Neural
Networks GNNs by propagating features between $k$-tuples of vertices. The
distinguishing power of these "higher-order'' GNNs is known to be bounded by
the $k$-dimensional Weisfeiler-Leman (WL) test, yet their $\mathcal O(n^k)$
memory requirements limit their applicability. Other proposals infuse GNNs with
local higher-order graph structural information from the start, hereby
inheriting the desirable $\mathcal O(n)$ memory requirement from GNNs at the
cost of a one-time, possibly non-linear, preprocessing step. We propose local
graph parameter enabled GNNs as a framework for studying the latter kind of
approaches and precisely characterize their distinguishing power, in terms of a
variant of the WL test, and in terms of the graph structural properties that
they can take into account. Local graph parameters can be added to any GNN
architecture, and are cheap to compute. In terms of expressive power, our
proposal lies in the middle of GNNs and their higher-order counterparts.
Further, we propose several techniques to aide in choosing the right local
graph parameters. Our results connect GNNs with deep results in finite model
theory and finite variable logics. Our experimental evaluation shows that
adding local graph parameters often has a positive effect for a variety of
GNNs, datasets and graph learning tasks.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06708" title="Abstract">arXiv:2106.06708</a> [<a href="/pdf/2106.06708" title="Download PDF">pdf</a>, <a href="/format/2106.06708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some Aspects of the Numerical Analysis of a Fractional Duffing  Oscillator with a Fractional Variable Order Derivative of the  Riemann-Liouville Type
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kim%2C+V">Valentine Kim</a>, 
<a href="/search/math?searchtype=author&query=Parovik%2C+R">Roman Parovik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Dynamical Systems (math.DS); Chaotic Dynamics (nlin.CD)

</div>
<p class="mathjax">In this paper, we consider some aspects of the numerical analysis of the
mathematical model of fractional Duffing with a derivative of variable
fractional order of the Riemann-Liouville type. Using numerical methods: an
explicit finite-difference scheme based on the Grunwald-Letnikov and
Adams-Bashford-Moulton approximations (predictor-corrector), the proposed
numerical model is found. These methods have been verified with a test case. It
is shown that the predictor-corrector method has a faster convergence than the
method according to the explicit finite-difference scheme. For these schemes,
using Runge's rule, estimates of the computational accuracy were made, which
tended to unity with an increase in the number of calculated grid nodes.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06712" title="Abstract">arXiv:2106.06712</a> [<a href="/pdf/2106.06712" title="Download PDF">pdf</a>, <a href="/format/2106.06712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Combinatorial Algorithms for Combinatorial Bandits: Corruptions  and Approximations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haike Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We consider the stochastic combinatorial semi-bandit problem with adversarial
corruptions. We provide a simple combinatorial algorithm that can achieve a
regret of $\tilde{O}\left(C+d^2K/\Delta_{min}\right)$ where $C$ is the total
amount of corruptions, $d$ is the maximal number of arms one can play in each
round, $K$ is the number of arms. If one selects only one arm in each round, we
achieves a regret of $\tilde{O}\left(C+\sum_{\Delta_i&gt;0}(1/\Delta_i)\right)$.
Our algorithm is combinatorial and improves on the previous combinatorial
algorithm by [Gupta et al., COLT2019] (their bound is
$\tilde{O}\left(KC+\sum_{\Delta_i&gt;0}(1/\Delta_i)\right)$), and almost matches
the best known bounds obtained by [Zimmert et al., ICML2019] and [Zimmert and
Seldin, AISTATS2019] (up to logarithmic factor). Note that the algorithms in
[Zimmert et al., ICML2019] and [Zimmert and Seldin, AISTATS2019] require one to
solve complex convex programs while our algorithm is combinatorial, very easy
to implement, requires weaker assumptions and has very low oracle complexity
and running time. We also study the setting where we only get access to an
approximation oracle for the stochastic combinatorial semi-bandit problem. Our
algorithm achieves an (approximation) regret bound of
$\tilde{O}\left(d\sqrt{KT}\right)$. Our algorithm is very simple, only worse
than the best known regret bound by $\sqrt{d}$, and has much lower oracle
complexity than previous work.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06713" title="Abstract">arXiv:2106.06713</a> [<a href="/pdf/2106.06713" title="Download PDF">pdf</a>, <a href="/format/2106.06713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoLoss: Automated Loss Function Search in Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haochen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Designing an effective loss function plays a crucial role in training deep
recommender systems. Most existing works often leverage a predefined and fixed
loss function that could lead to suboptimal recommendation quality and training
efficiency. Some recent efforts rely on exhaustively or manually searched
weights to fuse a group of candidate loss functions, which is exceptionally
costly in computation and time. They also neglect the various convergence
behaviors of different data examples. In this work, we propose an AutoLoss
framework that can automatically and adaptively search for the appropriate loss
function from a set of candidates. To be specific, we develop a novel
controller network, which can dynamically adjust the loss probabilities in a
differentiable manner. Unlike existing algorithms, the proposed controller can
adaptively generate the loss probabilities for different data examples
according to their varied convergence behaviors. Such design improves the
model's generalizability and transferability between deep recommender systems
and datasets. We evaluate the proposed framework on two benchmark datasets. The
results show that AutoLoss outperforms representative baselines. Further
experiments have been conducted to deepen our understandings of AutoLoss,
including its transferability, components and training efficiency.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06716" title="Abstract">arXiv:2106.06716</a> [<a href="/pdf/2106.06716" title="Download PDF">pdf</a>, <a href="/format/2106.06716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DS-TransUNet:Dual Swin Transformer U-Net for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+A">Ailiang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bingzhi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiayu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+G">Guangming Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Automatic medical image segmentation has made great progress benefit from the
development of deep learning. However, most existing methods are based on
convolutional neural networks (CNNs), which fail to build long-range
dependencies and global context connections due to the limitation of receptive
field in convolution operation. Inspired by the success of Transformer in
modeling the long-range contextual information, some researchers have expended
considerable efforts in designing the robust variants of Transformer-based
U-Net. Moreover, the patch division used in vision transformers usually ignores
the pixel-level intrinsic structural features inside each patch. To alleviate
these problems, we propose a novel deep medical image segmentation framework
called Dual Swin Transformer U-Net (DS-TransUNet), which might be the first
attempt to concurrently incorporate the advantages of hierarchical Swin
Transformer into both encoder and decoder of the standard U-shaped architecture
to enhance the semantic segmentation quality of varying medical images. Unlike
many prior Transformer-based solutions, the proposed DS-TransUNet first adopts
dual-scale encoder subnetworks based on Swin Transformer to extract the coarse
and fine-grained feature representations of different semantic scales. As the
core component for our DS-TransUNet, a well-designed Transformer Interactive
Fusion (TIF) module is proposed to effectively establish global dependencies
between features of different scales through the self-attention mechanism.
Furthermore, we also introduce the Swin Transformer block into decoder to
further explore the long-range contextual information during the up-sampling
process. Extensive experiments across four typical tasks for medical image
segmentation demonstrate the effectiveness of DS-TransUNet, and show that our
approach significantly outperforms the state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06719" title="Abstract">arXiv:2106.06719</a> [<a href="/pdf/2106.06719" title="Download PDF">pdf</a>, <a href="/format/2106.06719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair  Coherence Scoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+L">Linzi Xing</a>, 
<a href="/search/cs?searchtype=author&query=Carenini%2C+G">Giuseppe Carenini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long paper accepted at SIGDIAL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dialogue topic segmentation is critical in several dialogue modeling
problems. However, popular unsupervised approaches only exploit surface
features in assessing topical coherence among utterances. In this work, we
address this limitation by leveraging supervisory signals from the
utterance-pair coherence scoring task. First, we present a simple yet effective
strategy to generate a training corpus for utterance-pair coherence scoring.
Then, we train a BERT-based neural utterance-pair coherence model with the
obtained training corpus. Finally, such model is used to measure the topical
relevance between utterances, acting as the basis of the segmentation
inference. Experiments on three public datasets in English and Chinese
demonstrate that our proposal outperforms the state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06720" title="Abstract">arXiv:2106.06720</a> [<a href="/pdf/2106.06720" title="Download PDF">pdf</a>, <a href="/format/2106.06720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BIOPAK Flasher: Epidemic disease monitoring and detection in Pakistan  using text mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasir%2C+M">Muhammad Nasir</a>, 
<a href="/search/cs?searchtype=author&query=Bakhtyar%2C+M">Maheen Bakhtyar</a>, 
<a href="/search/cs?searchtype=author&query=Baber%2C+J">Junaid Baber</a>, 
<a href="/search/cs?searchtype=author&query=Lakho%2C+S">Sadia Lakho</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+B">Bilal Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Noor%2C+W">Waheed Noor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper is accepted in SOFTA 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Infectious disease outbreak has a significant impact on morbidity, mortality
and can cause economic instability of many countries. As global trade is
growing, goods and individuals are expected to travel across the border, an
infected epidemic area carrier can pose a great danger to his hostile. If a
disease outbreak is recognized promptly, then commercial products and travelers
(traders/visitors) will be effectively vaccinated, and therefore the disease
stopped. Early detection of outbreaks plays an important role here, and beware
of the rapid implementation of control measures by citizens, public health
organizations, and government. Many indicators have valuable information, such
as online news sources (RSS) and social media sources (Twitter, Facebook) that
can be used, but are unstructured and bulky, to extract information about
disease outbreaks. Few early warning outbreak systems exist with some
limitation of linguistic (Urdu) and covering areas (Pakistan). In Pakistan, few
channels are published the outbreak news in Urdu or English. The aim is to
procure information from Pakistan's English and Urdu news channels and then
investigate process, integrate, and visualize the disease epidemic. Urdu
ontology is not existed before to match extracted diseases, so we also build
that ontology of disease.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06721" title="Abstract">arXiv:2106.06721</a> [<a href="/pdf/2106.06721" title="Download PDF">pdf</a>, <a href="/format/2106.06721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A use case of Content Delivery Network raw logfile analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=La%2C+H">Hoang-Loc La</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+A+N">Anh-Tu Ngoc Tran</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+Q">Quang-Trai Le</a>, 
<a href="/search/cs?searchtype=author&query=Yoshimi%2C+M">Masato Yoshimi</a>, 
<a href="/search/cs?searchtype=author&query=Nakajima%2C+T">Takuma Nakajima</a>, 
<a href="/search/cs?searchtype=author&query=Thoai%2C+N">Nam Thoai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The growth of video streaming has stretched the Internet to its limitation.
In other words, the Internet was originally devised to connect a limited number
of computers so that they can share network resources, so the Internet cannot
handle a large amount of traffic at a time, which leads to network congestion.
To overcome this, CDNs are built on top of the Internet as an overlay to
efficiently store and swiftly disseminate contents to users by placing many
servers and data centers around the globe. The topic of CDNs has been
extensively studied in the last several decades. However, there is still a
certain gap between theories in academia and current technologies in industry.
In this paper, we take a close look at the design, implementation, solution,
and performance of a CDN system by analyzing its raw log files. Specifically,
its infrastructure and system design are first presented, and then we conduct a
trace-based study to understand user access patterns, the sources of requests,
system performance, and how such information can be used to improve the whole
CDN system.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06722" title="Abstract">arXiv:2106.06722</a> [<a href="/pdf/2106.06722" title="Download PDF">pdf</a>, <a href="/format/2106.06722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curriculum Pre-Training Heterogeneous Subgraph Transformer for Top-$N$  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Due to the flexibility in modelling data heterogeneity, heterogeneous
information network (HIN) has been adopted to characterize complex and
heterogeneous auxiliary data in top-$N$ recommender systems, called
\emph{HIN-based recommendation}. HIN characterizes complex, heterogeneous data
relations, containing a variety of information that may not be related to the
recommendation task. Therefore, it is challenging to effectively leverage
useful information from HINs for improving the recommendation performance. To
address the above issue, we propose a Curriculum pre-training based
HEterogeneous Subgraph Transformer (called \emph{CHEST}) with new \emph{data
characterization}, \emph{representation model} and \emph{learning algorithm}.
<br />Specifically, we consider extracting useful information from HIN to compose
the interaction-specific heterogeneous subgraph, containing both sufficient and
relevant context information for recommendation. Then we capture the rich
semantics (\eg graph structure and path semantics) within the subgraph via a
heterogeneous subgraph Transformer, where we encode the subgraph with
multi-slot sequence representations. Besides, we design a curriculum
pre-training strategy to provide an elementary-to-advanced learning process, by
which we smoothly transfer basic semantics in HIN for modeling user-item
interaction relation.
<br />Extensive experiments conducted on three real-world datasets demonstrate the
superiority of our proposed method over a number of competitive baselines,
especially when only limited training data is available.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06726" title="Abstract">arXiv:2106.06726</a> [<a href="/pdf/2106.06726" title="Download PDF">pdf</a>, <a href="/format/2106.06726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Go Small and Similar: A Simple Output Decay Brings Better Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tianshu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaomin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jiali Deng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Minghui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Regularization and data augmentation methods have been widely used and become
increasingly indispensable in deep learning training. Researchers who devote
themselves to this have considered various possibilities. But so far, there has
been little discussion about regularizing outputs of the model. This paper
begins with empirical observations that better performances are significantly
associated with output distributions, that have smaller average values and
variances. By audaciously assuming there is causality involved, we propose a
novel regularization term, called Output Decay, that enforces the model to
assign smaller and similar output values on each class. Though being
counter-intuitive, such a small modification result in a remarkable improvement
on performance. Extensive experiments demonstrate the wide applicability,
versatility, and compatibility of Output Decay.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06731" title="Abstract">arXiv:2106.06731</a> [<a href="/pdf/2106.06731" title="Download PDF">pdf</a>, <a href="/format/2106.06731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating External POS Tagger for Punctuation Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+N">Ning Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinfeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouhan Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Punctuation restoration is an important post-processing step in automatic
speech recognition. Among other kinds of external information, part-of-speech
(POS) taggers provide informative tags, suggesting each input token's syntactic
role, which has been shown to be beneficial for the punctuation restoration
task. In this work, we incorporate an external POS tagger and fuse its
predicted labels into the existing language model to provide syntactic
information. Besides, we propose sequence boundary sampling (SBS) to learn
punctuation positions more efficiently as a sequence tagging task. Experimental
results show that our methods can consistently obtain performance gains and
achieve a new state-of-the-art on the common IWSLT benchmark. Further ablation
studies illustrate that both large pre-trained language models and the external
POS tagger take essential parts to improve the model's performance.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06733" title="Abstract">arXiv:2106.06733</a> [<a href="/pdf/2106.06733" title="Download PDF">pdf</a>, <a href="/format/2106.06733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LE-NAS: Learning-based Ensenble with NAS for Dose Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingguang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guocai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yefeng Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Radiation therapy treatment planning is a complex process, as the target dose
prescription and normal tissue sparing are conflicting objectives. Automated
and accurate dose prediction for radiation therapy planning is in high demand.
In this study, we propose a novel learning-based ensemble approach, named
LE-NAS, which integrates neural architecture search (NAS) with knowledge
distillation for 3D radiotherapy dose prediction. Specifically, the prediction
network first exhaustively searches each block from enormous architecture
space. Then, multiple architectures are selected with promising performance and
diversity. To reduce the inference time, we adopt the teacher-student paradigm
by treating the combination of diverse outputs from multiple searched networks
as supervisions to guide the student network training. In addition, we apply
adversarial learning to optimize the student network to recover the knowledge
in teacher networks. To the best of our knowledge, we are the first to
investigate the combination of NAS and knowledge distillation. The proposed
method has been evaluated on the public OpenKBP dataset, and experimental
results demonstrate the effectiveness of our method and its superior
performance to the state-of-the-art method.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06736" title="Abstract">arXiv:2106.06736</a> [<a href="/pdf/2106.06736" title="Download PDF">pdf</a>, <a href="/format/2106.06736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-level Attention Fusion Network for Audio-visual Event Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brousmiche%2C+M">Mathilde Brousmiche</a>, 
<a href="/search/cs?searchtype=author&query=Rouat%2C+J">Jean Rouat</a>, 
<a href="/search/cs?searchtype=author&query=Dupont%2C+S">St&#xe9;phane Dupont</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to the Information Fusion journal in August 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Event classification is inherently sequential and multimodal. Therefore, deep
neural models need to dynamically focus on the most relevant time window and/or
modality of a video. In this study, we propose the Multi-level Attention Fusion
network (MAFnet), an architecture that can dynamically fuse visual and audio
information for event recognition. Inspired by prior studies in neuroscience,
we couple both modalities at different levels of visual and audio paths.
Furthermore, the network dynamically highlights a modality at a given time
window relevant to classify events. Experimental results in AVE (Audio-Visual
Event), UCF51, and Kinetics-Sounds datasets show that the approach can
effectively improve the accuracy in audio-visual event classification. Code is
available at: https://github.com/numediart/MAFnet
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06738" title="Abstract">arXiv:2106.06738</a> [<a href="/pdf/2106.06738" title="Download PDF">pdf</a>, <a href="/format/2106.06738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sentence-level Hierarchical BERT Model for Document Classification  with Limited Labelled Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jinghui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Henchion%2C+M">Maeve Henchion</a>, 
<a href="/search/cs?searchtype=author&query=Bacher%2C+I">Ivan Bacher</a>, 
<a href="/search/cs?searchtype=author&query=Mac+Namee%2C+B">Brian Mac Namee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Training deep learning models with limited labelled data is an attractive
scenario for many NLP tasks, including document classification. While with the
recent emergence of BERT, deep learning language models can achieve reasonably
good performance in document classification with few labelled instances, there
is a lack of evidence in the utility of applying BERT-like models on long
document classification. This work introduces a long-text-specific model -- the
Hierarchical BERT Model (HBM) -- that learns sentence-level features of the
text and works well in scenarios with limited labelled data. Various evaluation
experiments have demonstrated that HBM can achieve higher performance in
document classification than the previous state-of-the-art methods with only 50
to 200 labelled instances, especially when documents are long. Also, as an
extra benefit of HBM, the salient sentences identified by learned HBM are
useful as explanations for labelling documents based on a user study.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06739" title="Abstract">arXiv:2106.06739</a> [<a href="/pdf/2106.06739" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engineering Knowledge Graph from Patent Database
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siddharth%2C+L">L Siddharth</a>, 
<a href="/search/cs?searchtype=author&query=Blessing%2C+L+T+M">Lucienne T.M. Blessing</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+K+L">Kristin L. Wood</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jianxi Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB)

</div>
<p class="mathjax">We propose a large, scalable engineering knowledge graph, comprising sets of
(entity, relationship, entity) triples that are real-world engineering facts
found in the patent database. We apply a set of rules based on the syntactic
and lexical properties of claims in a patent document to extract facts. We
aggregate these facts within each patent document and integrate the aggregated
sets of facts across the patent database to obtain the engineering knowledge
graph. Such a knowledge graph is expected to support inference, reasoning, and
recalling in various engineering tasks. The knowledge graph has a greater size
and coverage in comparison with the previously used knowledge graphs and
semantic networks in the engineering literature.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06742" title="Abstract">arXiv:2106.06742</a> [<a href="/pdf/2106.06742" title="Download PDF">pdf</a>, <a href="/format/2106.06742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task Transformer Network for Joint MRI Reconstruction and  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chun-Mei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yunlu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Huazhu Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yong Xu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The core problem of Magnetic Resonance Imaging (MRI) is the trade off between
acceleration and image quality. Image reconstruction and super-resolution are
two crucial techniques in Magnetic Resonance Imaging (MRI). Current methods are
designed to perform these tasks separately, ignoring the correlations between
them. In this work, we propose an end-to-end task transformer network
(T$^2$Net) for joint MRI reconstruction and super-resolution, which allows
representations and feature transmission to be shared between multiple task to
achieve higher-quality, super-resolved and motion-artifacts-free images from
highly undersampled and degenerated MRI data. Our framework combines both
reconstruction and super-resolution, divided into two sub-branches, whose
features are expressed as queries and keys. Specifically, we encourage joint
feature learning between the two tasks, thereby transferring accurate task
information. We first use two separate CNN branches to extract task-specific
features. Then, a task transformer module is designed to embed and synthesize
the relevance between the two tasks. Experimental results show that our
multi-task model significantly outperforms advanced sequential methods, both
quantitatively and qualitatively.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06744" title="Abstract">arXiv:2106.06744</a> [<a href="/pdf/2106.06744" title="Download PDF">pdf</a>, <a href="/format/2106.06744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepMMSA: A Novel Multimodal Deep Learning Method for Non-small Cell  Lung Cancer Survival Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yujiao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jie Ma</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaoshui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+S+H">Sai Ho Ling</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+S+W">Steven Weidong Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 Submitted to IEEE TBME
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Lung cancer is the leading cause of cancer death worldwide. The critical
reason for the deaths is delayed diagnosis and poor prognosis. With the
accelerated development of deep learning techniques, it has been successfully
applied extensively in many real-world applications, including health sectors
such as medical image interpretation and disease diagnosis. By combining more
modalities that being engaged in the processing of information, multimodal
learning can extract better features and improve predictive ability. The
conventional methods for lung cancer survival analysis normally utilize
clinical data and only provide a statistical probability. To improve the
survival prediction accuracy and help prognostic decision-making in clinical
practice for medical experts, we for the first time propose a multimodal deep
learning method for non-small cell lung cancer (NSCLC) survival analysis, named
DeepMMSA. This method leverages CT images in combination with clinical data,
enabling the abundant information hold within medical images to be associate
with lung cancer survival information. We validate our method on the data of
422 NSCLC patients from The Cancer Imaging Archive (TCIA). Experimental results
support our hypothesis that there is an underlying relationship between
prognostic information and radiomic images. Besides, quantitative results
showing that the established multimodal model can be applied to traditional
method and has the potential to break bottleneck of existing methods and
increase the the percentage of concordant pairs(right predicted pairs) in
overall population by 4%.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06745" title="Abstract">arXiv:2106.06745</a> [<a href="/pdf/2106.06745" title="Download PDF">pdf</a>, <a href="/format/2106.06745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimization and Canonization of GFG Transition-Based Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radi%2C+B+A">Bader Abu Radi</a>, 
<a href="/search/cs?searchtype=author&query=Kupferman%2C+O">Orna Kupferman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 9 figures. arXiv admin note: substantial text overlap with <a href="/abs/2009.10885">arXiv:2009.10885</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">While many applications of automata in formal methods can use
nondeterministic automata, some applications, most notably synthesis, need
deterministic or good-for-games(GFG) automata. The latter are nondeterministic
automata that can resolve their nondeterministic choices in a way that only
depends on the past. The minimization problem for deterministic B\"uchi and
co-B\"uchi word automata is NP-complete. In particular, no canonical minimal
deterministic automaton exists, and a language may have different minimal
deterministic automata. We describe a polynomial minimization algorithm for GFG
co-B\"uchi word automata with transition-based acceptance. Thus, a run is
accepting if it traverses a set $\alpha$ of designated transitions only
finitely often. Our algorithm is based on a sequence of transformations we
apply to the automaton, on top of which a minimal quotient automaton is
defined. We use our minimization algorithm to show canonicity for
transition-based GFG co-B\"uchi word automata: all minimal automata have
isomorphic safe components (namely components obtained by restricting the
transitions to these not in $\alpha$) and once we saturate the automata with
$\alpha$-transitions, we get full isomorphism.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06747" title="Abstract">arXiv:2106.06747</a> [<a href="/pdf/2106.06747" title="Download PDF">pdf</a>, <a href="/format/2106.06747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Impact of Security Vulnerabilities in the npm and RubyGems  Dependency Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zerouali%2C+A">Ahmed Zerouali</a>, 
<a href="/search/cs?searchtype=author&query=Mens%2C+T">Tom Mens</a>, 
<a href="/search/cs?searchtype=author&query=Decan%2C+A">Alexandre Decan</a>, 
<a href="/search/cs?searchtype=author&query=De+Roover%2C+C">Coen De Roover</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The increasing interest in open source software has led to the emergence of
large package distributions of reusable software libraries, such as npm and
RubyGems. These software packages can be subject to security vulnerabilities
that may expose dependent packages through explicitly declared dependencies.
This article empirically studies security vulnerabilities affecting npm and
RubyGems packages. We analyse how and when these vulnerabilities are discovered
and fixed, and how their prevalence changes over time. We also analyse how
vulnerable packages expose their direct and indirect dependents to
vulnerabilities. We distinguish between two types of dependents: packages
distributed via the package manager, and external GitHub projects. Compared to
RubyGems, we observe that the number of vulnerabilities is increasing faster in
npm, but vulnerabilities are also discovered faster in npm. For both package
distributions, the time required to discover vulnerabilities is increasing, but
npm is improving the time needed to fix vulnerabilities. A large proportion of
external GitHub projects are exposed to vulnerabilities coming from direct or
indirect dependencies. Around one out of three direct vulnerable dependencies
to which projects or packages are exposed could be avoided, if software
developers would update their dependencies to more recent releases within the
same major release range.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06749" title="Abstract">arXiv:2106.06749</a> [<a href="/pdf/2106.06749" title="Download PDF">pdf</a>, <a href="/format/2106.06749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decreasing scaling transition from adaptive gradient descent to  stochastic gradient descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinlan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhixia Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongpo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23pages, 19figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Currently, researchers have proposed the adaptive gradient descent algorithm
and its variants, such as AdaGrad, RMSProp, Adam, AmsGrad, etc. Although these
algorithms have a faster speed in the early stage, the generalization ability
in the later stage of training is often not as good as the stochastic gradient
descent. Recently, some researchers have combined the adaptive gradient descent
and stochastic gradient descent to obtain the advantages of both and achieved
good results. Based on this research, we propose a decreasing scaling
transition from adaptive gradient descent to stochastic gradient descent
method(DSTAda). For the training stage of the stochastic gradient descent, we
use a learning rate that decreases linearly with the number of iterations
instead of a constant learning rate. We achieve a smooth and stable transition
from adaptive gradient descent to stochastic gradient descent through scaling.
At the same time, we give a theoretical proof of the convergence of DSTAda
under the framework of online learning. Our experimental results show that the
DSTAda algorithm has a faster convergence speed, higher accuracy, and better
stability and robustness. Our implementation is available at:
https://github.com/kunzeng/DSTAdam.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06751" title="Abstract">arXiv:2106.06751</a> [<a href="/pdf/2106.06751" title="Download PDF">pdf</a>, <a href="/format/2106.06751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Shuhao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Dengji Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+C">Chenze Shao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACL-IJCNLP 2021 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Although teacher forcing has become the main training paradigm for neural
machine translation, it usually makes predictions only conditioned on past
information, and hence lacks global planning for the future. To address this
problem, we introduce another decoder, called seer decoder, into the
encoder-decoder framework during training, which involves future information in
target predictions. Meanwhile, we force the conventional decoder to simulate
the behaviors of the seer decoder via knowledge distillation. In this way, at
test the conventional decoder can perform like the seer decoder without the
attendance of it. Experiment results on the Chinese-English, English-German and
English-Romanian translation tasks show our method can outperform competitive
baselines significantly and achieves greater improvements on the bigger data
sets. Besides, the experiments also prove knowledge distillation the best way
to transfer knowledge from the seer decoder to the conventional decoder
compared to adversarial learning and L2 regularization.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06753" title="Abstract">arXiv:2106.06753</a> [<a href="/pdf/2106.06753" title="Download PDF">pdf</a>, <a href="/format/2106.06753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling transition from momentum stochastic gradient descent to plain  stochastic gradient descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinlan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhixia Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongpo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The plain stochastic gradient descent and momentum stochastic gradient
descent have extremely wide applications in deep learning due to their simple
settings and low computational complexity. The momentum stochastic gradient
descent uses the accumulated gradient as the updated direction of the current
parameters, which has a faster training speed. Because the direction of the
plain stochastic gradient descent has not been corrected by the accumulated
gradient. For the parameters that currently need to be updated, it is the
optimal direction, and its update is more accurate. We combine the advantages
of the momentum stochastic gradient descent with fast training speed and the
plain stochastic gradient descent with high accuracy, and propose a scaling
transition from momentum stochastic gradient descent to plain stochastic
gradient descent(TSGD) method. At the same time, a learning rate that decreases
linearly with the iterations is used instead of a constant learning rate. The
TSGD algorithm has a larger step size in the early stage to speed up the
training, and training with a smaller step size in the later stage can steadily
converge. Our experimental results show that the TSGD algorithm has faster
training speed, higher accuracy and better stability. Our implementation is
available at: https://github.com/kunzeng/TSGD.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06755" title="Abstract">arXiv:2106.06755</a> [<a href="/pdf/2106.06755" title="Download PDF">pdf</a>, <a href="/ps/2106.06755" title="Download PostScript">ps</a>, <a href="/format/2106.06755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPT Approximation for Socially Fair Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goyal%2C+D">Dishant Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+R">Ragesh Jaiswal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we study the socially fair $k$-median/$k$-means problem. We are
given a set of points $P$ in a metric space $\mathcal{X}$ with a distance
function $d(.,.)$. There are $\ell$ groups: $P_1,\dotsc,P_{\ell} \subseteq P$.
We are also given a set $F$ of feasible centers in $\mathcal{X}$. The goal of
the socially fair $k$-median problem is to find a set $C \subseteq F$ of $k$
centers that minimizes the maximum average cost over all the groups. That is,
find $C$ that minimizes the objective function $\Phi(C,P) \equiv \max_{j}
\sum_{x \in P_j} d(C,x)/|P_j|$, where $d(C,x)$ is the distance of $x$ to the
closest center in $C$. The socially fair $k$-means problem is defined similarly
by using squared distances, i.e., $d^{2}(.,.)$ instead of $d(.,.)$. In this
work, we design $(5+\varepsilon)$ and $(33 + \varepsilon)$ approximation
algorithms for the socially fair $k$-median and $k$-means problems,
respectively. For the parameters: $k$ and $\ell$, the algorithms have an FPT
(fixed parameter tractable) running time of $f(k,\ell,\varepsilon) \cdot n$ for
$f(k,\ell,\varepsilon) = 2^{{O}(k \, \ell/\varepsilon)}$ and $n = |P \cup F|$.
We also study a special case of the problem where the centers are allowed to be
chosen from the point set $P$, i.e., $P \subseteq F$. For this special case,
our algorithms give better approximation guarantees of $(4+\varepsilon)$ and
$(18+\varepsilon)$ for the socially fair $k$-median and $k$-means problems,
respectively. Furthermore, we convert these algorithms to constant pass
log-space streaming algorithms. Lastly, we show FPT hardness of approximation
results for the problem with a small gap between our upper and lower bounds.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06758" title="Abstract">arXiv:2106.06758</a> [<a href="/pdf/2106.06758" title="Download PDF">pdf</a>, <a href="/format/2106.06758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Every Bite Is an Experience: Key Point Analysis of Business Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bar-Haim%2C+R">Roy Bar-Haim</a>, 
<a href="/search/cs?searchtype=author&query=Eden%2C+L">Lilach Eden</a>, 
<a href="/search/cs?searchtype=author&query=Kantor%2C+Y">Yoav Kantor</a>, 
<a href="/search/cs?searchtype=author&query=Friedman%2C+R">Roni Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Slonim%2C+N">Noam Slonim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL-IJCNLP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Previous work on review summarization focused on measuring the sentiment
toward the main aspects of the reviewed product or business, or on creating a
textual summary. These approaches provide only a partial view of the data:
aspect-based sentiment summaries lack sufficient explanation or justification
for the aspect rating, while textual summaries do not quantify the significance
of each element, and are not well-suited for representing conflicting views.
Recently, Key Point Analysis (KPA) has been proposed as a summarization
framework that provides both textual and quantitative summary of the main
points in the data. We adapt KPA to review data by introducing Collective Key
Point Mining for better key point extraction; integrating sentiment analysis
into KPA; identifying good key point candidates for review summaries; and
leveraging the massive amount of available reviews and their metadata. We show
empirically that these novel extensions of KPA substantially improve its
performance. We demonstrate that promising results can be achieved without any
domain-specific annotation, while human supervision can lead to further
improvement.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06761" title="Abstract">arXiv:2106.06761</a> [<a href="/pdf/2106.06761" title="Download PDF">pdf</a>, <a href="/format/2106.06761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relearning ensemble selection based on new generated features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burduk%2C+R">Robert Burduk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The ensemble methods are meta-algorithms that combine several base machine
learning techniques to increase the effectiveness of the classification. Many
existing committees of classifiers use the classifier selection process to
determine the optimal set of base classifiers. In this article, we propose the
classifiers selection framework with relearning base classifiers. Additionally,
we use in the proposed framework the new generated feature, which can be
obtained after the relearning process. The proposed technique was compared with
state-of-the-art ensemble methods using three benchmark datasets and one
synthetic dataset. Four classification performance measures are used to
evaluate the proposed method.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06762" title="Abstract">arXiv:2106.06762</a> [<a href="/pdf/2106.06762" title="Download PDF">pdf</a>, <a href="/format/2106.06762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Graph-based Public Good Games with Tree Search and Imitation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darvariu%2C+V">Victor-Alexandru Darvariu</a>, 
<a href="/search/cs?searchtype=author&query=Hailes%2C+S">Stephen Hailes</a>, 
<a href="/search/cs?searchtype=author&query=Musolesi%2C+M">Mirco Musolesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Public goods games represent insightful settings for studying incentives for
individual agents to make contributions that, while costly for each of them,
benefit the wider society. In this work, we adopt the perspective of a central
planner with a global view of a network of self-interested agents and the goal
of maximizing some desired property in the context of a best-shot public goods
game. Existing algorithms for this known NP-complete problem find solutions
that are sub-optimal and cannot optimize for criteria other than social
welfare.
<br />In order to efficiently solve public goods games, our proposed method
directly exploits the correspondence between equilibria and the Maximal
Independent Set (mIS) structural property of graphs. In particular, we define a
Markov Decision Process, which incrementally generates an mIS, and adopt a
planning method to search for equilibria, outperforming existing methods.
Furthermore, we devise an imitation learning technique that uses demonstrations
of the search to obtain a graph neural network parametrized policy which
quickly generalizes to unseen game instances. Our evaluation results show that
this policy is able to reach 99.5% of the performance of the planning method
while being approximately three orders of magnitude faster to evaluate on the
largest graphs tested. The methods presented in this work can be applied to a
large class of public goods games of potentially high societal impact.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06765" title="Abstract">arXiv:2106.06765</a> [<a href="/pdf/2106.06765" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Privacy-preserving Deep Learning-based Network Intrusion  Detection in Data Distribution Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abaimov%2C+S">Stanislav Abaimov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Data Distribution Service (DDS) is an innovative approach towards
communication in ICS/IoT infrastructure and robotics. Being based on the
cross-platform and cross-language API to be applicable in any computerised
device, it offers the benefits of modern programming languages and the
opportunities to develop more complex and advanced systems. However, the DDS
complexity equally increases its vulnerability, while the existing security
measures are limited to plug-ins and static rules, with the rest of the
security provided by third-party applications and operating system.
Specifically, traditional intrusion detection systems (IDS) do not detect any
anomalies in the publish/subscribe method. With the exponentially growing
global communication exchange, securing DDS is of the utmost importance to
futureproofing industrial, public, and even personal devices and systems. This
report presents an experimental work on the simulation of several specific
attacks against DDS, and the application of Deep Learning for their detection.
The findings show that even though Deep Learning allows to detect all simulated
attacks using only metadata analysis, their detection level varies, with some
of the advanced attacks being harder to detect. The limitations imposed by the
attempts to preserve privacy significantly decrease the detection rate. The
report also reviews the drawbacks and limitations of the Deep Learning approach
and proposes a set of selected solutions and configurations, that can further
improve the DDS security.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06766" title="Abstract">arXiv:2106.06766</a> [<a href="/pdf/2106.06766" title="Download PDF">pdf</a>, <a href="/ps/2106.06766" title="Download PostScript">ps</a>, <a href="/format/2106.06766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Parallel Corpora to Improve Multilingual Embedding based  Document and Sentence Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sachintha%2C+D">Dilan Sachintha</a>, 
<a href="/search/cs?searchtype=author&query=Piyarathna%2C+L">Lakmali Piyarathna</a>, 
<a href="/search/cs?searchtype=author&query=Rajitha%2C+C">Charith Rajitha</a>, 
<a href="/search/cs?searchtype=author&query=Ranathunga%2C+S">Surangika Ranathunga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 2 images
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multilingual sentence representations pose a great advantage for low-resource
languages that do not have enough data to build monolingual models on their
own. These multilingual sentence representations have been separately exploited
by few research for document and sentence alignment. However, most of the
low-resource languages are under-represented in these pre-trained models. Thus,
in the context of low-resource languages, these models have to be fine-tuned
for the task at hand, using additional data sources. This paper presents a
weighting mechanism that makes use of available small-scale parallel corpora to
improve the performance of multilingual sentence representations on document
and sentence alignment. Experiments are conducted with respect to two
low-resource languages, Sinhala and Tamil. Results on a newly created dataset
of Sinhala-English, Tamil-English, and Sinhala-Tamil show that this new
weighting mechanism significantly improves both document and sentence
alignment. This dataset, as well as the source-code, is publicly released.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06768" title="Abstract">arXiv:2106.06768</a> [<a href="/pdf/2106.06768" title="Download PDF">pdf</a>, <a href="/format/2106.06768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Planning Spatial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darvariu%2C+V">Victor-Alexandru Darvariu</a>, 
<a href="/search/cs?searchtype=author&query=Hailes%2C+S">Stephen Hailes</a>, 
<a href="/search/cs?searchtype=author&query=Musolesi%2C+M">Mirco Musolesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We tackle the problem of goal-directed graph construction: given a starting
graph, a global objective function (e.g., communication efficiency), and a
budget of modifications, the aim is to find a set of edges whose addition to
the graph maximally improves the objective. This problem emerges in many
networks of great importance for society such as transportation and critical
infrastructure networks. We identify two significant shortcomings with present
methods. Firstly, they focus exclusively on network topology while ignoring
spatial information; however, in many real-world networks, nodes are embedded
in space, which yields different global objectives and governs the range and
density of realizable connections. Secondly, existing RL methods scale poorly
to large networks due to the high cost of training a model and the scaling
factors of the action space and global objectives.
<br />In this work, we formulate the problem of goal-directed construction of
spatial networks as a deterministic MDP. We adopt the Monte Carlo Tree Search
framework for planning in this domain, prioritizing the optimality of final
solutions over the speed of policy evaluation. We propose several improvements
over the standard UCT algorithm for this family of problems, addressing their
single-agent nature, the trade-off between the costs of edges and their
contribution to the objective, and an action space linear in the number of
nodes. We demonstrate the suitability of this approach for improving the global
efficiency and attack resilience of a variety of synthetic and real-world
networks, including Internet backbone networks and metro systems. We obtain 24%
better solutions on average compared to UCT on the largest networks tested, and
scalability superior to previous methods.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06769" title="Abstract">arXiv:2106.06769</a> [<a href="/pdf/2106.06769" title="Download PDF">pdf</a>, <a href="/format/2106.06769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Subject Domain Adaptation for Multi-Frame EEG Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junfu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Working memory (WM) is a basic part of human cognition, which plays an
important role in the study of human cognitive load. Among various brain
imaging techniques, electroencephalography has shown its advantage on easy
access and reliability. However, one of the critical challenges is that
individual difference may cause the ineffective results, especially when the
established model meets an unfamiliar subject. In this work, we propose a
cross-subject deep adaptation model with spatial attention (CS-DASA) to
generalize the workload classifications across subjects. First, we transform
time-series EEG data into multi-frame EEG images incorporating more
spatio-temporal information. First, the subject-shared module in CS-DASA
receives multi-frame EEG image data from both source and target subjects and
learns the common feature representations. Then, in subject-specific module,
the maximum mean discrepancy is implemented to measure the domain distribution
divergence in a reproducing kernel Hilbert space, which can add an effective
penalty loss for domain adaptation. Additionally, the subject-to-subject
spatial attention mechanism is employed to focus on the most discriminative
spatial feature in EEG image data. Experiments conducted on a public WM EEG
dataset containing 13 subjects show that the proposed model is capable of
achieve better performance than existing state-of-the art methods.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06770" title="Abstract">arXiv:2106.06770</a> [<a href="/pdf/2106.06770" title="Download PDF">pdf</a>, <a href="/format/2106.06770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What can linearized neural networks actually say about generalization?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ortiz-Jim%C3%A9nez%2C+G">Guillermo Ortiz-Jim&#xe9;nez</a>, 
<a href="/search/cs?searchtype=author&query=Moosavi-Dezfooli%2C+S">Seyed-Mohsen Moosavi-Dezfooli</a>, 
<a href="/search/cs?searchtype=author&query=Frossard%2C+P">Pascal Frossard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">For certain infinitely-wide neural networks, the neural tangent kernel (NTK)
theory fully characterizes generalization. However, for the networks used in
practice, the empirical NTK represents only a rough first-order approximation
of these architectures. Still, a growing body of work keeps leveraging this
approximation to successfully analyze important deep learning phenomena and
derive algorithms for new applications. In our work, we provide strong
empirical evidence to determine the practical validity of such approximation by
conducting a systematic comparison of the behaviour of different neural
networks and their linear approximations on different tasks. We show that the
linear approximations can indeed rank the learning complexity of certain tasks
for neural networks, albeit with important nuances. Specifically, we discover
that, in contrast to what was previously observed, neural networks do not
always perform better than their kernel approximations, and reveal that their
performance gap heavily depends on architecture, number of samples and training
task. In fact, we show that during training, deep networks increase the
alignment of their empirical NTK with the target task, which explains why
linear approximations at the end of training can better explain the dynamics of
deep networks. Overall, our work provides concrete examples of novel deep
learning phenomena which can inspire future theoretical research, as well as
provides a new perspective on the use of the NTK approximation in deep
learning.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06772" title="Abstract">arXiv:2106.06772</a> [<a href="/pdf/2106.06772" title="Download PDF">pdf</a>, <a href="/ps/2106.06772" title="Download PostScript">ps</a>, <a href="/format/2106.06772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Mixed-Integer Linear Programming Formulation for Human Multi-Robot  Task Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lippi%2C+M">Martina Lippi</a>, 
<a href="/search/cs?searchtype=author&query=Marino%2C+A">Alessandro Marino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2021 IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this work, we address a task allocation problem for human multi-robot
settings. Given a set of tasks to perform, we formulate a general Mixed-Integer
Linear Programming (MILP) problem aiming at minimizing the overall execution
time while optimizing the quality of the executed tasks as well as human and
robotic workload. Different skills of the agents, both human and robotic, are
taken into account and human operators are enabled to either directly execute
tasks or play supervisory roles; moreover, multiple manipulators can tightly
collaborate if required to carry out a task. Finally, as realistic in human
contexts, human parameters are assumed to vary over time, e.g., due to
increasing human level of fatigue. Therefore, online monitoring is required and
re-allocation is performed if needed. Simulations in a realistic scenario with
two manipulators and a human operator performing an assembly task validate the
effectiveness of the approach.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06776" title="Abstract">arXiv:2106.06776</a> [<a href="/pdf/2106.06776" title="Download PDF">pdf</a>, <a href="/ps/2106.06776" title="Download PostScript">ps</a>, <a href="/format/2106.06776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A piecewise ellipsoidal reachable set estimation method for continuous  bimodal piecewise affine systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Quang%2C+T+L">Thuan Le Quang</a>, 
<a href="/search/eess?searchtype=author&query=Thanh%2C+N+P">Nam Phan Thanh</a>, 
<a href="/search/eess?searchtype=author&query=Baldi%2C+S">Simone Baldi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this work, the issue of estimation of reachable sets in continuous bimodal
piecewise affine systems is studied. A new method is proposed, in the framework
of ellipsoidal bounding, using piecewise quadratic Lyapunov functions. Although
bimodal piecewise affine systems can be seen as a special class of affine
hybrid systems, reachability methods developed for affine hybrid systems might
be inappropriately complex for bimodal dynamics. This work goes in the
direction of exploiting the dynamical structure of the system to propose a
simpler approach. More specifically, because of the piecewise nature of the
Lyapunov function, we first derive conditions to ensure that a given quadratic
function is positive on half spaces. Then, we exploit the property of bimodal
piecewise quadratic functions being continuous on a given hyperplane. Finally,
linear matrix characterizations of the estimate of the reachable set are
derived.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06777" title="Abstract">arXiv:2106.06777</a> [<a href="/pdf/2106.06777" title="Download PDF">pdf</a>, <a href="/format/2106.06777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-free Reinforcement Learning for Branching Markov Decision  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahn%2C+E+M">Ernst Moritz Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+M">Mateo Perez</a>, 
<a href="/search/cs?searchtype=author&query=Schewe%2C+S">Sven Schewe</a>, 
<a href="/search/cs?searchtype=author&query=Somenzi%2C+F">Fabio Somenzi</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Ashutosh Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Wojtczak%2C+D">Dominik Wojtczak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear in CAV 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Logic in Computer Science (cs.LO); Systems and Control (eess.SY)

</div>
<p class="mathjax">We study reinforcement learning for the optimal control of Branching Markov
Decision Processes (BMDPs), a natural extension of (multitype) Branching Markov
Chains (BMCs). The state of a (discrete-time) BMCs is a collection of entities
of various types that, while spawning other entities, generate a payoff. In
comparison with BMCs, where the evolution of a each entity of the same type
follows the same probabilistic pattern, BMDPs allow an external controller to
pick from a range of options. This permits us to study the best/worst behaviour
of the system. We generalise model-free reinforcement learning techniques to
compute an optimal control strategy of an unknown BMDP in the limit. We present
results of an implementation that demonstrate the practicality of the approach.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06778" title="Abstract">arXiv:2106.06778</a> [<a href="/pdf/2106.06778" title="Download PDF">pdf</a>, <a href="/format/2106.06778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Clone Transformer for Efficient Convolutional Neural Netwoks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+L">Longqing Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Convolutional networks (ConvNets) have shown impressive capability to solve
various vision tasks. Nevertheless, the trade-off between performance and
efficiency is still a challenge for a feasible model deployment on
resource-constrained platforms. In this paper, we introduce a novel concept
termed multi-path fully connected pattern (MPFC) to rethink the
interdependencies of topology pattern, accuracy and efficiency for ConvNets.
Inspired by MPFC, we further propose a dual-branch module named dynamic clone
transformer (DCT) where one branch generates multiple replicas from inputs and
another branch reforms those clones through a series of difference vectors
conditional on inputs itself to produce more variants. This operation allows
the self-expansion of channel-wise information in a data-driven way with little
computational cost while providing sufficient learning capacity, which is a
potential unit to replace computationally expensive pointwise convolution as an
expansion layer in the bottleneck structure.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06780" title="Abstract">arXiv:2106.06780</a> [<a href="/pdf/2106.06780" title="Download PDF">pdf</a>, <a href="/format/2106.06780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Context Systems: Dynamics and Evolution (Pre-Print of  &quot;Multi-context systems in dynamic environments&quot;)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabalar%2C+P">Pedro Cabalar</a>, 
<a href="/search/cs?searchtype=author&query=Costantini%2C+S">Stefania Costantini</a>, 
<a href="/search/cs?searchtype=author&query=De+Gasperis%2C+G">Giovanni De Gasperis</a>, 
<a href="/search/cs?searchtype=author&query=Formisano%2C+A">Andrea Formisano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages 2 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Annals of Mathematics and Artificial Intelligence 86, 87-120
  (2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Multi-Context Systems (MCS) model in Computational Logic distributed systems
composed of heterogeneous sources, or "contexts", interacting via special rules
called "bridge rules". In this paper, we consider how to enhance flexibility
and generality in bridge-rules definition and application. In particular, we
introduce and discuss some formal extensions of MCSs useful for a practical use
in dynamic environments, and we try to provide guidelines for implementations
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06781" title="Abstract">arXiv:2106.06781</a> [<a href="/pdf/2106.06781" title="Download PDF">pdf</a>, <a href="/ps/2106.06781" title="Download PostScript">ps</a>, <a href="/format/2106.06781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Driven Approach for Contact Detection, Classification and  Reaction in Physical Human-Robot Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lippi%2C+M">Martina Lippi</a>, 
<a href="/search/cs?searchtype=author&query=Gillini%2C+G">Giuseppe Gillini</a>, 
<a href="/search/cs?searchtype=author&query=Marino%2C+A">Alessandro Marino</a>, 
<a href="/search/cs?searchtype=author&query=Arrichiello%2C+F">Filippo Arrichiello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2021 IEEE International Conference on Robotics and Automation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper considers a scenario where a robot and a human operator share the
same workspace, and the robot is able to both carry out autonomous tasks and
physically interact with the human in order to achieve common goals. In this
context, both intentional and accidental contacts between human and robot might
occur due to the complexity of tasks and environment, to the uncertainty of
human behavior, and to the typical lack of awareness of each other actions.
Here, a two stage strategy based on Recurrent Neural Networks (RNNs) is
designed to detect intentional and accidental contacts: the occurrence of a
contact with the human is detected at the first stage, while the classification
between intentional and accidental is performed at the second stage. An
admittance control strategy or an evasive action is then performed by the
robot, respectively. The approach also works in the case the robot
simultaneously interacts with the human and the environment, where the
interaction wrench of the latter is modeled via Gaussian Mixture Models (GMMs).
Control Barrier Functions (CBFs) are included, at the control level, to
guarantee the satisfaction of robot and task constraints while performing the
proper interaction strategy. The approach has been validated on a real setup
composed of a Kinova Jaco2 robot.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06783" title="Abstract">arXiv:2106.06783</a> [<a href="/pdf/2106.06783" title="Download PDF">pdf</a>, <a href="/format/2106.06783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lvio-Fusion: A Self-adaptive Multi-sensor Fusion SLAM Framework Using  Actor-critic Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yupeng Jia</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haiyong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+G">Guanlin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jiaquan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhuqing Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">State estimation with sensors is essential for mobile robots. Due to sensors
have different performance in different environments, how to fuse measurements
of various sensors is a problem. In this paper, we propose a tightly-coupled
multi-sensor fusion framework, Lvio-Fusion, which fuses stereo camera, Lidar,
IMU, and GPS based on the graph optimization. Especially for urban traffic
scenes, we introduce a segmented global pose graph optimization with GPS and
loop-closure, which can eliminate accumulated drifts. Additionally, we
creatively use a actor-critic method in reinforcement learning to adaptively
adjust sensors' weight. After training, actor-critic agent can provide the
system with better and dynamic sensors' weight. We evaluate the performance of
our system on public datasets and compare it with other state-of-the-art
methods, showing that the proposed method achieves high estimation accuracy and
robustness to various environments. And our implementations are open source and
highly scalable.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06786" title="Abstract">arXiv:2106.06786</a> [<a href="/pdf/2106.06786" title="Download PDF">pdf</a>, <a href="/format/2106.06786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting the Ordering of Characters in Japanese Historical Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lamb%2C+A">Alex Lamb</a>, 
<a href="/search/cs?searchtype=author&query=Clanuwat%2C+T">Tarin Clanuwat</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Siyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Bober-Irizar%2C+M">Mikel Bober-Irizar</a>, 
<a href="/search/cs?searchtype=author&query=Kitamoto%2C+A">Asanobu Kitamoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Digital Libraries (cs.DL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Japan is a unique country with a distinct cultural heritage, which is
reflected in billions of historical documents that have been preserved.
However, the change in Japanese writing system in 1900 made these documents
inaccessible for the general public. A major research project has been to make
these historical documents accessible and understandable. An increasing amount
of research has focused on the character recognition task and the location of
characters on image, yet less research has focused on how to predict the
sequential ordering of the characters. This is because sequence in classical
Japanese is very different from modern Japanese. Ordering characters into a
sequence is important for making the document text easily readable and
searchable. Additionally, it is a necessary step for any kind of natural
language processing on the data (e.g. machine translation, language modeling,
and word embeddings). We explore a few approaches to the task of predicting the
sequential ordering of the characters: one using simple hand-crafted rules,
another using hand-crafted rules with adaptive thresholds, and another using a
deep recurrent sequence model trained with teacher forcing. We provide a
quantitative and qualitative comparison of these techniques as well as their
distinct trade-offs. Our best-performing system has an accuracy of 98.65\% and
has a perfect accuracy on 49\% of the books in our dataset, suggesting that the
technique is able to predict the order of the characters well enough for many
tasks.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06787" title="Abstract">arXiv:2106.06787</a> [<a href="/pdf/2106.06787" title="Download PDF">pdf</a>, <a href="/format/2106.06787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based Prior and Forward Models for Inverse Problems on Manifolds  with Boundaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Harlim%2C+J">John Harlim</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+S">Shixiao Jiang</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+H">Hwanwoo Kim</a>, 
<a href="/search/math?searchtype=author&query=Sanz-Alonso%2C+D">Daniel Sanz-Alonso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computation (stat.CO); Methodology (stat.ME)

</div>
<p class="mathjax">This paper develops manifold learning techniques for the numerical solution
of PDE-constrained Bayesian inverse problems on manifolds with boundaries. We
introduce graphical Mat\'ern-type Gaussian field priors that enable flexible
modeling near the boundaries, representing boundary values by superposition of
harmonic functions with appropriate Dirichlet boundary conditions. We also
investigate the graph-based approximation of forward models from PDE parameters
to observed quantities. In the construction of graph-based prior and forward
models, we leverage the ghost point diffusion map algorithm to approximate
second-order elliptic operators with classical boundary conditions. Numerical
results validate our graph-based approach and demonstrate the need to design
prior covariance models that account for boundary conditions.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06788" title="Abstract">arXiv:2106.06788</a> [<a href="/pdf/2106.06788" title="Download PDF">pdf</a>, <a href="/format/2106.06788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learngene: From Open-World to Your Learning Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiufeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xin Geng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shuxia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shiyu Xia</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lei Qi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Ning Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although deep learning has made significant progress on fixed large-scale
datasets, it typically encounters challenges regarding improperly detecting
new/unseen classes in the open-world classification, over-parametrized, and
overfitting small samples. In contrast, biological systems can overcome the
above difficulties very well. Individuals inherit an innate gene from
collective creatures that have evolved over hundreds of millions of years, and
can learn new skills through a few examples. Inspired by this, we propose a
practical collective-individual paradigm where open-world tasks are trained in
sequence using an evolution (expandable) network. To be specific, we
innovatively introduce learngene that inherits the meta-knowledge from the
collective model and reconstructs a new lightweight individual model for the
target task, to realize the collective-individual paradigm. Particularly, we
present a novel criterion that can discover the learngene in the collective
model, according to the gradient information. Finally, the individual model is
trained only with a few samples in the absence of the source data. We
demonstrate the effectiveness of our approach in an extensive empirical study
and theoretical analysis.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06792" title="Abstract">arXiv:2106.06792</a> [<a href="/pdf/2106.06792" title="Download PDF">pdf</a>, <a href="/ps/2106.06792" title="Download PostScript">ps</a>, <a href="/format/2106.06792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A One-Shot Texture-Perceiving Generative Adversarial Network for  Unsupervised Surface Inspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+L">Lingyun Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaokui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICIP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual surface inspection is a challenging task owing to the highly diverse
appearance of target surfaces and defective regions. Previous attempts heavily
rely on vast quantities of training examples with manual annotation. However,
in some practical cases, it is difficult to obtain a large number of samples
for inspection. To combat it, we propose a hierarchical texture-perceiving
generative adversarial network (HTP-GAN) that is learned from the one-shot
normal image in an unsupervised scheme. Specifically, the HTP-GAN contains a
pyramid of convolutional GANs that can capture the global structure and
fine-grained representation of an image simultaneously. This innovation helps
distinguishing defective surface regions from normal ones. In addition, in the
discriminator, a texture-perceiving module is devised to capture the spatially
invariant representation of normal image via directional convolutions, making
it more sensitive to defective areas. Experiments on a variety of datasets
consistently demonstrate the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06795" title="Abstract">arXiv:2106.06795</a> [<a href="/pdf/2106.06795" title="Download PDF">pdf</a>, <a href="/format/2106.06795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Consolidation based Class Incremental Online Learning with  Limited Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karim%2C+M+A">Mohammed Asad Karim</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+V+K">Vinay Kumar Verma</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+P">Pravendra Singh</a>, 
<a href="/search/cs?searchtype=author&query=Namboodiri%2C+V">Vinay Namboodiri</a>, 
<a href="/search/cs?searchtype=author&query=Rai%2C+P">Piyush Rai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Joint Conference on Artificial Intelligence (IJCAI-2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose a novel approach for class incremental online learning in a
limited data setting. This problem setting is challenging because of the
following constraints: (1) Classes are given incrementally, which necessitates
a class incremental learning approach; (2) Data for each class is given in an
online fashion, i.e., each training example is seen only once during training;
(3) Each class has very few training examples; and (4) We do not use or assume
access to any replay/memory to store data from previous classes. Therefore, in
this setting, we have to handle twofold problems of catastrophic forgetting and
overfitting. In our approach, we learn robust representations that are
generalizable across tasks without suffering from the problems of catastrophic
forgetting and overfitting to accommodate future classes with limited samples.
Our proposed method leverages the meta-learning framework with knowledge
consolidation. The meta-learning framework helps the model for rapid learning
when samples appear in an online fashion. Simultaneously, knowledge
consolidation helps to learn a robust representation against forgetting under
online updates to facilitate future learning. Our approach significantly
outperforms other methods on several benchmarks.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06796" title="Abstract">arXiv:2106.06796</a> [<a href="/pdf/2106.06796" title="Download PDF">pdf</a>, <a href="/format/2106.06796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Client Scheduling and Resource Allocation under Channel  Uncertainty in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wadu%2C+M+M">Madhusanka Manimel Wadu</a>, 
<a href="/search/cs?searchtype=author&query=Samarakoon%2C+S">Sumudu Samarakoon</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2002.00802">arXiv:2002.00802</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The performance of federated learning (FL) over wireless networks depend on
the reliability of the client-server connectivity and clients' local
computation capabilities. In this article we investigate the problem of client
scheduling and resource block (RB) allocation to enhance the performance of
model training using FL, over a pre-defined training duration under imperfect
channel state information (CSI) and limited local computing resources. First,
we analytically derive the gap between the training losses of FL with clients
scheduling and a centralized training method for a given training duration.
Then, we formulate the gap of the training loss minimization over client
scheduling and RB allocation as a stochastic optimization problem and solve it
using Lyapunov optimization. A Gaussian process regression-based channel
prediction method is leveraged to learn and track the wireless channel, in
which, the clients' CSI predictions and computing power are incorporated into
the scheduling decision. Using an extensive set of simulations, we validate the
robustness of the proposed method under both perfect and imperfect CSI over an
array of diverse data distributions. Results show that the proposed method
reduces the gap of the training accuracy loss by up to 40.7% compared to
state-of-theart client scheduling and RB allocation methods.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06797" title="Abstract">arXiv:2106.06797</a> [<a href="/pdf/2106.06797" title="Download PDF">pdf</a>, <a href="/format/2106.06797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Translation into Low-resource Language Varieties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sachin Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Wintner%2C+S">Shuly Wintner</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">State-of-the-art machine translation (MT) systems are typically trained to
generate the "standard" target language; however, many languages have multiple
varieties (regional varieties, dialects, sociolects, non-native varieties) that
are different from the standard language. Such varieties are often
low-resource, and hence do not benefit from contemporary NLP solutions, MT
included. We propose a general framework to rapidly adapt MT systems to
generate language varieties that are close to, but different from, the standard
target language, using no parallel (source--variety) data. This also includes
adaptation of MT systems to low-resource typologically-related target
languages. We experiment with adapting an English--Russian MT system to
generate Ukrainian and Belarusian, an English--Norwegian Bokm{\aa}l system to
generate Nynorsk, and an English--Arabic system to generate four Arabic
dialects, obtaining significant improvements over competitive baselines.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06799" title="Abstract">arXiv:2106.06799</a> [<a href="/pdf/2106.06799" title="Download PDF">pdf</a>, <a href="/format/2106.06799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Cost Proxies Meet Differentiable Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+L">Lichuan Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Dudziak%2C+%C5%81">&#x141;ukasz Dudziak</a>, 
<a href="/search/cs?searchtype=author&query=Abdelfattah%2C+M+S">Mohamed S. Abdelfattah</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+T">Thomas Chau</a>, 
<a href="/search/cs?searchtype=author&query=Lane%2C+N+D">Nicholas D. Lane</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hongkai Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Differentiable neural architecture search (NAS) has attracted significant
attention in recent years due to its ability to quickly discover promising
architectures of deep neural networks even in very large search spaces. Despite
its success, DARTS lacks robustness in certain cases, e.g. it may degenerate to
trivial architectures with excessive parametric-free operations such as skip
connection or random noise, leading to inferior performance. In particular,
operation selection based on the magnitude of architectural parameters was
recently proven to be fundamentally wrong showcasing the need to rethink this
aspect. On the other hand, zero-cost proxies have been recently studied in the
context of sample-based NAS showing promising results -- speeding up the search
process drastically in some cases but also failing on some of the large search
spaces typical for differentiable NAS. In this work we propose a novel
operation selection paradigm in the context of differentiable NAS which
utilises zero-cost proxies. Our perturbation-based zero-cost operation
selection (Zero-Cost-PT) improves searching time and, in many cases, accuracy
compared to the best available differentiable architecture search, regardless
of the search space size. Specifically, we are able to find comparable
architectures to DARTS-PT on the DARTS CNN search space while being over 40x
faster (total searching time 25 minutes on a single GPU).
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06801" title="Abstract">arXiv:2106.06801</a> [<a href="/pdf/2106.06801" title="Download PDF">pdf</a>, <a href="/format/2106.06801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandey%2C+P">Prashant Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+A">Ajey Pai</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+N">Nisarg Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+P">Prasenjit Das</a>, 
<a href="/search/cs?searchtype=author&query=Makharia%2C+G">Govind Makharia</a>, 
<a href="/search/cs?searchtype=author&query=AP%2C+P">Prathosh AP</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Contrastive Learning (CL) is a recent representation learning approach, which
achieves promising results by encouraging inter-class separability and
intra-class compactness in learned image representations. Because medical
images often contain multiple classes of interest per image, a standard
image-level CL for these images is not applicable. In this work, we present a
novel semi-supervised 2D medical segmentation solution that applies CL on image
patches, instead of full images. These patches are meaningfully constructed
using the semantic information of different classes obtained via pseudo
labeling. We also propose a novel consistency regularization scheme, which
works in synergy with contrastive learning. It addresses the problem of
confirmation bias often observed in semi-supervised settings, and encourages
better clustering in the feature space. We evaluate our method on four public
medical segmentation datasets along with a novel histopathology dataset that we
introduce. Our method obtains consistent improvements over the state-of-the-art
semi-supervised segmentation approaches for all datasets.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06804" title="Abstract">arXiv:2106.06804</a> [<a href="/pdf/2106.06804" title="Download PDF">pdf</a>, <a href="/format/2106.06804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-based Logic Explanations of Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barbiero%2C+P">Pietro Barbiero</a>, 
<a href="/search/cs?searchtype=author&query=Ciravegna%2C+G">Gabriele Ciravegna</a>, 
<a href="/search/cs?searchtype=author&query=Giannini%2C+F">Francesco Giannini</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B3%2C+P">Pietro Li&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Gori%2C+M">Marco Gori</a>, 
<a href="/search/cs?searchtype=author&query=Melacci%2C+S">Stefano Melacci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06807" title="Abstract">arXiv:2106.06807</a> [<a href="/pdf/2106.06807" title="Download PDF">pdf</a>, <a href="/format/2106.06807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redirected Walking in Static and Dynamic Scenes Using Visibility  Polygons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Williams%2C+N+L">Niall L. Williams</a>, 
<a href="/search/cs?searchtype=author&query=Bera%2C+A">Aniket Bera</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We present a new approach for redirected walking in static and dynamic scenes
that uses techniques from robot motion planning to compute the redirection
gains that steer the user on collision-free paths in the physical space. Our
first contribution is a mathematical framework for redirected walking using
concepts from motion planning and configuration spaces. This framework
highlights various geometric and perceptual constraints that tend to make
collision-free redirected walking difficult. We use our framework to propose an
efficient solution to the redirection problem that uses the notion of
visibility polygons to compute the free spaces in the physical environment and
the virtual environment. The visibility polygon provides a concise
representation of the entire space that is visible, and therefore walkable, to
the user from their position within an environment. Using this representation
of walkable space, we apply redirected walking to steer the user to regions of
the visibility polygon in the physical environment that closely match the
region that the user occupies in the visibility polygon in the virtual
environment. We show that our algorithm is able to steer the user along paths
that result in significantly fewer red{resets than existing state-of-the-art
algorithms in both static and dynamic scenes.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06811" title="Abstract">arXiv:2106.06811</a> [<a href="/pdf/2106.06811" title="Download PDF">pdf</a>, <a href="/format/2106.06811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Case Study on Detecting COVID-19 Health-Related Misinformation in Social  Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pritom%2C+M+M+A">Mir Mehedi A. Pritom</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+R+M">Rosana Montanez Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A+A">Asad Ali Khan</a>, 
<a href="/search/cs?searchtype=author&query=Nugroho%2C+S+A">Sebastian A. Nugroho</a>, 
<a href="/search/cs?searchtype=author&query=Alrashydah%2C+E">Esra&#x27;a Alrashydah</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz%2C+B+N">Beatrice N. Ruiz</a>, 
<a href="/search/cs?searchtype=author&query=Rios%2C+A">Anthony Rios</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">COVID-19 pandemic has generated what public health officials called an
infodemic of misinformation. As social distancing and stay-at-home orders came
into effect, many turned to social media for socializing. This increase in
social media usage has made it a prime vehicle for the spreading of
misinformation. This paper presents a mechanism to detect COVID-19
health-related misinformation in social media following an interdisciplinary
approach. Leveraging social psychology as a foundation and existing
misinformation frameworks, we defined misinformation themes and associated
keywords incorporated into the misinformation detection mechanism using applied
machine learning techniques. Next, using the Twitter dataset, we explored the
performance of the proposed methodology using multiple state-of-the-art machine
learning classifiers. Our method shows promising results with at most 78%
accuracy in classifying health-related misinformation versus true information
using uni-gram-based NLP feature generations from tweets and the Decision Tree
classifier. We also provide suggestions on alternatives for countering
misinformation and ethical consideration for the study.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06815" title="Abstract">arXiv:2106.06815</a> [<a href="/pdf/2106.06815" title="Download PDF">pdf</a>, <a href="/ps/2106.06815" title="Download PostScript">ps</a>, <a href="/format/2106.06815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying the Conceptual Error in Dimensionality Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanika%2C+T">Tom Hanika</a>, 
<a href="/search/cs?searchtype=author&query=Hirth%2C+J">Johannes Hirth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
<p class="mathjax">Dimension reduction of data sets is a standard problem in the realm of
machine learning and knowledge reasoning. They affect patterns in and
dependencies on data dimensions and ultimately influence any decision-making
processes. Therefore, a wide variety of reduction procedures are in use, each
pursuing different objectives. A so far not considered criterion is the
conceptual continuity of the reduction mapping, i.e., the preservation of the
conceptual structure with respect to the original data set. Based on the notion
scale-measure from formal concept analysis we present in this work a) the
theoretical foundations to detect and quantify conceptual errors in data
scalings; b) an experimental investigation of our approach on eleven data sets
that were respectively treated with a variant of non-negative matrix
factorization.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06816" title="Abstract">arXiv:2106.06816</a> [<a href="/pdf/2106.06816" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System Identification and Model-based Robust Nonlinear Disturbance  Rejection Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oveisi%2C+A">Atta Oveisi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD Dissertation, 238 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">The robust disturbance rejection controller has been the subject of intensive
research due to its undeniable importance for automation. Modern control theory
tends to use model-based approaches versus model-free approaches, especially
when it comes to highly modern applications. The backbone of the dissertation
is based on the systematic modeling of dynamic systems and the development of
advanced control methods. Accordingly, the dissertation begins with the
investigation of nonlinearities in dynamic systems. The extension of classic
subspace algorithms for linear systems in the frequency domain is tackled using
the new local polynomial approach. Next, the problem of disturbance control is
addressed, namely modeling of uncertainties and non-modeled high-order
dynamics, fragility of the controller and observer systems, and the
non-linearities are analyzed separately.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06817" title="Abstract">arXiv:2106.06817</a> [<a href="/pdf/2106.06817" title="Download PDF">pdf</a>, <a href="/format/2106.06817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Foveated Video Quality Using Entropic Differencing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yize Jin</a>, 
<a href="/search/cs?searchtype=author&query=Patney%2C+A">Anjul Patney</a>, 
<a href="/search/cs?searchtype=author&query=Bovik%2C+A">Alan Bovik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Virtual Reality is regaining attention due to recent advancements in hardware
technology. Immersive images / videos are becoming widely adopted to carry
omnidirectional visual information. However, due to the requirements for higher
spatial and temporal resolution of real video data, immersive videos require
significantly larger bandwidth consumption. To reduce stresses on bandwidth,
foveated video compression is regaining popularity, whereby the space-variant
spatial resolution of the retina is exploited. Towards advancing the progress
of foveated video compression, we propose a full reference (FR) foveated image
quality assessment algorithm, which we call foveated entropic differencing
(FED), which employs the natural scene statistics of bandpass responses by
applying differences of local entropies weighted by a foveation-based error
sensitivity function. We evaluate the proposed algorithm by measuring the
correlations of the predictions that FED makes against human judgements on the
newly created 2D and 3D LIVE-FBT-FCVR databases for Virtual Reality (VR). The
performance of the proposed algorithm yields state-of-the-art as compared with
other existing full reference algorithms. Software for FED has been made
available at: <a href="http://live.ece.utexas.edu/research/Quality/FED.zip">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06819" title="Abstract">arXiv:2106.06819</a> [<a href="/pdf/2106.06819" title="Download PDF">pdf</a>, <a href="/format/2106.06819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D2C: Diffusion-Denoising Models for Few-shot Conditional Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Abhishek Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiaming Song</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Chenlin Meng</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Conditional generative models of high-dimensional images have many
applications, but supervision signals from conditions to images can be
expensive to acquire. This paper describes Diffusion-Decoding models with
Contrastive representations (D2C), a paradigm for training unconditional
variational autoencoders (VAEs) for few-shot conditional image generation. D2C
uses a learned diffusion-based prior over the latent representations to improve
generation and contrastive self-supervised learning to improve representation
quality. D2C can adapt to novel generation tasks conditioned on labels or
manipulation constraints, by learning from as few as 100 labeled examples. On
conditional generation from new labels, D2C achieves superior performance over
state-of-the-art VAEs and diffusion models. On conditional image manipulation,
D2C generations are two orders of magnitude faster to produce over StyleGAN2
ones and are preferred by 50% - 60% of the human evaluators in a double-blind
study.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06822" title="Abstract">arXiv:2106.06822</a> [<a href="/pdf/2106.06822" title="Download PDF">pdf</a>, <a href="/format/2106.06822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Pseudo Label-wise Attention Network for Automatic ICD Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yifan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+M">Min Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Ying Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Min Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic International Classification of Diseases (ICD) coding is defined as
a kind of text multi-label classification problem, which is difficult because
the number of labels is very large and the distribution of labels is
unbalanced. The label-wise attention mechanism is widely used in automatic ICD
coding because it can assign weights to every word in full Electronic Medical
Records (EMR) for different ICD codes. However, the label-wise attention
mechanism is computational redundant and costly. In this paper, we propose a
pseudo label-wise attention mechanism to tackle the problem. Instead of
computing different attention modes for different ICD codes, the pseudo
label-wise attention mechanism automatically merges similar ICD codes and
computes only one attention mode for the similar ICD codes, which greatly
compresses the number of attention modes and improves the predicted accuracy.
In addition, we apply a more convenient and effective way to obtain the ICD
vectors, and thus our model can predict new ICD codes by calculating the
similarities between EMR vectors and ICD vectors. Extensive experiments show
the superior performance of our model. On the public MIMIC-III dataset and
private Xiangya dataset, our model achieves micro f1 of 0.575 and 0.796,
respectively, which outperforms other competing models. Furthermore, we verify
the ability of our model in predicting new ICD codes. The case study shows how
pseudo label-wise attention works, and demonstrates the effectiveness of pseudo
label-wise attention mechanism.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06823" title="Abstract">arXiv:2106.06823</a> [<a href="/pdf/2106.06823" title="Download PDF">pdf</a>, <a href="/format/2106.06823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Contrastive Explanations for Commonsense Reasoning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paranjape%2C+B">Bhargavi Paranjape</a>, 
<a href="/search/cs?searchtype=author&query=Michael%2C+J">Julian Michael</a>, 
<a href="/search/cs?searchtype=author&query=Ghazvininejad%2C+M">Marjan Ghazvininejad</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2021 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Many commonsense reasoning NLP tasks involve choosing between one or more
possible answers to a question or prompt based on knowledge that is often
implicit. Large pretrained language models (PLMs) can achieve near-human
performance on such tasks, while providing little human-interpretable evidence
of the underlying reasoning they use. In this work, we show how to use these
same models to generate such evidence: inspired by the contrastive nature of
human explanations, we use PLMs to complete explanation prompts which contrast
alternatives according to the key attribute(s) required to justify the correct
answer (for example, peanuts are usually salty while raisins are sweet).
Conditioning model decisions on these explanations improves performance on two
commonsense reasoning benchmarks, as compared to previous non-contrastive
alternatives. These explanations are also judged by humans to be more relevant
for solving the task, and facilitate a novel method to evaluate explanation
faithfulfness.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06828" title="Abstract">arXiv:2106.06828</a> [<a href="/pdf/2106.06828" title="Download PDF">pdf</a>, <a href="/format/2106.06828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Game-Theoretic Approach to Multi-Agent Trust Region Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zheng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minne Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A Multi-Agent Trust Region Learning (MATRL) algorithm that augments the single-agent trust region policy optimization with a weak stable fixed point approximated by the policy-space meta-game
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Trust region methods are widely applied in single-agent reinforcement
learning problems due to their monotonic performance-improvement guarantee at
every iteration. Nonetheless, when applied in multi-agent settings, the
guarantee of trust region methods no longer holds because an agent's payoff is
also affected by other agents' adaptive behaviors. To tackle this problem, we
conduct a game-theoretical analysis in the policy space, and propose a
multi-agent trust region learning method (MATRL), which enables trust region
optimization for multi-agent learning. Specifically, MATRL finds a stable
improvement direction that is guided by the solution concept of Nash
equilibrium at the meta-game level. We derive the monotonic improvement
guarantee in multi-agent settings and empirically show the local convergence of
MATRL to stable fixed points in the two-player rotational differential game. To
test our method, we evaluate MATRL in both discrete and continuous multiplayer
general-sum games including checker and switch grid worlds, multi-agent MuJoCo,
and Atari games. Results suggest that MATRL significantly outperforms strong
multi-agent reinforcement learning baselines.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06830" title="Abstract">arXiv:2106.06830</a> [<a href="/pdf/2106.06830" title="Download PDF">pdf</a>, <a href="/format/2106.06830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Entity Disambiguation and the Role of Popularity in  Retrieval-Based NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Anthony Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gudipati%2C+P">Pallavi Gudipati</a>, 
<a href="/search/cs?searchtype=author&query=Longpre%2C+S">Shayne Longpre</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+X">Xiao Ling</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sameer Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Retrieval is a core component for open-domain NLP tasks. In open-domain
tasks, multiple entities can share a name, making disambiguation an inherent
yet under-explored problem. We propose an evaluation benchmark for assessing
the entity disambiguation capabilities of these retrievers, which we call
Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection
of entities that share a name along with queries about those entities. By
covering the set of entities for polysemous names, AmbER sets act as a
challenging test of entity disambiguation. We create AmbER sets for three
popular open-domain tasks: fact checking, slot filling, and question answering,
and evaluate a diverse set of retrievers. We find that the retrievers exhibit
popularity bias, significantly under-performing on rarer entities that share a
name, e.g., they are twice as likely to retrieve erroneous documents on queries
for the less popular entity under the same name. These experiments on AmbER
sets show their utility as an evaluation tool and highlight the weaknesses of
popular retrieval systems.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06831" title="Abstract">arXiv:2106.06831</a> [<a href="/pdf/2106.06831" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward the Optimized Crowdsourcing Strategy for OCR Post-Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suissa%2C+O">Omri Suissa</a>, 
<a href="/search/cs?searchtype=author&query=Elmalech%2C+A">Avshalom Elmalech</a>, 
<a href="/search/cs?searchtype=author&query=Zhitomirsky-Geffet%2C+M">Maayan Zhitomirsky-Geffet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 12 figures, 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Aslib Journal of Information Management, Vol. 72 No. 2, pp.
  179-197 (2020)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Digitization of historical documents is a challenging task in many digital
humanities projects. A popular approach for digitization is to scan the
documents into images, and then convert images into text using Optical
Character Recognition (OCR) algorithms. However, the outcome of OCR processing
of historical documents is usually inaccurate and requires post-processing
error correction. This study investigates how crowdsourcing can be utilized to
correct OCR errors in historical text collections, and which crowdsourcing
methodology is the most effective in different scenarios and for various
research objectives. A series of experiments with different micro-task's
structures and text lengths was conducted with 753 workers on the Amazon's
Mechanical Turk platform. The workers had to fix OCR errors in a selected
historical text. To analyze the results, new accuracy and efficiency measures
have been devised. The analysis suggests that in terms of accuracy, the optimal
text length is medium (paragraph-size) and the optimal structure of the
experiment is two-phase with a scanned image. In terms of efficiency, the best
results were obtained when using longer text in the single-stage structure with
no image. The study provides practical recommendations to researchers on how to
build the optimal crowdsourcing task for OCR post-correction. The developed
methodology can also be utilized to create golden standard historical texts for
automatic OCR post-correction. This is the first attempt to systematically
investigate the influence of various factors on crowdsourcing-based OCR
post-correction and propose an optimal strategy for this process.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06836" title="Abstract">arXiv:2106.06836</a> [<a href="/pdf/2106.06836" title="Download PDF">pdf</a>, <a href="/format/2106.06836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cox Models for Vehicular Networks: SIR Performance and Equivalence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeyaraj%2C+J+P">Jeya Pradha Jeyaraj</a>, 
<a href="/search/cs?searchtype=author&query=Haenggi%2C+M">Martin Haenggi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Transactions on Wireless Communications, vol. 20, no. 1,
  pp. 171-185, Jan. 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We introduce a general framework for the modeling and analysis of vehicular
networks by defining street systems as random 1D subsets of $\mathbb{R}^{2}$.
The street system, in turn, specifies the random intensity measure of a Cox
process of vehicles, i.e., vehicles form independent 1D Poisson point processes
on each street. Models in this Coxian framework can characterize streets of
different lengths and orientations forming intersections or T-junctions. The
lengths of the streets can be infinite or finite and mutually independent or
dependent. We analyze the reliability of communication for different models,
where reliability is the probability that a vehicle at an intersection, a
T-junction, or a general location can receive a message successfully from a
transmitter at a certain distance. Further, we introduce a notion of
equivalence between vehicular models, which means that a representative model
can be used as a proxy for other models in terms of reliability. Specifically,
we prove that the Poisson stick process-based vehicular network is equivalent
to the Poisson line process-based and Poisson lilypond model-based vehicular
networks, and their rotational variants.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06837" title="Abstract">arXiv:2106.06837</a> [<a href="/pdf/2106.06837" title="Download PDF">pdf</a>, <a href="/format/2106.06837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crouzeix-Raviart finite element method for non-autonomous variational  problems with Lavrentiev gap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Balci%2C+A+K">Anna Kh.Balci</a>, 
<a href="/search/math?searchtype=author&query=Ortner%2C+C">Christoph Ortner</a>, 
<a href="/search/math?searchtype=author&query=Storn%2C+J">Johannes Storn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We investigate the convergence of the Crouzeix-Raviart finite element method
for variational problems with non-autonomous integrands that exhibit
non-standard growth conditions. While conforming schemes fail due to the
Lavrentiev gap phenomenon, we prove that the solution of the Crouzeix-Raviart
scheme converges to a global minimiser. Numerical experiments illustrate the
performance of the scheme and give additional analytical insights.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06838" title="Abstract">arXiv:2106.06838</a> [<a href="/pdf/2106.06838" title="Download PDF">pdf</a>, <a href="/ps/2106.06838" title="Download PostScript">ps</a>, <a href="/format/2106.06838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Low-Compexity Deep Learning Framework For Acoustic Scene  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+L">Lam Pham</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hieu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jalali%2C+A">Anahid Jalali</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+A">Alexander Schindler</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+R">Ross King</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>

</div>
<p class="mathjax">In this paper, we presents a low-complexity deep learning frameworks for
acoustic scene classification (ASC). The proposed framework can be separated
into three main steps: Front-end spectrogram extraction, back-end
classification, and late fusion of predicted probabilities. First, we use Mel
filter, Gammatone filter and Constant Q Transfrom (CQT) to transform raw audio
signal into spectrograms, where both frequency and temporal features are
presented. Three spectrograms are then fed into three individual back-end
convolutional neural networks (CNNs), classifying into ten urban scenes.
Finally, a late fusion of three predicted probabilities obtained from three
CNNs is conducted to achieve the final classification result. To reduce the
complexity of our proposed CNN network, we apply two model compression
techniques: model restriction and decomposed convolution. Our extensive
experiments, which are conducted on DCASE 2021 (IEEE AASP Challenge on
Detection and Classification of Acoustic Scenes and Events) Task 1A development
dataset, achieve a low-complexity CNN based framework with 128 KB trainable
parameters and the best classification accuracy of 66.7%, improving DCASE
baseline by 19.0%
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06839" title="Abstract">arXiv:2106.06839</a> [<a href="/pdf/2106.06839" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Vision Based Wear Forecasting on Surfaces of Machine Tool  Elements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schlagenhauf%2C+T">Tobias Schlagenhauf</a>, 
<a href="/search/cs?searchtype=author&query=Burghardt%2C+N">Niklas Burghardt</a>, 
<a href="/search/cs?searchtype=author&query=Fleischer%2C+J">J&#xfc;rgen Fleischer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">This paper addresses the ability to enable machines to automatically detect
failures on machine tool components as well as estimating the severity of the
failures, which is a critical step towards autonomous production machines.
Extracting information about the severity of failures has been a substantial
part of classical, as well as Machine Learning based machine vision systems.
Efforts have been undertaken to automatically predict the severity of failures
on machine tool components for predictive maintenance purposes. Though, most
approaches only partly cover a completely automatic system from detecting
failures to the prognosis of their future severity. To the best of the authors
knowledge, this is the first time a vision-based system for defect detection
and prognosis of failures on metallic surfaces in general and on Ball Screw
Drives in specific has been proposed. The authors show that they can do both,
detect and prognose the evolution of a failure on the surface of a Ball Screw
Drive.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06840" title="Abstract">arXiv:2106.06840</a> [<a href="/pdf/2106.06840" title="Download PDF">pdf</a>, <a href="/ps/2106.06840" title="Download PostScript">ps</a>, <a href="/format/2106.06840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Frameworks Applied For Audio-Visual Scene Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+L">Lam Pham</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+A">Alexander Schindler</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtz%2C+M">Mina Sch&#xfc;tz</a>, 
<a href="/search/cs?searchtype=author&query=Lampert%2C+J">Jasmin Lampert</a>, 
<a href="/search/cs?searchtype=author&query=Schlarb%2C+S">Sven Schlarb</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+R">Ross King</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>

</div>
<p class="mathjax">In this paper, we present deep learning frameworks for audio-visual scene
classification (SC) and indicate how individual visual and audio features as
well as their combination affect SC performance. Our extensive experiments,
which are conducted on DCASE (IEEE AASP Challenge on Detection and
Classification of Acoustic Scenes and Events) Task 1B development dataset,
achieve the best classification accuracy of 82.2%, 91.1%, and 93.9% with audio
input only, visual input only, and both audio-visual input, respectively. The
highest classification accuracy of 93.9%, obtained from an ensemble of
audio-based and visual-based frameworks, shows an improvement of 16.5% compared
with DCASE baseline.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06842" title="Abstract">arXiv:2106.06842</a> [<a href="/pdf/2106.06842" title="Download PDF">pdf</a>, <a href="/format/2106.06842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recomposing the Reinforcement Learning Building Blocks with  Hypernetworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keynan%2C+S">Shai Keynan</a>, 
<a href="/search/cs?searchtype=author&query=Sarafian%2C+E">Elad Sarafian</a>, 
<a href="/search/cs?searchtype=author&query=Kraus%2C+S">Sarit Kraus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The Reinforcement Learning (RL) building blocks, i.e. Q-functions and policy
networks, usually take elements from the cartesian product of two domains as
input. In particular, the input of the Q-function is both the state and the
action, and in multi-task problems (Meta-RL) the policy can take a state and a
context. Standard architectures tend to ignore these variables' underlying
interpretations and simply concatenate their features into a single vector. In
this work, we argue that this choice may lead to poor gradient estimation in
actor-critic algorithms and high variance learning steps in Meta-RL algorithms.
To consider the interaction between the input variables, we suggest using a
Hypernetwork architecture where a primary network determines the weights of a
conditional dynamic network. We show that this approach improves the gradient
approximation and reduces the learning step variance, which both accelerates
learning and improves the final performance. We demonstrate a consistent
improvement across different locomotion tasks and different algorithms both in
RL (TD3 and SAC) and in Meta-RL (MAML and PEARL).
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06843" title="Abstract">arXiv:2106.06843</a> [<a href="/pdf/2106.06843" title="Download PDF">pdf</a>, <a href="/ps/2106.06843" title="Download PostScript">ps</a>, <a href="/format/2106.06843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning on Non-IID Data: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hangyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinjin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaochu Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning is an emerging distributed machine learning framework for
privacy preservation. However, models trained in federated learning usually
have worse performance than those trained in the standard centralized learning
mode, especially when the training data are not independent and identically
distributed (Non-IID) on the local devices. In this survey, we pro-vide a
detailed analysis of the influence of Non-IID data on both parametric and
non-parametric machine learning models in both horizontal and vertical
federated learning. In addition, cur-rent research work on handling challenges
of Non-IID data in federated learning are reviewed, and both advantages and
disadvantages of these approaches are discussed. Finally, we suggest several
future research directions before concluding the paper.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06844" title="Abstract">arXiv:2106.06844</a> [<a href="/pdf/2106.06844" title="Download PDF">pdf</a>, <a href="/format/2106.06844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amplifying Privacy: Scaling Up Transparency Research Through Delegated  Access Requests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asghari%2C+H">Hadi Asghari</a>, 
<a href="/search/cs?searchtype=author&query=van+Biemen%2C+T">Thomas van Biemen</a>, 
<a href="/search/cs?searchtype=author&query=Warnier%2C+M">Martijn Warnier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Peer-reviewed and presented at IEEE Workshop on Technology and Consumer Protection 2021 (ConPro '21)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In recent years, numerous studies have used 'data subject access requests' in
a collective manner, to tackle information asymmetries and shed light on data
collection and privacy practices of organizations. While successful at
increasing transparency, such studies are quite hard to conduct for the simple
fact that right of access is an individual right. This means that researchers
have to recruit participants and guide them through the often-cumbersome
process of access. In this paper, we present an alternative method: to ask
participants to delegate their right of access to the researchers. We discuss
the legal grounds for doing this, the advantages it can bring to both
researchers and data subjects, and present a procedural and technical design to
execute it in a manner that ensures data subjects stay informed and in charge
during the process. We tested our method in a pilot study in the Netherlands,
and found that it creates a win-win for both the researchers and the
participants. We also noted differences in how data controllers from various
sectors react to such requests and discuss some remaining challenges.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06845" title="Abstract">arXiv:2106.06845</a> [<a href="/pdf/2106.06845" title="Download PDF">pdf</a>, <a href="/format/2106.06845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harmonization with Flow-based Causal Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rongguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+P">Pratik Chaudhari</a>, 
<a href="/search/cs?searchtype=author&query=Davatzikos%2C+C">Christos Davatzikos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Heterogeneity in medical data, e.g., from data collected at different sites
and with different protocols in a clinical study, is a fundamental hurdle for
accurate prediction using machine learning models, as such models often fail to
generalize well. This paper presents a normalizing-flow-based method to perform
counterfactual inference upon a structural causal model (SCM) to harmonize such
data. We formulate a causal model for observed effects (brain magnetic
resonance imaging data) that result from known confounders (site, gender and
age) and exogenous noise variables. Our method exploits the bijection induced
by flow for harmonization. We can infer the posterior of exogenous variables,
intervene on observations, and draw samples from the resultant SCM to obtain
counterfactuals. We evaluate on multiple, large, real-world medical datasets to
observe that this method leads to better cross-domain generalization compared
to state-of-the-art algorithms. Further experiments that evaluate the quality
of confounder-independent data generated by our model using regression and
classification tasks are provided.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06847" title="Abstract">arXiv:2106.06847</a> [<a href="/pdf/2106.06847" title="Download PDF">pdf</a>, <a href="/format/2106.06847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Super-Resolution Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiezhang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video super-resolution (VSR), with the aim to restore a high-resolution video
from its corresponding low-resolution version, is a spatial-temporal sequence
prediction problem. Recently, Transformer has been gaining popularity due to
its parallel computing ability for sequence-to-sequence modeling. Thus, it
seems to be straightforward to apply the vision Transformer to solve VSR.
However, the typical block design of Transformer with a fully connected
self-attention layer and a token-wise feed-forward layer does not fit well for
VSR due to the following two reasons. First, the fully connected self-attention
layer neglects to exploit the data locality because this layer relies on linear
layers to compute attention maps. Second, the token-wise feed-forward layer
lacks the feature alignment which is important for VSR since this layer
independently processes each of the input token embeddings without any
interaction among them. In this paper, we make the first attempt to adapt
Transformer for VSR. Specifically, to tackle the first issue, we present a
spatial-temporal convolutional self-attention layer with a theoretical
understanding to exploit the locality information. For the second issue, we
design a bidirectional optical flow-based feed-forward layer to discover the
correlations across different video frames and also align features. Extensive
experiments on several benchmark datasets demonstrate the effectiveness of our
proposed method. The code will be available at
https://github.com/caojiezhang/VSR-Transformer.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06848" title="Abstract">arXiv:2106.06848</a> [<a href="/pdf/2106.06848" title="Download PDF">pdf</a>, <a href="/ps/2106.06848" title="Download PostScript">ps</a>, <a href="/format/2106.06848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guaranteed Fixed-Confidence Best Arm Identification in Multi-Armed  Bandit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azizi%2C+M">MohammadJavad Azizi</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+S+M">Sheldon M Ross</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We consider the problem of finding, through adaptive sampling, which of n
populations (arms) has the largest mean. Our objective is to determine a rule
which identifies the best population with a fixed minimum confidence using as
few observations as possible, i.e. fixed-confidence (FC) best arm
identification (BAI) in multi-armed bandits. We study such problems under the
Bayesian setting with both Bernoulli and Gaussian populations. We propose to
use the classical vector at a time (VT) rule, which samples each alive
population once in each round. We show how VT can be implemented and analyzed
in our Bayesian setting and be improved by early elimination. We also propose
and analyze a variant of the classical play the winner (PW) algorithm.
Numerical results show that these rules compare favorably with state-of-art
algorithms.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06849" title="Abstract">arXiv:2106.06849</a> [<a href="/pdf/2106.06849" title="Download PDF">pdf</a>, <a href="/format/2106.06849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Transformer Language Models Predict Psychometric Properties?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laverghetta%2C+A">Antonio Laverghetta Jr.</a>, 
<a href="/search/cs?searchtype=author&query=Nighojkar%2C+A">Animesh Nighojkar</a>, 
<a href="/search/cs?searchtype=author&query=Mirzakhalov%2C+J">Jamshidbek Mirzakhalov</a>, 
<a href="/search/cs?searchtype=author&query=Licato%2C+J">John Licato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 10th Joint Conference on Lexical and Computational Semantics (*SEM 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Transformer-based language models (LMs) continue to advance state-of-the-art
performance on NLP benchmark tasks, including tasks designed to mimic
human-inspired "commonsense" competencies. To better understand the degree to
which LMs can be said to have certain linguistic reasoning skills, researchers
are beginning to adapt the tools and concepts of the field of psychometrics.
But to what extent can the benefits flow in the other direction? I.e., can LMs
be of use in predicting what the psychometric properties of test items will be
when those items are given to human participants? We gather responses from
numerous human participants and LMs (transformer and non-transformer-based) on
a broad diagnostic test of linguistic competencies. We then use the responses
to calculate standard psychometric properties of the items in the diagnostic
test, using the human responses and the LM responses separately. We then
determine how well these two sets of predictions match. We find cases in which
transformer-based LMs predict psychometric properties consistently well in
certain categories but consistently poorly in others, thus providing new
insights into fundamental similarities and differences between human and LM
reasoning.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06854" title="Abstract">arXiv:2106.06854</a> [<a href="/pdf/2106.06854" title="Download PDF">pdf</a>, <a href="/format/2106.06854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Reinforcement Learning Approach to Marginalized Importance  Sampling with the Successor Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+S">Scott Fujimoto</a>, 
<a href="/search/cs?searchtype=author&query=Meger%2C+D">David Meger</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Marginalized importance sampling (MIS), which measures the density ratio
between the state-action occupancy of a target policy and that of a sampling
distribution, is a promising approach for off-policy evaluation. However,
current state-of-the-art MIS methods rely on complex optimization tricks and
succeed mostly on simple toy problems. We bridge the gap between MIS and deep
reinforcement learning by observing that the density ratio can be computed from
the successor representation of the target policy. The successor representation
can be trained through deep reinforcement learning methodology and decouples
the reward optimization from the dynamics of the environment, making the
resulting algorithm stable and applicable to high-dimensional domains. We
evaluate the empirical performance of our approach on a variety of challenging
Atari and MuJoCo environments.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06856" title="Abstract">arXiv:2106.06856</a> [<a href="/pdf/2106.06856" title="Download PDF">pdf</a>, <a href="/format/2106.06856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate  Multi-Camera Multiple Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quach%2C+K+G">Kha Gia Quach</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+P">Pha Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Huu Le</a>, 
<a href="/search/cs?searchtype=author&query=Truong%2C+T">Thanh-Dat Truong</a>, 
<a href="/search/cs?searchtype=author&query=Duong%2C+C+N">Chi Nhan Duong</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+M">Minh-Triet Tran</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+K">Khoa Luu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at CVPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-Camera Multiple Object Tracking (MC-MOT) is a significant computer
vision problem due to its emerging applicability in several real-world
applications. Despite a large number of existing works, solving the data
association problem in any MC-MOT pipeline is arguably one of the most
challenging tasks. Developing a robust MC-MOT system, however, is still highly
challenging due to many practical issues such as inconsistent lighting
conditions, varying object movement patterns, or the trajectory occlusions of
the objects between the cameras. To address these problems, this work,
therefore, proposes a new Dynamic Graph Model with Link Prediction (DyGLIP)
approach to solve the data association task. Compared to existing methods, our
new model offers several advantages, including better feature representations
and the ability to recover from lost tracks during camera transitions.
Moreover, our model works gracefully regardless of the overlapping ratios
between the cameras. Experimental results show that we outperform existing
MC-MOT algorithms by a large margin on several practical datasets. Notably, our
model works favorably on online settings but can be extended to an incremental
approach for large-scale datasets.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06860" title="Abstract">arXiv:2106.06860</a> [<a href="/pdf/2106.06860" title="Download PDF">pdf</a>, <a href="/format/2106.06860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Minimalist Approach to Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+S">Scott Fujimoto</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S+S">Shixiang Shane Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Offline reinforcement learning (RL) defines the task of learning from a fixed
batch of data. Due to errors in value estimation from out-of-distribution
actions, most offline RL algorithms take the approach of constraining or
regularizing the policy with the actions contained in the dataset. Built on
pre-existing RL algorithms, modifications to make an RL algorithm work offline
comes at the cost of additional complexity. Offline RL algorithms introduce new
hyperparameters and often leverage secondary components such as generative
models, while adjusting the underlying RL algorithm. In this paper we aim to
make a deep RL algorithm work while making minimal changes. We find that we can
match the performance of state-of-the-art offline RL algorithms by simply
adding a behavior cloning term to the policy update of an online RL algorithm
and normalizing the data. The resulting algorithm is a simple to implement and
tune baseline, while more than halving the overall run time by removing the
additional computational overheads of previous methods.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06863" title="Abstract">arXiv:2106.06863</a> [<a href="/pdf/2106.06863" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Wavelet Vocoder-based Decomposition of Parametric Speech  Waveform Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Radhi%2C+M+S">Mohammed Salah Al-Radhi</a>, 
<a href="/search/cs?searchtype=author&query=Csap%C3%B3%2C+T+G">Tam&#xe1;s G&#xe1;bor Csap&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Zaink%C3%B3%2C+C">Csaba Zaink&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=N%C3%A9meth%2C+G">G&#xe9;za N&#xe9;meth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, accepted to the conference of Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To date, various speech technology systems have adopted the vocoder approach,
a method for synthesizing speech waveform that shows a major role in the
performance of statistical parametric speech synthesis. WaveNet one of the best
models that nearly resembles the human voice, has to generate a waveform in a
time consuming sequential manner with an extremely complex structure of its
neural networks.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06866" title="Abstract">arXiv:2106.06866</a> [<a href="/pdf/2106.06866" title="Download PDF">pdf</a>, <a href="/format/2106.06866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Implicit Neural Representation for Fonts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reddy%2C+P">Pradyumna Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fisher%2C+M">Matthew Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hailin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+N+J">Niloy J. Mitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Fonts are ubiquitous across documents and come in a variety of styles. They
are either represented in a native vector format or rasterized to produce fixed
resolution images. In the first case, the non-standard representation prevents
benefiting from latest network architectures for neural representations; while,
in the latter case, the rasterized representation, when encoded via networks,
results in loss of data fidelity, as font-specific discontinuities like edges
and corners are difficult to represent using neural networks. Based on the
observation that complex fonts can be represented by a superposition of a set
of simpler occupancy functions, we introduce \textit{multi-implicits} to
represent fonts as a permutation-invariant set of learned implict functions,
without losing features (e.g., edges and corners). However, while
multi-implicits locally preserve font features, obtaining supervision in the
form of ground truth multi-channel signals is a problem in itself. Instead, we
propose how to train such a representation with only local supervision, while
the proposed neural architecture directly finds globally consistent
multi-implicits for font families. We extensively evaluate the proposed
representation for various tasks including reconstruction, interpolation, and
synthesis to demonstrate clear advantages with existing alternatives.
Additionally, the representation naturally enables glyph completion, wherein a
single characteristic font is used to synthesize a whole font family in the
target style.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06867" title="Abstract">arXiv:2106.06867</a> [<a href="/pdf/2106.06867" title="Download PDF">pdf</a>, <a href="/format/2106.06867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spatially Dependent Probabilistic Model for House Hunting in Ant  Colonies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+G">Grace Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wendy Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wayne Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiajia Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lynch%2C+N">Nancy Lynch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Ant species such as Temnothorax albipennis select a new nest site in a
distributed fashion that, if modeled correctly, can serve as useful information
for site selection algorithms for robotic swarms and other applications.
Studying and replicating the ants' house hunting behavior will also illuminate
useful distributed strategies that have evolved in nature. Many of the existing
models of househunting behaviour for T. albipennis make the assumption that all
candidate nest sites are equally distant from the ants' home nest, or that an
ant has an equal probability of finding each candidate nest site. However,
realistically this is not the case, as nests that are further away from the
home nest and nests that are difficult to access are less likely to be found,
even if they are of higher quality. We extend previous house-hunting models to
account for a pairwise distance metric between nests, compare our results to
those of real colonies, and use our results to examine the effects of house
hunting in nests of different spatial orientations. Our incorporation of
distances in the ant model appear to match empirical data in situations where a
distance-quality tradeoff between nests is relevant. Furthermore, the model
continues to be on par with previous house-hunting models in experiments where
all candidate nests are equidistant from the home nest, as is typically
assumed.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06868" title="Abstract">arXiv:2106.06868</a> [<a href="/pdf/2106.06868" title="Download PDF">pdf</a>, <a href="/format/2106.06868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Short-term forecasting of global solar irradiance with incomplete data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoyos-G%C3%B3mez%2C+L+S">Laura S. Hoyos-G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz-Mu%C3%B1oz%2C+J+F">Jose F. Ruiz-Mu&#xf1;oz</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz-Mendoza%2C+B+J">Belizza J. Ruiz-Mendoza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Accurate mechanisms for forecasting solar irradiance and insolation provide
important information for the planning of renewable energy and agriculture
projects as well as for environmental and socio-economical studies. This
research introduces a pipeline for the one-day ahead forecasting of solar
irradiance and insolation that only requires solar irradiance historical data
for training. Furthermore, our approach is able to deal with missing data since
it includes a data imputation state. In the prediction stage, we consider four
data-driven approaches: Autoregressive Integrated Moving Average (ARIMA),
Single Layer Feed Forward Network (SL-FNN), Multiple Layer Feed Forward Network
(FL-FNN), and Long Short-Term Memory (LSTM). The experiments are performed in a
real-world dataset collected with 12 Automatic Weather Stations (AWS) located
in the Nari\~no - Colombia. The results show that the neural network-based
models outperform ARIMA in most cases. Furthermore, LSTM exhibits better
performance in cloudy environments (where more randomness is expected).
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06873" title="Abstract">arXiv:2106.06873</a> [<a href="/pdf/2106.06873" title="Download PDF">pdf</a>, <a href="/format/2106.06873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly-supervised Graph Meta-learning for Few-shot Node Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Kaize Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jundong Li</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graphs are widely used to model the relational structure of data, and the
research of graph machine learning (ML) has a wide spectrum of applications
ranging from drug design in molecular graphs to friendship recommendation in
social networks. Prevailing approaches for graph ML typically require abundant
labeled instances in achieving satisfactory results, which is commonly
infeasible in real-world scenarios since labeled data for newly emerged
concepts (e.g., new categorizations of nodes) on graphs is limited. Though
meta-learning has been applied to different few-shot graph learning problems,
most existing efforts predominately assume that all the data from those seen
classes is gold-labeled, while those methods may lose their efficacy when the
seen data is weakly-labeled with severe label noise. As such, we aim to
investigate a novel problem of weakly-supervised graph meta-learning for
improving the model robustness in terms of knowledge transfer. To achieve this
goal, we propose a new graph meta-learning framework -- Graph Hallucination
Networks (Meta-GHN) in this paper. Based on a new robustness-enhanced episodic
training, Meta-GHN is meta-learned to hallucinate clean node representations
from weakly-labeled data and extracts highly transferable meta-knowledge, which
enables the model to quickly adapt to unseen tasks with few labeled instances.
Extensive experiments demonstrate the superiority of Meta-GHN over existing
graph meta-learning studies on the task of weakly-supervised few-shot node
classification.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06875" title="Abstract">arXiv:2106.06875</a> [<a href="/pdf/2106.06875" title="Download PDF">pdf</a>, <a href="/format/2106.06875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine  Translation Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+R">Rajat Bhatnagar</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+A">Ananya Ganesh</a>, 
<a href="/search/cs?searchtype=author&query=Kann%2C+K">Katharina Kann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure, ACL-IJCNLP 2021 submission, Natural Language Processing, Data Collection, Monolingual Speakers, Machine Translation, GIFs, Images
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">High-performing machine translation (MT) systems can help overcome language
barriers while making it possible for everyone to communicate and use language
technologies in the language of their choice. However, such systems require
large amounts of parallel sentences for training, and translators can be
difficult to find and expensive. Here, we present a data collection strategy
for MT which, in contrast, is cheap and simple, as it does not require
bilingual speakers. Based on the insight that humans pay specific attention to
movements, we use graphics interchange formats (GIFs) as a pivot to collect
parallel sentences from monolingual annotators. We use our strategy to collect
data in Hindi, Tamil and English. As a baseline, we also collect data using
images as a pivot. We perform an intrinsic evaluation by manually evaluating a
subset of the sentence pairs and an extrinsic evaluation by finetuning mBART on
the collected data. We find that sentences collected via GIFs are indeed of
higher quality.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06876" title="Abstract">arXiv:2106.06876</a> [<a href="/pdf/2106.06876" title="Download PDF">pdf</a>, <a href="/format/2106.06876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Affine OneMax
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berny%2C+A">Arnaud Berny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An extended two-page abstract of this work will appear in 2021 Genetic and Evolutionary Computation Conference Companion (GECCO '21 Companion)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">A new class of test functions for black box optimization is introduced.
Affine OneMax (AOM) functions are defined as compositions of OneMax and
invertible affine maps on bit vectors. The black box complexity of the class is
upper bounded by a polynomial of large degree in the dimension. The proof
relies on discrete Fourier analysis and the Kushilevitz-Mansour algorithm.
Tunable complexity is achieved by expressing invertible linear maps as finite
products of transvections. The black box complexity of sub-classes of AOM
functions is studied. Finally, experimental results are given to illustrate the
performance of search algorithms on AOM functions.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06878" title="Abstract">arXiv:2106.06878</a> [<a href="/pdf/2106.06878" title="Download PDF">pdf</a>, <a href="/format/2106.06878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Group Testing with a Linear Number of Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Flodin%2C+L">Larkin Flodin</a>, 
<a href="/search/cs?searchtype=author&query=Mazumdar%2C+A">Arya Mazumdar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure. To be published in ISIT 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In probabilistic nonadaptive group testing (PGT), we aim to characterize the
number of pooled tests necessary to identify a random $k$-sparse vector of
defectives with high probability. Recent work has shown that $n$ tests are
necessary when $k =\omega(n/\log n)$. It is also known that $O(k \log n)$ tests
are necessary and sufficient in other regimes. This leaves open the important
sparsity regime where the probability of a defective item is $\sim 1/\log n$
(or $k = \Theta(n/\log n)$) where the number of tests required is linear in
$n$. In this work we aim to exactly characterize the number of tests in this
sparsity regime. In particular, we seek to determine the number of defectives
$\lambda(\alpha)n / \log n$ that can be identified if the number of tests is
$\alpha n$. In the process, we give upper and lower bounds on the exact point
at which individual testing becomes suboptimal, and the use of a carefully
constructed pooled test design is beneficial.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06880" title="Abstract">arXiv:2106.06880</a> [<a href="/pdf/2106.06880" title="Download PDF">pdf</a>, <a href="/format/2106.06880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Shuffling Beats SGD Only After Many Epochs on Ill-Conditioned  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Safran%2C+I">Itay Safran</a>, 
<a href="/search/cs?searchtype=author&query=Shamir%2C+O">Ohad Shamir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, there has been much interest in studying the convergence rates of
without-replacement SGD, and proving that it is faster than with-replacement
SGD in the worst case. However, these works ignore or do not provide tight
bounds in terms of the problem's geometry, including its condition number.
Perhaps surprisingly, we prove that when the condition number is taken into
account, without-replacement SGD \emph{does not} significantly improve on
with-replacement SGD in terms of worst-case bounds, unless the number of epochs
(passes over the data) is larger than the condition number. Since many problems
in machine learning and other areas are both ill-conditioned and involve large
datasets, this indicates that without-replacement does not necessarily improve
over with-replacement sampling for realistic iteration budgets. We show this by
providing new lower and upper bounds which are tight (up to log factors), for
quadratic problems with commuting quadratic terms, precisely quantifying the
dependence on the problem parameters.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06882" title="Abstract">arXiv:2106.06882</a> [<a href="/pdf/2106.06882" title="Download PDF">pdf</a>, <a href="/format/2106.06882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse PointPillars: Exploiting Sparsity in Birds-Eye-View Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vedder%2C+K">Kyle Vedder</a>, 
<a href="/search/cs?searchtype=author&query=Eaton%2C+E">Eric Eaton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Bird's Eye View (BEV) is a popular representation for processing 3D point
clouds, and by its nature is fundamentally sparse. Motivated by the
computational limitations of mobile robot platforms, we take a fast
high-performance BEV 3D object detector - PointPillars - and modify its
backbone to exploit this sparsity, leading to decreased runtimes. We present
preliminary results demonstrating decreased runtimes with either the same
performance or a modest decrease in performance, which we anticipate will be
remedied by model specific hyperparameter tuning. Our work is a first step
towards a new class of 3D object detectors that exploit sparsity throughout
their entire pipeline in order to reduce runtime and resource usage while
maintaining good detection performance.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06885" title="Abstract">arXiv:2106.06885</a> [<a href="/pdf/2106.06885" title="Download PDF">pdf</a>, <a href="/format/2106.06885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Learning with Optimism and Delay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Flaspohler%2C+G">Genevieve Flaspohler</a>, 
<a href="/search/cs?searchtype=author&query=Orabona%2C+F">Francesco Orabona</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+J">Judah Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Mouatadid%2C+S">Soukayna Mouatadid</a>, 
<a href="/search/cs?searchtype=author&query=Oprescu%2C+M">Miruna Oprescu</a>, 
<a href="/search/cs?searchtype=author&query=Orenstein%2C+P">Paulo Orenstein</a>, 
<a href="/search/cs?searchtype=author&query=Mackey%2C+L">Lester Mackey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021. 9 pages of main paper and 26 pages of appendix text
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Inspired by the demands of real-time climate and weather forecasting, we
develop optimistic online learning algorithms that require no parameter tuning
and have optimal regret guarantees under delayed feedback. Our algorithms --
DORM, DORMP, and AdaHedgeD -- arise from a novel reduction of delayed online
learning to optimistic online learning that reveals how optimistic hints can
mitigate the regret penalty caused by delay. We pair this delay-as-optimism
perspective with a new analysis of optimistic learning that exposes its
robustness to hinting errors and a new meta-algorithm for learning effective
hinting strategies in the presence of delay. We conclude by benchmarking our
algorithms on four subseasonal climate forecasting tasks, demonstrating low
regret relative to state-of-the-art forecasting models.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06887" title="Abstract">arXiv:2106.06887</a> [<a href="/pdf/2106.06887" title="Download PDF">pdf</a>, <a href="/format/2106.06887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Spatio-Temporal Poisson Point Process: A Simple Model for the  Alignment of Event Camera Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+C">Cheng Gu</a>, 
<a href="/search/cs?searchtype=author&query=Learned-Miller%2C+E">Erik Learned-Miller</a>, 
<a href="/search/cs?searchtype=author&query=Sheldon%2C+D">Daniel Sheldon</a>, 
<a href="/search/cs?searchtype=author&query=Gallego%2C+G">Guillermo Gallego</a>, 
<a href="/search/cs?searchtype=author&query=Bideau%2C+P">Pia Bideau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Event cameras, inspired by biological vision systems, provide a natural and
data efficient representation of visual information. Visual information is
acquired in the form of events that are triggered by local brightness changes.
Each pixel location of the camera's sensor records events asynchronously and
independently with very high temporal resolution. However, because most
brightness changes are triggered by relative motion of the camera and the
scene, the events recorded at a single sensor location seldom correspond to the
same world point. To extract meaningful information from event cameras, it is
helpful to register events that were triggered by the same underlying world
point. In this work we propose a new model of event data that captures its
natural spatio-temporal structure. We start by developing a model for aligned
event data. That is, we develop a model for the data as though it has been
perfectly registered already. In particular, we model the aligned data as a
spatio-temporal Poisson point process. Based on this model, we develop a
maximum likelihood approach to registering events that are not yet aligned.
That is, we find transformations of the observed events that make them as
likely as possible under our model. In particular we extract the camera
rotation that leads to the best event alignment. We show new state of the art
accuracy for rotational velocity estimation on the DAVIS 240C dataset. In
addition, our method is also faster and has lower computational complexity than
several competing methods.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06889" title="Abstract">arXiv:2106.06889</a> [<a href="/pdf/2106.06889" title="Download PDF">pdf</a>, <a href="/format/2106.06889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G-TADOC: Enabling Efficient GPU-Based Text Analytics without  Decompression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Feng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zaifeng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yanliang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+J">Jidong Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xipeng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xiaoyong Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th IEEE International Conference on Data Engineering (ICDE 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Text analytics directly on compression (TADOC) has proven to be a promising
technology for big data analytics. GPUs are extremely popular accelerators for
data analytics systems. Unfortunately, no work so far shows how to utilize GPUs
to accelerate TADOC. We describe G-TADOC, the first framework that provides
GPU-based text analytics directly on compression, effectively enabling
efficient text analytics on GPUs without decompressing the input data. G-TADOC
solves three major challenges. First, TADOC involves a large amount of
dependencies, which makes it difficult to exploit massive parallelism on a GPU.
We develop a novel fine-grained thread-level workload scheduling strategy for
GPU threads, which partitions heavily-dependent loads adaptively in a
fine-grained manner. Second, in developing G-TADOC, thousands of GPU threads
writing to the same result buffer leads to inconsistency while directly using
locks and atomic operations lead to large synchronization overheads. We develop
a memory pool with thread-safe data structures on GPUs to handle such
difficulties. Third, maintaining the sequence information among words is
essential for lossless compression. We design a sequence-support strategy,
which maintains high GPU parallelism while ensuring sequence information. Our
experimental evaluations show that G-TADOC provides 31.1x average speedup
compared to state-of-the-art TADOC.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06892" title="Abstract">arXiv:2106.06892</a> [<a href="/pdf/2106.06892" title="Download PDF">pdf</a>, <a href="/ps/2106.06892" title="Download PostScript">ps</a>, <a href="/format/2106.06892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Guarantees for Offline Stochastic Matching via new Ordered  Contention Resolution Schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brubach%2C+B">Brian Brubach</a>, 
<a href="/search/cs?searchtype=author&query=Grammel%2C+N">Nathaniel Grammel</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Will Ma</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+A">Aravind Srinivasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Matching is one of the most fundamental and broadly applicable problems
across many domains. In these diverse real-world applications, there is often a
degree of uncertainty in the input which has led to the study of stochastic
matching models. Here, each edge in the graph has a known, independent
probability of existing derived from some prediction. Algorithms must probe
edges to determine existence and match them irrevocably if they exist. Further,
each vertex may have a patience constraint denoting how many of its neighboring
edges can be probed. We present new ordered contention resolution schemes
yielding improved approximation guarantees for some of the foundational
problems studied in this area. For stochastic matching with patience
constraints in general graphs, we provide a 0.382-approximate algorithm,
significantly improving over the previous best 0.31-approximation of Baveja et
al. (2018). When the vertices do not have patience constraints, we describe a
0.432-approximate random order probing algorithm with several corollaries such
as an improved guarantee for the Prophet Secretary problem under Edge Arrivals.
Finally, for the special case of bipartite graphs with unit patience
constraints on one of the partitions, we show a 0.632-approximate algorithm
that improves on the recent $1/3$-guarantee of Hikima et al. (2021).
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06895" title="Abstract">arXiv:2106.06895</a> [<a href="/pdf/2106.06895" title="Download PDF">pdf</a>, <a href="/format/2106.06895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Odetola%2C+T">Tolulope Odetola</a>, 
<a href="/search/cs?searchtype=author&query=Khalid%2C+F">Faiq Khalid</a>, 
<a href="/search/cs?searchtype=author&query=Sandefur%2C+T">Travis Sandefur</a>, 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+H">Hawzhin Mohammed</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+S+R">Syed Rafay Hasan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Convolutional Neural Networks (CNN) have shown impressive performance in
computer vision, natural language processing, and many other applications, but
they exhibit high computations and substantial memory requirements. To address
these limitations, especially in resource-constrained devices, the use of cloud
computing for CNNs is becoming more popular. This comes with privacy and
latency concerns that have motivated the designers to develop embedded hardware
accelerators for CNNs. However, designing a specialized accelerator increases
the time-to-market and cost of production. Therefore, to reduce the
time-to-market and access to state-of-the-art techniques, CNN hardware mapping
and deployment on embedded accelerators are often outsourced to untrusted third
parties, which is going to be more prevalent in futuristic artificial
intelligence of things (AIoT) systems. These AIoT systems anticipate horizontal
collaboration among different resource-constrained AIoT node devices, where CNN
layers are partitioned and these devices collaboratively compute complex CNN
tasks Therefore, there is a dire need to explore this attack surface for
designing secure embedded hardware accelerators for CNNs. Towards this goal, in
this paper, we exploited this attack surface to propose an HT-based attack
called FeSHI. This attack exploits the statistical distribution i.e., Gaussian
distribution, of the layer-by-layer feature maps of the CNN to design two
triggers for stealthy HT with a very low probability of triggering. To
illustrate the effectiveness of the proposed attack, we deployed the LeNet and
LeNet-3D on PYNQ to classify the MNIST and CIFAR-10 datasets, respectively, and
tested FeSHI. The experimental results show that FeSHI utilizes up to 2% extra
LUTs, and the overall resource overhead is less than 1% compared to the
original designs
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06896" title="Abstract">arXiv:2106.06896</a> [<a href="/pdf/2106.06896" title="Download PDF">pdf</a>, <a href="/format/2106.06896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperspectral and Multispectral Classification for Coastal Wetland Using  Depthwise Feature Interaction Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yunhao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengmeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianbu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weiwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+R">Ran Tao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Q">Qian Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06898" title="Abstract">arXiv:2106.06898</a> [<a href="/pdf/2106.06898" title="Download PDF">pdf</a>, <a href="/format/2106.06898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Markov Neural Operators for Learning Chaotic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Kovachki%2C+N">Nikola Kovachki</a>, 
<a href="/search/cs?searchtype=author&query=Azizzadenesheli%2C+K">Kamyar Azizzadenesheli</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Burigede Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+K">Kaushik Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Stuart%2C+A">Andrew Stuart</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">Chaotic systems are notoriously challenging to predict because of their
instability. Small errors accumulate in the simulation of each time step,
resulting in completely different trajectories. However, the trajectories of
many prominent chaotic systems live in a low-dimensional subspace (attractor).
If the system is Markovian, the attractor is uniquely determined by the Markov
operator that maps the evolution of infinitesimal time steps. This makes it
possible to predict the behavior of the chaotic system by learning the Markov
operator even if we cannot predict the exact trajectory. Recently, a new
framework for learning resolution-invariant solution operators for PDEs was
proposed, known as neural operators. In this work, we train a Markov neural
operator (MNO) with only the local one-step evolution information. We then
compose the learned operator to obtain the global attractor and invariant
measure. Such a Markov neural operator forms a discrete semigroup and we
empirically observe that does not collapse or blow up. Experiments show neural
operators are more accurate and stable compared to previous methods on chaotic
systems such as the Kuramoto-Sivashinsky and Navier-Stokes equations.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06899" title="Abstract">arXiv:2106.06899</a> [<a href="/pdf/2106.06899" title="Download PDF">pdf</a>, <a href="/format/2106.06899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-efficient Transformers via Top-$k$ Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Ankit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Dar%2C+G">Guy Dar</a>, 
<a href="/search/cs?searchtype=author&query=Goodman%2C+S">Shaya Goodman</a>, 
<a href="/search/cs?searchtype=author&query=Ciprut%2C+D">David Ciprut</a>, 
<a href="/search/cs?searchtype=author&query=Berant%2C+J">Jonathan Berant</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Following the success of dot-product attention in Transformers, numerous
approximations have been recently proposed to address its quadratic complexity
with respect to the input length. While these variants are memory and compute
efficient, it is not possible to directly use them with popular pre-trained
language models trained using vanilla attention, without an expensive
corrective pre-training stage. In this work, we propose a simple yet highly
accurate approximation for vanilla attention. We process the queries in chunks,
and for each query, compute the top-$k$ scores with respect to the keys. Our
approach offers several advantages: (a) its memory usage is linear in the input
size, similar to linear attention variants, such as Performer and RFA (b) it is
a drop-in replacement for vanilla attention that does not require any
corrective pre-training, and (c) it can also lead to significant memory savings
in the feed-forward layers after casting them into the familiar query-key-value
framework. We evaluate the quality of top-$k$ approximation for multi-head
attention layers on the Long Range Arena Benchmark, and for feed-forward layers
of T5 and UnifiedQA on multiple QA datasets. We show our approach leads to
accuracy that is nearly-identical to vanilla attention in multiple setups
including training from scratch, fine-tuning, and zero-shot inference.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06900" title="Abstract">arXiv:2106.06900</a> [<a href="/pdf/2106.06900" title="Download PDF">pdf</a>, <a href="/format/2106.06900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning based Group Recommender System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zefang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+S">Shuran Wen</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+Y">Yinzhu Quan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Group recommender systems are widely used in current web applications. In
this paper, we propose a novel group recommender system based on the deep
reinforcement learning. We introduce the MovieLens data at first and generate
one random group dataset, MovieLens-Rand, from it. This randomly generated
dataset is described and analyzed. We also present experimental settings and
two state-of-art baselines, AGREE and GroupIM. The framework of our novel
model, the Deep Reinforcement learning based Group Recommender system (DRGR),
is proposed. Actor-critic networks are implemented with the deep deterministic
policy gradient algorithm. The DRGR model is applied on the MovieLens-Rand
dataset with two baselines. Compared with baselines, we conclude that DRGR
performs better than GroupIM due to long interaction histories but worse than
AGREE because of the self-attention mechanism. We express advantages and
shortcomings of DRGR and also give future improvement directions at the end.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06901" title="Abstract">arXiv:2106.06901</a> [<a href="/pdf/2106.06901" title="Download PDF">pdf</a>, <a href="/ps/2106.06901" title="Download PostScript">ps</a>, <a href="/format/2106.06901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-User Communication with Extremely Large-Scale MIMO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haiquan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yong Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Extremely large-scale multiple-input multiple-output (XL-MIMO) communication
aims to further boost the antenna size significantly than current massive MIMO
systems, for which conventional far-field assumption with uniform plane wave
(UPW) model may become invalid. This paper studies the modelling and
performance analysis for multi-user XL-MIMO communication. With the spherical
wavefront phase modelling, and also by taking into account the variations of
signal amplitude and projected aperture across array elements, the performance
of the three typical beamforming schemes are analyzed, namely the maximal-ratio
combining (MRC), zero-forcing (ZF), and minimum mean-square error (MMSE)
beamforming. For the special case of two-users, we analytically show that the
signal-to-interference-plus-noise ratio (SINR) of all the three beamforming
schemes increases as the channels' correlation coefficient decreases.
Furthermore, compared to existing UPW model where inter-user interference (IUI)
can only be suppressed in angular domain, XL-MIMO enables a new
degree-of-freedom (DoF) for IUI suppression by distance separation, even for
users along the same direction. Simulation results are provided to validate the
modelling and performance analysis of multi-user XL-MIMO communications.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06905" title="Abstract">arXiv:2106.06905</a> [<a href="/pdf/2106.06905" title="Download PDF">pdf</a>, <a href="/format/2106.06905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfoBehavior: Self-supervised Representation Learning for Ultra-long  Behavior Sequence via Hierarchical Grouping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Runshi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+P">Pengda Qin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Weigao Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dong Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+K">Kefeng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiang Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">E-commerce companies have to face abnormal sellers who sell potentially-risky
products. Typically, the risk can be identified by jointly considering product
content (e.g., title and image) and seller behavior. This work focuses on
behavior feature extraction as behavior sequences can provide valuable clues
for the risk discovery by reflecting the sellers' operation habits. Traditional
feature extraction techniques heavily depend on domain experts and adapt poorly
to new tasks. In this paper, we propose a self-supervised method InfoBehavior
to automatically extract meaningful representations from ultra-long raw
behavior sequences instead of the costly feature selection procedure.
InfoBehavior utilizes Bidirectional Transformer as feature encoder due to its
excellent capability in modeling long-term dependency. However, it is
intractable for commodity GPUs because the time and memory required by
Transformer grow quadratically with the increase of sequence length. Thus, we
propose a hierarchical grouping strategy to aggregate ultra-long raw behavior
sequences to length-processable high-level embedding sequences. Moreover, we
introduce two types of pretext tasks. Sequence-related pretext task defines a
contrastive-based training objective to correctly select the masked-out
coarse-grained/fine-grained behavior sequences against other "distractor"
behavior sequences; Domain-related pretext task designs a classification
training objective to correctly predict the domain-specific statistical results
of anomalous behavior. We show that behavior representations from the
pre-trained InfoBehavior can be directly used or integrated with features from
other side information to support a wide range of downstream tasks.
Experimental results demonstrate that InfoBehavior significantly improves the
performance of Product Risk Management and Intellectual Property Protection.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06906" title="Abstract">arXiv:2106.06906</a> [<a href="/pdf/2106.06906" title="Download PDF">pdf</a>, <a href="/ps/2106.06906" title="Download PostScript">ps</a>, <a href="/format/2106.06906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Sensor Precision for Multi-Rate Sensing for Bounded Estimation  Error
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Das%2C+N">Niladri Das</a>, 
<a href="/search/eess?searchtype=author&query=Bhattacharya%2C+R">Raktim Bhattacharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP); Optimization and Control (math.OC)

</div>
<p class="mathjax">We address the problem of determining optimal sensor precisions for
estimating the states of linear time-varying discrete-time stochastic dynamical
systems, with guaranteed bounds on the estimation errors. This is performed in
the Kalman filtering framework, where the sensor precisions are treated as
variables. They are determined by solving a constrained convex optimization
problem, which guarantees the specified upper bound on the posterior error
variance. Optimal sensor precisions are determined by minimizing the l1 norm,
which promotes sparseness in the solution and indirectly addresses the sensor
selection problem. The theory is applied to realistic flight mechanics and
astrodynamics problems to highlight its engineering value. These examples
demonstrate the application of the presented theory to a) determine redundant
sensing architectures for linear time invariant systems, b) accurately estimate
states with low-cost sensors, and c) optimally schedule sensors for linear
time-varying systems.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06907" title="Abstract">arXiv:2106.06907</a> [<a href="/pdf/2106.06907" title="Download PDF">pdf</a>, <a href="/format/2106.06907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INADVERT: An Interactive and Adaptive Counterdeception Platform for  Attention Enhancement and Phishing Prevention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Linan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Quanyan Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Deceptive attacks exploiting the innate and the acquired vulnerabilities of
human users have posed severe threats to information and infrastructure
security. This work proposes INADVERT, a systematic solution that generates
interactive visual aids in real-time to prevent users from inadvertence and
counter visual-deception attacks. Based on the eye-tracking outcomes and proper
data compression, the INADVERT platform automatically adapts the visual aids to
the user's varying attention status captured by the gaze location and duration.
We extract system-level metrics to evaluate the user's average attention level
and characterize the magnitude and frequency of the user's mind-wandering
behaviors. These metrics contribute to an adaptive enhancement of the user's
attention through reinforcement learning. To determine the optimal
hyper-parameters in the attention enhancement mechanism, we develop an
algorithm based on Bayesian optimization to efficiently update the design of
the INADVERT platform and maximize the accuracy of the users' phishing
recognition.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06908" title="Abstract">arXiv:2106.06908</a> [<a href="/pdf/2106.06908" title="Download PDF">pdf</a>, <a href="/format/2106.06908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Generalization on Medical Imaging Classification using Episodic  Training with Task Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Q">Qi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xinghao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Dong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yizhou Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Medical imaging datasets usually exhibit domain shift due to the variations
of scanner vendors, imaging protocols, etc. This raises the concern about the
generalization capacity of machine learning models. Domain generalization (DG),
which aims to learn a model from multiple source domains such that it can be
directly generalized to unseen test domains, seems particularly promising to
medical imaging community. To address DG, recent model-agnostic meta-learning
(MAML) has been introduced, which transfers the knowledge from previous
training tasks to facilitate the learning of novel testing tasks. However, in
clinical practice, there are usually only a few annotated source domains
available, which decreases the capacity of training task generation and thus
increases the risk of overfitting to training tasks in the paradigm. In this
paper, we propose a novel DG scheme of episodic training with task augmentation
on medical imaging classification. Based on meta-learning, we develop the
paradigm of episodic training to construct the knowledge transfer from episodic
training-task simulation to the real testing task of DG. Motivated by the
limited number of source domains in real-world medical deployment, we consider
the unique task-level overfitting and we propose task augmentation to enhance
the variety during training task generation to alleviate it. With the
established learning framework, we further exploit a novel meta-objective to
regularize the deep embedding of training domains. To validate the
effectiveness of the proposed method, we perform experiments on
histopathological images and abdominal CT images.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06909" title="Abstract">arXiv:2106.06909</a> [<a href="/pdf/2106.06909" title="Download PDF">pdf</a>, <a href="/format/2106.06909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of  Transcribed Audio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guoguo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+S">Shuzhou Chai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guanbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jiayu Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei-Qiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+C">Chao Weng</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+D">Dan Su</a>, 
<a href="/search/cs?searchtype=author&query=Povey%2C+D">Daniel Povey</a>, 
<a href="/search/cs?searchtype=author&query=Trmal%2C+J">Jan Trmal</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingjie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Khudanpur%2C+S">Sanjeev Khudanpur</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuaijiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+W">Wei Zou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xuchen Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yujun Wang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Z">Zhao You</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiyong Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper introduces GigaSpeech, an evolving, multi-domain English speech
recognition corpus with 10,000 hours of high quality labeled audio suitable for
supervised training, and 40,000 hours of total audio suitable for
semi-supervised and unsupervised training. Around 40,000 hours of transcribed
audio is first collected from audiobooks, podcasts and YouTube, covering both
read and spontaneous speaking styles, and a variety of topics, such as arts,
science, sports, etc. A new forced alignment and segmentation pipeline is
proposed to create sentence segments suitable for speech recognition training,
and to filter out segments with low-quality transcription. For system training,
GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h,
and 10000h. For our 10,000-hour XL training subset, we cap the word error rate
at 4% during the filtering/validation stage, and for all our other smaller
training subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the
other hand, are re-processed by professional human transcribers to ensure high
transcription quality. Baseline systems are provided for popular speech
recognition toolkits, namely Athena, ESPnet, Kaldi and Pika.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06910" title="Abstract">arXiv:2106.06910</a> [<a href="/pdf/2106.06910" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentiment Analysis of Covid-19 Tweets using Evolutionary  Classification-Based LSTM Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+A+K">Arunava Kumar Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sourav Das</a>, 
<a href="/search/cs?searchtype=author&query=Kolya%2C+A+K">Anup Kumar Kolya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, 5 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In RAAI 2020. Advances in Intelligent Systems and Computing, vol
  1355 (2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">As the Covid-19 outbreaks rapidly all over the world day by day and also
affects the lives of million, a number of countries declared complete lock-down
to check its intensity. During this lockdown period, social media plat-forms
have played an important role to spread information about this pandemic across
the world, as people used to express their feelings through the social
networks. Considering this catastrophic situation, we developed an experimental
approach to analyze the reactions of people on Twitter taking into ac-count the
popular words either directly or indirectly based on this pandemic. This paper
represents the sentiment analysis on collected large number of tweets on
Coronavirus or Covid-19. At first, we analyze the trend of public sentiment on
the topics related to Covid-19 epidemic using an evolutionary classification
followed by the n-gram analysis. Then we calculated the sentiment ratings on
collected tweet based on their class. Finally, we trained the long-short term
network using two types of rated tweets to predict sentiment on Covid-19 data
and obtained an overall accuracy of 84.46%.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06911" title="Abstract">arXiv:2106.06911</a> [<a href="/pdf/2106.06911" title="Download PDF">pdf</a>, <a href="/format/2106.06911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interaction-based Convolutional Neural Network (ICNN) Towards Better  Understanding of COVID-19 X-ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+S">Shaw-Hwa Lo</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yiqiao Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The field of Explainable Artificial Intelligence (XAI) aims to build
explainable and interpretable machine learning (or deep learning) methods
without sacrificing prediction performance. Convolutional Neural Networks
(CNNs) have been successful in making predictions, especially in image
classification. However, these famous deep learning models use tens of millions
of parameters based on a large number of pre-trained filters which have been
repurposed from previous data sets. We propose a novel Interaction-based
Convolutional Neural Network (ICNN) that does not make assumptions about the
relevance of local information. Instead, we use a model-free Influence Score
(I-score) to directly extract the influential information from images to form
important variable modules. We demonstrate that the proposed method produces
state-of-the-art prediction performance of 99.8% on a real-world data set
classifying COVID-19 Chest X-ray images without sacrificing the explanatory
power of the model. This proposed design can efficiently screen COVID-19
patients before human diagnosis, and will be the benchmark for addressing
future XAI problems in large-scale data sets.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06916" title="Abstract">arXiv:2106.06916</a> [<a href="/pdf/2106.06916" title="Download PDF">pdf</a>, <a href="/format/2106.06916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Transferable Learning: A New Approach for Model Verification and  Authorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lixu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shichao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruiqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qi Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">As Artificial Intelligence as a Service gains popularity, protecting
well-trained models as intellectual property is becoming increasingly
important. Generally speaking, there are two common protection methods:
ownership verification and usage authorization. In this paper, we propose
Non-Transferable Learning (NTL), a novel approach that captures the exclusive
data representation in the learned model and restricts the model generalization
ability to certain domains. This approach provides effective solutions to both
model verification and authorization. For ownership verification, watermarking
techniques are commonly used but are often vulnerable to sophisticated
watermark removal methods. Our NTL-based model verification approach instead
provides robust resistance to state-of-the-art watermark removal methods, as
shown in extensive experiments for four of such methods over the digits,
CIFAR10 &amp; STL10, and VisDA datasets. For usage authorization, prior solutions
focus on authorizing specific users to use the model, but authorized users can
still apply the model to any data without restriction. Our NTL-based
authorization approach instead provides data-centric usage protection by
significantly degrading the performance of usage on unauthorized data. Its
effectiveness is also shown through experiments on a variety of datasets.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06917" title="Abstract">arXiv:2106.06917</a> [<a href="/pdf/2106.06917" title="Download PDF">pdf</a>, <a href="/format/2106.06917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ATRAS: Adversarially Trained Robust Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alparslan%2C+Y">Yigit Alparslan</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+E">Edward Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we explore the effect of architecture completeness on
adversarial robustness. We train models with different architectures on
CIFAR-10 and MNIST dataset. For each model, we vary different number of layers
and different number of nodes in the layer. For every architecture candidate,
we use Fast Gradient Sign Method (FGSM) to generate untargeted adversarial
attacks and use adversarial training to defend against those attacks. For each
architecture candidate, we report pre-attack, post-attack and post-defense
accuracy for the model as well as the architecture parameters and the impact of
completeness to the model accuracies.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06920" title="Abstract">arXiv:2106.06920</a> [<a href="/pdf/2106.06920" title="Download PDF">pdf</a>, <a href="/format/2106.06920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modal Scene-compliant User Intention Estimation for Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katuwandeniya%2C+K">Kavindie Katuwandeniya</a>, 
<a href="/search/cs?searchtype=author&query=Kiss%2C+S+H">Stefan H. Kiss</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Lei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Miro%2C+J+V">Jaime Valls Miro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A multi-modal framework to generated user intention distributions when
operating a mobile vehicle is proposed in this work. The model learns from past
observed trajectories and leverages traversability information derived from the
visual surroundings to produce a set of future trajectories, suitable to be
directly embedded into a perception-action shared control strategy on a mobile
agent, or as a safety layer to supervise the prudent operation of the vehicle.
We base our solution on a conditional Generative Adversarial Network with
Long-Short Term Memory cells to capture trajectory distributions conditioned on
past trajectories, further fused with traversability probabilities derived from
visual segmentation with a Convolutional Neural Network. The proposed
data-driven framework results in a significant reduction in error of the
predicted trajectories (versus the ground truth) from comparable strategies in
the literature (e.g. Social-GAN) that fail to account for information other
than the agent's past history. Experiments were conducted on a dataset
collected with a custom wheelchair model built onto the open-source urban
driving simulator CARLA, proving also that the proposed framework can be used
with a small, un-annotated dataset.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06921" title="Abstract">arXiv:2106.06921</a> [<a href="/pdf/2106.06921" title="Download PDF">pdf</a>, <a href="/format/2106.06921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Dynamic Pruning for Non-IID Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sixing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+P">Phuong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+A">Ali Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Federated Learning~(FL) has emerged as a new paradigm of training machine
learning models without sacrificing data security and privacy. Learning models
at edge devices such as cell phones is one of the most common use case of FL.
However, the limited computing power and energy constraints of edge devices
hinder the adoption of FL for both model training and deployment, especially
for the resource-hungry Deep Neural Networks~(DNNs). To this end, many model
compression methods have been proposed and network pruning is among the most
well-known. However, a pruning policy for a given model is highly
dataset-dependent, which is not suitable for non-Independent and Identically
Distributed~(Non-IID) FL edge devices. In this paper, we present an adaptive
pruning scheme for edge devices in an FL system, which applies dataset-aware
dynamic pruning for inference acceleration on Non-IID datasets. Our evaluation
shows that the proposed method accelerates inference by $2\times$~($50\%$ FLOPs
reduction) while maintaining the model's quality on edge devices.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06922" title="Abstract">arXiv:2106.06922</a> [<a href="/pdf/2106.06922" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-sentence Neural Language Models for Conversational Speech  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiu%2C+S">Shih-Hsuan Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+T">Tien-Hong Lo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Berlin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, Accepted to IEEE IJCNN 2021. arXiv admin note: substantial text overlap with <a href="/abs/2104.04950">arXiv:2104.04950</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">An important research direction in automatic speech recognition (ASR) has
centered around the development of effective methods to rerank the output
hypotheses of an ASR system with more sophisticated language models (LMs) for
further gains. A current mainstream school of thoughts for ASR N-best
hypothesis reranking is to employ a recurrent neural network (RNN)-based LM or
its variants, with performance superiority over the conventional n-gram LMs
across a range of ASR tasks. In real scenarios such as a long conversation, a
sequence of consecutive sentences may jointly contain ample cues of
conversation-level information such as topical coherence, lexical entrainment
and adjacency pairs, which however remains to be underexplored. In view of
this, we first formulate ASR N-best reranking as a prediction problem, putting
forward an effective cross-sentence neural LM approach that reranks the ASR
N-best hypotheses of an upcoming sentence by taking into consideration the word
usage in its precedent sentences. Furthermore, we also explore to extract
task-specific global topical information of the cross-sentence history in an
unsupervised manner for better ASR performance. Extensive experiments conducted
on the AMI conversational benchmark corpus indicate the effectiveness and
feasibility of our methods in comparison to several state-of-the-art reranking
methods.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06924" title="Abstract">arXiv:2106.06924</a> [<a href="/pdf/2106.06924" title="Download PDF">pdf</a>, <a href="/format/2106.06924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Reversible Steganography: Principles and Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Ching-Chun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sisheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Echizen%2C+I">Isao Echizen</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+V">Victor Sanchez</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chang-Tsun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep-learning\textendash{centric} reversible steganography has emerged as a
promising research paradigm. A direct way of applying deep learning to
reversible steganography is to construct a pair of encoder and decoder, whose
parameters are trained jointly, thereby learning the steganographic system as a
whole. This end-to-end framework, however, falls short of the reversibility
requirement because it is difficult for this kind of monolithic system, as a
black box, to create or duplicate intricate reversible mechanisms. In response
to this issue, a recent approach is to carve up the steganographic system and
work on modules independently. In particular, neural networks are deployed in
an analytics module to learn the data distribution, while an established
mechanism is called upon to handle the remaining tasks. In this paper, we
investigate the modular framework and deploy deep neural networks in a
reversible steganographic scheme referred to as prediction-error modulation, in
which an analytics module serves the purpose of pixel intensity prediction. The
primary focus of this study is on deep-learning\textendash{based} context-aware
pixel intensity prediction. We address the unsolved issues reported in related
literature, including the impact of pixel initialisation on prediction accuracy
and the influence of uncertainty propagation in dual-layer embedding.
Furthermore, we establish a connection between context-aware pixel intensity
prediction and low-level computer vision and analyse the performance of several
advanced neural networks.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06925" title="Abstract">arXiv:2106.06925</a> [<a href="/pdf/2106.06925" title="Download PDF">pdf</a>, <a href="/ps/2106.06925" title="Download PostScript">ps</a>, <a href="/format/2106.06925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Complexity of Fair House Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamiyama%2C+N">Naoyuki Kamiyama</a>, 
<a href="/search/cs?searchtype=author&query=Manurangsi%2C+P">Pasin Manurangsi</a>, 
<a href="/search/cs?searchtype=author&query=Suksompong%2C+W">Warut Suksompong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We study fairness in house allocation, where $m$ houses are to be allocated
among $n$ agents so that every agent receives one house. We show that
maximizing the number of envy-free agents is hard to approximate to within a
factor of $n^{1-\gamma}$ for any constant $\gamma&gt;0$, and that the exact
version is NP-hard even for binary utilities. Moreover, we prove that deciding
whether a proportional allocation exists is computationally hard, whereas the
corresponding problem for equitability can be solved efficiently.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06926" title="Abstract">arXiv:2106.06926</a> [<a href="/pdf/2106.06926" title="Download PDF">pdf</a>, <a href="/format/2106.06926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bellman-consistent Pessimism for Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tengyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Ching-An Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Mineiro%2C+P">Paul Mineiro</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Alekh Agarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">The use of pessimism, when reasoning about datasets lacking exhaustive
exploration has recently gained prominence in offline reinforcement learning.
Despite the robustness it adds to the algorithm, overly pessimistic reasoning
can be equally damaging in precluding the discovery of good policies, which is
an issue for the popular bonus-based pessimism. In this paper, we introduce the
notion of Bellman-consistent pessimism for general function approximation:
instead of calculating a point-wise lower bound for the value function, we
implement pessimism at the initial state over the set of functions consistent
with the Bellman equations. Our theoretical guarantees only require Bellman
closedness as standard in the exploratory setting, in which case bonus-based
pessimism fails to provide guarantees. Even in the special case of linear MDPs
where stronger function-approximation assumptions hold, our result improves
upon a recent bonus-based approach by $\mathcal{O}(d)$ in its sample complexity
when the action space is finite. Remarkably, our algorithms automatically adapt
to the best bias-variance tradeoff in the hindsight, whereas most prior
approaches require tuning extra hyperparameters a priori.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06927" title="Abstract">arXiv:2106.06927</a> [<a href="/pdf/2106.06927" title="Download PDF">pdf</a>, <a href="/format/2106.06927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverting Adversarially Robust Networks for Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rojas-Gomez%2C+R+A">Renan A. Rojas-Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+R+A">Raymond A. Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+M+N">Minh N. Do</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Anh Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Recent research in adversarially robust classifiers suggests their
representations tend to be aligned with human perception, which makes them
attractive for image synthesis and restoration applications. Despite favorable
empirical results on a few downstream tasks, their advantages are limited to
slow and sensitive optimization-based techniques. Moreover, their use on
generative models remains unexplored. This work proposes the use of robust
representations as a perceptual primitive for feature inversion models, and
show its benefits with respect to standard non-robust image features. We
empirically show that adopting robust representations as an image prior
significantly improves the reconstruction accuracy of CNN-based feature
inversion models. Furthermore, it allows reconstructing images at multiple
scales out-of-the-box. Following these findings, we propose an
encoding-decoding network based on robust representations and show its
advantages for applications such as anomaly detection, style transfer and image
denoising.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06931" title="Abstract">arXiv:2106.06931</a> [<a href="/pdf/2106.06931" title="Download PDF">pdf</a>, <a href="/format/2106.06931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning on Abstract Domains: A New Approach for Verifiable Guarantee in  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+P">Peng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianwen Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Li Han</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xuejun Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Formally verifying Deep Reinforcement Learning (DRL) systems is a challenging
task due to the dynamic continuity of system behaviors and the black-box
feature of embedded neural networks. In this paper, we propose a novel
abstraction-based approach to train DRL systems on finite abstract domains
instead of concrete system states. It yields neural networks whose input states
are finite, making hosting DRL systems directly verifiable using model checking
techniques. Our approach is orthogonal to existing DRL algorithms and
off-the-shelf model checkers. We implement a resulting prototype training and
verification framework and conduct extensive experiments on the
state-of-the-art benchmark. The results show that the systems trained in our
approach can be verified more efficiently while they retain comparable
performance against those that are trained without abstraction.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06932" title="Abstract">arXiv:2106.06932</a> [<a href="/pdf/2106.06932" title="Download PDF">pdf</a>, <a href="/format/2106.06932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing the Gap Between Actor-Critic and Policy Gradient
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Junfeng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Saurabh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Gummadi%2C+R">Ramki Gummadi</a>, 
<a href="/search/cs?searchtype=author&query=Schuurmans%2C+D">Dale Schuurmans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Actor-critic (AC) methods are ubiquitous in reinforcement learning. Although
it is understood that AC methods are closely related to policy gradient (PG),
their precise connection has not been fully characterized previously. In this
paper, we explain the gap between AC and PG methods by identifying the exact
adjustment to the AC objective/gradient that recovers the true policy gradient
of the cumulative reward objective (PG). Furthermore, by viewing the AC method
as a two-player Stackelberg game between the actor and critic, we show that the
Stackelberg policy gradient can be recovered as a special case of our more
general analysis. Based on these results, we develop practical algorithms,
Residual Actor-Critic and Stackelberg Actor-Critic, for estimating the
correction between AC and PG and use these to modify the standard AC algorithm.
Experiments on popular tabular and continuous environments show the proposed
corrections can improve both the sample efficiency and final performance of
existing AC methods.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06933" title="Abstract">arXiv:2106.06933</a> [<a href="/pdf/2106.06933" title="Download PDF">pdf</a>, <a href="/format/2106.06933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Learning for Network Traffic Classification: A Technical Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahraki%2C+A">Amin Shahraki</a>, 
<a href="/search/cs?searchtype=author&query=Abbasi%2C+M">Mahmoud Abbasi</a>, 
<a href="/search/cs?searchtype=author&query=Taherkordi%2C+A">Amir Taherkordi</a>, 
<a href="/search/cs?searchtype=author&query=Jurcut%2C+A+D">Anca Delia Jurcut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE Transactions on Cognitive Communications and Networking journal for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Network Traffic Classification (NTC) has become an important component in a
wide variety of network management operations, e.g., Quality of Service (QoS)
provisioning and security purposes. Machine Learning (ML) algorithms as a
common approach for NTC methods can achieve reasonable accuracy and handle
encrypted traffic. However, ML-based NTC techniques suffer from the shortage of
labeled traffic data which is the case in many real-world applications. This
study investigates the applicability of an active form of ML, called Active
Learning (AL), which reduces the need for a high number of labeled examples by
actively choosing the instances that should be labeled. The study first
provides an overview of NTC and its fundamental challenges along with surveying
the literature in the field of using ML techniques in NTC. Then, it introduces
the concepts of AL, discusses it in the context of NTC, and review the
literature in this field. Further, challenges and open issues in the use of AL
for NTC are discussed. Additionally, as a technical survey, some experiments
are conducted to show the broad applicability of AL in NTC. The simulation
results show that AL can achieve high accuracy with a small amount of data.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06934" title="Abstract">arXiv:2106.06934</a> [<a href="/pdf/2106.06934" title="Download PDF">pdf</a>, <a href="/format/2106.06934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning Over Wireless Channels: Dynamic Resource Allocation  and Task Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+S">Shunfeng Chu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zang%2C+Y">Yijin Zang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yuwen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">With the development of federated learning (FL), mobile devices (MDs) are
able to train their local models with private data and sends them to a central
server for aggregation, thereby preventing sensitive raw data leakage. In this
paper, we aim to improve the training performance of FL systems in the context
of wireless channels and stochastic energy arrivals of MDs. To this purpose, we
dynamically optimize MDs' transmission power and training task scheduling. We
first model this dynamic programming problem as a constrained Markov decision
process (CMDP). Due to high dimensions rooted from our CMDP problem, we propose
online stochastic learning methods to simplify the CMDP and design online
algorithms to obtain an efficient policy for all MDs. Since there are long-term
constraints in our CMDP, we utilize Lagrange multipliers approach to tackle
this issue. Furthermore, we prove the convergence of the proposed online
stochastic learning algorithm. Numerical results indicate that the proposed
algorithms can achieve better performance than the benchmark algorithms.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06935" title="Abstract">arXiv:2106.06935</a> [<a href="/pdf/2106.06935" title="Download PDF">pdf</a>, <a href="/ps/2106.06935" title="Download PostScript">ps</a>, <a href="/format/2106.06935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Bellman-Ford Networks: A General Graph Neural Network Framework  for Link Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhaocheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zuobai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xhonneux%2C+L">Louis-Pascal Xhonneux</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Link prediction is a very fundamental task on graphs. Inspired by traditional
path-based methods, in this paper we propose a general and flexible
representation learning framework based on paths for link prediction.
Specifically, we define the representation of a pair of nodes as the
generalized sum of all path representations, with each path representation as
the generalized product of the edge representations in the path. Motivated by
the Bellman-Ford algorithm for solving the shortest path problem, we show that
the proposed path formulation can be efficiently solved by the generalized
Bellman-Ford algorithm. To further improve the capacity of the path
formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general
graph neural network framework that solves the path formulation with learned
operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes
the generalized Bellman-Ford algorithm with 3 neural components, namely
INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary
condition, multiplication operator, and summation operator respectively. The
NBFNet is very general, covers many traditional path-based methods, and can be
applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge
graphs) in both transductive and inductive settings. Experiments on both
homogeneous graphs and knowledge graphs show that the proposed NBFNet
outperforms existing methods by a large margin in both transductive and
inductive settings, achieving new state-of-the-art results.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06937" title="Abstract">arXiv:2106.06937</a> [<a href="/pdf/2106.06937" title="Download PDF">pdf</a>, <a href="/format/2106.06937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Common Sense Beyond English: Evaluating and Improving Multilingual  Language Models for Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+B+Y">Bill Yuchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seyeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+X">Xiaoyang Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACL-IJCNLP 2021 (long paper at main conference). Project website: <a href="https://inklab.usc.edu/XCSR/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Commonsense reasoning research has so far been limited to English. We aim to
evaluate and improve popular multilingual language models (ML-LMs) to help
advance commonsense reasoning (CSR) beyond English. We collect the Mickey
Corpus, consisting of 561k sentences in 11 different languages, which can be
used for analyzing and improving ML-LMs. We propose Mickey Probe, a
language-agnostic probing task for fairly evaluating the common sense of
popular ML-LMs across different languages. In addition, we also create two new
datasets, X-CSQA and X-CODAH, by translating their English versions to 15 other
languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense
reasoning. To improve the performance beyond English, we propose a simple yet
effective method -- multilingual contrastive pre-training (MCP). It
significantly enhances sentence representations, yielding a large performance
gain on both benchmarks.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06939" title="Abstract">arXiv:2106.06939</a> [<a href="/pdf/2106.06939" title="Download PDF">pdf</a>, <a href="/format/2106.06939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Modal Attention Consistency for Video-Audio Unsupervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+S">Shaobo Min</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Q">Qi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hongtao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+C">Chuang Gan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingdong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Cross-modal correlation provides an inherent supervision for video
unsupervised representation learning. Existing methods focus on distinguishing
different video clips by visual and audio representations. We human visual
perception could attend to regions where sounds are made, and our auditory
perception could also ground their frequencies of sounding objects, which we
call bidirectional local correspondence. Such supervision is intuitive but not
well explored in the contrastive learning framework. This paper introduces a
pretext task, Cross-Modal Attention Consistency (CMAC), for exploring the
bidirectional local correspondence property. The CMAC approach aims to align
the regional attention generated purely from the visual signal with the target
attention generated under the guidance of acoustic signal, and do a similar
alignment for frequency grounding on the acoustic attention. Accompanied by a
remoulded cross-modal contrastive loss where we consider additional
within-modal interactions, the CMAC approach works effectively for enforcing
the bidirectional alignment. Extensive experiments on six downstream benchmarks
demonstrate that CMAC can improve the state-of-the-art performance on both
visual and audio modalities.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06942" title="Abstract">arXiv:2106.06942</a> [<a href="/pdf/2106.06942" title="Download PDF">pdf</a>, <a href="/format/2106.06942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stronger Baseline for Ego-Centric Action Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qing%2C+Z">Zhiwu Qing</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yutong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jianwen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+M">Mingqian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Changxin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ang%2C+M+H">Marcelo H. Ang Jr</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+N">Nong Sang</a>, 
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPRW21, EPIC-KITCHENS-100 Competition Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This technical report analyzes an egocentric video action detection method we
used in the 2021 EPIC-KITCHENS-100 competition hosted in CVPR2021 Workshop. The
goal of our task is to locate the start time and the end time of the action in
the long untrimmed video, and predict action category. We adopt sliding window
strategy to generate proposals, which can better adapt to short-duration
actions. In addition, we show that classification and proposals are conflict in
the same network. The separation of the two tasks boost the detection
performance with high efficiency. By simply employing these strategy, we
achieved 16.10\% performance on the test set of EPIC-KITCHENS-100 Action
Detection challenge using a single model, surpassing the baseline method by
11.7\% in terms of average mAP.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06944" title="Abstract">arXiv:2106.06944</a> [<a href="/pdf/2106.06944" title="Download PDF">pdf</a>, <a href="/format/2106.06944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SASICM A Multi-Task Benchmark For Subtext Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hua Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+W">Weikang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+F">Feng Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+F">Furao Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 6 figures, 6 tables. Submitted to the journal of artificial intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Subtext is a kind of deep semantics which can be acquired after one or more
rounds of expression transformation. As a popular way of expressing one's
intentions, it is well worth studying. In this paper, we try to make computers
understand whether there is a subtext by means of machine learning. We build a
Chinese dataset whose source data comes from the popular social media (e.g.
Weibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a
baseline model called SASICM to deal with subtext recognition. The F1 score of
SASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97%
higher than that of BERT based model, 12.7% higher than that of traditional
methods on average, including support vector machine, logistic regression
classifier, maximum entropy classifier, naive bayes classifier and decision
tree and 2.39% higher than that of the state-of-the-art, including MARIN and
BTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%,
which is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and
SASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of
other methods which are mentioned before.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06945" title="Abstract">arXiv:2106.06945</a> [<a href="/pdf/2106.06945" title="Download PDF">pdf</a>, <a href="/format/2106.06945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Status Update for Caching Enabled IoT Networks: A Dueling Deep  R-Network Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yiping Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H+H">Howard H. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q. S. Quek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In the Internet of Things (IoT) networks, caching is a promising technique to
alleviate energy consumption of sensors by responding to users' data requests
with the data packets cached in the edge caching node (ECN). However, without
an efficient status update strategy, the information obtained by users may be
stale, which in return would inevitably deteriorate the accuracy and
reliability of derived decisions for real-time applications. In this paper, we
focus on striking the balance between the information freshness, in terms of
age of information (AoI), experienced by users and energy consumed by sensors,
by appropriately activating sensors to update their current status.
Particularly, we first depict the evolutions of the AoI with each sensor from
different users' perspective with time steps of non-uniform duration, which are
determined by both the users' data requests and the ECN's status update
decision. Then, we formulate a non-uniform time step based dynamic status
update optimization problem to minimize the long-term average cost, jointly
considering the average AoI and energy consumption. To this end, a Markov
Decision Process is formulated and further, a dueling deep R-network based
dynamic status update algorithm is devised by combining dueling deep Q-network
and tabular R-learning, with which challenges from the curse of dimensionality
and unknown of the environmental dynamics can be addressed. Finally, extensive
simulations are conducted to validate the effectiveness of our proposed
algorithm by comparing it with five baseline deep reinforcement learning
algorithms and policies.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06946" title="Abstract">arXiv:2106.06946</a> [<a href="/pdf/2106.06946" title="Download PDF">pdf</a>, <a href="/format/2106.06946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Randomized Smoothing with Variance Reduced Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Horv%C3%A1th%2C+M+Z">Mikl&#xf3;s Z. Horv&#xe1;th</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M+N">Mark Niklas M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+M">Marc Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Randomized Smoothing (RS) is a promising method for obtaining robustness
certificates by evaluating a base model under noise. In this work we: (i)
theoretically motivate why ensembles are a particularly suitable choice as base
models for RS, and (ii) empirically confirm this choice, obtaining state of the
art results in multiple settings. The key insight of our work is that the
reduced variance of ensembles over the perturbations introduced in RS leads to
significantly more consistent classifications for a given input, in turn
leading to substantially increased certifiable radii for difficult samples. We
also introduce key optimizations which enable an up to 50-fold decrease in
sample complexity of RS, thus drastically reducing its computational overhead.
Experimentally, we show that ensembles of only 3 to 10 classifiers consistently
improve on the strongest single model with respect to their average certified
radius (ACR) by 5% to 21% on both CIFAR-10 and ImageNet. On the latter, we
achieve a state-of-the-art ACR of 1.11. We release all code and models required
to reproduce our results upon publication.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06947" title="Abstract">arXiv:2106.06947</a> [<a href="/pdf/2106.06947" title="Download PDF">pdf</a>, <a href="/format/2106.06947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Network-Based Anomaly Detection in Multivariate Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+A">Ailin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI Conference on Artificial Intelligence (AAAI), 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Given high-dimensional time series data (e.g., sensor data), how can we
detect anomalous events, such as system faults and attacks? More challengingly,
how can we do this in a way that captures complex inter-sensor relationships,
and detects and explains anomalies which deviate from these relationships?
Recently, deep learning approaches have enabled improvements in anomaly
detection in high-dimensional datasets; however, existing methods do not
explicitly learn the structure of existing relationships between variables, or
use them to predict the expected behavior of time series. Our approach combines
a structure learning approach with graph neural networks, additionally using
attention weights to provide explainability for the detected anomalies.
Experiments on two real-world sensor datasets with ground truth anomalies show
that our method detects anomalies more accurately than baseline approaches,
accurately captures correlations between sensors, and allows users to deduce
the root cause of a detected anomaly.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06949" title="Abstract">arXiv:2106.06949</a> [<a href="/pdf/2106.06949" title="Download PDF">pdf</a>, <a href="/format/2106.06949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Crucial is it for 6G Networks to be Autonomous?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adem%2C+N">Nadia Adem</a>, 
<a href="/search/cs?searchtype=author&query=Benfaid%2C+A">Ahmed Benfaid</a>, 
<a href="/search/cs?searchtype=author&query=Harib%2C+R">Ramy Harib</a>, 
<a href="/search/cs?searchtype=author&query=Alarabi%2C+A">Anas Alarabi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The sixth generation (6G), unlike any previous generations, is envisioned by
2030 to connect everything. Moreover, in addition to the new use cases 6G is
expected to support, it will need to provide a superior performance over 5G.
The global connectivity, large-network dimensions, users heterogeneity,
extremely-low power consumption, high-throughput, ultra-reliability,
efficient-network operation and maintenance, and low-latency requirements to be
met by future networks inevitably necessitate the autonomy of 6G. Intelligence,
facilitated mainly by the advancement and innovation of the artificial
intelligence (AI) technique, is a key to achieve autonomy. In this paper we
provide a bird's-eye view of future networks, vision, progress, and objectives.
We review some of the 6G technologies that would be mainly enabling the
globally-intelligent connected world. We, in addition to discussing the role of
AI in future networks, unlike any other review papers provide our original
results that emphasize the necessity of deploying AI for 6G networks. We also
very importantly identify 6G implementation challenges and key innovative
techniques like quantum and blockchain to solve these challenges. This article
serves as a starting point for learner to acquire more knowledge about 6G as it
combines some of the main contributions in the area and provide some references
for getting a more deeper knowledge, and also for researchers to contribute to
the field.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06950" title="Abstract">arXiv:2106.06950</a> [<a href="/pdf/2106.06950" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient way to manage blocks of data with Wise Red-Black Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boffi%2C+A">Alberto Boffi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">This paper describes the most efficient way to manage operations on groups of
consecutive elements, or "blocks" of elements, within an ordered set. This will
be done by introducing a new data structure called Wise Red-Black Trees, a
variation of classical Red-Black Trees. The goal is to improve time complexity,
while also keeping spatial complexity to a minimum. The optimization will be
visible both at the asymptote and in terms of multiplicative constants,
affecting not only the worst-case, but also the average one.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06951" title="Abstract">arXiv:2106.06951</a> [<a href="/pdf/2106.06951" title="Download PDF">pdf</a>, <a href="/format/2106.06951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effects of Eavesdropper on the Performance of Mixed &#x3b7;-&#x3bc; and DGG  Cooperative Relaying System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarker%2C+N+A">Noor Ahmed Sarker</a>, 
<a href="/search/cs?searchtype=author&query=Badrudduza%2C+A+S+M">A. S. M. Badrudduza</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+M+K">Milton Kumar Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Ansari%2C+I+S">Imran Shafique Ansari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Free-space optical (FSO) channel offers line-of-sight wireless communication
with high data rates and high secrecy utilizing unlicensed optical spectrum and
also paves the way to the solution of the last-mile access problem. Since
atmospheric turbulence is a hindrance to an enhanced secrecy performance, the
mixed radio frequency (RF)-FSO system is gaining enormous research interest in
recent days. But conventional FSO models except for the double generalized
Gamma (DGG) model can not demonstrate secrecy performance for all ranges of
turbulence severity. This reason has led us to propose a dual-hop eta-mu and
unified DGG mixed RF-FSO network while considering eavesdropping at both RF and
FSO hops. The security of these proposed scenarios is investigated in terms of
two metrics, i.e., strictly positive secrecy capacity and secure outage
probability. Exploiting these expressions, we further investigate how the
secrecy performance is affected by various system parameters, i.e., fading,
turbulence, and pointing errors. A demonstration is made between heterodyne
detection (HD) and intensity modulation and direct detection (IM/DD) techniques
while exhibiting superior secrecy performance for HD technique over IM/DD
technique. Finally, all analytical results are corroborated via Monte-Carlo
simulations.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06955" title="Abstract">arXiv:2106.06955</a> [<a href="/pdf/2106.06955" title="Download PDF">pdf</a>, <a href="/format/2106.06955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding Iterative Magnitude Pruning: Why Lottery Tickets  Win
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maene%2C+J">Jaron Maene</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The lottery ticket hypothesis states that sparse subnetworks exist in
randomly initialized dense networks that can be trained to the same accuracy as
the dense network they reside in. However, the subsequent work has failed to
replicate this on large-scale models and required rewinding to an early stable
state instead of initialization. We show that by using a training method that
is stable with respect to linear mode connectivity, large networks can also be
entirely rewound to initialization. Our subsequent experiments on common vision
tasks give strong credence to the hypothesis in Evci et al. (2020b) that
lottery tickets simply retrain to the same regions (although not necessarily to
the same basin). These results imply that existing lottery tickets could not
have been found without the preceding dense training by iterative magnitude
pruning, raising doubts about the use of the lottery ticket hypothesis.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06957" title="Abstract">arXiv:2106.06957</a> [<a href="/pdf/2106.06957" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoScore-Survival: Developing interpretable machine learning-based  time-to-event scores with right-censored survival data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+F">Feng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yilin Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Han Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+B+A">Benjamin Alan Goldstein</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+M+E+H">Marcus Eng Hock Ong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+B">Bibhas Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
<br />AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
<br />The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
<br />Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06959" title="Abstract">arXiv:2106.06959</a> [<a href="/pdf/2106.06959" title="Download PDF">pdf</a>, <a href="/format/2106.06959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Not Escape From the Manifold: Discovering the Local Coordinates on  the Latent Space of GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaewoong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+C">Changyeon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+H">Jung Ho Park</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+G">Geonho Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Myungjoo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose a method to find local-geometry-aware traversal
directions on the intermediate latent space of Generative Adversarial Networks
(GANs). These directions are defined as an ordered basis of tangent space at a
latent code. Motivated by the intrinsic sparsity of the latent space, the basis
is discovered by solving the low-rank approximation problem of the differential
of the partial network. Moreover, the local traversal basis leads to a natural
iterative traversal on the latent space. Iterative Curve-Traversal shows stable
traversal on images, since the trajectory of latent code stays close to the
latent space even under the strong perturbations compared to the linear
traversal. This stability provides far more diverse variations of the given
image. Although the proposed method can be applied to various GAN models, we
focus on the W-space of the StyleGAN2, which is renowned for showing the better
disentanglement of the latent factors of variation. Our quantitative and
qualitative analysis provides evidence showing that the W-space is still
globally warped while showing a certain degree of global consistency of
interpretable variation. In particular, we introduce some metrics on the
Grassmannian manifolds to quantify the global warpage of the W-space and the
subspace traversal to test the stability of traversal directions.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06960" title="Abstract">arXiv:2106.06960</a> [<a href="/pdf/2106.06960" title="Download PDF">pdf</a>, <a href="/format/2106.06960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation and Correlation Enhanced Encoder-Decoder Framework for  Scene Text Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+M">Mengmeng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinjin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Attention-based encoder-decoder framework is widely used in the scene text
recognition task. However, for the current state-of-the-art(SOTA) methods,
there is room for improvement in terms of the efficient usage of local visual
and global context information of the input text image, as well as the robust
correlation between the scene processing module(encoder) and the text
processing module(decoder). In this paper, we propose a Representation and
Correlation Enhanced Encoder-Decoder Framework(RCEED) to address these
deficiencies and break performance bottleneck. In the encoder module, local
visual feature, global context feature, and position information are aligned
and fused to generate a small-size comprehensive feature map. In the decoder
module, two methods are utilized to enhance the correlation between scene and
text feature space. 1) The decoder initialization is guided by the holistic
feature and global glimpse vector exported from the encoder. 2) The feature
enriched glimpse vector produced by the Multi-Head General Attention is used to
assist the RNN iteration and the character prediction at each time step.
Meanwhile, we also design a Layernorm-Dropout LSTM cell to improve model's
generalization towards changeable texts. Extensive experiments on the
benchmarks demonstrate the advantageous performance of RCEED in scene text
recognition tasks, especially the irregular ones.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06963" title="Abstract">arXiv:2106.06963</a> [<a href="/pdf/2106.06963" title="Download PDF">pdf</a>, <a href="/format/2106.06963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring and Distilling Posterior and Prior Knowledge for Radiology  Report Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fenglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Shen Ge</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yuexian Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Automatically generating radiology reports can improve current clinical
practice in diagnostic radiology. On one hand, it can relieve radiologists from
the heavy burden of report writing; On the other hand, it can remind
radiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.
Yet, this task remains a challenging job for data-driven neural networks, due
to the serious visual and textual data biases. To this end, we propose a
Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to
imitate the working patterns of radiologists, who will first examine the
abnormal regions and assign the disease topic tags to the abnormal regions, and
then rely on the years of prior medical knowledge and prior working experience
accumulations to write reports. Thus, the PPKED includes three modules:
Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and
Multi-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior
knowledge, which provides explicit abnormal visual regions to alleviate visual
data bias; PrKE explores the prior knowledge from the prior medical knowledge
graph (medical knowledge) and prior radiology reports (working experience) to
alleviate textual data bias. The explored knowledge is distilled by the MKD to
generate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our
method is able to outperform previous state-of-the-art models on these two
datasets.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06964" title="Abstract">arXiv:2106.06964</a> [<a href="/pdf/2106.06964" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tikhonov%2C+A">Alexey Tikhonov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 2 figures, EEML-2021 poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Pre-trained word representations became a key component in many NLP tasks.
However, the global geometry of the word embeddings remains poorly understood.
In this paper, we demonstrate that a typical word embeddings cloud is shaped as
a high-dimensional simplex with interpretable vertices and propose a simple yet
effective method for enumeration of these vertices. We show that the proposed
method can detect and describe vertices of the simplex for GloVe and fasttext
spaces.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06965" title="Abstract">arXiv:2106.06965</a> [<a href="/pdf/2106.06965" title="Download PDF">pdf</a>, <a href="/format/2106.06965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Attention for Automatic Chest X-ray Report Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fenglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+C">Changchang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Shen Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Ping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xu Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appear in Findings of ACL 2021 (The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021))
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recently, chest X-ray report generation, which aims to automatically generate
descriptions of given chest X-ray images, has received growing research
interests. The key challenge of chest X-ray report generation is to accurately
capture and describe the abnormal regions. In most cases, the normal regions
dominate the entire chest X-ray image, and the corresponding descriptions of
these normal regions dominate the final report. Due to such data bias,
learning-based models may fail to attend to abnormal regions. In this work, to
effectively capture and describe abnormal regions, we propose the Contrastive
Attention (CA) model. Instead of solely focusing on the current input image,
the CA model compares the current input image with normal images to distill the
contrastive information. The acquired contrastive information can better
represent the visual features of abnormal regions. According to the experiments
on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into
several existing models can boost their performance across most metrics. In
addition, according to the analysis, the CA model can help existing models
better attend to the abnormal regions and provide more accurate descriptions
which are crucial for an interpretable diagnosis. Specifically, we achieve the
state-of-the-art results on the two public datasets.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06966" title="Abstract">arXiv:2106.06966</a> [<a href="/pdf/2106.06966" title="Download PDF">pdf</a>, <a href="/format/2106.06966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feedback Pyramid Attention Networks for Single Image Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huapeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+J">Jie Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhihui Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, convolutional neural network (CNN) based image super-resolution
(SR) methods have achieved significant performance improvement. However, most
CNN-based methods mainly focus on feed-forward architecture design and neglect
to explore the feedback mechanism, which usually exists in the human visual
system. In this paper, we propose feedback pyramid attention networks (FPAN) to
fully exploit the mutual dependencies of features. Specifically, a novel
feedback connection structure is developed to enhance low-level feature
expression with high-level information. In our method, the output of each layer
in the first stage is also used as the input of the corresponding layer in the
next state to re-update the previous low-level filters. Moreover, we introduce
a pyramid non-local structure to model global contextual information in
different scales and improve the discriminative representation of the network.
Extensive experimental results on various datasets demonstrate the superiority
of our FPAN in comparison with the state-of-the-art SR methods.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06969" title="Abstract">arXiv:2106.06969</a> [<a href="/pdf/2106.06969" title="Download PDF">pdf</a>, <a href="/format/2106.06969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoundDet: Polyphonic Sound Event Detection and Localization from Raw  Waveform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuhang He</a>, 
<a href="/search/cs?searchtype=author&query=Trigoni%2C+N">Niki Trigoni</a>, 
<a href="/search/cs?searchtype=author&query=Markham%2C+A">Andrew Markham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a new framework SoundDet, which is an end-to-end trainable and
light-weight framework, for polyphonic moving sound event detection and
localization. Prior methods typically approach this problem by preprocessing
raw waveform into time-frequency representations, which is more amenable to
process with well-established image processing pipelines. Prior methods also
detect in segment-wise manner, leading to incomplete and partial detections.
SoundDet takes a novel approach and directly consumes the raw, multichannel
waveform and treats the spatio-temporal sound event as a complete
``sound-object" to be detected. Specifically, SoundDet consists of a backbone
neural network and two parallel heads for temporal detection and spatial
localization, respectively. Given the large sampling rate of raw waveform, the
backbone network first learns a set of phase-sensitive and frequency-selective
bank of filters to explicitly retain direction-of-arrival information, whilst
being highly computationally and parametrically efficient than standard 1D/2D
convolution. A dense sound event proposal map is then constructed to handle the
challenges of predicting events with large varying temporal duration.
Accompanying the dense proposal map are a temporal overlapness map and a motion
smoothness map that measure a proposal's confidence to be an event from
temporal detection accuracy and movement consistency perspective. Involving the
two maps guarantees SoundDet to be trained in a spatio-temporally unified
manner. Experimental results on the public DCASE dataset show the advantage of
SoundDet on both segment-based and our newly proposed event-based evaluation
system.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06971" title="Abstract">arXiv:2106.06971</a> [<a href="/pdf/2106.06971" title="Download PDF">pdf</a>, <a href="/format/2106.06971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLHD: A Pixel-Level Non-Local Retinex Model for Low-Light Image  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hou Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yingkun%2C+H">Hou Yingkun</a>, 
<a href="/search/cs?searchtype=author&query=Yuxuan%2C+S">Shi Yuxuan</a>, 
<a href="/search/cs?searchtype=author&query=Benzheng%2C+W">Wei Benzheng</a>, 
<a href="/search/cs?searchtype=author&query=Jun%2C+X">Xu Jun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Retinex model has been applied to low-light image enhancement in many
existing methods. More appropriate decomposition of a low-light image can help
achieve better image enhancement. In this paper, we propose a new pixel-level
non-local Haar transform based illumination and reflectance decomposition
method (NLHD). The unique low-frequency coefficient of Haar transform on each
similar pixel group is used to reconstruct the illumination component, and the
rest of all high-frequency coefficients are employed to reconstruct the
reflectance component. The complete similarity of pixels in a matched similar
pixel group and the simple separable Haar transform help to obtain more
appropriate image decomposition; thus, the image is hardly sharpened in the
image brightness enhancement procedure. The exponential transform and
logarithmic transform are respectively implemented on the illumination
component. Then a minimum fusion strategy on the results of these two
transforms is utilized to achieve more natural illumination component
enhancement. It can alleviate the mosaic artifacts produced in the darker
regions by the exponential transform with a gamma value less than 1 and reduce
information loss caused by excessive enhancement of the brighter regions due to
the logarithmic transform. Finally, the Retinex model is applied to the
enhanced illumination and reflectance to achieve image enhancement. We also
develop a local noise level estimation based noise suppression method and a
non-local saturation reduction based color deviation correction method. These
two methods can respectively attenuate noise or color deviation usually
presented in the enhanced results of the extremely dark low-light images.
Experiments on benchmark datasets show that the proposed method can achieve
better low-light image enhancement results on subjective and objective
evaluations than most existing methods.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06972" title="Abstract">arXiv:2106.06972</a> [<a href="/pdf/2106.06972" title="Download PDF">pdf</a>, <a href="/format/2106.06972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RCURRENCY: Live Digital Asset Trading Using a Recurrent Neural  Network-based Forecasting System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y+J">Yapeng Jasper Hu</a>, 
<a href="/search/cs?searchtype=author&query=van+Gurp%2C+R">Ralph van Gurp</a>, 
<a href="/search/cs?searchtype=author&query=Somai%2C+A">Ashay Somai</a>, 
<a href="/search/cs?searchtype=author&query=Kooijman%2C+H">Hugo Kooijman</a>, 
<a href="/search/cs?searchtype=author&query=Rellermeyer%2C+J+S">Jan S. Rellermeyer</a> (Distributed Systems Group, Delft University of Technology)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Consistent alpha generation, i.e., maintaining an edge over the market,
underpins the ability of asset traders to reliably generate profits. Technical
indicators and trading strategies are commonly used tools to determine when to
buy/hold/sell assets, yet these are limited by the fact that they operate on
known values. Over the past decades, multiple studies have investigated the
potential of artificial intelligence in stock trading in conventional markets,
with some success. In this paper, we present RCURRENCY, an RNN-based trading
engine to predict data in the highly volatile digital asset market which is
able to successfully manage an asset portfolio in a live environment. By
combining asset value prediction and conventional trading tools, RCURRENCY
determines whether to buy, hold or sell digital currencies at a given point in
time. Experimental results show that, given the data of an interval $t$, a
prediction with an error of less than 0.5\% of the data at the subsequent
interval $t+1$ can be obtained. Evaluation of the system through backtesting
shows that RCURRENCY can be used to successfully not only maintain a stable
portfolio of digital assets in a simulated live environment using real
historical trading data but even increase the portfolio value over time.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06976" title="Abstract">arXiv:2106.06976</a> [<a href="/pdf/2106.06976" title="Download PDF">pdf</a>, <a href="/ps/2106.06976" title="Download PostScript">ps</a>, <a href="/format/2106.06976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game of GANs: Game Theoretical Models for Generative Adversarial  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moghadam%2C+M+M">Monireh Mohebbi Moghadam</a>, 
<a href="/search/cs?searchtype=author&query=Boroumand%2C+B">Bahar Boroumand</a>, 
<a href="/search/cs?searchtype=author&query=Jalali%2C+M">Mohammad Jalali</a>, 
<a href="/search/cs?searchtype=author&query=Zareian%2C+A">Arman Zareian</a>, 
<a href="/search/cs?searchtype=author&query=Javad%2C+A+D">Alireza Daei Javad</a>, 
<a href="/search/cs?searchtype=author&query=Manshaei%2C+M+H">Mohammad Hossein Manshaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 Tables, 2 Figures, Review paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Generative Adversarial Network, as a promising research direction in the AI
community, recently attracts considerable attention due to its ability to
generating high-quality realistic data. GANs are a competing game between two
neural networks trained in an adversarial manner to reach a Nash equilibrium.
Despite the improvement accomplished in GANs in the last years, there remain
several issues to solve. In this way, how to tackle these issues and make
advances leads to rising research interests. This paper reviews literature that
leverages the game theory in GANs and addresses how game models can relieve
specific generative models' challenges and improve the GAN's performance. In
particular, we firstly review some preliminaries, including the basic GAN model
and some game theory backgrounds. After that, we present our taxonomy to
summarize the state-of-the-art solutions into three significant categories:
modified game model, modified architecture, and modified learning method. The
classification is based on the modifications made in the basic model by the
proposed approaches from the game-theoretic perspective. We further classify
each category into several subcategories. Following the proposed taxonomy, we
explore the main objective of each class and review the recent work in each
group. Finally, we discuss the remaining challenges in this field and present
the potential future research topics.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06978" title="Abstract">arXiv:2106.06978</a> [<a href="/pdf/2106.06978" title="Download PDF">pdf</a>, <a href="/format/2106.06978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Study of Joint Activity Detection and Channel Estimation Based on  Message Passing with RBP Scheduling for MTC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Renna%2C+R+B">R. B. Di Renna</a>, 
<a href="/search/cs?searchtype=author&query=de+Lamare%2C+R+C">R. C. de Lamare</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures. arXiv admin note: substantial text overlap with <a href="/abs/2103.04486">arXiv:2103.04486</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this work, based on the hybrid generalized approximate message passing
(HyGAMP) algorithm, we propose the message-scheduling GAMP (MSGAMP) algorithm
in order to address the problem of joint active device detection and channel
estimation in an uplink grant-free massive MIMO system scenario. In MSGAMP, we
apply three different scheduling techniques based on the Residual Belief
Propagation (RBP) in which messages are generated using the latest available
information. With a much lower computational cost than the state-of-the-art
algorithms, MSGAMP-type schemes exhibits good performance in terms of activity
error rate and normalized mean squared error, requiring a small number of
iterations for convergence. %
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06981" title="Abstract">arXiv:2106.06981</a> [<a href="/pdf/2106.06981" title="Download PDF">pdf</a>, <a href="/format/2106.06981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thinking Like Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weiss%2C+G">Gail Weiss</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Yahav%2C+E">Eran Yahav</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">What is the computational model behind a Transformer? Where recurrent neural
networks have direct parallels in finite state machines, allowing clear
discussion and thought around architecture variants or trained models,
Transformers have no such familiar parallel. In this paper we aim to change
that, proposing a computational model for the transformer-encoder in the form
of a programming language. We map the basic components of a transformer-encoder
-- attention and feed-forward computation -- into simple primitives, around
which we form a programming language: the Restricted Access Sequence Processing
Language (RASP). We show how RASP can be used to program solutions to tasks
that could conceivably be learned by a Transformer, and how a Transformer can
be trained to mimic a RASP solution. In particular, we provide RASP programs
for histograms, sorting, and Dyck-languages. We further use our model to relate
their difficulty in terms of the number of required layers and attention heads:
analyzing a RASP program implies a maximum number of heads and layers necessary
to encode a task in a transformer. Finally, we see how insights gained from our
abstraction might be used to explain phenomena seen in recent works.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06983" title="Abstract">arXiv:2106.06983</a> [<a href="/pdf/2106.06983" title="Download PDF">pdf</a>, <a href="/format/2106.06983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-way Spectrum Pursuit for CUR Decomposition and Its Application in  Joint Column/Row Subset Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esmaeili%2C+A">Ashkan Esmaeili</a>, 
<a href="/search/cs?searchtype=author&query=Joneidi%2C+M">Mohsen Joneidi</a>, 
<a href="/search/cs?searchtype=author&query=Salimitari%2C+M">Mehrdad Salimitari</a>, 
<a href="/search/cs?searchtype=author&query=Khalid%2C+U">Umar Khalid</a>, 
<a href="/search/cs?searchtype=author&query=Rahnavard%2C+N">Nazanin Rahnavard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The problem of simultaneous column and row subset selection is addressed in
this paper. The column space and row space of a matrix are spanned by its left
and right singular vectors, respectively. However, the singular vectors are not
within actual columns/rows of the matrix. In this paper, an iterative approach
is proposed to capture the most structural information of columns/rows via
selecting a subset of actual columns/rows. This algorithm is referred to as
two-way spectrum pursuit (TWSP) which provides us with an accurate solution for
the CUR matrix decomposition. TWSP is applicable in a wide range of
applications since it enjoys a linear complexity w.r.t. number of original
columns/rows. We demonstrated the application of TWSP for joint channel and
sensor selection in cognitive radio networks, informative users and contents
detection, and efficient supervised data reduction.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06984" title="Abstract">arXiv:2106.06984</a> [<a href="/pdf/2106.06984" title="Download PDF">pdf</a>, <a href="/format/2106.06984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural  Networks Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shikuang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ruihao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Shi Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spiking Neural Network (SNN) has been recognized as one of the next
generation of neural networks. Conventionally, SNN can be converted from a
pre-trained ANN by only replacing the ReLU activation to spike activation while
keeping the parameters intact. Perhaps surprisingly, in this work we show that
a proper way to calibrate the parameters during the conversion of ANN to SNN
can bring significant improvements. We introduce SNN Calibration, a cheap but
extraordinarily effective method by leveraging the knowledge within a
pre-trained Artificial Neural Network (ANN). Starting by analyzing the
conversion error and its propagation through layers theoretically, we propose
the calibration algorithm that can correct the error layer-by-layer. The
calibration only takes a handful number of training data and several minutes to
finish. Moreover, our calibration algorithm can produce SNN with
state-of-the-art architecture on the large-scale ImageNet dataset, including
MobileNet and RegNet. Extensive experiments demonstrate the effectiveness and
efficiency of our algorithm. For example, our advanced pipeline can increase up
to 69% top-1 accuracy when converting MobileNet on ImageNet compared to
baselines. Codes are released at https://github.com/yhhhli/SNN_Calibration.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06988" title="Abstract">arXiv:2106.06988</a> [<a href="/pdf/2106.06988" title="Download PDF">pdf</a>, <a href="/format/2106.06988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NDPNet: A novel non-linear data projection network for few-shot  fine-gained image classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhangy%2C+W">Weichuan Zhangy</a>, 
<a href="/search/cs?searchtype=author&query=Liuy%2C+X">Xuefang Liuy</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhe Xue</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yongsheng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Changming Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Metric-based few-shot fine-grained image classification (FSFGIC) aims to
learn a transferable feature embedding network by estimating the similarities
between query images and support classes from very few examples. In this work,
we propose, for the first time, to introduce the non-linear data projection
concept into the design of FSFGIC architecture in order to address the limited
sample problem in few-shot learning and at the same time to increase the
discriminability of the model for fine-grained image classification.
Specifically, we first design a feature re-abstraction embedding network that
has the ability to not only obtain the required semantic features for effective
metric learning but also re-enhance such features with finer details from input
images. Then the descriptors of the query images and the support classes are
projected into different non-linear spaces in our proposed similarity metric
learning network to learn discriminative projection factors. This design can
effectively operate in the challenging and restricted condition of a FSFGIC
task for making the distance between the samples within the same class smaller
and the distance between samples from different classes larger and for reducing
the coupling relationship between samples from different categories.
Furthermore, a novel similarity measure based on the proposed non-linear data
project is presented for evaluating the relationships of feature information
between a query image and a support set. It is worth to note that our proposed
architecture can be easily embedded into any episodic training mechanisms for
end-to-end training from scratch. Extensive experiments on FSFGIC tasks
demonstrate the superiority of the proposed methods over the state-of-the-art
benchmarks.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06989" title="Abstract">arXiv:2106.06989</a> [<a href="/pdf/2106.06989" title="Download PDF">pdf</a>, <a href="/format/2106.06989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The DEformer: An Order-Agnostic Distribution Estimating Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alcorn%2C+M+A">Michael A. Alcorn</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Anh Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Order-agnostic autoregressive distribution estimation (OADE), i.e.,
autoregressive distribution estimation where the features can occur in an
arbitrary order, is a challenging problem in generative machine learning. Prior
work on OADE has encoded feature identity (e.g., pixel location) by assigning
each feature to a distinct fixed position in an input vector. As a result,
architectures built for these inputs must strategically mask either the input
or model weights to learn the various conditional distributions necessary for
inferring the full joint distribution of the dataset in an order-agnostic way.
In this paper, we propose an alternative approach for encoding feature
identities, where each feature's identity is included alongside its value in
the input. This feature identity encoding strategy allows neural architectures
designed for sequential data to be applied to the OADE task without
modification. As a proof of concept, we show that a Transformer trained on this
input (which we refer to as "the DEformer", i.e., the distribution estimating
Transformer) can effectively model binarized-MNIST, approaching the average
negative log-likelihood of fixed order autoregressive distribution estimating
algorithms while still being entirely order-agnostic.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06991" title="Abstract">arXiv:2106.06991</a> [<a href="/pdf/2106.06991" title="Download PDF">pdf</a>, <a href="/format/2106.06991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BoolNet: Minimizing The Energy Consumption of Binary Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+N">Nianhui Guo</a>, 
<a href="/search/cs?searchtype=author&query=Bethge%2C+J">Joseph Bethge</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haojin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+K">Kai Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xuefei Ning</a>, 
<a href="/search/cs?searchtype=author&query=Meinel%2C+C">Christoph Meinel</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent works on Binary Neural Networks (BNNs) have made promising progress in
narrowing the accuracy gap of BNNs to their 32-bit counterparts. However, the
accuracy gains are often based on specialized model designs using additional
32-bit components. Furthermore, almost all previous BNNs use 32-bit for feature
maps and the shortcuts enclosing the corresponding binary convolution blocks,
which helps to effectively maintain the accuracy, but is not friendly to
hardware accelerators with limited memory, energy, and computing resources.
Thus, we raise the following question: How can accuracy and energy consumption
be balanced in a BNN network design? We extensively study this fundamental
problem in this work and propose a novel BNN architecture without most commonly
used 32-bit components: \textit{BoolNet}. Experimental results on ImageNet
demonstrate that BoolNet can achieve 4.6x energy reduction coupled with 1.2\%
higher accuracy than the commonly used BNN architecture Bi-RealNet. Code and
trained models are available at: https://github.com/hpi-xnor/BoolNet.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06992" title="Abstract">arXiv:2106.06992</a> [<a href="/pdf/2106.06992" title="Download PDF">pdf</a>, <a href="/format/2106.06992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Perfect Filtering Enough Leading to Perfect Phase Correction for dMRI  data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feihong%2C+L">Liu Feihong</a>, 
<a href="/search/cs?searchtype=author&query=Junwei%2C+Y">Yang Junwei</a>, 
<a href="/search/cs?searchtype=author&query=Xiaowei%2C+H">He Xiaowei</a>, 
<a href="/search/cs?searchtype=author&query=Luping%2C+Z">Zhou Luping</a>, 
<a href="/search/cs?searchtype=author&query=Jun%2C+F">Feng Jun</a>, 
<a href="/search/cs?searchtype=author&query=Dinggang%2C+S">Shen Dinggang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Being complex-valued and low in signal-to-noise ratios, magnitude-based
diffusion MRI is confounded by the noise-floor that falsely elevates signal
magnitude and incurs bias to the commonly used diffusion indices, such as
fractional anisotropy (FA). To avoid noise-floor, most existing phase
correction methods explore improving filters to estimate the noise-free
background phase. In this work, after diving into the phase correction
procedures, we argue that even a perfect filter is insufficient for phase
correction because the correction procedures are incapable of distinguishing
sign-symbols of noise, resulting in artifacts (\textit{i.e.}, arbitrary signal
loss). With this insight, we generalize the definition of noise-floor to a
complex polar coordinate system and propose a calibration procedure that could
conveniently distinguish noise sign symbols. The calibration procedure is
conceptually simple and easy to implement without relying on any external
technique while keeping distinctly effective.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06996" title="Abstract">arXiv:2106.06996</a> [<a href="/pdf/2106.06996" title="Download PDF">pdf</a>, <a href="/format/2106.06996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pyramidal Dense Attention Networks for Lightweight Image  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huapeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+J">Jie Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhihui Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, deep convolutional neural network methods have achieved an
excellent performance in image superresolution (SR), but they can not be easily
applied to embedded devices due to large memory cost. To solve this problem, we
propose a pyramidal dense attention network (PDAN) for lightweight image
super-resolution in this paper. In our method, the proposed pyramidal dense
learning can gradually increase the width of the densely connected layer inside
a pyramidal dense block to extract deep features efficiently. Meanwhile, the
adaptive group convolution that the number of groups grows linearly with dense
convolutional layers is introduced to relieve the parameter explosion. Besides,
we also present a novel joint attention to capture cross-dimension interaction
between the spatial dimensions and channel dimension in an efficient way for
providing rich discriminative feature representations. Extensive experimental
results show that our method achieves superior performance in comparison with
the state-of-the-art lightweight SR methods.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06997" title="Abstract">arXiv:2106.06997</a> [<a href="/pdf/2106.06997" title="Download PDF">pdf</a>, <a href="/format/2106.06997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Post-hoc loss-calibration for Bayesian neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vadera%2C+M+P">Meet P. Vadera</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Soumya Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+K">Kenney Ng</a>, 
<a href="/search/cs?searchtype=author&query=Marlin%2C+B+M">Benjamin M. Marlin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Conference on Uncertainty in AI (UAI) '21
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Bayesian decision theory provides an elegant framework for acting optimally
under uncertainty when tractable posterior distributions are available. Modern
Bayesian models, however, typically involve intractable posteriors that are
approximated with, potentially crude, surrogates. This difficulty has
engendered loss-calibrated techniques that aim to learn posterior
approximations that favor high-utility decisions. In this paper, focusing on
Bayesian neural networks, we develop methods for correcting approximate
posterior predictive distributions encouraging them to prefer high-utility
decisions. In contrast to previous work, our approach is agnostic to the choice
of the approximate inference algorithm, allows for efficient test time decision
making through amortization, and empirically produces higher quality decisions.
We demonstrate the effectiveness of our approach through controlled experiments
spanning a diversity of tasks and datasets.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06998" title="Abstract">arXiv:2106.06998</a> [<a href="/pdf/2106.06998" title="Download PDF">pdf</a>, <a href="/format/2106.06998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-memory stochastic backpropagation with multi-channel randomized  trace estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Louboutin%2C+M">Mathias Louboutin</a>, 
<a href="/search/cs?searchtype=author&query=Siahkoohi%2C+A">Ali Siahkoohi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rongrong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Herrmann%2C+F+J">Felix J. Herrmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Thanks to the combination of state-of-the-art accelerators and highly
optimized open software frameworks, there has been tremendous progress in the
performance of deep neural networks. While these developments have been
responsible for many breakthroughs, progress towards solving large-scale
problems, such as video encoding and semantic segmentation in 3D, is hampered
because access to on-premise memory is often limited. Instead of relying on
(optimal) checkpointing or invertibility of the network layers -- to recover
the activations during backpropagation -- we propose to approximate the
gradient of convolutional layers in neural networks with a multi-channel
randomized trace estimation technique. Compared to other methods, this approach
is simple, amenable to analyses, and leads to a greatly reduced memory
footprint. Even though the randomized trace estimation introduces stochasticity
during training, we argue that this is of little consequence as long as the
induced errors are of the same order as errors in the gradient due to the use
of stochastic gradient descent. We discuss the performance of networks trained
with stochastic backpropagation and how the error can be controlled while
maximizing memory usage and minimizing computational overhead.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07000" title="Abstract">arXiv:2106.07000</a> [<a href="/pdf/2106.07000" title="Download PDF">pdf</a>, <a href="/format/2106.07000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Large Scale Aerial Terrestrial Networks with mmWave  Backhauling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kouzayha%2C+N">Nour Kouzayha</a>, 
<a href="/search/cs?searchtype=author&query=ElSawy%2C+H">Hesham ElSawy</a>, 
<a href="/search/cs?searchtype=author&query=Dahrouj%2C+H">Hayssam Dahrouj</a>, 
<a href="/search/cs?searchtype=author&query=Alshaikh%2C+K">Khlod Alshaikh</a>, 
<a href="/search/cs?searchtype=author&query=Al-Naffouri%2C+T+Y">Tareq Y. Al-Naffouri</a>, 
<a href="/search/cs?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Service providers are considering the use of unmanned aerial vehicles (UAVs)
to enhance wireless connectivity of cellular networks. To provide connectivity,
UAVs have to be backhauled through terrestrial base stations (BSs) to the core
network. In particular, we consider millimeter-wave (mmWave) backhauling in the
downlink of a hybrid aerial-terrestrial network, where the backhaul links are
subject to beamforming misalignment errors. In the proposed model, the user
equipment (UE) can connect to either a ground BS or a UAV, where we
differentiate between two transmission schemes according to the backhaul
status. In one scheme, the UEs are served by the UAVs regardless of whether the
backhaul links are good or not. In the other scheme, the UAVs are aware of the
backhaul links status, and hence, only the subset of successfully backhauled
UAVs can serve the UEs. Using stochastic geometry, the performance of the
proposed model is assessed in terms of coverage probability and validated
against Monte-Carlo simulations. Several insights are provided for determining
some system parameters including the UAVs altitude and required number and the
beamforming misalignment error of the backhaul link. The obtained results
highlight the impact of the UAVs backhaul link on the UE experience.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07003" title="Abstract">arXiv:2106.07003</a> [<a href="/pdf/2106.07003" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Analysis of Trajectory Control Using Computer Vision and  Artificial Intelligence for Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbas%2C+A+N">Ammar N. Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Irshad%2C+M+A">Muhammad Asad Irshad</a>, 
<a href="/search/cs?searchtype=author&query=Ammar%2C+H+H">Hossam Hassan Ammar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Perception of the lane boundaries is crucial for the tasks related to
autonomous trajectory control. In this paper, several methodologies for lane
detection are discussed with an experimental illustration: Hough
transformation, Blob analysis, and Bird's eye view. Following the abstraction
of lane marks from the boundary, the next approach is applying a control law
based on the perception to control steering and speed control. In the
following, a comparative analysis is made between an open-loop response, PID
control, and a neural network control law through graphical statistics. To get
the perception of the surrounding a wireless streaming camera connected to
Raspberry Pi is used. After pre-processing the signal received by the camera
the output is sent back to the Raspberry Pi that processes the input and
communicates the control to the motors through Arduino via serial
communication.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07009" title="Abstract">arXiv:2106.07009</a> [<a href="/pdf/2106.07009" title="Download PDF">pdf</a>, <a href="/format/2106.07009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise2Score: Tweedie&#x27;s Approach to Self-Supervised Image Denoising  without Clean Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kwanyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recently, there has been extensive research interest in training deep
networks to denoise images without clean reference. However, the representative
approaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator
(SURE), etc. seem to differ from one another and it is difficult to find the
coherent mathematical structure. To address this, here we present a novel
approach, called Noise2Score, which reveals a missing link in order to unite
these seemingly different approaches. Specifically, we show that image
denoising problems without clean images can be addressed by finding the mode of
the posterior distribution and that the Tweedie's formula offers an explicit
solution through the score function (i.e. the gradient of log likelihood). Our
method then uses the recent finding that the score function can be stably
estimated from the noisy images using the amortized residual denoising
autoencoder, the method of which is closely related to Noise2Noise or
Nose2Void. Our Noise2Score approach is so universal that the same network
training can be used to remove noises from images that are corrupted by any
exponential family distributions and noise parameters. Using extensive
experiments with Gaussian, Poisson, and Gamma noises, we show that Noise2Score
significantly outperforms the state-of-the-art self-supervised denoising
methods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07011" title="Abstract">arXiv:2106.07011</a> [<a href="/pdf/2106.07011" title="Download PDF">pdf</a>, <a href="/format/2106.07011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Underwater Soft Robotic Hand with Multi-Source Coupling Bio-Inspired  Soft Palm and Six Fingers Driven by Water Hydraulic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haihang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">He Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siqing Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">A new fluid-driven soft robot hand in this study uses the idea of the bionics
and has the anthropomorphic form, which is oriented to the flexible grasp
function. The soft robot hand is composed of a new kind of multi-freedom soft
finger and soft palm, which realizes the characteristic grasping function of
forehand and backhand. Combined with the fine fluid control system, the soft
hand can realize flexible grasping under high pressure, so as to realize
flexible grasping operation for different types of target objects in the
underwater environment. The soft robot hand was controlled based on water
hydraulic platform, Finally, the soft robot hand and the fine fluid control
system were connected to form the underwater soft robot hand experiment
platform.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07012" title="Abstract">arXiv:2106.07012</a> [<a href="/pdf/2106.07012" title="Download PDF">pdf</a>, <a href="/format/2106.07012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incomplete Gamma Integrals for Deep Cascade Prediction using Content,  Network, and Exogenous Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Subhabrata Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Shravika Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+D">Dipankar Das</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarti%2C+S">Soumen Chakrabarti</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The behaviour of information cascades (such as retweets) has been modelled
extensively. While point process-based generative models have long been in use
for estimating cascade growths, deep learning has greatly enhanced diverse
feature integration. We observe two significant temporal signals in cascade
data that have not been emphasized or reported to our knowledge. First, the
popularity of the cascade root is known to influence cascade size strongly; but
the effect falls off rapidly with time. Second, there is a measurable positive
correlation between the novelty of the root content (with respect to a
streaming external corpus) and the relative size of the resulting cascade.
Responding to these observations, we propose GammaCas, a new cascade growth
model as a parametric function of time, which combines deep influence signals
from content (e.g., tweet text), network features (e.g., followers of the root
user), and exogenous event sources (e.g., online news). Specifically, our model
processes these signals through a customized recurrent network, whose states
then provide the parameters of the cascade rate function, which is integrated
over time to predict the cascade size. The network parameters are trained
end-to-end using observed cascades. GammaCas outperforms seven recent and
diverse baselines significantly on a large-scale dataset of retweet cascades
coupled with time-aligned online news -- it beats the best baseline with an
18.98% increase in terms of Kendall's $\tau$ correlation and $35.63$ reduction
in Mean Absolute Percentage Error. Extensive ablation and case studies unearth
interesting insights regarding retweet cascade dynamics.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07015" title="Abstract">arXiv:2106.07015</a> [<a href="/pdf/2106.07015" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Siamese Network Training Using Sampled Triplets and Image Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbas%2C+A+N">Ammar N. Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Moser%2C+D">David Moser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The device used in this work detects the objects over the surface of the
water using two thermal cameras which aid the users to detect and avoid the
objects in scenarios where the human eyes cannot (night, fog, etc.). To avoid
the obstacle collision autonomously, it is required to track the objects in
real-time and assign a specific identity to each object to determine its
dynamics (trajectory, velocity, etc.) for making estimated collision
predictions. In the following work, a Machine Learning (ML) approach for
Computer Vision (CV) called Convolutional Neural Network (CNN) was used using
TensorFlow as the high-level programming environment in Python. To validate the
algorithm a test set was generated using an annotation tool that was created
during the work for proper evaluation. Once validated, the algorithm was
deployed on the platform and tested with the sequence generated by the test
boat.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07017" title="Abstract">arXiv:2106.07017</a> [<a href="/pdf/2106.07017" title="Download PDF">pdf</a>, <a href="/format/2106.07017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The k-mappability problem revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amir%2C+A">Amihood Amir</a>, 
<a href="/search/cs?searchtype=author&query=Boneh%2C+I">Itai Boneh</a>, 
<a href="/search/cs?searchtype=author&query=Kondratovsky%2C+E">Eitan Kondratovsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The $k$-mappability problem has two integers parameters $m$ and $k$. For
every subword of size $m$ in a text $S$, we wish to report the number of
indices in $S$ in which the word occurs with at most $k$ mismatches.
<br />The problem was lately tackled by Alzamel et al. For a text with constant
alphabet $\Sigma$ and $k \in O(1)$, they present an algorithm with linear space
and $O(n\log^{k+1}n)$ time. For the case in which $k = 1$ and a constant size
alphabet, a faster algorithm with linear space and $O(n\log(n)\log\log(n))$
time was presented in a 2020 paper by Alzamel et al.
<br />In this work, we enhance the techniques of Alzamel et al.'s 2020 paper to
obtain an algorithm with linear space and $O(n \log(n))$ time for $k = 1$. Our
algorithm removes the constraint of the alphabet being of constant size. We
also present linear algorithms for the case of $k=1$, $|\Sigma|\in O(1)$ and
$m=\Omega(\sqrt{n})$.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07020" title="Abstract">arXiv:2106.07020</a> [<a href="/pdf/2106.07020" title="Download PDF">pdf</a>, <a href="/format/2106.07020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generation of the NIR spectral Band for Satellite Images with  Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Illarionova%2C+S">Svetlana Illarionova</a>, 
<a href="/search/cs?searchtype=author&query=Shadrin%2C+D">Dmitrii Shadrin</a>, 
<a href="/search/cs?searchtype=author&query=Trekin%2C+A">Alexey Trekin</a>, 
<a href="/search/cs?searchtype=author&query=Ignatiev%2C+V">Vladimir Ignatiev</a>, 
<a href="/search/cs?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The near-infrared (NIR) spectral range (from 780 to 2500 nm) of the
multispectral remote sensing imagery provides vital information for the
landcover classification, especially concerning the vegetation assessment.
Despite the usefulness of NIR, common RGB is not always accompanied by it.
Modern achievements in image processing via deep neural networks allow
generating artificial spectral information, such as for the image colorization
problem. In this research, we aim to investigate whether this approach can
produce not only visually similar images but also an artificial spectral band
that can improve the performance of computer vision algorithms for solving
remote sensing tasks. We study the generative adversarial network (GAN)
approach in the task of the NIR band generation using just RGB channels of
high-resolution satellite imagery. We evaluate the impact of a generated
channel on the model performance for solving the forest segmentation task. Our
results show an increase in model accuracy when using generated NIR comparing
to the baseline model that uses only RGB (0.947 and 0.914 F1-score
accordingly). Conducted study shows the advantages of generating the extra band
and its implementation in applied challenges reducing the required amount of
labeled data.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07022" title="Abstract">arXiv:2106.07022</a> [<a href="/pdf/2106.07022" title="Download PDF">pdf</a>, <a href="/format/2106.07022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Speed Control Methodology for Variable Speed Wind Turbines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Al-Jodah%2C+A">Ammar Al-Jodah</a>, 
<a href="/search/eess?searchtype=author&query=Alwan%2C+M">Marwah Alwan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Improving wind turbine efficiency is essential for reducing the costs of
energy production. The highly nonlinear dynamics of the wind turbines and their
uncertain operating conditions have posed many challenges for their control
methods. In this work, a robust control strategy based on sliding mode and
adaptive fuzzy disturbance observer is proposed for speed tracking in a
variable speed wind turbine. First, the nonlinear mathematical model that
describes the dynamics of the variable speed wind turbine is derived. This
nonlinear model is then used to derive the control methodology and to find
stability and robustness conditions. The control approach is designed to track
the optimal wind speed that causes maximum energy extraction. The stability
condition was verified using the Lyapunov stability theory. A simulation study
was conducted to verify the method, and a comparative analysis was used to
measure its effectiveness. The results showed a high tracking ability and
robustness of the developed methodology. Moreover, higher power extraction was
observed when compared to a classical control method.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07023" title="Abstract">arXiv:2106.07023</a> [<a href="/pdf/2106.07023" title="Download PDF">pdf</a>, <a href="/format/2106.07023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Styleformer: Transformer based Generative Adversarial Networks with  Style Vector
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jeeseung Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Younggeun Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose Styleformer, which is a style-based generator for GAN
architecture, but a convolution-free transformer-based generator. In our paper,
we explain how a transformer can generate high-quality images, overcoming the
disadvantage that convolution operations are difficult to capture global
features in an image. Furthermore, we change the demodulation of StyleGAN2 and
modify the existing transformer structure (e.g., residual connection, layer
normalization) to create a strong style-based generator with a convolution-free
structure. We also make Styleformer lighter by applying Linformer, enabling
Styleformer to generate higher resolution images and result in improvements in
terms of speed and memory. We experiment with the low-resolution image dataset
such as CIFAR-10, as well as the high-resolution image dataset like
LSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark
dataset, which is comparable performance to the current state-of-the-art and
outperforms all GAN-based generative models, including StyleGAN2-ADA with fewer
parameters on the unconditional setting. We also both achieve new
state-of-the-art with FID 20.11, IS 10.16, and FID 3.66, respectively on STL-10
and CelebA. We release our code at
https://github.com/Jeeseung-Park/Styleformer.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07024" title="Abstract">arXiv:2106.07024</a> [<a href="/pdf/2106.07024" title="Download PDF">pdf</a>, <a href="/format/2106.07024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite-Length Bounds on Hypothesis Testing Subject to Vanishing Type I  Error Restrictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Espinosa%2C+S">Sebastian Espinosa</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+J+F">Jorge F. Silva</a>, 
<a href="/search/cs?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Vol. 28, 2021, 229 - 233
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">A central problem in Binary Hypothesis Testing (BHT) is to determine the
optimal tradeoff between the Type I error (referred to as false alarm) and Type
II (referred to as miss) error. In this context, the exponential rate of
convergence of the optimal miss error probability -- as the sample size tends
to infinity -- given some (positive) restrictions on the false alarm
probabilities is a fundamental question to address in theory. Considering the
more realistic context of a BHT with a finite number of observations, this
paper presents a new non-asymptotic result for the scenario with monotonic
(sub-exponential decreasing) restriction on the Type I error probability, which
extends the result presented by Strassen in 2009. Building on the use of
concentration inequalities, we offer new upper and lower bounds to the optimal
Type II error probability for the case of finite observations. Finally, the
derived bounds are evaluated and interpreted numerically (as a function of the
number samples) for some vanishing Type I error restrictions.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07026" title="Abstract">arXiv:2106.07026</a> [<a href="/pdf/2106.07026" title="Download PDF">pdf</a>, <a href="/format/2106.07026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reborn Mechanism: Rethinking the Negative Phase Information Flow in  Convolutional Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhicheng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaizhu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Chenglei Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper proposes a novel nonlinear activation mechanism typically for
convolutional neural network (CNN), named as reborn mechanism. In sharp
contrast to ReLU which cuts off the negative phase value, the reborn mechanism
enjoys the capacity to reborn and reconstruct dead neurons. Compared to other
improved ReLU functions, reborn mechanism introduces a more proper way to
utilize the negative phase information. Extensive experiments validate that
this activation mechanism is able to enhance the model representation ability
more significantly and make the better use of the input data information while
maintaining the advantages of the original ReLU function. Moreover, reborn
mechanism enables a non-symmetry that is hardly achieved by traditional CNNs
and can act as a channel compensation method, offering competitive or even
better performance but with fewer learned parameters than traditional methods.
Reborn mechanism was tested on various benchmark datasets, all obtaining better
performance than previous nonlinear activation functions.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07029" title="Abstract">arXiv:2106.07029</a> [<a href="/pdf/2106.07029" title="Download PDF">pdf</a>, <a href="/format/2106.07029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSS-PRNU: Privacy-Preserving PRNU Based Camera Attribution using Shamir  Secret Sharing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jena%2C+R">Riyanka Jena</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+P">Priyanka Singh</a>, 
<a href="/search/cs?searchtype=author&query=Mohanty%2C+M">Manoranjan Mohanty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Photo Response Non-Uniformity(PRNU) noise has proven to be very effective
tool in camera based forensics. It helps to match a photo to the device that
clicked it. In today's scenario, where millions and millions of images are
uploaded every hour, it is very easy to compute this unique PRNU pattern from a
couple of shared images on social profiles. This endangers the privacy of the
camera owner and becomes a cause of major concern for the privacy-aware
society. We propose SSS-PRNU scheme that facilitates the forensic investigators
to carry out their crime investigation without breaching the privacy of the
people. Thus, maintaining a balance between the two. To preserve privacy,
extraction of camera fingerprint and PRNU noise for a suspicious image is
computed in a trusted execution environment such as ARM TrustZone. After
extraction, the sensitive information of camera fingerprint and PRNU noise is
distributed into multiple obfuscated shares using Shamir secret sharing(SSS)
scheme. These shares are information-theoretically secure and leak no
information of underlying content. The encrypted information is distributed to
multiple third-part servers where correlation is computed on a share basis
between the camera fingerprint and the PRNU noise. These partial correlation
values are combined together to obtain the final correlation value that becomes
the basis for a match decision. Transforming the computation of the correlation
value in the encrypted domain and making it well suited for a distributed
environment is the main contribution of the paper. Experiment results validate
the feasibility of the proposed scheme that provides a secure framework for
PRNU based source camera attribution. The security analysis and evaluation of
computational and storage overheads are performed to analysis the practical
feasibility of the scheme.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07030" title="Abstract">arXiv:2106.07030</a> [<a href="/pdf/2106.07030" title="Download PDF">pdf</a>, <a href="/format/2106.07030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Backpropagation Algorithm Implemented on Spiking Neuromorphic  Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Renner%2C+A">Alpha Renner</a>, 
<a href="/search/cs?searchtype=author&query=Sheldon%2C+F">Forrest Sheldon</a>, 
<a href="/search/cs?searchtype=author&query=Zlotnik%2C+A">Anatoly Zlotnik</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+L">Louis Tao</a>, 
<a href="/search/cs?searchtype=author&query=Sornborger%2C+A">Andrew Sornborger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">The capabilities of natural neural systems have inspired new generations of
machine learning algorithms as well as neuromorphic very large-scale integrated
(VLSI) circuits capable of fast, low-power information processing. However,
most modern machine learning algorithms are not neurophysiologically plausible
and thus are not directly implementable in neuromorphic hardware. In
particular, the workhorse of modern deep learning, the backpropagation
algorithm, has proven difficult to translate to neuromorphic hardware. In this
study, we present a neuromorphic, spiking backpropagation algorithm based on
pulse-gated dynamical information coordination and processing, implemented on
Intel's Loihi neuromorphic research processor. We demonstrate a
proof-of-principle three-layer circuit that learns to classify digits from the
MNIST dataset. This implementation shows a path for using massively parallel,
low-power, low-latency neuromorphic processors in modern deep learning
applications.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07032" title="Abstract">arXiv:2106.07032</a> [<a href="/pdf/2106.07032" title="Download PDF">pdf</a>, <a href="/ps/2106.07032" title="Download PostScript">ps</a>, <a href="/format/2106.07032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Category Theory in Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shiebler%2C+D">Dan Shiebler</a>, 
<a href="/search/cs?searchtype=author&query=Gavranovi%C4%87%2C+B">Bruno Gavranovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+P">Paul Wilson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Over the past two decades machine learning has permeated almost every realm
of technology. At the same time, many researchers have begun using category
theory as a unifying language, facilitating communication between different
scientific disciplines. It is therefore unsurprising that there is a burgeoning
interest in applying category theory to machine learning. We aim to document
the motivations, goals and common themes across these applications. We touch on
gradient-based learning, probability, and equivariant learning.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07033" title="Abstract">arXiv:2106.07033</a> [<a href="/pdf/2106.07033" title="Download PDF">pdf</a>, <a href="/format/2106.07033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Interplay between Privacy and Robustness in Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yaowei Han</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yoshikawa%2C+M">Masatoshi Yoshikawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated Learning (FL) is emerging as a promising paradigm of
privacy-preserving machine learning, which trains an algorithm across multiple
clients without exchanging their data samples. Recent works highlighted several
privacy and robustness weaknesses in FL and addressed these concerns using
local differential privacy (LDP) and some well-studied methods used in
conventional ML, separately. However, it is still not clear how LDP affects
adversarial robustness in FL. To fill this gap, this work attempts to develop a
comprehensive understanding of the effects of LDP on adversarial robustness in
FL. Clarifying the interplay is significant since this is the first step
towards a principled design of private and robust FL systems. We certify that
local differential privacy has both positive and negative effects on
adversarial robustness using theoretical analysis and empirical verification.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07034" title="Abstract">arXiv:2106.07034</a> [<a href="/pdf/2106.07034" title="Download PDF">pdf</a>, <a href="/format/2106.07034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Extended Multi-Model Regression Approach for Compressive Strength  Prediction and Optimization of a Concrete Mixture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Motlagh%2C+S+A+T">Seyed Arman Taghizadeh Motlagh</a> (1), 
<a href="/search/cs?searchtype=author&query=Naghizadehrokni%2C+M">Mehran Naghizadehrokni</a> (2) ((1) Azad University, Central Tehran Branch (IAUCTB), (2) RWTH Aachen University, Lehrstuhl fur Geotechnik im Bauwesen und Institut fur Geomechanik und Untergrundtechnik)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Due to the significant delay and cost associated with experimental tests, a
model based evaluation of concrete compressive strength is of high value, both
for the purpose of strength prediction as well as the mixture optimization. In
this regard, several recent studies have employed state-of-the-art regression
models in order to achieve a good prediction model, employing available
experimental data sets. Nevertheless, while each of the employed models can
better adapt to a specific nature of the input data, the accuracy of each
individual model is limited due to the sensitivity to the choice of
hyperparameters and the learning strategy. In the present work, we take a
further step towards improving the accuracy of the prediction model via the
weighted combination of multiple regression methods. Moreover, a (GA)-based
multi-objective mixture optimization is proposed, building on the obtained
multi-regression model. In particular, we present a data aided framework where
the regression methods based on artificial neural network, random forest
regression, and polynomial regression are jointly implemented to predict the
compressive strength of concrete. The outcome of the individual regression
models are then combined via a linear weighting strategy and optimized over the
training data set as a quadratic convex optimization problem. It is worth
mentioning that due to the convexity of the formulated problem, the globally
optimum weighting strategy is obtained via standard numerical solvers.
Employing the proposed GA-based optimization, a Pareto front of the cost-CS
trade-of has been obtained employing the available data set. Moreover, the
resulting accuracy of the proposed multi-model prediction method is shown to
outperform the available single-model regression methods in the literature by a
valuable margin, via numerical simulations.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07035" title="Abstract">arXiv:2106.07035</a> [<a href="/pdf/2106.07035" title="Download PDF">pdf</a>, <a href="/format/2106.07035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Bayesian Unsupervised Lifelong Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tingting Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Masoomi%2C+A">Aria Masoomi</a>, 
<a href="/search/cs?searchtype=author&query=Dy%2C+J">Jennifer Dy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Lifelong Learning (LL) refers to the ability to continually learn and solve
new problems with incremental available information over time while retaining
previous knowledge. Much attention has been given lately to Supervised Lifelong
Learning (SLL) with a stream of labelled data. In contrast, we focus on
resolving challenges in Unsupervised Lifelong Learning (ULL) with streaming
unlabelled data when the data distribution and the unknown class labels evolve
over time. Bayesian framework is natural to incorporate past knowledge and
sequentially update the belief with new data. We develop a fully Bayesian
inference framework for ULL with a novel end-to-end Deep Bayesian Unsupervised
Lifelong Learning (DBULL) algorithm, which can progressively discover new
clusters without forgetting the past with unlabelled data while learning latent
representations. To efficiently maintain past knowledge, we develop a novel
knowledge preservation mechanism via sufficient statistics of the latent
representation for raw data. To detect the potential new clusters on the fly,
we develop an automatic cluster discovery and redundancy removal strategy in
our inference inspired by Nonparametric Bayesian statistics techniques. We
demonstrate the effectiveness of our approach using image and text corpora
benchmark datasets in both LL and batch settings.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07036" title="Abstract">arXiv:2106.07036</a> [<a href="/pdf/2106.07036" title="Download PDF">pdf</a>, <a href="/format/2106.07036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protein-Ligand Docking Surrogate Models: A SARS-CoV-2 Benchmark for Deep  Learning Accelerated Virtual Screening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clyde%2C+A">Austin Clyde</a>, 
<a href="/search/cs?searchtype=author&query=Brettin%2C+T">Thomas Brettin</a>, 
<a href="/search/cs?searchtype=author&query=Partin%2C+A">Alex Partin</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+H">Hyunseung Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Babuji%2C+Y">Yadu Babuji</a>, 
<a href="/search/cs?searchtype=author&query=Blaiszik%2C+B">Ben Blaiszik</a>, 
<a href="/search/cs?searchtype=author&query=Merzky%2C+A">Andre Merzky</a>, 
<a href="/search/cs?searchtype=author&query=Turilli%2C+M">Matteo Turilli</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Shantenu Jha</a>, 
<a href="/search/cs?searchtype=author&query=Ramanathan%2C+A">Arvind Ramanathan</a>, 
<a href="/search/cs?searchtype=author&query=Stevens%2C+R">Rick Stevens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a benchmark to study surrogate model accuracy for protein-ligand
docking. We share a dataset consisting of 200 million 3D complex structures and
2D structure scores across a consistent set of 13 million ``in-stock''
molecules over 15 receptors, or binding sites, across the SARS-CoV-2 proteome.
Our work shows surrogate docking models have six orders of magnitude more
throughput than standard docking protocols on the same supercomputer node
types. We demonstrate the power of high-speed surrogate models by running each
target against 1 billion molecules in under a day (50k predictions per GPU
seconds). We showcase a workflow for docking utilizing surrogate ML models as a
pre-filter. Our workflow is ten times faster at screening a library of
compounds than the standard technique, with an error rate less than 0.01\% of
detecting the underlying best scoring 0.1\% of compounds. Our analysis of the
speedup explains that to screen more molecules under a docking paradigm,
another order of magnitude speedup must come from model accuracy rather than
computing speed (which, if increased, will not anymore alter our throughput to
screen molecules). We believe this is strong evidence for the community to
begin focusing on improving the accuracy of surrogate models to improve the
ability to screen massive compound libraries 100x or even 1000x faster than
current techniques.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07037" title="Abstract">arXiv:2106.07037</a> [<a href="/pdf/2106.07037" title="Download PDF">pdf</a>, <a href="/format/2106.07037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hash Adaptive Bloom Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Rongbiao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+Z">Zheyu Miao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+R">Rong Gu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">He Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Haipeng Dai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guihai Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, accepted by ICDE 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Bloom filter is a compact memory-efficient probabilistic data structure
supporting membership testing, i.e., to check whether an element is in a given
set. However, as Bloom filter maps each element with uniformly random hash
functions, few flexibilities are provided even if the information of negative
keys (elements are not in the set) are available. The problem gets worse when
the misidentification of negative keys brings different costs. To address the
above problems, we propose a new Hash Adaptive Bloom Filter (HABF) that
supports the customization of hash functions for keys. The key idea of HABF is
to customize the hash functions for positive keys (elements are in the set) to
avoid negative keys with high cost, and pack customized hash functions into a
lightweight data structure named HashExpressor. Then, given an element at query
time, HABF follows a two-round pattern to check whether the element is in the
set. Further, we theoretically analyze the performance of HABF and bound the
expected false positive rate. We conduct extensive experiments on
representative datasets, and the results show that HABF outperforms the
standard Bloom filter and its cutting-edge variants on the whole in terms of
accuracy, construction time, query time, and memory space consumption (Note
that source codes are available in [1]).
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07039" title="Abstract">arXiv:2106.07039</a> [<a href="/pdf/2106.07039" title="Download PDF">pdf</a>, <a href="/format/2106.07039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contingency-Aware Influence Maximization: A Reinforcement Learning  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haipeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+W">Wei Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+H">Han-Ching Ou</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bo An</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+M">Milind Tambe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages; accepted for publication at UAI 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The influence maximization (IM) problem aims at finding a subset of seed
nodes in a social network that maximize the spread of influence. In this study,
we focus on a sub-class of IM problems, where whether the nodes are willing to
be the seeds when being invited is uncertain, called contingency-aware IM. Such
contingency aware IM is critical for applications for non-profit organizations
in low resource communities (e.g., spreading awareness of disease prevention).
Despite the initial success, a major practical obstacle in promoting the
solutions to more communities is the tremendous runtime of the greedy
algorithms and the lack of high performance computing (HPC) for the non-profits
in the field -- whenever there is a new social network, the non-profits usually
do not have the HPCs to recalculate the solutions. Motivated by this and
inspired by the line of works that use reinforcement learning (RL) to address
combinatorial optimization on graphs, we formalize the problem as a Markov
Decision Process (MDP), and use RL to learn an IM policy over historically seen
networks, and generalize to unseen networks with negligible runtime at test
phase. To fully exploit the properties of our targeted problem, we propose two
technical innovations that improve the existing methods, including
state-abstraction and theoretically grounded reward shaping. Empirical results
show that our method achieves influence as high as the state-of-the-art methods
for contingency-aware IM, while having negligible runtime at test phase.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07041" title="Abstract">arXiv:2106.07041</a> [<a href="/pdf/2106.07041" title="Download PDF">pdf</a>, <a href="/format/2106.07041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correcting Exposure Bias for Link Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shantanu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Link prediction methods are frequently applied in recommender systems, e.g.,
to suggest citations for academic papers or friends in social networks.
However, exposure bias can arise when users are systematically underexposed to
certain relevant items. For example, in citation networks, authors might be
more likely to encounter papers from their own field and thus cite them
preferentially. This bias can propagate through naively trained link
predictors, leading to both biased evaluation and high generalization error (as
assessed by true relevance). Moreover, this bias can be exacerbated by feedback
loops. We propose estimators that leverage known exposure probabilities to
mitigate this bias and consequent feedback loops. Next, we provide a loss
function for learning the exposure probabilities from data. Finally,
experiments on semi-synthetic data based on real-world citation networks, show
that our methods reliably identify (truly) relevant citations. Additionally,
our methods lead to greater diversity in the recommended papers' fields of
study. The code is available at
https://github.com/shantanu95/exposure-bias-link-rec.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07045" title="Abstract">arXiv:2106.07045</a> [<a href="/pdf/2106.07045" title="Download PDF">pdf</a>, <a href="/format/2106.07045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VeriFly: On-the-fly Assertion Checking via Incrementality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanchez-Ordaz%2C+M+A">Miguel A. Sanchez-Ordaz</a>, 
<a href="/search/cs?searchtype=author&query=Garcia-Contreras%2C+I">Isabel Garcia-Contreras</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Carrasco%2C+V">Victor Perez-Carrasco</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+J+F">Jose F. Morales</a>, 
<a href="/search/cs?searchtype=author&query=lopez-Garcia%2C+P">Pedro lopez-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Hermenegildo%2C+M+V">Manuel V. Hermenegildo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 11 figures, 2 tables; submitted to ICLP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Assertion checking is an invaluable programmer's tool for finding many
classes of errors or verifying their absence in dynamic languages such as
Prolog. For Prolog programmers this means being able to have relevant
properties such as modes, types, determinacy, non-failure, sharing,
constraints, cost, etc., checked and errors flagged without having to actually
run the program. Such global static analysis tools are arguably most useful the
earlier they are used in the software development cycle, and fast response
times are essential for interactive use. Triggering a full and precise semantic
analysis of a software project every time a change is made can be prohibitively
expensive. In our static analysis and verification framework this challenge is
addressed through a combination of modular and incremental (context- and
path-sensitive) analysis that is responsive to program edits, at different
levels of granularity. We describe how the combination of this framework within
an integrated development environment (IDE) takes advantage of such
incrementality to achieve a high level of reactivity when reflecting analysis
and verification results back as colorings and tooltips directly on the program
text -- the tool's VeriFly mode. The concrete implementation that we describe
is Emacs-based and reuses in part off-the-shelf "on-the-fly" syntax checking
facilities (flycheck). We believe that similar extensions are also reproducible
with low effort in other mature development environments. Our initial
experience with the tool shows quite promising results, with low latency times
that provide early, continuous, and precise assertion checking and other
semantic feedback to programmers during the development process. The tool
supports Prolog natively, as well as other languages by semantic transformation
into Horn clauses.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07046" title="Abstract">arXiv:2106.07046</a> [<a href="/pdf/2106.07046" title="Download PDF">pdf</a>, <a href="/format/2106.07046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Tight Bounds on the Sample Complexity of Average-reward MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yujia Jin</a>, 
<a href="/search/cs?searchtype=author&query=Sidford%2C+A">Aaron Sidford</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">We prove new upper and lower bounds for sample complexity of finding an
$\epsilon$-optimal policy of an infinite-horizon average-reward Markov decision
process (MDP) given access to a generative model. When the mixing time of the
probability transition matrix of all policies is at most $t_\mathrm{mix}$, we
provide an algorithm that solves the problem using
$\widetilde{O}(t_\mathrm{mix} \epsilon^{-3})$ (oblivious) samples per
state-action pair. Further, we provide a lower bound showing that a linear
dependence on $t_\mathrm{mix}$ is necessary in the worst case for any algorithm
which computes oblivious samples. We obtain our results by establishing
connections between infinite-horizon average-reward MDPs and discounted MDPs of
possible further utility.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07047" title="Abstract">arXiv:2106.07047</a> [<a href="/pdf/2106.07047" title="Download PDF">pdf</a>, <a href="/format/2106.07047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Target Model Agnostic Adversarial Attacks with Query Budgets on Language  Understanding Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+J">Jatin Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Bhukar%2C+K">Karan Bhukar</a>, 
<a href="/search/cs?searchtype=author&query=Kaul%2C+M">Manohar Kaul</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Despite significant improvements in natural language understanding models
with the advent of models like BERT and XLNet, these neural-network based
classifiers are vulnerable to blackbox adversarial attacks, where the attacker
is only allowed to query the target model outputs. We add two more realistic
restrictions on the attack methods, namely limiting the number of queries
allowed (query budget) and crafting attacks that easily transfer across
different pre-trained models (transferability), which render previous attack
models impractical and ineffective. Here, we propose a target model agnostic
adversarial attack method with a high degree of attack transferability across
the attacked models. Our empirical studies show that in comparison to baseline
methods, our method generates highly transferable adversarial sentences under
the restriction of limited query budgets.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07049" title="Abstract">arXiv:2106.07049</a> [<a href="/pdf/2106.07049" title="Download PDF">pdf</a>, <a href="/format/2106.07049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly-supervised High-resolution Segmentation of Mammography Images for  Breast Cancer Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kangning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yiqiu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+N">Nan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ch%C5%82%C4%99dowski%2C+J">Jakub Ch&#x142;&#x119;dowski</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Granda%2C+C">Carlos Fernandez-Granda</a>, 
<a href="/search/cs?searchtype=author&query=Geras%2C+K+J">Krzysztof J. Geras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The last two authors contributed equally. Accepted to Medical Imaging with Deep Learning (MIDL) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the last few years, deep learning classifiers have shown promising results
in image-based medical diagnosis. However, interpreting the outputs of these
models remains a challenge. In cancer diagnosis, interpretability can be
achieved by localizing the region of the input image responsible for the
output, i.e. the location of a lesion. Alternatively, segmentation or detection
models can be trained with pixel-wise annotations indicating the locations of
malignant lesions. Unfortunately, acquiring such labels is labor-intensive and
requires medical expertise. To overcome this difficulty, weakly-supervised
localization can be utilized. These methods allow neural network classifiers to
output saliency maps highlighting the regions of the input most relevant to the
classification task (e.g. malignant lesions in mammograms) using only
image-level labels (e.g. whether the patient has cancer or not) during
training. When applied to high-resolution images, existing methods produce
low-resolution saliency maps. This is problematic in applications in which
suspicious lesions are small in relation to the image size. In this work, we
introduce a novel neural network architecture to perform weakly-supervised
segmentation of high-resolution images. The proposed model selects regions of
interest via coarse-level localization, and then performs fine-grained
segmentation of those regions. We apply this model to breast cancer diagnosis
with screening mammography, and validate it on a large clinically-realistic
dataset. Measured by Dice similarity score, our approach outperforms existing
methods by a large margin in terms of localization performance of benign and
malignant lesions, relatively improving the performance by 39.6% and 20.0%,
respectively. Code and the weights of some of the models are available at
https://github.com/nyukat/GLAM
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07051" title="Abstract">arXiv:2106.07051</a> [<a href="/pdf/2106.07051" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative analysis of quality of service scheduling classes in mobile  ad-hoc networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phakathi%2C+T">Thulani Phakathi</a>, 
<a href="/search/cs?searchtype=author&query=Esiefarienrhe%2C+B+M">Bukohwo Michael Esiefarienrhe</a>, 
<a href="/search/cs?searchtype=author&query=Lugayizi%2C+F">Francis Lugayizi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NCWMC - 2021 pp. 211-220, 2021. CS &amp; IT - CSCP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Quality of Service (QoS) is now regarded as a requirement for all networks in
managing resources like bandwidth and avoidance of network impairments like
packet loss, jitter, and delay. Media transfer or streaming would be virtually
impossible if QoS parameters were not used even if the streaming protocols were
perfectly designed. QoS Scheduling classes help in network traffic optimization
and the priority management of packets. This paper presents an analysis of QoS
scheduling classes using video traffic in a MANET. The main objective was to
identify a scheduling class that provides better QoS for video streaming. A
simulation was conducted using NetSim and results were analyzed according to
throughput, jitter, and delay. The overall results showed that extended
real-time Polling Service (ertPS) outperformed the other classes. ertPS has
hybrid features of both real-time Polling Service (rtPS) and Unsolicited Grant
Service(UGS) hence the enhanced performance. It is recommended that ertPS
scheduling class should be used in MANET where QoS consideration is utmost
particularly in multimedia streaming applications.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07052" title="Abstract">arXiv:2106.07052</a> [<a href="/pdf/2106.07052" title="Download PDF">pdf</a>, <a href="/format/2106.07052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wide Mean-Field Variational Bayesian Neural Networks Ignore the Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coker%2C+B">Beau Coker</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Weiwei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Doshi-Velez%2C+F">Finale Doshi-Velez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Variational inference enables approximate posterior inference of the highly
over-parameterized neural networks that are popular in modern machine learning.
Unfortunately, such posteriors are known to exhibit various pathological
behaviors. We prove that as the number of hidden units in a single-layer
Bayesian neural network tends to infinity, the function-space posterior mean
under mean-field variational inference actually converges to zero, completely
ignoring the data. This is in contrast to the true posterior, which converges
to a Gaussian process. Our work provides insight into the over-regularization
of the KL divergence in variational inference.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07053" title="Abstract">arXiv:2106.07053</a> [<a href="/pdf/2106.07053" title="Download PDF">pdf</a>, <a href="/format/2106.07053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convex Sparse Blind Deconvolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qingyun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Donoho%2C+D">David Donoho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Statistics Theory (math.ST); Other Statistics (stat.OT)

</div>
<p class="mathjax">In the blind deconvolution problem, we observe the convolution of an unknown
filter and unknown signal and attempt to reconstruct the filter and signal. The
problem seems impossible in general, since there are seemingly many more
unknowns than knowns . Nevertheless, this problem arises in many application
fields; and empirically, some of these fields have had success using heuristic
methods -- even economically very important ones, in wireless communications
and oil exploration. Today's fashionable heuristic formulations pose non-convex
optimization problems which are then attacked heuristically as well. The fact
that blind deconvolution can be solved under some repeatable and
naturally-occurring circumstances poses a theoretical puzzle.
<br />To bridge the gulf between reported successes and theory's limited
understanding, we exhibit a convex optimization problem that -- assuming signal
sparsity -- can convert a crude approximation to the true filter into a
high-accuracy recovery of the true filter. Our proposed formulation is based on
L1 minimization of inverse filter outputs. We give sharp guarantees on
performance of the minimizer assuming sparsity of signal, showing that our
proposal precisely recovers the true inverse filter, up to shift and rescaling.
There is a sparsity/initial accuracy tradeoff: the less accurate the initial
approximation, the greater we rely on sparsity to enable exact recovery. To our
knowledge this is the first reported tradeoff of this kind. We consider it
surprising that this tradeoff is independent of dimension.
<br />We also develop finite-$N$ guarantees, for highly accurate reconstruction
under $N\geq O(k \log(k) )$ with high probability. We further show stable
approximation when the true inverse filter is infinitely long and extend our
guarantees to the case where the observations are contaminated by stochastic or
adversarial noise.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07055" title="Abstract">arXiv:2106.07055</a> [<a href="/pdf/2106.07055" title="Download PDF">pdf</a>, <a href="/format/2106.07055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenSF: Simultaneous Adaptation of Generative Pre-trained Models and Slot  Filling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehri%2C+S">Shikib Mehri</a>, 
<a href="/search/cs?searchtype=author&query=Eskenazi%2C+M">Maxine Eskenazi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SIGDial 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In transfer learning, it is imperative to achieve strong alignment between a
pre-trained model and a downstream task. Prior work has done this by proposing
task-specific pre-training objectives, which sacrifices the inherent
scalability of the transfer learning paradigm. We instead achieve strong
alignment by simultaneously modifying both the pre-trained model and the
formulation of the downstream task, which is more efficient and preserves the
scalability of transfer learning. We present GenSF (Generative Slot Filling),
which leverages a generative pre-trained open-domain dialog model for slot
filling. GenSF (1) adapts the pre-trained model by incorporating inductive
biases about the task and (2) adapts the downstream task by reformulating slot
filling to better leverage the pre-trained model's capabilities. GenSF achieves
state-of-the-art results on two slot filling datasets with strong gains in
few-shot and zero-shot settings. We achieve a 9 F1 score improvement in
zero-shot slot filling. This highlights the value of strong alignment between
the pre-trained model and the downstream task.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07056" title="Abstract">arXiv:2106.07056</a> [<a href="/pdf/2106.07056" title="Download PDF">pdf</a>, <a href="/format/2106.07056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Schema-Guided Paradigm for Zero-Shot Dialog
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehri%2C+S">Shikib Mehri</a>, 
<a href="/search/cs?searchtype=author&query=Eskenazi%2C+M">Maxine Eskenazi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SIGDial 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Developing mechanisms that flexibly adapt dialog systems to unseen tasks and
domains is a major challenge in dialog research. Neural models implicitly
memorize task-specific dialog policies from the training data. We posit that
this implicit memorization has precluded zero-shot transfer learning. To this
end, we leverage the schema-guided paradigm, wherein the task-specific dialog
policy is explicitly provided to the model. We introduce the Schema Attention
Model (SAM) and improved schema representations for the STAR corpus. SAM
obtains significant improvement in zero-shot settings, with a +22 F1 score
improvement over prior work. These results validate the feasibility of
zero-shot generalizability in dialog. Ablation experiments are also presented
to demonstrate the efficacy of SAM.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07057" title="Abstract">arXiv:2106.07057</a> [<a href="/pdf/2106.07057" title="Download PDF">pdf</a>, <a href="/format/2106.07057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairCanary: Rapid Continuous Explainable Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Avijit Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Shanbhag%2C+A">Aalok Shanbhag</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine Learning (ML) models are being used in all facets of today's society
to make high stake decisions like bail granting or credit lending, with very
minimal regulations. Such systems are extremely vulnerable to both propagating
and amplifying social biases, and have therefore been subject to growing
research interest. One of the main issues with conventional fairness metrics is
their narrow definitions which hide the complete extent of the bias by focusing
primarily on positive and/or negative outcomes, whilst not paying attention to
the overall distributional shape. Moreover, these metrics are often
contradictory to each other, are severely restrained by the contextual and
legal landscape of the problem, have technical constraints like poor support
for continuous outputs, the requirement of class labels, and are not
explainable.
<br />In this paper, we present Quantile Demographic Drift, which addresses the
shortcomings mentioned above. This metric can also be used to measure
intra-group privilege. It is easily interpretable via existing attribution
techniques, and also extends naturally to individual fairness via the principle
of like-for-like comparison. We make this new fairness score the basis of a new
system that is designed to detect bias in production ML models without the need
for labels. We call the system FairCanary because of its capability to detect
bias in a live deployed model and narrow down the alert to the responsible set
of features, like the proverbial canary in a coal mine.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07059" title="Abstract">arXiv:2106.07059</a> [<a href="/pdf/2106.07059" title="Download PDF">pdf</a>, <a href="/format/2106.07059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Resource List Scheduling of Moldable Parallel Jobs under  Precedence Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perotin%2C+L">Lucas Perotin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hongyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Raghavan%2C+P">Padma Raghavan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The scheduling literature has traditionally focused on a single type of
resource (e.g., computing nodes). However, scientific applications in modern
High-Performance Computing (HPC) systems process large amounts of data, hence
have diverse requirements on different types of resources (e.g., cores, cache,
memory, I/O). All of these resources could potentially be exploited by the
runtime scheduler to improve the application performance. In this paper, we
study multi-resource scheduling to minimize the makespan of computational
workflows comprised of parallel jobs subject to precedence constraints. The
jobs are assumed to be moldable, allowing the scheduler to flexibly select a
variable set of resources before execution. We propose a multi-resource,
list-based scheduling algorithm, and prove that, on a system with $d$ types of
schedulable resources, our algorithm achieves an approximation ratio of
$1.619d+2.545\sqrt{d}+1$ for any $d$, and a ratio of $d+O(\sqrt[3]{d^2})$ for
large $d$. We also present improved results for independent jobs and for jobs
with special precedence constraints (e.g., series-parallel graphs and trees).
Finally, we prove a lower bound of $d$ on the approximation ratio of any list
scheduling scheme with local priority considerations. To the best of our
knowledge, these are the first approximation results for moldable workflows
with multiple resource requirements.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07062" title="Abstract">arXiv:2106.07062</a> [<a href="/pdf/2106.07062" title="Download PDF">pdf</a>, <a href="/format/2106.07062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Atlas Based Representation and Metric Learning on Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korman%2C+E+O">Eric O. Korman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We explore the use of a topological manifold, represented as a collection of
charts, as the target space of neural network based representation learning
tasks. This is achieved by a simple adjustment to the output of an encoder's
network architecture plus the addition of a maximal mean discrepancy (MMD)
based loss function for regularization. Most algorithms in representation and
metric learning are easily adaptable to our framework and we demonstrate its
effectiveness by adjusting SimCLR (for representation learning) and standard
triplet loss training (for metric learning) to have manifold encoding spaces.
Our experiments show that we obtain a substantial performance boost over the
baseline for low dimensional encodings. In the case of triplet training, we
also find, independent of the manifold setup, that the MMD loss alone (i.e.
keeping a flat, euclidean target space but using an MMD loss to regularize it)
increases performance over the baseline in the typical, high-dimensional
Euclidean target spaces. Code for reproducing experiments is provided at
https://github.com/ekorman/neurve .
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07068" title="Abstract">arXiv:2106.07068</a> [<a href="/pdf/2106.07068" title="Download PDF">pdf</a>, <a href="/format/2106.07068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HistoTransfer: Understanding Transfer Learning for Histopathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+Y">Yash Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Ehsan%2C+L">Lubaina Ehsan</a>, 
<a href="/search/cs?searchtype=author&query=Syed%2C+S">Sana Syed</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+D+E">Donald E. Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE International Conference on Biomedical and Health Informatics (BHI'21). arXiv admin note: text overlap with <a href="/abs/2103.10626">arXiv:2103.10626</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Advancement in digital pathology and artificial intelligence has enabled deep
learning-based computer vision techniques for automated disease diagnosis and
prognosis. However, WSIs present unique computational and algorithmic
challenges. WSIs are gigapixel-sized, making them infeasible to be used
directly for training deep neural networks. Hence, for modeling, a two-stage
approach is adopted: Patch representations are extracted first, followed by the
aggregation for WSI prediction. These approaches require detailed pixel-level
annotations for training the patch encoder. However, obtaining these
annotations is time-consuming and tedious for medical experts. Transfer
learning is used to address this gap and deep learning architectures
pre-trained on ImageNet are used for generating patch-level representation.
Even though ImageNet differs significantly from histopathology data,
pre-trained networks have been shown to perform impressively on histopathology
data. Also, progress in self-supervised and multi-task learning coupled with
the release of multiple histopathology data has led to the release of
histopathology-specific networks. In this work, we compare the performance of
features extracted from networks trained on ImageNet and histopathology data.
We use an attention pooling network over these extracted features for
slide-level aggregation. We investigate if features learned using more complex
networks lead to gain in performance. We use a simple top-k sampling approach
for fine-tuning framework and study the representation similarity between
frozen and fine-tuned networks using Centered Kernel Alignment. Further, to
examine if intermediate block representation is better suited for feature
extraction and ImageNet architectures are unnecessarily large for
histopathology, we truncate the blocks of ResNet18 and DenseNet121 and examine
the performance.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07069" title="Abstract">arXiv:2106.07069</a> [<a href="/pdf/2106.07069" title="Download PDF">pdf</a>, <a href="/format/2106.07069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A finite element model for a coupled thermo-mechanical system: nonlinear  strain-limiting thermoelastic body
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yoon%2C+H+C">Hyun C. Yoon</a>, 
<a href="/search/math?searchtype=author&query=Vasudeva%2C+K+K">Karthik K. Vasudeva</a>, 
<a href="/search/math?searchtype=author&query=Mallikarjunaiah%2C+S+M">S. M. Mallikarjunaiah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We investigate a specific finite element model to study the thermoelastic
behavior of an elastic body within the context of nonlinear strain-limiting
constitutive relation. As a special subclass of implicit relations, the
thermoelastic response of our interest is such that stresses can be arbitrarily
large, but strains remain small, especially in the neighborhood of crack-tips.
Thus, the proposed model can be inherently consistent with the assumption of
the small strain theory. In the present communication, we consider a
two-dimensional coupled system-linear and quasilinear partial differential
equations for temperature and displacements, respectively. Two distinct
temperature distributions of the Dirichlet type are considered for boundary
condition, and a standard finite element method of continuous Galerkin is
employed to obtain the numerical solutions for the field variables. For a
domain with an edge-crack, we find that the near-tip strain growth of our model
is much slower than the growth of stress, which is the salient feature compared
to the inconsistent results of the classical linearized description of the
elastic body. Current study can provide a theoretical and computational
framework to develop physically meaningful models and examine other coupled
multi-physics such as an evolution of complex network of cracks induced by
thermal shocks.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07074" title="Abstract">arXiv:2106.07074</a> [<a href="/pdf/2106.07074" title="Download PDF">pdf</a>, <a href="/format/2106.07074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RadArnomaly: Protecting Radar Systems from Data Manipulation Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S">Shai Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+E">Efrat Levy</a>, 
<a href="/search/cs?searchtype=author&query=Shaked%2C+A">Avi Shaked</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+T">Tair Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Elovici%2C+Y">Yuval Elovici</a>, 
<a href="/search/cs?searchtype=author&query=Shabtai%2C+A">Asaf Shabtai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Radar systems are mainly used for tracking aircraft, missiles, satellites,
and watercraft. In many cases, information regarding the objects detected by
the radar system is sent to, and used by, a peripheral consuming system, such
as a missile system or a graphical user interface used by an operator. Those
systems process the data stream and make real-time, operational decisions based
on the data received. Given this, the reliability and availability of
information provided by radar systems has grown in importance. Although the
field of cyber security has been continuously evolving, no prior research has
focused on anomaly detection in radar systems. In this paper, we present a deep
learning-based method for detecting anomalies in radar system data streams. We
propose a novel technique which learns the correlation between numerical
features and an embedding representation of categorical features in an
unsupervised manner. The proposed technique, which allows the detection of
malicious manipulation of critical fields in the data stream, is complemented
by a timing-interval anomaly detection mechanism proposed for the detection of
message dropping attempts. Real radar system data is used to evaluate the
proposed method. Our experiments demonstrate the method's high detection
accuracy on a variety of data stream manipulation attacks (average detection
rate of 88% with 1.59% false alarms) and message dropping attacks (average
detection rate of 92% with 2.2% false alarms).
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07075" title="Abstract">arXiv:2106.07075</a> [<a href="/pdf/2106.07075" title="Download PDF">pdf</a>, <a href="/format/2106.07075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A baseline for semi-supervised learning of efficient semantic  segmentation models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grubi%C5%A1i%C4%87%2C+I">Ivan Grubi&#x161;i&#x107;</a> (1), 
<a href="/search/cs?searchtype=author&query=Or%C5%A1i%C4%87%2C+M">Marin Or&#x161;i&#x107;</a> (1), 
<a href="/search/cs?searchtype=author&query=%C5%A0egvi%C4%87%2C+S">Sini&#x161;a &#x160;egvi&#x107;</a> (1) ((1) University of Zagreb, Faculty of Electrical Engineering and Computing)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Semi-supervised learning is especially interesting in the dense prediction
context due to high cost of pixel-level ground truth. Unfortunately, most such
approaches are evaluated on outdated architectures which hamper research due to
very slow training and high requirements on GPU RAM. We address this concern by
presenting a simple and effective baseline which works very well both on
standard and efficient architectures. Our baseline is based on one-way
consistency and non-linear geometric and photometric perturbations. We show
advantage of perturbing only the student branch and present a plausible
explanation of such behaviour. Experiments on Cityscapes and CIFAR-10
demonstrate competitive performance with respect to prior work.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07080" title="Abstract">arXiv:2106.07080</a> [<a href="/pdf/2106.07080" title="Download PDF">pdf</a>, <a href="/ps/2106.07080" title="Download PostScript">ps</a>, <a href="/format/2106.07080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-verified Learning from the Crowd with Pairwise Comparisons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shiwei Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jie Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study the problem of {\em crowdsourced PAC learning} of Boolean-valued
functions through enriched queries, a problem that has attracted a surge of
recent research interests. In particular, we consider that the learner may
query the crowd to obtain a label of a given instance or a comparison tag of a
pair of instances. This is a challenging problem and only recently have
budget-efficient algorithms been established for the scenario where the
majority of the crowd are correct. In this work, we investigate the
significantly more challenging case that the majority are incorrect which
renders learning impossible in general. We show that under the {semi-verified
model} of Charikar~et~al.~(2017), where we have (limited) access to a trusted
oracle who always returns the correct annotation, it is possible to learn the
underlying function while the labeling cost is significantly mitigated by the
enriched and more easily obtained queries.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07084" title="Abstract">arXiv:2106.07084</a> [<a href="/pdf/2106.07084" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security Analysis of the Silver Bullet Technique for RowHammer  Prevention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ya%C4%9Fl%C4%B1k%C3%A7%C4%B1%2C+A+G">Abdullah Giray Ya&#x11f;l&#x131;k&#xe7;&#x131;</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J+S">Jeremie S. Kim</a>, 
<a href="/search/cs?searchtype=author&query=Devaux%2C+F">Fabrice Devaux</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">The purpose of this document is to study the security properties of the
Silver Bullet algorithm against worst-case RowHammer attacks. We mathematically
demonstrate that Silver Bullet, when properly configured and implemented in a
DRAM chip, can securely prevent RowHammer attacks. The demonstration focuses on
the most representative implementation of Silver Bullet, the patent claiming
many implementation possibilities not covered in this demonstration. Our study
concludes that Silver Bullet is a promising RowHammer prevention mechanism that
can be configured to operate securely against RowHammer attacks at various
efficiency-area tradeoff points, supporting relatively small hammer count
values (e.g., 1000) and Silver Bullet table sizes (e.g., 1.06KB).
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07085" title="Abstract">arXiv:2106.07085</a> [<a href="/pdf/2106.07085" title="Download PDF">pdf</a>, <a href="/format/2106.07085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survey: Image Mixing and Deleting for Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naveed%2C+H">Humza Naveed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Data augmentation has been widely used to improve deep nerual networks
performance. Numerous approaches are suggested, for example, dropout,
regularization and image augmentation, to avoid over-ftting and enhancing
generalization of neural networks. One of the sub-area within data augmentation
is image mixing and deleting. This specific type of augmentation either mixes
two images or delete image regions to hide or make certain characteristics of
images confusing for the network to force it to emphasize on overall structure
of object in image. The model trained with this approach has shown to perform
and generalize well as compared to one trained without imgage mixing or
deleting. Additional benefit achieved with this method of training is
robustness against image corruptions. Due to its low compute cost and success
in recent past, many techniques of image mixing and deleting are proposed. This
paper provides detailed review on these devised approaches, dividing
augmentation strategies in three main categories cut and delete, cut and mix
and mixup. The second part of paper emprically evaluates these approaches for
image classification, finegrained image recognition and object detection where
it is shown that this category of data augmentation improves the overall
performance for deep neural networks.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07087" title="Abstract">arXiv:2106.07087</a> [<a href="/pdf/2106.07087" title="Download PDF">pdf</a>, <a href="/format/2106.07087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Koios: A Deep Learning Benchmark Suite for FPGA Architecture and CAD  Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Aman Arora</a>, 
<a href="/search/cs?searchtype=author&query=Boutros%2C+A">Andrew Boutros</a>, 
<a href="/search/cs?searchtype=author&query=Rauch%2C+D">Daniel Rauch</a>, 
<a href="/search/cs?searchtype=author&query=Rajen%2C+A">Aishwarya Rajen</a>, 
<a href="/search/cs?searchtype=author&query=Borda%2C+A">Aatman Borda</a>, 
<a href="/search/cs?searchtype=author&query=Damghani%2C+S+A">Seyed Alireza Damghani</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Samidh Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Kate%2C+S">Sangram Kate</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+P">Pragnesh Patel</a>, 
<a href="/search/cs?searchtype=author&query=Kent%2C+K+B">Kenneth B. Kent</a>, 
<a href="/search/cs?searchtype=author&query=Betz%2C+V">Vaughn Betz</a>, 
<a href="/search/cs?searchtype=author&query=John%2C+L+K">Lizy K. John</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">With the prevalence of deep learning (DL) in many applications, researchers
are investigating different ways of optimizing FPGA architecture and CAD to
achieve better quality-of-results (QoR) on DL-based workloads. In this
optimization process, benchmark circuits are an essential component; the QoR
achieved on a set of benchmarks is the main driver for architecture and CAD
design choices. However, current academic benchmark suites are inadequate, as
they do not capture any designs from the DL domain. This work presents a new
suite of DL acceleration benchmark circuits for FPGA architecture and CAD
research, called Koios. This suite of 19 circuits covers a wide variety of
accelerated neural networks, design sizes, implementation styles, abstraction
levels, and numerical precisions. These designs are larger, more data parallel,
more heterogeneous, more deeply pipelined, and utilize more FPGA architectural
features compared to existing open-source benchmarks. This enables researchers
to pin-point architectural inefficiencies for this class of workloads and
optimize CAD tools on more realistic benchmarks that stress the CAD algorithms
in different ways. In this paper, we describe the designs in our benchmark
suite, present results of running them through the Verilog-to-Routing (VTR)
flow using a recent FPGA architecture model, and identify key insights from the
resulting metrics. On average, our benchmarks have 3.7x more netlist
primitives, 1.8x and 4.7x higher DSP and BRAM densities, and 1.7x higher
frequency with 1.9x more near-critical paths compared to the widely-used VTR
suite. Finally, we present two example case studies showing how architectural
exploration for DL-optimized FPGAs can be performed using our new benchmark
suite.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07088" title="Abstract">arXiv:2106.07088</a> [<a href="/pdf/2106.07088" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new soft computing method for integration of expert&#x27;s knowledge in  reinforcement learn-ing problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Annabestani%2C+M">Mohsen Annabestani</a>, 
<a href="/search/cs?searchtype=author&query=Abedi%2C+A">Ali Abedi</a>, 
<a href="/search/cs?searchtype=author&query=Nematollahi%2C+M+R">Mohammad Reza Nematollahi</a>, 
<a href="/search/cs?searchtype=author&query=Sis-tani%2C+M+B+N">Mohammad Bagher Naghibi Sis-tani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This paper proposes a novel fuzzy action selection method to leverage human
knowledge in reinforcement learning problems. Based on the estimates of the
most current action-state values, the proposed fuzzy nonlinear mapping as-signs
each member of the action set to its probability of being chosen in the next
step. A user tunable parameter is introduced to control the action selection
policy, which determines the agent's greedy behavior throughout the learning
process. This parameter resembles the role of the temperature parameter in the
softmax action selection policy, but its tuning process can be more
knowledge-oriented since this parameter reflects the human knowledge into the
learning agent by making modifications in the fuzzy rule base. Simulation
results indicate that including fuzzy logic within the reinforcement learning
in the proposed manner improves the learning algorithm's convergence rate, and
provides superior performance.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07091" title="Abstract">arXiv:2106.07091</a> [<a href="/pdf/2106.07091" title="Download PDF">pdf</a>, <a href="/format/2106.07091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-Off Center-Surround Receptive Fields for Accurate and Robust Image  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Babaiee%2C+Z">Zahra Babaiee</a>, 
<a href="/search/cs?searchtype=author&query=Hasani%2C+R">Ramin Hasani</a>, 
<a href="/search/cs?searchtype=author&query=Lechner%2C+M">Mathias Lechner</a>, 
<a href="/search/cs?searchtype=author&query=Rus%2C+D">Daniela Rus</a>, 
<a href="/search/cs?searchtype=author&query=Grosu%2C+R">Radu Grosu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 Pages. Accepted for publication in the proceedings of the 38th International Conference on Machine Learning (ICML) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Robustness to variations in lighting conditions is a key objective for any
deep vision system. To this end, our paper extends the receptive field of
convolutional neural networks with two residual components, ubiquitous in the
visual processing system of vertebrates: On-center and off-center pathways,
with excitatory center and inhibitory surround; OOCS for short. The on-center
pathway is excited by the presence of a light stimulus in its center but not in
its surround, whereas the off-center one is excited by the absence of a light
stimulus in its center but not in its surround. We design OOCS pathways via a
difference of Gaussians, with their variance computed analytically from the
size of the receptive fields. OOCS pathways complement each other in their
response to light stimuli, ensuring this way a strong edge-detection
capability, and as a result, an accurate and robust inference under challenging
lighting conditions. We provide extensive empirical evidence showing that
networks supplied with the OOCS edge representation gain accuracy and
illumination-robustness compared to standard deep models.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07094" title="Abstract">arXiv:2106.07094</a> [<a href="/pdf/2106.07094" title="Download PDF">pdf</a>, <a href="/format/2106.07094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DP-NormFedAvg: Normalizing Client Updates for Privacy-Preserving  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+R">Rudrajit Das</a>, 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+A">Abolfazl Hashemi</a>, 
<a href="/search/cs?searchtype=author&query=Sanghavi%2C+S">Sujay Sanghavi</a>, 
<a href="/search/cs?searchtype=author&query=Dhillon%2C+I+S">Inderjit S. Dhillon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Signal Processing (eess.SP); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we focus on facilitating differentially private quantized
communication between the clients and server in federated learning (FL).
Towards this end, we propose to have the clients send a \textit{private
quantized} version of only the \textit{unit vector} along the change in their
local parameters to the server, \textit{completely throwing away the magnitude
information}. We call this algorithm \texttt{DP-NormFedAvg} and show that it
has the same order-wise convergence rate as \texttt{FedAvg} on smooth
quasar-convex functions (an important class of non-convex functions for
modeling optimization of deep neural networks), thereby establishing that
discarding the magnitude information is not detrimental from an optimization
point of view. We also introduce QTDL, a new differentially private
quantization mechanism for unit-norm vectors, which we use in
\texttt{DP-NormFedAvg}. QTDL employs \textit{discrete} noise having a
Laplacian-like distribution on a \textit{finite support} to provide privacy. We
show that under a growth-condition assumption on the per-sample client losses,
the extra per-coordinate communication cost in each round incurred due to
privacy by our method is $\mathcal{O}(1)$ with respect to the model dimension,
which is an improvement over prior work. Finally, we show the efficacy of our
proposed method with experiments on fully-connected neural networks trained on
CIFAR-10 and Fashion-MNIST.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07095" title="Abstract">arXiv:2106.07095</a> [<a href="/pdf/2106.07095" title="Download PDF">pdf</a>, <a href="/format/2106.07095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear representation of categorical values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berny%2C+A">Arnaud Berny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An extended two-page abstract of this work will appear in 2021 Genetic and Evolutionary Computation Conference Companion (GECCO '21 Companion)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">We propose a binary representation of categorical values using a linear map.
This linear representation preserves the neighborhood structure of categorical
values. In the context of evolutionary algorithms, it means that every
categorical value can be reached in a single mutation. The linear
representation is embedded into standard metaheuristics, applied to the problem
of Sudoku puzzles, and compared to the more traditional direct binary encoding.
It shows promising results in fixed-budget experiments and empirical cumulative
distribution functions with high dimension instances, and also in fixed-target
experiments with small dimension instances.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07098" title="Abstract">arXiv:2106.07098</a> [<a href="/pdf/2106.07098" title="Download PDF">pdf</a>, <a href="/format/2106.07098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security Analysis of Camera-LiDAR Semantic-Level Fusion Against  Black-Box Attacks on Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hallyburton%2C+R+S">R. Spencer Hallyburton</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yupei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pajic%2C+M">Miroslav Pajic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">To enable safe and reliable decision-making, autonomous vehicles (AVs) feed
sensor data to perception algorithms to understand the environment. Sensor
fusion, and particularly semantic fusion, with multi-frame tracking is becoming
increasingly popular for detecting 3D objects. Recently, it was shown that
LiDAR-based perception built on deep neural networks is vulnerable to LiDAR
spoofing attacks. Thus, in this work, we perform the first analysis of
camera-LiDAR fusion under spoofing attacks and the first security analysis of
semantic fusion in any AV context. We find first that fusion is more successful
than existing defenses at guarding against naive spoofing. However, we then
define the frustum attack as a new class of attacks on AVs and find that
semantic camera-LiDAR fusion exhibits widespread vulnerability to frustum
attacks with between 70% and 90% success against target models. Importantly,
the attacker needs less than 20 random spoof points on average for successful
attacks - an order of magnitude less than established maximum capability.
Finally, we are the first to analyze the longitudinal impact of perception
attacks by showing the impact of multi-frame attacks.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07100" title="Abstract">arXiv:2106.07100</a> [<a href="/pdf/2106.07100" title="Download PDF">pdf</a>, <a href="/ps/2106.07100" title="Download PostScript">ps</a>, <a href="/format/2106.07100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Irrational Behaviours in the Optional Prisoner&#x27;s Dilemma  with Game-Environment Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stella%2C+L">Leonardo Stella</a>, 
<a href="/search/cs?searchtype=author&query=Bauso%2C+D">Dario Bauso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">In the optional prisoner's dilemma (OPD), players can choose to cooperate and
defect as usual, but can also abstain as a third possible strategy. This
strategy models the players' participation in the game and is a relevant aspect
in many settings, e.g. social networks or opinion dynamics where abstention is
an option during an election. In this paper, we provide a formulation of the
OPD where we consider irrational behaviours in the population inspired by
prospect theory. Prospect theory has gained increasing popularity in recent
times thanks to its ability to capture aspects such as reference dependence or
loss aversion which are common in human behaviour. This element is original in
our formulation of the game and is incorporated in our framework through
pairwise comparison dynamics. Recently, the impact of the environment has been
studied in the form of feedback on the population dynamics. Another element of
novelty in our work is the extension of the game-environment feedback to the
OPD in two forms of dynamics, the replicator and the pairwise comparison. The
contribution of this paper is threefold. First, we propose a modelling
framework where prospect theory is used to capture irrational behaviours in an
evolutionary game with game-environment feedback. Second, we carry out the
stability analysis of the system equilibria and discuss the oscillating
behaviours arising from the game-environment feedback. Finally, we extend our
previous results to the OPD and we discuss the main differences between the
model resulting from the replicator dynamics and the one resulting from the
pairwise comparison dynamics.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07102" title="Abstract">arXiv:2106.07102</a> [<a href="/pdf/2106.07102" title="Download PDF">pdf</a>, <a href="/format/2106.07102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Farview: Disaggregated Memory with Operator Off-loading for Database  Engines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korolija%2C+D">Dario Korolija</a>, 
<a href="/search/cs?searchtype=author&query=Koutsoukos%2C+D">Dimitrios Koutsoukos</a>, 
<a href="/search/cs?searchtype=author&query=Keeton%2C+K">Kimberly Keeton</a>, 
<a href="/search/cs?searchtype=author&query=Taranov%2C+K">Konstantin Taranov</a>, 
<a href="/search/cs?searchtype=author&query=Miloji%C4%8Di%C4%87%2C+D">Dejan Miloji&#x10d;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Alonso%2C+G">Gustavo Alonso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Cloud deployments disaggregate storage from compute, providing more
flexibility to both the storage and compute layers. In this paper, we explore
disaggregation by taking it one step further and applying it to memory (DRAM).
Disaggregated memory uses network attached DRAM as a way to decouple memory
from CPU. In the context of databases, such a design offers significant
advantages in terms of making a larger memory capacity available as a central
pool to a collection of smaller processing nodes. To explore these
possibilities, we have implemented Farview, a disaggregated memory solution for
databases, operating as a remote buffer cache with operator offloading
capabilities. Farview is implemented as an FPGA-based smart NIC making DRAM
available as a disaggregated, network attached memory module capable of
performing data processing at line rate over data streams to/from disaggregated
memory. Farview supports query offloading using operators such as selection,
projection, aggregation, regular expression matching and encryption. In this
paper we focus on analytical queries and demonstrate the viability of the idea
through an extensive experimental evaluation of Farview under different
workloads. Farview is competitive with a local buffer cache solution for all
the workloads and outperforms it in a number of cases, proving that a smart
disaggregated memory can be a viable alternative for databases deployed in
cloud environments.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07105" title="Abstract">arXiv:2106.07105</a> [<a href="/pdf/2106.07105" title="Download PDF">pdf</a>, <a href="/format/2106.07105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SRAM-SUC: Ultra-Low Latency Robust Digital PUF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mars%2C+A">Ayoub Mars</a>, 
<a href="/search/cs?searchtype=author&query=Ghandour%2C+H">Hussam Ghandour</a>, 
<a href="/search/cs?searchtype=author&query=Adi%2C+W">Wael Adi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 15 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Secret Unknown Ciphers (SUC) have been proposed recently as digital
clone-resistant functions overcoming some of Physical(ly) Unclonable Functions
(PUF) downsides, mainly their inconsistency because of PUFs analog nature. In
this paper, we propose a new practical mechanism for creating internally random
ciphers in modern volatile and non-volatile SoC FPGAs, coined as SRAM-SUC. Each
created random cipher inside a SoC FPGA constitutes a robust digital PUF. This
work also presents a class of involutive SUCs, optimized for the targeted SoC
FPGA architecture, as sample realization of the concept; it deploys a generated
class of involutive 8-bit S-Boxes, that are selected randomly from a defined
large set through an internal process inside the SoC FPGA. Hardware and
software implementations show that the resulting SRAM-SUC has ultra-low latency
compared to well-known PUF-based authentication mechanisms. SRAM-SUC requires
only $2.88/0.72 \mu s$ to generate a response for a challenge at 50/200 MHz
respectively. This makes SRAM-SUC a promising and appealing solution for
Ultra-Reliable Low Latency Communication (URLLC).
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07106" title="Abstract">arXiv:2106.07106</a> [<a href="/pdf/2106.07106" title="Download PDF">pdf</a>, <a href="/format/2106.07106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Optimal Transport with Transition Couplings of Random Walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=O%27Connor%2C+K">Kevin O&#x27;Connor</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+B">Bongsoo Yi</a>, 
<a href="/search/cs?searchtype=author&query=McGoff%2C+K">Kevin McGoff</a>, 
<a href="/search/cs?searchtype=author&query=Nobel%2C+A+B">Andrew B. Nobel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a novel approach to optimal transport between graphs from the
perspective of stationary Markov chains. A weighted graph may be associated
with a stationary Markov chain by means of a random walk on the vertex set with
transition distributions depending on the edge weights of the graph. After
drawing this connection, we describe how optimal transport techniques for
stationary Markov chains may be used in order to perform comparison and
alignment of the graphs under study. In particular, we propose the graph
optimal transition coupling problem, referred to as GraphOTC, in which the
Markov chains associated to two given graphs are optimally synchronized to
minimize an expected cost. The joint synchronized chain yields an alignment of
the vertices and edges in the two graphs, and the expected cost of the
synchronized chain acts as a measure of distance or dissimilarity between the
two graphs. We demonstrate that GraphOTC performs equal to or better than
existing state-of-the-art techniques in graph optimal transport for several
tasks and datasets. Finally, we also describe a generalization of the GraphOTC
problem, called the FusedOTC problem, from which we recover the GraphOTC and OT
costs as special cases.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07108" title="Abstract">arXiv:2106.07108</a> [<a href="/pdf/2106.07108" title="Download PDF">pdf</a>, <a href="/format/2106.07108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pointwise Feasibility of Gaussian Process-based Safety-Critical Control  under Model Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Casta%C3%B1eda%2C+F">Fernando Casta&#xf1;eda</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+J+J">Jason J. Choi</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+B">Bike Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Tomlin%2C+C+J">Claire J. Tomlin</a>, 
<a href="/search/eess?searchtype=author&query=Sreenath%2C+K">Koushil Sreenath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs) are
popular tools for enforcing safety and stability of a controlled system,
respectively. They are commonly utilized to build constraints that can be
incorporated in a min-norm quadratic program (CBF-CLF-QP) which solves for a
safety-critical control input. However, since these constraints rely on a model
of the system, when this model is inaccurate the guarantees of safety and
stability can be easily lost. In this paper, we present a Gaussian Process
(GP)-based approach to tackle the problem of model uncertainty in
safety-critical controllers that use CBFs and CLFs. The considered model
uncertainty is affected by both state and control input. We derive
probabilistic bounds on the effects that such model uncertainty has on the
dynamics of the CBF and CLF. Then, we use these bounds to build safety and
stability chance constraints that can be incorporated in a min-norm convex
optimization program, called GP-CBF-CLF-SOCP. As the main theoretical result of
the paper, we present necessary and sufficient conditions for pointwise
feasibility of the proposed optimization problem. We believe that these
conditions could serve as a starting point towards understanding what are the
minimal requirements on the distribution of data collected from the real system
in order to guarantee safety. Finally, we validate the proposed framework with
numerical simulations of an adaptive cruise controller for an automotive
system.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07111" title="Abstract">arXiv:2106.07111</a> [<a href="/pdf/2106.07111" title="Download PDF">pdf</a>, <a href="/format/2106.07111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computational Information Criterion for Particle-Tracking with Sparse  or Noisy Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tran%2C+N+T">Nhat Thanh Tran</a>, 
<a href="/search/math?searchtype=author&query=Benson%2C+D+A">David A. Benson</a>, 
<a href="/search/math?searchtype=author&query=Schmidt%2C+M+J">Michael J. Schmidt</a>, 
<a href="/search/math?searchtype=author&query=Pankavich%2C+S+D">Stephen D. Pankavich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Water Resources (2021) 151: 103893
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Traditional probabilistic methods for the simulation of advection-diffusion
equations (ADEs) often overlook the entropic contribution of the
discretization, e.g., the number of particles, within associated numerical
methods. Many times, the gain in accuracy of a highly discretized numerical
model is outweighed by its associated computational costs or the noise within
the data. We address the question of how many particles are needed in a
simulation to best approximate and estimate parameters in one-dimensional
advective-diffusive transport. To do so, we use the well-known Akaike
Information Criterion (AIC) and a recently-developed correction called the
Computational Information Criterion (COMIC) to guide the model selection
process. Random-walk and mass-transfer particle tracking methods are employed
to solve the model equations at various levels of discretization. Numerical
results demonstrate that the COMIC provides an optimal number of particles that
can describe a more efficient model in terms of parameter estimation and model
prediction compared to the model selected by the AIC even when the data is
sparse or noisy, the sampling volume is not uniform throughout the physical
domain, or the error distribution of the data is non-IID Gaussian.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07112" title="Abstract">arXiv:2106.07112</a> [<a href="/pdf/2106.07112" title="Download PDF">pdf</a>, <a href="/format/2106.07112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bias: Friend or Foe? User Acceptance of Gender Stereotypes in Automated  Career Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Clarice Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kathryn Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+A">Andrew Bian</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+R">Rashidul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Keya%2C+K+N">Kamrun Naher Keya</a>, 
<a href="/search/cs?searchtype=author&query=Foulde%2C+J">James Foulde</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shimei Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Currently, there is a surge of interest in fair Artificial Intelligence (AI)
and Machine Learning (ML) research which aims to mitigate discriminatory bias
in AI algorithms, e.g. along lines of gender, age, and race. While most
research in this domain focuses on developing fair AI algorithms, in this work,
we show that a fair AI algorithm on its own may be insufficient to achieve its
intended results in the real world. Using career recommendation as a case
study, we build a fair AI career recommender by employing gender debiasing
machine learning techniques. Our offline evaluation showed that the debiased
recommender makes fairer career recommendations without sacrificing its
accuracy. Nevertheless, an online user study of more than 200 college students
revealed that participants on average prefer the original biased system over
the debiased system. Specifically, we found that perceived gender disparity is
a determining factor for the acceptance of a recommendation. In other words,
our results demonstrate we cannot fully address the gender bias issue in AI
recommendations without addressing the gender bias in humans.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07113" title="Abstract">arXiv:2106.07113</a> [<a href="/pdf/2106.07113" title="Download PDF">pdf</a>, <a href="/format/2106.07113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models  for NASA MODIS Instruments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sarah Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+E">Esther Cao</a>, 
<a href="/search/cs?searchtype=author&query=Koul%2C+A">Anirudh Koul</a>, 
<a href="/search/cs?searchtype=author&query=Ganju%2C+S">Siddha Ganju</a>, 
<a href="/search/cs?searchtype=author&query=Praveen%2C+S">Satyarth Praveen</a>, 
<a href="/search/cs?searchtype=author&query=Kasam%2C+M+A">Meher Anand Kasam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Due to the nature of their pathways, NASA Terra and NASA Aqua satellites
capture imagery containing swath gaps, which are areas of no data. Swath gaps
can overlap the region of interest (ROI) completely, often rendering the entire
imagery unusable by Machine Learning (ML) models. This problem is further
exacerbated when the ROI rarely occurs (e.g. a hurricane) and, on occurrence,
is partially overlapped with a swath gap. With annotated data as supervision, a
model can learn to differentiate between the area of focus and the swath gap.
However, annotation is expensive and currently the vast majority of existing
data is unannotated. Hence, we propose an augmentation technique that
considerably removes the existence of swath gaps in order to allow CNNs to
focus on the ROI, and thus successfully use data with swath gaps for training.
We experiment on the UC Merced Land Use Dataset, where we add swath gaps
through empty polygons (up to 20 percent areas) and then apply augmentation
techniques to fill the swath gaps. We compare the model trained with our
augmentation techniques on the swath gap-filled data with the model trained on
the original swath gap-less data and note highly augmented performance.
Additionally, we perform a qualitative analysis using activation maps that
visualizes the effectiveness of our trained network in not paying attention to
the swath gaps. We also evaluate our results with a human baseline and show
that, in certain cases, the filled swath gaps look so realistic that even a
human evaluator did not distinguish between original satellite images and swath
gap-filled images. Since this method is aimed at unlabeled data, it is widely
generalizable and impactful for large scale unannotated datasets from various
space data domains.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07114" title="Abstract">arXiv:2106.07114</a> [<a href="/pdf/2106.07114" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Agent for Hurricane Emergency Identification and Text  Information Extraction from Streaming Social Media Big Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jingwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Khallouli%2C+W">Wael Khallouli</a>, 
<a href="/search/cs?searchtype=author&query=Rabadi%2C+G">Ghaith Rabadi</a>, 
<a href="/search/cs?searchtype=author&query=Seck%2C+M">Mamadou Seck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper presents our research on leveraging social media Big Data and AI
to support hurricane disaster emergency response. The current practice of
hurricane emergency response for rescue highly relies on emergency call
centres. The more recent Hurricane Harvey event reveals the limitations of the
current systems. We use Hurricane Harvey and the associated Houston flooding as
the motivating scenario to conduct research and develop a prototype as a
proof-of-concept of using an intelligent agent as a complementary role to
support emergency centres in hurricane emergency response. This intelligent
agent is used to collect real-time streaming tweets during a natural disaster
event, to identify tweets requesting rescue, to extract key information such as
address and associated geocode, and to visualize the extracted information in
an interactive map in decision supports. Our experiment shows promising
outcomes and the potential application of the research in support of hurricane
emergency response.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07115" title="Abstract">arXiv:2106.07115</a> [<a href="/pdf/2106.07115" title="Download PDF">pdf</a>, <a href="/format/2106.07115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Correlation-Based Multiview Learning and Self-Supervision: A  Unifying Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Q">Qi Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xiao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Songtao Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Multiple views of data, both naturally acquired (e.g., image and audio) and
artificially produced (e.g., via adding different noise to data samples), have
proven useful in enhancing representation learning. Natural views are often
handled by multiview analysis tools, e.g., (deep) canonical correlation
analysis [(D)CCA], while the artificial ones are frequently used in
self-supervised learning (SSL) paradigms, e.g., \texttt{SimCLR} and
\texttt{Barlow Twins}. Both types of approaches often involve learning neural
feature extractors such that the embeddings of data exhibit high cross-view
correlations. Although intuitive, the effectiveness of correlation-based neural
embedding is only empirically validated. This work puts forth a theory-backed
framework for unsupervised multiview learning. Our development starts with
proposing a multiview model, where each view is a nonlinear mixture of shared
and private components. Consequently, the learning problem boils down to
shared/private component identification and disentanglement. Under this model,
latent correlation maximization is shown to guarantee the extraction of the
shared components across views (up to certain ambiguities). In addition, the
private information in each view can be provably disentangled from the shared
using proper regularization design. The method is tested on a series of tasks,
e.g., downstream clustering, which all show promising performance. Our
development also provides a unifying perspective for understanding various DCCA
and SSL schemes.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07116" title="Abstract">arXiv:2106.07116</a> [<a href="/pdf/2106.07116" title="Download PDF">pdf</a>, <a href="/format/2106.07116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Power of Randomization: Efficient and Effective Algorithms for  Constrained Submodular Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Shuang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianshuai Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Benwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">He Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Part of the contribution appears in ICML2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Submodular optimization has numerous applications such as crowdsourcing and
viral marketing. In this paper, we study the fundamental problem of
non-negative submodular function maximization subject to a $k$-system
constraint, which generalizes many other important constraints in submodular
optimization such as cardinality constraint, matroid constraint, and
$k$-extendible system constraint. The existing approaches for this problem
achieve the best-known approximation ratio of $k+2\sqrt{k+2}+3$ (for a general
submodular function) based on deterministic algorithmic frameworks. We propose
several randomized algorithms that improve upon the state-of-the-art algorithms
in terms of approximation ratio and time complexity, both under the
non-adaptive setting and the adaptive setting. The empirical performance of our
algorithms is extensively evaluated in several applications related to data
mining and social computing, and the experimental results demonstrate the
superiorities of our algorithms in terms of both utility and efficiency.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07117" title="Abstract">arXiv:2106.07117</a> [<a href="/pdf/2106.07117" title="Download PDF">pdf</a>, <a href="/format/2106.07117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Diverse Precondition Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+H">Heeyoung Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Chambers%2C+N">Nathanael Chambers</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+N">Niranjan Balasubramanian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language understanding must identify the logical connections between events
in a discourse, but core events are often unstated due to their commonsense
nature. This paper fills in these missing events by generating precondition
events. Precondition generation can be framed as a sequence-to-sequence
problem: given a target event, generate a possible precondition. However, in
most real-world scenarios, an event can have several preconditions, requiring
diverse generation -- a challenge for standard seq2seq approaches. We propose
DiP, a Diverse Precondition generation system that can generate unique and
diverse preconditions. DiP uses a generative process with three components --
an event sampler, a candidate generator, and a post-processor. The event
sampler provides control codes (precondition triggers) which the candidate
generator uses to focus its generation. Unlike other conditional generation
systems, DiP automatically generates control codes without training on diverse
examples. Analysis against baselines reveals that DiP improves the diversity of
preconditions significantly while also generating more preconditions.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07124" title="Abstract">arXiv:2106.07124</a> [<a href="/pdf/2106.07124" title="Download PDF">pdf</a>, <a href="/ps/2106.07124" title="Download PostScript">ps</a>, <a href="/format/2106.07124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-orthogonal codes over a non-unital ring and combinatorial matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Minjia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shukai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jon-Lark Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sol%C3%A9%2C+P">Patrick Sol&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">There is a local ring $E$ of order $4,$ without identity for the
multiplication, defined by generators and relations as $E=\langle a,b \mid
2a=2b=0,\, a^2=a,\, b^2=b,\,ab=a,\, ba=b\rangle.$
<br />We study a special construction of self-orthogonal codes over $E,$ based on
combinatorial matrices related to two-class association schemes, Strongly
Regular Graphs (SRG), and Doubly Regular Tournaments (DRT).
<br />We construct quasi self-dual codes over $E,$ and Type IV codes, that is,
quasi self-dual codes whose all codewords have even Hamming weight. All these
codes can be represented as formally self-dual additive codes over $\F_4.$ The
classical invariant theory bound for the weight enumerators of this class of
codesimproves the known bound on the minimum distance of Type IV codes over
$E.$
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07125" title="Abstract">arXiv:2106.07125</a> [<a href="/pdf/2106.07125" title="Download PDF">pdf</a>, <a href="/format/2106.07125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Policy Search using Sparse Gaussian Process Priors for  Learning Multimodal Optimal Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sasaki%2C+H">Hikaru Sasaki</a>, 
<a href="/search/cs?searchtype=author&query=Matsubara%2C+T">Takamitsu Matsubara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Neural Networks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Policy search reinforcement learning has been drawing much attention as a
method of learning a robot control policy. In particular, policy search using
such non-parametric policies as Gaussian process regression can learn optimal
actions with high-dimensional and redundant sensors as input. However, previous
methods implicitly assume that the optimal action becomes unique for each
state. This assumption can severely limit such practical applications as robot
manipulations since designing a reward function that appears in only one
optimal action for complex tasks is difficult. The previous methods might have
caused critical performance deterioration because the typical non-parametric
policies cannot capture the optimal actions due to their unimodality. We
propose novel approaches in non-parametric policy searches with multiple
optimal actions and offer two different algorithms commonly based on a sparse
Gaussian process prior and variational Bayesian inference. The following are
the key ideas: 1) multimodality for capturing multiple optimal actions and 2)
mode-seeking for capturing one optimal action by ignoring the others. First, we
propose a multimodal sparse Gaussian process policy search that uses multiple
overlapped GPs as a prior. Second, we propose a mode-seeking sparse Gaussian
process policy search that uses the student-t distribution for a likelihood
function. The effectiveness of those algorithms is demonstrated through
applications to object manipulation tasks with multiple optimal actions in
simulations.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07127" title="Abstract">arXiv:2106.07127</a> [<a href="/pdf/2106.07127" title="Download PDF">pdf</a>, <a href="/format/2106.07127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transition Motion Planning for Multi-Limbed Vertical Climbing Robots  Using Complementarity Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+D+W">Dennis W Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2021 Accepted. Optimization, Climbing motion planning, Complementarity Constraints
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In order to achieve autonomous vertical wall climbing, the transition phase
from the ground to the wall requires extra consideration inevitably. This paper
focuses on the contact sequence planner to transition between flat terrain and
vertical surfaces for multi-limbed climbing robots. To overcome the transition
phase, it requires planning both multi-contact and contact wrenches
simultaneously which makes it difficult. Instead of using a predetermined
contact sequence, we consider various motions on different environment setups
via modeling contact constraints and limb switchability as complementarity
conditions. Two safety factors for toe sliding and motor over-torque are the
main tuning parameters for different contact sequences. By solving as a
nonlinear program (NLP), we can generate several feasible sequences of foot
placements and contact forces to avoid failure cases. We verified feasibility
with demonstrations on the hardware SiLVIA, a six-legged robot capable of
vertically climbing between two walls by bracing itself in-between using only
friction.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07131" title="Abstract">arXiv:2106.07131</a> [<a href="/pdf/2106.07131" title="Download PDF">pdf</a>, <a href="/format/2106.07131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT3-to-plan: Extracting plans from text using GPT-3
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olmo%2C+A">Alberto Olmo</a>, 
<a href="/search/cs?searchtype=author&query=Sreedharan%2C+S">Sarath Sreedharan</a>, 
<a href="/search/cs?searchtype=author&query=Kambhampati%2C+S">Subbarao Kambhampati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Operations in many essential industries including finance and banking are
often characterized by the need to perform repetitive sequential tasks. Despite
their criticality to the business, workflows are rarely fully automated or even
formally specified, though there may exist a number of natural language
documents describing these procedures for the employees of the company. Plan
extraction methods provide us with the possibility of extracting structure
plans from such natural language descriptions of the plans/workflows, which
could then be leveraged by an automated system. In this paper, we investigate
the utility of generalized language models in performing such extractions
directly from such texts. Such models have already been shown to be quite
effective in multiple translation tasks, and our initial results seem to point
to their effectiveness also in the context of plan extractions. Particularly,
we show that GPT-3 is able to generate plan extraction results that are
comparable to many of the current state of the art plan extraction methods.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07134" title="Abstract">arXiv:2106.07134</a> [<a href="/pdf/2106.07134" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discerning the painter&#x27;s hand: machine learning on surface topography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+F">F. Ji</a>, 
<a href="/search/cs?searchtype=author&query=McMaster%2C+M+S">M. S. McMaster</a>, 
<a href="/search/cs?searchtype=author&query=Schwab%2C+S">S. Schwab</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">G. Singh</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+L+N">L. N. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Adhikari%2C+S">S. Adhikari</a>, 
<a href="/search/cs?searchtype=author&query=O%27Dwyer%2C+M">M. O&#x27;Dwyer</a>, 
<a href="/search/cs?searchtype=author&query=Sayed%2C+F">F. Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Ingrisano%2C+A">A. Ingrisano</a>, 
<a href="/search/cs?searchtype=author&query=Yoder%2C+D">D. Yoder</a>, 
<a href="/search/cs?searchtype=author&query=Bolman%2C+E+S">E. S. Bolman</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+I+T">I. T. Martin</a>, 
<a href="/search/cs?searchtype=author&query=Hinczewski%2C+M">M. Hinczewski</a>, 
<a href="/search/cs?searchtype=author&query=Singer%2C+K+D">K. D. Singer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> main text: 24 pages, 6 figures; SI: 6 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Attribution of paintings is a critical problem in art history. This study
extends machine learning analysis to surface topography of painted works. A
controlled study of positive attribution was designed with paintings produced
by a class of art students. The paintings were scanned using a confocal optical
profilometer to produce surface data. The surface data were divided into
virtual patches and used to train an ensemble of convolutional neural networks
(CNNs) for attribution. Over a range of patch sizes from 0.5 to 60 mm, the
resulting attribution was found to be 60 to 96% accurate, and, when comparing
regions of different color, was nearly twice as accurate as CNNs using color
images of the paintings. Remarkably, short length scales, as small as twice a
bristle diameter, were the key to reliably distinguishing among artists. These
results show promise for real-world attribution, particularly in the case of
workshop practice.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07135" title="Abstract">arXiv:2106.07135</a> [<a href="/pdf/2106.07135" title="Download PDF">pdf</a>, <a href="/format/2106.07135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MTC: Multiresolution Tensor Completion from Partial and Coarse  Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yang%2C+C">Chaoqi Yang</a>, 
<a href="/search/math?searchtype=author&query=Singh%2C+N">Navjot Singh</a>, 
<a href="/search/math?searchtype=author&query=Xiao%2C+C">Cao Xiao</a>, 
<a href="/search/math?searchtype=author&query=Qian%2C+C">Cheng Qian</a>, 
<a href="/search/math?searchtype=author&query=Solomonik%2C+E">Edgar Solomonik</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in SIGKDD 2021. Code in <a href="https://github.com/ycq091044/MTC">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing tensor completion formulation mostly relies on partial observations
from a single tensor. However, tensors extracted from real-world data are often
more complex due to: (i) Partial observation: Only a small subset (e.g., 5%) of
tensor elements are available. (ii) Coarse observation: Some tensor modes only
present coarse and aggregated patterns (e.g., monthly summary instead of daily
reports). In this paper, we are given a subset of the tensor and some
aggregated/coarse observations (along one or more modes) and seek to recover
the original fine-granular tensor with low-rank factorization. We formulate a
coupled tensor completion problem and propose an efficient Multi-resolution
Tensor Completion model (MTC) to solve the problem. Our MTC model explores
tensor mode properties and leverages the hierarchy of resolutions to
recursively initialize an optimization setup, and optimizes on the coupled
system using alternating least squares. MTC ensures low computational and space
complexity. We evaluate our model on two COVID-19 related spatio-temporal
tensors. The experiments show that MTC could provide 65.20% and 75.79%
percentage of fitness (PoF) in tensor completion with only 5% fine granular
observations, which is 27.96% relative improvement over the best baseline. To
evaluate the learned low-rank factors, we also design a tensor prediction task
for daily and cumulative disease case predictions, where MTC achieves 50% in
PoF and 30% relative improvements over the best baseline.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07136" title="Abstract">arXiv:2106.07136</a> [<a href="/pdf/2106.07136" title="Download PDF">pdf</a>, <a href="/format/2106.07136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian dense inverse searching algorithm for real-time stereo matching  in minimally invasive surgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jingwei Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qiuchen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+M">Maani Ghaffari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper reports a CPU-level real-time stereo matching method for surgical
images (10 Hz on 640 * 480 image with a single core of i5-9400). The proposed
method is built on the fast ''dense inverse searching'' algorithm, which
estimates the disparity of the stereo images. The overlapping image patches
(arbitrary squared image segment) from the images at different scales are
aligned based on the photometric consistency presumption. We propose a Bayesian
framework to evaluate the probability of the optimized patch disparity at
different scales. Moreover, we introduce a spatial Gaussian mixed probability
distribution to address the pixel-wise probability within the patch. In-vivo
and synthetic experiments show that our method can handle ambiguities resulted
from the textureless surfaces and the photometric inconsistency caused by the
Lambertian reflectance. Our Bayesian method correctly balances the probability
of the patch for stereo images at different scales. Experiments indicate that
the estimated depth has higher accuracy and fewer outliers than the baseline
methods in the surgical scenario.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07137" title="Abstract">arXiv:2106.07137</a> [<a href="/pdf/2106.07137" title="Download PDF">pdf</a>, <a href="/format/2106.07137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why Can You Lay Off Heads? Investigating How BERT Heads Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Ting-Rui Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yun-Nung Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The huge size of the widely used BERT family models has led to recent efforts
about model distillation. The main goal of distillation is to create a
task-agnostic pre-trained model that can be fine-tuned on downstream tasks
without fine-tuning its full-sized version. Despite the progress of
distillation, to what degree and for what reason a task-agnostic model can be
created from distillation has not been well studied. Also, the mechanisms
behind transfer learning of those BERT models are not well investigated either.
Therefore, this work focuses on analyzing the acceptable deduction when
distillation for guiding the future distillation procedure. Specifically, we
first inspect the prunability of the Transformer heads in RoBERTa and ALBERT
using their head importance estimation proposed by Michel et al. (2019), and
then check the coherence of the important heads between the pre-trained task
and downstream tasks. Hence, the acceptable deduction of performance on the
pre-trained task when distilling a model can be derived from the results, and
we further compare the behavior of the pruned model before and after
fine-tuning. Our studies provide guidance for future directions about BERT
family model distillation.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07139" title="Abstract">arXiv:2106.07139</a> [<a href="/pdf/2106.07139" title="Download PDF">pdf</a>, <a href="/format/2106.07139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-Trained Models: Past, Present and Future
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhengyan%2C+Z">Zhang Zhengyan</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+D">Ding Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yuxian%2C+G">Gu Yuxian</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Liu Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Yuqi%2C+H">Huo Yuqi</a>, 
<a href="/search/cs?searchtype=author&query=Jiezhong%2C+Q">Qiu Jiezhong</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wentao%2C+H">Han Wentao</a>, 
<a href="/search/cs?searchtype=author&query=Minlie%2C+H">Huang Minlie</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jin Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yanyan%2C+L">Lan Yanyan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhiyuan%2C+L">Liu Zhiyuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhiwu%2C+L">Lu Zhiwu</a>, 
<a href="/search/cs?searchtype=author&query=Xipeng%2C+Q">Qiu Xipeng</a>, 
<a href="/search/cs?searchtype=author&query=Ruihua%2C+S">Song Ruihua</a>, 
<a href="/search/cs?searchtype=author&query=Jie%2C+T">Tang Jie</a>, 
<a href="/search/cs?searchtype=author&query=Ji-Rong%2C+W">Wen Ji-Rong</a>, 
<a href="/search/cs?searchtype=author&query=Jinhui%2C+Y">Yuan Jinhui</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+Z+W">Zhao Wayne Xin</a>, 
<a href="/search/cs?searchtype=author&query=Jun%2C+Z">Zhu Jun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large-scale pre-trained models (PTMs) such as BERT and GPT have recently
achieved great success and become a milestone in the field of artificial
intelligence (AI). Owing to sophisticated pre-training objectives and huge
model parameters, large-scale PTMs can effectively capture knowledge from
massive labeled and unlabeled data. By storing knowledge into huge parameters
and fine-tuning on specific tasks, the rich knowledge implicitly encoded in
huge parameters can benefit a variety of downstream tasks, which has been
extensively demonstrated via experimental verification and empirical analysis.
It is now the consensus of the AI community to adopt PTMs as backbone for
downstream tasks rather than learning models from scratch. In this paper, we
take a deep look into the history of pre-training, especially its special
relation with transfer learning and self-supervised learning, to reveal the
crucial position of PTMs in the AI development spectrum. Further, we
comprehensively review the latest breakthroughs of PTMs. These breakthroughs
are driven by the surge of computational power and the increasing availability
of data, towards four important directions: designing effective architectures,
utilizing rich contexts, improving computational efficiency, and conducting
interpretation and theoretical analysis. Finally, we discuss a series of open
problems and research directions of PTMs, and hope our view can inspire and
advance the future study of PTMs.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07140" title="Abstract">arXiv:2106.07140</a> [<a href="/pdf/2106.07140" title="Download PDF">pdf</a>, <a href="/format/2106.07140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SinIR: Efficient General Image Manipulation with Single Image  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jihyeong Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose SinIR, an efficient reconstruction-based framework trained on a
single natural image for general image manipulation, including
super-resolution, editing, harmonization, paint-to-image, photo-realistic style
transfer, and artistic style transfer. We train our model on a single image
with cascaded multi-scale learning, where each network at each scale is
responsible for image reconstruction. This reconstruction objective greatly
reduces the complexity and running time of training, compared to the GAN
objective. However, the reconstruction objective also exacerbates the output
quality. Therefore, to solve this problem, we further utilize simple random
pixel shuffling, which also gives control over manipulation, inspired by the
Denoising Autoencoder. With quantitative evaluation, we show that SinIR has
competitive performance on various image manipulation tasks. Moreover, with a
much simpler training objective (i.e., reconstruction), SinIR is trained 33.5
times faster than SinGAN (for 500 X 500 images) that solves similar tasks. Our
code is publicly available at github.com/YooJiHyeong/SinIR.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07141" title="Abstract">arXiv:2106.07141</a> [<a href="/pdf/2106.07141" title="Download PDF">pdf</a>, <a href="/format/2106.07141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selection of Source Images Heavily Influences the Effectiveness of  Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozbulak%2C+U">Utku Ozbulak</a>, 
<a href="/search/cs?searchtype=author&query=Anzaku%2C+E+T">Esla Timothy Anzaku</a>, 
<a href="/search/cs?searchtype=author&query=De+Neve%2C+W">Wesley De Neve</a>, 
<a href="/search/cs?searchtype=author&query=Van+Messem%2C+A">Arnout Van Messem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Although the adoption rate of deep neural networks (DNNs) has tremendously
increased in recent years, a solution for their vulnerability against
adversarial examples has not yet been found. As a result, substantial research
efforts are dedicated to fix this weakness, with many studies typically using a
subset of source images to generate adversarial examples, treating every image
in this subset as equal. We demonstrate that, in fact, not every source image
is equally suited for this kind of assessment. To do so, we devise a
large-scale model-to-model transferability scenario for which we meticulously
analyze the properties of adversarial examples, generated from every suitable
source image in ImageNet by making use of two of the most frequently deployed
attacks. In this transferability scenario, which involves seven distinct DNN
models, including the recently proposed vision transformers, we reveal that it
is possible to have a difference of up to $12.5\%$ in model-to-model
transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$
($8/225$) in average $L_{\infty}$ perturbation when $1,000$ source images are
sampled randomly among all suitable candidates. We then take one of the first
steps in evaluating the robustness of images used to create adversarial
examples, proposing a number of simple but effective methods to identify
unsuitable source images, thus making it possible to mitigate extreme cases in
experimentation and support high-quality benchmarking.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07152" title="Abstract">arXiv:2106.07152</a> [<a href="/pdf/2106.07152" title="Download PDF">pdf</a>, <a href="/ps/2106.07152" title="Download PostScript">ps</a>, <a href="/format/2106.07152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Construction of 4-Additive Spanners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Dhalaan%2C+B">Bandar Al-Dhalaan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">A $k$-additive spanner of a graph is a subgraph that preserves the distance
between any two nodes up to a total additive error of $+k$. Efficient
algorithms have been devised for constructing 2 [Aingworth et al. SIAM '99], 6
[Baswana et al. ACM '10, Woodruff ICALP '13], and 8-additive spanners [Knudsen
'17], but efficiency hasn't been studied for 4-additive spanner constructions.
In this paper we present a modification of Chechik's 4-additive spanner
construction [Chechik SODA '13] that produces a 4-additive spanner on
$\widetilde{O}(n^{7/5})$ edges, with an improved runtime of
$\widetilde{O}(mn^{3/5})$ from $O(mn)$.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07153" title="Abstract">arXiv:2106.07153</a> [<a href="/pdf/2106.07153" title="Download PDF">pdf</a>, <a href="/format/2106.07153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Methods for Private Synthetic Data: Unifying Framework and New  Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Terrance Liu</a>, 
<a href="/search/cs?searchtype=author&query=Vietri%2C+G">Giuseppe Vietri</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study private synthetic data generation for query release, where the goal
is to construct a sanitized version of a sensitive dataset, subject to
differential privacy, that approximately preserves the answers to a large
collection of statistical queries. We first present an algorithmic framework
that unifies a long line of iterative algorithms in the literature. Under this
framework, we propose two new methods. The first method, private entropy
projection (PEP), can be viewed as an advanced variant of MWEM that adaptively
reuses past query measurements to boost accuracy. Our second method, generative
networks with the exponential mechanism (GEM), circumvents computational
bottlenecks in algorithms such as MWEM and PEP by optimizing over generative
models parameterized by neural networks, which capture a rich family of
distributions while enabling fast gradient-based optimization. We demonstrate
that PEP and GEM empirically outperform existing algorithms. Furthermore, we
show that GEM nicely incorporates prior information from public data while
overcoming limitations of PMW^Pub, the existing state-of-the-art method that
also leverages public data.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07154" title="Abstract">arXiv:2106.07154</a> [<a href="/pdf/2106.07154" title="Download PDF">pdf</a>, <a href="/format/2106.07154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local time stepping for the shallow water equations in MPAS-Ocean
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Capodaglio%2C+G">Giacomo Capodaglio</a>, 
<a href="/search/math?searchtype=author&query=Petersen%2C+M">Mark Petersen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We assess the performance of a set of local time-stepping schemes for the
shallow water equations implemented in the global ocean model MPAS-Ocean. The
availability of local time-stepping tools is of major relevance for ocean codes
such as MPAS-Ocean, which rely on a multi-resolution approach to perform
regional grid refinement, for instance in proximity of the coast. In presence
of variable resolution, the size of the time-step of explicit numerical
integrators is bounded above by the size of the smallest cell on the grid,
according to the Courant-Friedrichs-Lewy (CFL) condition. This constraint means
that the time-step size used in low resolution regions must be the same as the
one used in high resolution regions, resulting in an unnecessary computational
effort. Local time-stepping, on the other hand, allows one to select different
time-step sizes according to local, rather than global, CFL conditions,
resulting in a more tailored integration process and reduced computational
times. The present work is a preliminary but necessary effort aimed at paving
the way for a more comprehensive work on local time-stepping for the primitive
equation set with realistic geography.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07155" title="Abstract">arXiv:2106.07155</a> [<a href="/pdf/2106.07155" title="Download PDF">pdf</a>, <a href="/ps/2106.07155" title="Download PostScript">ps</a>, <a href="/format/2106.07155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFedAvg: Achieving Efficient Communication and Fast Convergence in  Non-IID Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haibo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bentley%2C+E+S">Elizabeth S. Bentley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning (FL) is a prevailing distributed learning paradigm, where
a large number of workers jointly learn a model without sharing their training
data. However, high communication costs could arise in FL due to large-scale
(deep) learning models and bandwidth-constrained connections. In this paper, we
introduce a communication-efficient algorithmic framework called CFedAvg for FL
with non-i.i.d. datasets, which works with general (biased or unbiased)
SNR-constrained compressors. We analyze the convergence rate of CFedAvg for
non-convex functions with constant and decaying learning rates. The CFedAvg
algorithm can achieve an $\mathcal{O}(1 / \sqrt{mKT} + 1 / T)$ convergence rate
with a constant learning rate, implying a linear speedup for convergence as the
number of workers increases, where $K$ is the number of local steps, $T$ is the
number of total communication rounds, and $m$ is the total worker number. This
matches the convergence rate of distributed/federated learning without
compression, thus achieving high communication efficiency while not sacrificing
learning accuracy in FL. Furthermore, we extend CFedAvg to cases with
heterogeneous local steps, which allows different workers to perform a
different number of local steps to better adapt to their own circumstances. The
interesting observation in general is that the noise/variance introduced by
compressors does not affect the overall convergence rate order for non-i.i.d.
FL. We verify the effectiveness of our CFedAvg algorithm on three datasets with
two gradient compression schemes of different compression ratios.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07156" title="Abstract">arXiv:2106.07156</a> [<a href="/pdf/2106.07156" title="Download PDF">pdf</a>, <a href="/format/2106.07156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Predictive Coding For Model-Based Planning In Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Tung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+R">Rui Shu</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+T">Tuan Pham</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+H">Hung Bui</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Machine Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">High-dimensional observations are a major challenge in the application of
model-based reinforcement learning (MBRL) to real-world environments. To handle
high-dimensional sensory inputs, existing approaches use representation
learning to map high-dimensional observations into a lower-dimensional latent
space that is more amenable to dynamics estimation and planning. In this work,
we present an information-theoretic approach that employs temporal predictive
coding to encode elements in the environment that can be predicted across time.
Since this approach focuses on encoding temporally-predictable information, we
implicitly prioritize the encoding of task-relevant components over nuisance
information within the environment that are provably task-irrelevant. By
learning this representation in conjunction with a recurrent state space model,
we can then perform planning in latent space. We evaluate our model on a
challenging modification of standard DMControl tasks where the background is
replaced with natural videos that contain complex but irrelevant information to
the planning task. Our experiments show that our model is superior to existing
methods in the challenging complex-background setting while remaining
competitive with current state-of-the-art models in the standard setting.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07157" title="Abstract">arXiv:2106.07157</a> [<a href="/pdf/2106.07157" title="Download PDF">pdf</a>, <a href="/format/2106.07157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple scattering ambisonics: three-dimensional sound foeld estimation  using interacting spheres
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+S">Shoken Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Duraiswami%2C+R">Ramani Duraiswami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Rigid spherical microphone arrays (RSMAs) have been widely used in ambisonics
sound field recording. While it is desired to combine the information captured
by a grid of densely arranged RSMAs for expanding the area of accurate
reconstruction, or sweet-spots, this is not trivial due to inter-array
interference. Here we propose multiple scattering ambisonics, a method for
three-dimensional ambisonics sound field recording using multiple acoustically
interacting RSMAs. Numerical experiments demonstrate the sweet-spot expansion
realized by the proposed method. The proposed method can be used with existing
RSMAs as building blocks and opens possibilities including higher
degrees-of-freedom spatial audio.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07158" title="Abstract">arXiv:2106.07158</a> [<a href="/pdf/2106.07158" title="Download PDF">pdf</a>, <a href="/ps/2106.07158" title="Download PostScript">ps</a>, <a href="/format/2106.07158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Variable K-Pseudonym Scheme Applied to 5G Anonymous Access  Authentication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Dong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xixiang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+R">Renpeng Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Anonymous access authentication schemes provide users with massive
application services while protecting the privacy of users' identities. The
identity protection schemes in 3G and 4G are not suitable for 5G anonymous
access authentication due to complex computation and pseudonym asynchrony. In
this paper, we consider mobile devices with limited resources in the 5G network
and propose an anonymous access authentication scheme without the Public Key
Infrastructure. The anonymous access authentication scheme provides users with
variable shard pseudonyms to protect users' identities asynchronously. With the
variable shared pseudonym, our scheme can ensure user anonymity and resist the
mark attack, a novel attack aimed at the basic k-pseudonym scheme. Finally, we
analyze the scheme with BAN logic analysis and verify the user anonymity.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07159" title="Abstract">arXiv:2106.07159</a> [<a href="/pdf/2106.07159" title="Download PDF">pdf</a>, <a href="/format/2106.07159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object-Guided Instance Segmentation With Auxiliary Feature Refinement  for Biological Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jingru Yi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Pengxiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hui Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qiaoying Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Hui Qu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lianyi Han</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Hoeppner%2C+D+J">Daniel J. Hoeppner</a>, 
<a href="/search/cs?searchtype=author&query=Metaxas%2C+D+N">Dimitris N. Metaxas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in TMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Instance segmentation is of great importance for many biological
applications, such as study of neural cell interactions, plant phenotyping, and
quantitatively measuring how cells react to drug treatment. In this paper, we
propose a novel box-based instance segmentation method. Box-based instance
segmentation methods capture objects via bounding boxes and then perform
individual segmentation within each bounding box region. However, existing
methods can hardly differentiate the target from its neighboring objects within
the same bounding box region due to their similar textures and low-contrast
boundaries. To deal with this problem, in this paper, we propose an
object-guided instance segmentation method. Our method first detects the center
points of the objects, from which the bounding box parameters are then
predicted. To perform segmentation, an object-guided coarse-to-fine
segmentation branch is built along with the detection branch. The segmentation
branch reuses the object features as guidance to separate target object from
the neighboring ones within the same bounding box region. To further improve
the segmentation quality, we design an auxiliary feature refinement module that
densely samples and refines point-wise features in the boundary regions.
Experimental results on three biological image datasets demonstrate the
advantages of our method. The code will be available at
https://github.com/yijingru/ObjGuided-Instance-Segmentation.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07160" title="Abstract">arXiv:2106.07160</a> [<a href="/pdf/2106.07160" title="Download PDF">pdf</a>, <a href="/format/2106.07160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Intrusion Prevention Policies through Optimal Stopping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hammar%2C+K">Kim Hammar</a>, 
<a href="/search/cs?searchtype=author&query=Stadler%2C+R">Rolf Stadler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">We study automated intrusion prevention using reinforcement learning. In a
novel approach, we formulate the problem of intrusion prevention as an optimal
stopping problem. This formulation allows us insight into the structure of the
optimal policies, which turn out to be threshold based. Since the computation
of the optimal defender policy using dynamic programming is not feasible for
practical cases, we approximate the optimal policy through reinforcement
learning in a simulation environment. To define the dynamics of the simulation,
we emulate the target infrastructure and collect measurements. Our evaluations
show that the learned policies are close to optimal and that they indeed can be
expressed using thresholds.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07161" title="Abstract">arXiv:2106.07161</a> [<a href="/pdf/2106.07161" title="Download PDF">pdf</a>, <a href="/format/2106.07161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneous Edge-Enhanced Graph Attention Network For Multi-Agent  Trajectory Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+X">Xiaoyu Mo</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chen Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Simultaneous trajectory prediction for multiple heterogeneous traffic
participants is essential for the safe and efficient operation of connected
automated vehicles under complex driving situations in the real world. The
multi-agent prediction task is challenging, as the motions of traffic
participants are affected by many factors, including their individual dynamics,
their interactions with surrounding agents, the traffic infrastructures, and
the number and modalities of the target agents. To further advance the
trajectory prediction techniques, in this work we propose a three-channel
framework together with a novel Heterogeneous Edge-enhanced graph ATtention
network (HEAT), which is able to deal with the heterogeneity of the target
agents and traffic participants involved. Specifically, the agent's dynamics
are extracted from their historical states using type-specific encoders. The
inter-agent interactions are represented with a directed edge-featured
heterogeneous graph, and then interaction features are extracted using the
proposed HEAT network. Besides, the map features are shared across all agents
by introducing a selective gate mechanism. And finally, the trajectories of
multi-agent are executed simultaneously. Validations using both urban and
highway driving datasets show that the proposed model can realize simultaneous
trajectory predictions for multiple agents under complex traffic situations,
and achieve state-of-the-art performance with respect to prediction accuracy,
demonstrating its feasibility and effectiveness.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07162" title="Abstract">arXiv:2106.07162</a> [<a href="/pdf/2106.07162" title="Download PDF">pdf</a>, <a href="/format/2106.07162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Goal-Aware Neural SAT Solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozolins%2C+E">Emils Ozolins</a>, 
<a href="/search/cs?searchtype=author&query=Freivalds%2C+K">Karlis Freivalds</a>, 
<a href="/search/cs?searchtype=author&query=Draguns%2C+A">Andis Draguns</a>, 
<a href="/search/cs?searchtype=author&query=Gaile%2C+E">Eliza Gaile</a>, 
<a href="/search/cs?searchtype=author&query=Zakovskis%2C+R">Ronalds Zakovskis</a>, 
<a href="/search/cs?searchtype=author&query=Kozlovics%2C+S">Sergejs Kozlovics</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Modern neural networks obtain information about the problem and calculate the
output solely from the input values. We argue that it is not always optimal,
and the network's performance can be significantly improved by augmenting it
with a query mechanism that allows the network to make several solution trials
at run time and get feedback on the loss value on each trial. To demonstrate
the capabilities of the query mechanism, we formulate an unsupervised (not
dependant on labels) loss function for Boolean Satisfiability Problem (SAT) and
theoretically show that it allows the network to extract rich information about
the problem. We then propose a neural SAT solver with a query mechanism called
QuerySAT and show that it outperforms the neural baseline on a wide range of
SAT tasks and the classical baselines on SHA-1 preimage attack and 3-SAT task.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07165" title="Abstract">arXiv:2106.07165</a> [<a href="/pdf/2106.07165" title="Download PDF">pdf</a>, <a href="/ps/2106.07165" title="Download PostScript">ps</a>, <a href="/format/2106.07165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-training Guided Adversarial Domain Adaptation For Thermal Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akkaya%2C+I+B">Ibrahim Batuhan Akkaya</a>, 
<a href="/search/cs?searchtype=author&query=Altinel%2C+F">Fazil Altinel</a>, 
<a href="/search/cs?searchtype=author&query=Halici%2C+U">Ugur Halici</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2021 Perception Beyond the Visible Spectrum (PBVS) workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep models trained on large-scale RGB image datasets have shown tremendous
success. It is important to apply such deep models to real-world problems.
However, these models suffer from a performance bottleneck under illumination
changes. Thermal IR cameras are more robust against such changes, and thus can
be very useful for the real-world problems. In order to investigate efficacy of
combining feature-rich visible spectrum and thermal image modalities, we
propose an unsupervised domain adaptation method which does not require
RGB-to-thermal image pairs. We employ large-scale RGB dataset MS-COCO as source
domain and thermal dataset FLIR ADAS as target domain to demonstrate results of
our method. Although adversarial domain adaptation methods aim to align the
distributions of source and target domains, simply aligning the distributions
cannot guarantee perfect generalization to the target domain. To this end, we
propose a self-training guided adversarial domain adaptation method to promote
generalization capabilities of adversarial domain adaptation methods. To
perform self-training, pseudo labels are assigned to the samples on the target
thermal domain to learn more generalized representations for the target domain.
Extensive experimental analyses show that our proposed method achieves better
results than the state-of-the-art adversarial domain adaptation methods. The
code and models are publicly available.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07166" title="Abstract">arXiv:2106.07166</a> [<a href="/pdf/2106.07166" title="Download PDF">pdf</a>, <a href="/format/2106.07166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 2rd Place Solutions in the HC-STVG track of Person in Context Challenge  2021
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=YiYu">YiYu</a>, 
<a href="/search/cs?searchtype=author&query=XinyingWang">XinyingWang</a>, 
<a href="/search/cs?searchtype=author&query=WeiHu">WeiHu</a>, 
<a href="/search/cs?searchtype=author&query=XunLuo">XunLuo</a>, 
<a href="/search/cs?searchtype=author&query=ChengLi">ChengLi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this technical report, we present our solution to localize a
spatio-temporal person in an untrimmed video based on a sentence. We achieve
the second vIOU(0.30025) in the HC-STVG track of the 3rd Person in Context(PIC)
Challenge. Our solution contains three parts: 1) human attributes information
is extracted from the sentence, it is helpful to filter out tube proposals in
the testing phase and supervise our classifier to learn appearance information
in the training phase. 2) we detect humans with YoloV5 and track humans based
on the DeepSort framework but replace the original ReID network with FastReID.
3) a visual transformer is used to extract cross-modal representations for
localizing a spatio-temporal tube of the target person.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07167" title="Abstract">arXiv:2106.07167</a> [<a href="/pdf/2106.07167" title="Download PDF">pdf</a>, <a href="/format/2106.07167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Neural Diarization: From Transformer to Conformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y+C">Yi Chieh Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+E">Eunjung Han</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chul Lee</a>, 
<a href="/search/cs?searchtype=author&query=Stolcke%2C+A">Andreas Stolcke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose a new end-to-end neural diarization (EEND) system that is based on
Conformer, a recently proposed neural architecture that combines convolutional
mappings and Transformer to model both local and global dependencies in speech.
We first show that data augmentation and convolutional subsampling layers
enhance the original self-attentive EEND in the Transformer-based EEND, and
then Conformer gives an additional gain over the Transformer-based EEND.
However, we notice that the Conformer-based EEND does not generalize as well
from simulated to real conversation data as the Transformer-based model. This
leads us to quantify the mismatch between simulated data and real speaker
behavior in terms of temporal statistics reflecting turn-taking between
speakers, and investigate its correlation with diarization error. By mixing
simulated and real data in EEND training, we mitigate the mismatch further,
with Conformer-based EEND achieving 24% error reduction over the baseline
SA-EEND system, and 10% improvement over the best augmented Transformer-based
system, on two-speaker CALLHOME data.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07171" title="Abstract">arXiv:2106.07171</a> [<a href="/pdf/2106.07171" title="Download PDF">pdf</a>, <a href="/format/2106.07171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining and Combating Spurious Features under Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunting Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xuezhe Ma</a>, 
<a href="/search/cs?searchtype=author&query=Michel%2C+P">Paul Michel</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICML2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A central goal of machine learning is to learn robust representations that
capture the causal relationship between inputs features and output labels.
However, minimizing empirical risk over finite or biased datasets often results
in models latching on to spurious correlations between the training
input/output pairs that are not fundamental to the problem at hand. In this
paper, we define and analyze robust and spurious representations using the
information-theoretic concept of minimal sufficient statistics. We prove that
even when there is only bias of the input distribution (i.e. covariate shift),
models can still pick up spurious features from their training data. Group
distributionally robust optimization (DRO) provides an effective tool to
alleviate covariate shift by minimizing the worst-case training loss over a set
of pre-defined groups. Inspired by our analysis, we demonstrate that group DRO
can fail when groups do not directly account for various spurious correlations
that occur in the data. To address this, we further propose to minimize the
worst-case losses over a more flexible set of distributions that are defined on
the joint distribution of groups and instances, instead of treating each group
as a whole at optimization time. Through extensive experiments on one image and
two language tasks, we show that our model is significantly more robust than
comparable baselines under various partitions. Our code is available at
https://github.com/violet-zct/group-conditional-DRO.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07172" title="Abstract">arXiv:2106.07172</a> [<a href="/pdf/2106.07172" title="Download PDF">pdf</a>, <a href="/format/2106.07172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-efficient Knowledge Distillation for Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongjin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seongsik Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jongwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Doh%2C+W">Wuhyeong Doh</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sungroh Yoon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Spiking neural networks (SNNs) have been gaining interest as energy-efficient
alternatives of conventional artificial neural networks (ANNs) due to their
event-driven computation. Considering the future deployment of SNN models to
constrained neuromorphic devices, many studies have applied techniques
originally used for ANN model compression, such as network quantization,
pruning, and knowledge distillation, to SNNs. Among them, existing works on
knowledge distillation reported accuracy improvements of student SNN model.
However, analysis on energy efficiency, which is also an important feature of
SNN, was absent. In this paper, we thoroughly analyze the performance of the
distilled SNN model in terms of accuracy and energy efficiency. In the process,
we observe a substantial increase in the number of spikes, leading to energy
inefficiency, when using the conventional knowledge distillation methods. Based
on this analysis, to achieve energy efficiency, we propose a novel knowledge
distillation method with heterogeneous temperature parameters. We evaluate our
method on two different datasets and show that the resulting SNN student
satisfies both accuracy improvement and reduction of the number of spikes. On
MNIST dataset, our proposed student SNN achieves up to 0.09% higher accuracy
and produces 65% less spikes compared to the student SNN trained with
conventional knowledge distillation method. We also compare the results with
other SNN compression techniques and training methods.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07174" title="Abstract">arXiv:2106.07174</a> [<a href="/pdf/2106.07174" title="Download PDF">pdf</a>, <a href="/format/2106.07174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Mutual Information Maximization Approach for the Spurious Solution  Problem in Weakly Supervised Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL2021 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Weakly supervised question answering usually has only the final answers as
supervision signals while the correct solutions to derive the answers are not
provided. This setting gives rise to the spurious solution problem: there may
exist many spurious solutions that coincidentally derive the correct answer,
but training on such solutions can hurt model performance (e.g., producing
wrong solutions or answers). For example, for discrete reasoning tasks as on
DROP, there may exist many equations to derive a numeric answer, and typically
only one of them is correct. Previous learning methods mostly filter out
spurious solutions with heuristics or using model confidence, but do not
explicitly exploit the semantic correlations between a question and its
solution. In this paper, to alleviate the spurious solution problem, we propose
to explicitly exploit such semantic correlations by maximizing the mutual
information between question-answer pairs and predicted solutions. Extensive
experiments on four question answering datasets show that our method
significantly outperforms previous learning methods in terms of task
performance and is more effective in training models to produce correct
solutions.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07175" title="Abstract">arXiv:2106.07175</a> [<a href="/pdf/2106.07175" title="Download PDF">pdf</a>, <a href="/format/2106.07175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Combine Per-Example Solutions for Neural Program Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+D">Disha Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Larochelle%2C+H">Hugo Larochelle</a>, 
<a href="/search/cs?searchtype=author&query=Tarlow%2C+D">Daniel Tarlow</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
<p class="mathjax">The goal of program synthesis from examples is to find a computer program
that is consistent with a given set of input-output examples. Most
learning-based approaches try to find a program that satisfies all examples at
once. Our work, by contrast, considers an approach that breaks the problem into
two stages: (a) find programs that satisfy only one example, and (b) leverage
these per-example solutions to yield a program that satisfies all examples. We
introduce the Cross Aggregator neural network module based on a multi-head
attention mechanism that learns to combine the cues present in these
per-example solutions to synthesize a global solution. Evaluation across
programs of different lengths and under two different experimental settings
reveal that when given the same time budget, our technique significantly
improves the success rate over PCCoder <a href="/abs/1809.04682">arXiv:1809.04682v2</a> [cs.LG] and other
ablation baselines. The code, data and trained models for our work can be found
at https://github.com/shrivastavadisha/N-PEPS.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07176" title="Abstract">arXiv:2106.07176</a> [<a href="/pdf/2106.07176" title="Download PDF">pdf</a>, <a href="/format/2106.07176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAS: Self-Augmented Strategy for Language Model Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yifei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingqiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ru He</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+L">Liangzhu Ge</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The core of a self-supervised learning method for pre-training language
models includes the design of appropriate data augmentation and corresponding
pre-training task(s). Most data augmentations in language model pre-training
are context-independent. The seminal contextualized augmentation recently
proposed by the ELECTRA requires a separate generator, which leads to extra
computation cost as well as the challenge in adjusting the capability of its
generator relative to that of the other model component(s). We propose a
self-augmented strategy (SAS) that uses a single forward pass through the model
to augment the input data for model training in the next epoch. Essentially our
strategy eliminates a separate generator network and uses only one network to
generate the data augmentation and undertake two pre-training tasks (the MLM
task and the RTD task) jointly, which naturally avoids the challenge in
adjusting the generator's capability as well as reduces the computation cost.
Additionally, our SAS is a general strategy such that it can seamlessly
incorporate many new techniques emerging recently or in the future, such as the
disentangled attention mechanism recently proposed by the DeBERTa model. Our
experiments show that our SAS is able to outperform the ELECTRA and other
state-of-the-art models in the GLUE tasks with the same or less computation
cost.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07178" title="Abstract">arXiv:2106.07178</a> [<a href="/pdf/2106.07178" title="Download PDF">pdf</a>, <a href="/format/2106.07178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on Graph Anomaly Detection with Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoxiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Shan Xue</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Q+Z">Quan Z. Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Anomalies represent rare observations (e.g., data records or events) that are
deviating significantly from others. Over the last forty years, researches on
anomalies have received great interests because of their significance in many
disciplines (e.g., computer science, chemistry, and biology). Anomaly
detection, which aims to identify these rare observations, is among the most
vital tasks and has shown its power in preventing detrimental events, such as
financial fraud and network intrusion, from happening. The detection task is
typically solved by detecting outlying data points in the features space and
inherently overlooks the structural information in real-world data. Graphs have
been prevalently used to preserve the structural information, and this raises
the graph anomaly detection problem - identifying anomalous graph objects
(i.e., nodes, edges and sub-graphs). However, conventional anomaly detection
techniques cannot well solve this problem because of the complexity of graph
data (e.g., irregular structures, non-independent and large-scale). For the
aptitudes of deep learning in breaking these limitations, graph anomaly
detection with deep learning has received intensified studies recently. In this
survey, we aim to provide a systematic and comprehensive review of the
contemporary deep learning techniques for graph anomaly detection.
Specifically, our categorization follows a task-driven strategy and classifies
existing works according to the anomalous graph objects they can detect. We
especially focus on the motivations, key intuitions and technical details of
existing works. We also summarize open-sourced implementations, public
datasets, and commonly-used evaluation metrics for future studies. Finally, we
highlight twelve future research directions according to our survey results
covering emerging problems introduced by graph data, anomaly detection and real
applications.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07182" title="Abstract">arXiv:2106.07182</a> [<a href="/pdf/2106.07182" title="Download PDF">pdf</a>, <a href="/format/2106.07182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deploying COTS Legged Robot Platforms into a Heterogeneous Robot Team
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tam%2C+B">Benjamin Tam</a>, 
<a href="/search/cs?searchtype=author&query=Molnar%2C+T">Thomas Molnar</a>, 
<a href="/search/cs?searchtype=author&query=Talbot%2C+F">Fletcher Talbot</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+B">Brett Wood</a>, 
<a href="/search/cs?searchtype=author&query=Steindl%2C+R">Ryan Steindl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2021: 5th Full-Day Workshop on Towards Real-World Deployment of Legged Robots
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The recent availability of commercial-off-the-shelf (COTS) legged robot
platforms have opened up new opportunities in deploying legged systems into
different scenarios. While the main advantage of legged robots is their ability
to traverse unstructured terrain, there are still large gaps between what robot
platforms can achieve and their animal counterparts. Therefore, when deploying
as part of a heterogeneous robot team of different platforms, it is beneficial
to understand the different scenarios where a legged platform would perform
better than a wheeled, tracked or aerial platform. Two COTS quadruped robots,
Ghost Robotics' Vision 60 and Boston Dynamics' Spot, were deployed into a
heterogeneous team. A description of some of the challenges faced while
integrating the platforms, as well as some experiments in traversing different
terrains are provided to give insight into the real-world deployment of legged
robots.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07185" title="Abstract">arXiv:2106.07185</a> [<a href="/pdf/2106.07185" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Object Recognition in Newborn Chicks using Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Donsuk Lee</a>, 
<a href="/search/cs?searchtype=author&query=Pak%2C+D">Denizhan Pak</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+J+N">Justin N. Wood</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at CogSci 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">In recent years, the brain and cognitive sciences have made great strides
developing a mechanistic understanding of object recognition in mature brains.
Despite this progress, fundamental questions remain about the origins and
computational foundations of object recognition. What learning algorithms
underlie object recognition in newborn brains? Since newborn animals learn
largely through unsupervised learning, we explored whether unsupervised
learning algorithms can be used to predict the view-invariant object
recognition behavior of newborn chicks. Specifically, we used feature
representations derived from unsupervised deep neural networks (DNNs) as inputs
to cognitive models of categorization. We show that features derived from
unsupervised DNNs make competitive predictions about chick behavior compared to
supervised features. More generally, we argue that linking controlled-rearing
studies to image-computable DNN models opens new experimental avenues for
studying the origins and computational basis of object recognition in newborn
animals.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07186" title="Abstract">arXiv:2106.07186</a> [<a href="/pdf/2106.07186" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sejong Face Database: A Multi-Modal Disguise Face Database
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheema%2C+U">Usman Cheema</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Seungbin Moon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Database Access Link: <a href="https://github.com/usmancheema89/SejongFaceDatabase">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Vision and Image Understanding, Volumes 208-209, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Commercial application of facial recognition demands robustness to a variety
of challenges such as illumination, occlusion, spoofing, disguise, etc.
Disguised face recognition is one of the emerging issues for access control
systems, such as security checkpoints at the borders. However, the lack of
availability of face databases with a variety of disguise addons limits the
development of academic research in the area. In this paper, we present a
multimodal disguised face dataset to facilitate the disguised face recognition
research. The presented database contains 8 facial add-ons and 7 additional
combinations of these add-ons to create a variety of disguised face images.
Each facial image is captured in visible, visible plus infrared, infrared, and
thermal spectra. Specifically, the database contains 100 subjects divided into
subset-A (30 subjects, 1 image per modality) and subset-B (70 subjects, 5 plus
images per modality). We also present baseline face detection results performed
on the proposed database to provide reference results and compare the
performance in different modalities. Qualitative and quantitative analysis is
performed to evaluate the challenging nature of disguise addons. The dataset
will be publicly available with the acceptance of the research article. The
database is available at: https://github.com/usmancheema89/SejongFaceDatabase.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07189" title="Abstract">arXiv:2106.07189</a> [<a href="/pdf/2106.07189" title="Download PDF">pdf</a>, <a href="/format/2106.07189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capacity of Gaussian Arbitrarily-Varying Fading Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseinigoki%2C+F">Fatemeh Hosseinigoki</a>, 
<a href="/search/cs?searchtype=author&query=Kosut%2C+O">Oliver Kosut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper considers an arbitrarily-varying fading channel consisting of one
transmitter, one receiver and an arbitrarily varying adversary. The channel is
assumed to have additive Gaussian noise and fast fading of the gain from the
legitimate user to the receiver. We study four variants of the problem
depending on whether the transmitter and/or adversary have access to the fading
gains; we assume the receiver always knows the fading gains. In two variants
the adversary does not have access to the gains, we show that the capacity
corresponds to the capacity of a standard point-to-point fading channel with
increased noise variance. The capacity of the other two cases, in which the
adversary has knowledge of the channel gains, are determined by the worst-case
noise variance as a function of the channel gain subject to the jammer's power
constraint; if the jammer has enough power, then it can imitate the legitimate
user's channel, causing the capacity to drop to zero. We also show that having
the channel gains causally or non-causally at the encoder and/or the adversary
does not change the capacity, except for the case where all parties know the
channel gains. In this case, if the transmitter knows the gains non-causally,
while the adversary knows the gains causally, then it is possible for the
legitimate users to keep a secret from the adversary. We show that in this case
the capacity is always positive.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07190" title="Abstract">arXiv:2106.07190</a> [<a href="/pdf/2106.07190" title="Download PDF">pdf</a>, <a href="/format/2106.07190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group-based Bi-Directional Recurrent Wavelet Neural Networks for Video  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Young-Ju Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Young-Woon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Byung-Gyu Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Video super-resolution (VSR) aims to estimate a high-resolution (HR) frame
from a low-resolution (LR) frames. The key challenge for VSR lies in the
effective exploitation of spatial correlation in an intra-frame and temporal
dependency between consecutive frames. However, most of the previous methods
treat different types of the spatial features identically and extract spatial
and temporal features from the separated modules. It leads to lack of obtaining
meaningful information and enhancing the fine details. In VSR, there are three
types of temporal modeling frameworks: 2D convolutional neural networks (CNN),
3D CNN, and recurrent neural networks (RNN). Among them, the RNN-based approach
is suitable for sequential data. Thus the SR performance can be greatly
improved by using the hidden states of adjacent frames. However, at each of
time step in a recurrent structure, the RNN-based previous works utilize the
neighboring features restrictively. Since the range of accessible motion per
time step is narrow, there are still limitations to restore the missing details
for dynamic or large motion. In this paper, we propose a group-based
bi-directional recurrent wavelet neural networks (GBR-WNN) to exploit the
sequential data and spatio-temporal information effectively for VSR. The
proposed group-based bi-directional RNN (GBR) temporal modeling framework is
built on the well-structured process with the group of pictures (GOP). We
propose a temporal wavelet attention (TWA) module, in which attention is
adopted for both spatial and temporal features. Experimental results
demonstrate that the proposed method achieves superior performance compared
with state-of-the-art methods in both of quantitative and qualitative
evaluations.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07192" title="Abstract">arXiv:2106.07192</a> [<a href="/pdf/2106.07192" title="Download PDF">pdf</a>, <a href="/format/2106.07192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Document Sketching: Generating Drafts from Analogous Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zeqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Galley%2C+M">Michel Galley</a>, 
<a href="/search/cs?searchtype=author&query=Brockett%2C+C">Chris Brockett</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dolan%2C+B">Bill Dolan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The advent of large pre-trained language models has made it possible to make
high-quality predictions on how to add or change a sentence in a document.
However, the high branching factor inherent to text generation impedes the
ability of even the strongest language models to offer useful editing
suggestions at a more global or document level. We introduce a new task,
document sketching, which involves generating entire draft documents for the
writer to review and revise. These drafts are built from sets of documents that
overlap in form - sharing large segments of potentially reusable text - while
diverging in content. To support this task, we introduce a Wikipedia-based
dataset of analogous documents and investigate the application of weakly
supervised methods, including use of a transformer-based mixture of experts,
together with reinforcement learning. We report experiments using automated and
human evaluation methods and discuss relative merits of these models.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07193" title="Abstract">arXiv:2106.07193</a> [<a href="/pdf/2106.07193" title="Download PDF">pdf</a>, <a href="/format/2106.07193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crowdsourcing via Annotator Co-occurrence Imputation and Provable  Symmetric Nonnegative Matrix Factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+S">Shahana Ibrahim</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xiao Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Unsupervised learning of the Dawid-Skene (D&amp;S) model from noisy, incomplete
and crowdsourced annotations has been a long-standing challenge, and is a
critical step towards reliably labeling massive data. A recent work takes a
coupled nonnegative matrix factorization (CNMF) perspective, and shows
appealing features: It ensures the identifiability of the D\&amp;S model and enjoys
low sample complexity, as only the estimates of the co-occurrences of annotator
labels are involved. However, the identifiability holds only when certain
somewhat restrictive conditions are met in the context of crowdsourcing.
Optimizing the CNMF criterion is also costly -- and convergence assurances are
elusive. This work recasts the pairwise co-occurrence based D&amp;S model learning
problem as a symmetric NMF (SymNMF) problem -- which offers enhanced
identifiability relative to CNMF. In practice, the SymNMF model is often
(largely) incomplete, due to the lack of co-labeled items by some annotators.
Two lightweight algorithms are proposed for co-occurrence imputation. Then, a
low-complexity shifted rectified linear unit (ReLU)-empowered SymNMF algorithm
is proposed to identify the D&amp;S model. Various performance characterizations
(e.g., missing co-occurrence recoverability, stability, and convergence) and
evaluations are also presented.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07197" title="Abstract">arXiv:2106.07197</a> [<a href="/pdf/2106.07197" title="Download PDF">pdf</a>, <a href="/ps/2106.07197" title="Download PostScript">ps</a>, <a href="/format/2106.07197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DAGs with No Curl: An Efficient DAG Structure Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tian Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+N">Naiyu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Q">Qiang Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML2021, Code is available at <a href="https://github.com/fishmoon1234/DAG-NoCurl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recently directed acyclic graph (DAG) structure learning is formulated as a
constrained continuous optimization problem with continuous acyclicity
constraints and was solved iteratively through subproblem optimization. To
further improve efficiency, we propose a novel learning framework to model and
learn the weighted adjacency matrices in the DAG space directly. Specifically,
we first show that the set of weighted adjacency matrices of DAGs are
equivalent to the set of weighted gradients of graph potential functions, and
one may perform structure learning by searching in this equivalent set of DAGs.
To instantiate this idea, we propose a new algorithm, DAG-NoCurl, which solves
the optimization problem efficiently with a two-step procedure: 1) first we
find an initial cyclic solution to the optimization problem, and 2) then we
employ the Hodge decomposition of graphs and learn an acyclic graph by
projecting the cyclic graph to the gradient of a potential function.
Experimental studies on benchmark datasets demonstrate that our method provides
comparable accuracy but better efficiency than baseline DAG structure learning
methods on both linear and generalized structural equation models, often by
more than one order of magnitude.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07200" title="Abstract">arXiv:2106.07200</a> [<a href="/pdf/2106.07200" title="Download PDF">pdf</a>, <a href="/format/2106.07200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Continuous Safety Assessment in Context of DevOps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeller%2C+M">Marc Zeller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Traditionally, promoted by the internet companies, continuous delivery is
more and more appealing to industries which develop systems with
safety-critical functions. Since safety-critical systems must meet regulatory
requirements and require specific safety assessment processes in addition to
the normal development steps, enabling continuous delivery of software in
safety-critical systems requires the automation of the safety assessment
process in the delivery pipeline. In this paper, we outline a continuous
delivery pipeline for realizing continuous safety assessment in
software-intensive safety-critical systems based on model-based safety
assessment methods.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07203" title="Abstract">arXiv:2106.07203</a> [<a href="/pdf/2106.07203" title="Download PDF">pdf</a>, <a href="/ps/2106.07203" title="Download PostScript">ps</a>, <a href="/format/2106.07203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Sub-Sampling for Reinforcement Learning with General Function  Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Dingwen Kong</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruosong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L+F">Lin F. Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Designing provably efficient algorithms with general function approximation
is an important open problem in reinforcement learning. Recently, Wang et
al.~[2020c] establish a value-based algorithm with general function
approximation that enjoys
$\widetilde{O}(\mathrm{poly}(dH)\sqrt{K})$\footnote{Throughout the paper, we
use $\widetilde{O}(\cdot)$ to suppress logarithm factors. } regret bound, where
$d$ depends on the complexity of the function class, $H$ is the planning
horizon, and $K$ is the total number of episodes. However, their algorithm
requires $\Omega(K)$ computation time per round, rendering the algorithm
inefficient for practical use. In this paper, by applying online sub-sampling
techniques, we develop an algorithm that takes
$\widetilde{O}(\mathrm{poly}(dH))$ computation time per round on average, and
enjoys nearly the same regret bound. Furthermore, the algorithm achieves low
switching cost, i.e., it changes the policy only
$\widetilde{O}(\mathrm{poly}(dH))$ times during its execution, making it
appealing to be implemented in real-life scenarios. Moreover, by using an
upper-confidence based exploration-driven reward function, the algorithm
provably explores the environment in the reward-free setting. In particular,
after $\widetilde{O}(\mathrm{poly}(dH))/\epsilon^2$ rounds of exploration, the
algorithm outputs an $\epsilon$-optimal policy for any given reward function.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07204" title="Abstract">arXiv:2106.07204</a> [<a href="/pdf/2106.07204" title="Download PDF">pdf</a>, <a href="/format/2106.07204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hard Samples Rectification for Unsupervised Cross-domain Person  Re-identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chih-Ting Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Man-Yu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tsai-Shien Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chien%2C+S">Shao-Yi Chien</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was accepted by IEEE International Conference on Image Processing (ICIP) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Person re-identification (re-ID) has received great success with the
supervised learning methods. However, the task of unsupervised cross-domain
re-ID is still challenging. In this paper, we propose a Hard Samples
Rectification (HSR) learning scheme which resolves the weakness of original
clustering-based methods being vulnerable to the hard positive and negative
samples in the target unlabelled dataset. Our HSR contains two parts, an
inter-camera mining method that helps recognize a person under different views
(hard positive) and a part-based homogeneity technique that makes the model
discriminate different persons but with similar appearance (hard negative). By
rectifying those two hard cases, the re-ID model can learn effectively and
achieve promising results on two large-scale benchmarks.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07207" title="Abstract">arXiv:2106.07207</a> [<a href="/pdf/2106.07207" title="Download PDF">pdf</a>, <a href="/format/2106.07207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Straight to the Gradient: Learning to Use Novel Tokens for Neural Text  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Simeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Advanced large-scale neural language models have led to significant success
in many language generation tasks. However, the most commonly used training
objective, Maximum Likelihood Estimation (MLE), has been shown problematic,
where the trained model prefers using dull and repetitive phrases. In this
work, we introduce ScaleGrad, a modification straight to the gradient of the
loss function, to remedy the degeneration issue of the standard MLE objective.
By directly maneuvering the gradient information, ScaleGrad makes the model
learn to use novel tokens. Empirical results show the effectiveness of our
method not only in open-ended generation, but also in directed generation
tasks. With the simplicity in architecture, our method can serve as a general
training objective that is applicable to most of the neural text generation
tasks.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07211" title="Abstract">arXiv:2106.07211</a> [<a href="/pdf/2106.07211" title="Download PDF">pdf</a>, <a href="/format/2106.07211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Neural Architecture Search with Morphism-based  Transformable Backbone Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jie%2C+R">Renlong Jie</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junbin Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This study aims at making the architecture search process more adaptive for
one-shot or online training. It is extended from the existing study on
differentiable neural architecture search, and we made the backbone
architecture transformable rather than fixed during the training process. As is
known, differentiable neural architecture search (DARTS) requires a pre-defined
over-parameterized backbone architecture, while its size is to be determined
manually. Also, in DARTS backbone, Hadamard product of two elements is not
introduced, which exists in both LSTM and GRU cells for recurrent nets. This
study introduces a growing mechanism for differentiable neural architecture
search based on network morphism. It enables growing of the cell structures
from small size towards large size ones with one-shot training. Two modes can
be applied in integrating the growing and original pruning process. We also
implement a recently proposed two-input backbone architecture for recurrent
neural networks. Initial experimental results indicate that our approach and
the two-input backbone structure can be quite effective compared with other
baseline architectures including LSTM, in a variety of learning tasks including
multi-variate time series forecasting and language modeling. On the other hand,
we find that dynamic network transformation is promising in improving the
efficiency of differentiable architecture search.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07213" title="Abstract">arXiv:2106.07213</a> [<a href="/pdf/2106.07213" title="Download PDF">pdf</a>, <a href="/format/2106.07213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TweetPap: A Dataset to Study the Social Media Discourse of Scientific  Papers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+N">Naman Jain</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mayank Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 Pages, JCDL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">Nowadays, researchers have moved to platforms like Twitter to spread
information about their ideas and empirical evidence. Recent studies have shown
that social media affects the scientific impact of a paper. However, these
studies only utilize the tweet counts to represent Twitter activity. In this
paper, we propose TweetPap, a large-scale dataset that introduces temporal
information of citation/tweets and the metadata of the tweets to quantify and
understand the discourse of scientific papers on social media. The dataset is
publicly available at https://github.com/lingo-iitgn/TweetPap
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07214" title="Abstract">arXiv:2106.07214</a> [<a href="/pdf/2106.07214" title="Download PDF">pdf</a>, <a href="/format/2106.07214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence  Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cin%C3%A0%2C+A+E">Antonio Emanuele Cin&#xe0;</a>, 
<a href="/search/cs?searchtype=author&query=Grosse%2C+K">Kathrin Grosse</a>, 
<a href="/search/cs?searchtype=author&query=Vascon%2C+S">Sebastiano Vascon</a>, 
<a href="/search/cs?searchtype=author&query=Demontis%2C+A">Ambra Demontis</a>, 
<a href="/search/cs?searchtype=author&query=Biggio%2C+B">Battista Biggio</a>, 
<a href="/search/cs?searchtype=author&query=Roli%2C+F">Fabio Roli</a>, 
<a href="/search/cs?searchtype=author&query=Pelillo%2C+M">Marcello Pelillo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, submitted to NeurIPS 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Backdoor attacks inject poisoning samples during training, with the goal of
enforcing a machine-learning model to output an attacker-chosen class when
presented a specific trigger at test time. Although backdoor attacks have been
demonstrated in a variety of settings and against different models, the factors
affecting their success are not yet well understood. In this work, we provide a
unifying framework to study the process of backdoor learning under the lens of
incremental learning and influence functions. We show that the success of
backdoor attacks inherently depends on (i) the complexity of the learning
algorithm, controlled by its hyperparameters, and (ii) the fraction of backdoor
samples injected into the training set. These factors affect how fast a
machine-learning model learns to correlate the presence of a backdoor trigger
with the target class. Interestingly, our analysis shows that there exists a
region in the hyperparameter space in which the accuracy on clean test samples
is still high while backdoor attacks become ineffective, thereby suggesting
novel criteria to improve existing defenses.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07217" title="Abstract">arXiv:2106.07217</a> [<a href="/pdf/2106.07217" title="Download PDF">pdf</a>, <a href="/format/2106.07217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Over-Fit: Noisy-Label Detection based on the Overfitted Model Property
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seulki Park</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+D+U">Dae Ung Jo</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J+Y">Jin Young Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Due to the increasing need to handle the noisy label problem in a massive
dataset, learning with noisy labels has received much attention in recent
years. As a promising approach, there have been recent studies to select clean
training data by finding small-loss instances before a deep neural network
overfits the noisy-label data. However, it is challenging to prevent
overfitting. In this paper, we propose a novel noisy-label detection algorithm
by employing the property of overfitting on individual data points. To this
end, we present two novel criteria that statistically measure how much each
training sample abnormally affects the model and clean validation data. Using
the criteria, our iterative algorithm removes noisy-label samples and retrains
the model alternately until no further performance improvement is made. In
experiments on multiple benchmark datasets, we demonstrate the validity of our
algorithm and show that our algorithm outperforms the state-of-the-art methods
when the exact noise rates are not given. Furthermore, we show that our method
can not only be expanded to a real-world video dataset but also can be viewed
as a regularization method to solve problems caused by overfitting.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07218" title="Abstract">arXiv:2106.07218</a> [<a href="/pdf/2106.07218" title="Download PDF">pdf</a>, <a href="/format/2106.07218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Aware Downsampling with Deep Learning for Scalable Flood  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giladi%2C+N">Niv Giladi</a>, 
<a href="/search/cs?searchtype=author&query=Ben-Haim%2C+Z">Zvika Ben-Haim</a>, 
<a href="/search/cs?searchtype=author&query=Nevo%2C+S">Sella Nevo</a>, 
<a href="/search/cs?searchtype=author&query=Matias%2C+Y">Yossi Matias</a>, 
<a href="/search/cs?searchtype=author&query=Soudry%2C+D">Daniel Soudry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Background: Floods are the most common natural disaster in the world,
affecting the lives of hundreds of millions. Flood forecasting is therefore a
vitally important endeavor, typically achieved using physical water flow
simulations, which rely on accurate terrain elevation maps. However, such
simulations, based on solving partial differential equations, are
computationally prohibitive on a large scale. This scalability issue is
commonly alleviated using a coarse grid representation of the elevation map,
though this representation may distort crucial terrain details, leading to
significant inaccuracies in the simulation. Contributions: We train a deep
neural network to perform physics-informed downsampling of the terrain map: we
optimize the coarse grid representation of the terrain maps, so that the flood
prediction will match the fine grid solution. For the learning process to
succeed, we configure a dataset specifically for this task. We demonstrate that
with this method, it is possible to achieve a significant reduction in
computational cost, while maintaining an accurate solution. A reference
implementation accompanies the paper as well as documentation and code for
dataset reproduction.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07220" title="Abstract">arXiv:2106.07220</a> [<a href="/pdf/2106.07220" title="Download PDF">pdf</a>, <a href="/format/2106.07220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-Aware Image Inpainting with Learned Semantic Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wendong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Junwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tai%2C+Y">Ying Tai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+W">Wenqing Chu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+B">Bingbing Ni</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IJCAI 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in image inpainting have shown impressive results for
generating plausible visual details on rather simple backgrounds. However, for
complex scenes, it is still challenging to restore reasonable contents as the
contextual information within the missing regions tends to be ambiguous. To
tackle this problem, we introduce pretext tasks that are semantically
meaningful to estimating the missing contents. In particular, we perform
knowledge distillation on pretext models and adapt the features to image
inpainting. The learned semantic priors ought to be partially invariant between
the high-level pretext task and low-level image inpainting, which not only help
to understand the global context but also provide structural guidance for the
restoration of local textures. Based on the semantic priors, we further propose
a context-aware image inpainting model, which adaptively integrates global
semantics and local features in a unified image generator. The semantic learner
and the image generator are trained in an end-to-end manner. We name the model
SPL to highlight its ability to learn and leverage semantic priors. It achieves
the state of the art on Places2, CelebA, and Paris StreetView datasets.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07221" title="Abstract">arXiv:2106.07221</a> [<a href="/pdf/2106.07221" title="Download PDF">pdf</a>, <a href="/format/2106.07221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certification of embedded systems based on Machine Learning: A survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidot%2C+G">Guillaume Vidot</a> (IRIT-ARGOS), 
<a href="/search/cs?searchtype=author&query=Gabreau%2C+C">Christophe Gabreau</a>, 
<a href="/search/cs?searchtype=author&query=Ober%2C+I">Ileana Ober</a> (IRIT-ARGOS), 
<a href="/search/cs?searchtype=author&query=Ober%2C+I">Iulian Ober</a> (IRIT-ARGOS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Advances in machine learning (ML) open the way to innovating functions in the
avionic domain, such as navigation/surveillance assistance (e.g. vision-based
navigation, obstacle sensing, virtual sensing), speechto-text applications,
autonomous flight, predictive maintenance or cockpit assistance. Current
certification standards and practices, which were defined and refined decades
over decades with classical programming in mind, do not however support this
new development paradigm. This article provides an overview of the main
challenges raised by the use ML in the demonstration of compliance with
regulation requirements, and a survey of literature relevant to these
challenges, with particular focus on the issues of robustness and
explainability of ML results.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07224" title="Abstract">arXiv:2106.07224</a> [<a href="/pdf/2106.07224" title="Download PDF">pdf</a>, <a href="/format/2106.07224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SGE net: Video object detection with squeezed GRU and information  entropy map
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+R">Rui Su</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haoyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaowei Song</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinglu Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICIP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, deep learning based video object detection has attracted more and
more attention. Compared with object detection of static images, video object
detection is more challenging due to the motion of objects, while providing
rich temporal information. The RNN-based algorithm is an effective way to
enhance detection performance in videos with temporal information. However,
most studies in this area only focus on accuracy while ignoring the calculation
cost and the number of parameters.
<br />In this paper, we propose an efficient method that combines channel-reduced
convolutional GRU (Squeezed GRU), and Information Entropy map for video object
detection (SGE-Net). The experimental results validate the accuracy
improvement, computational savings of the Squeezed GRU, and superiority of the
information entropy attention mechanism on the classification performance. The
mAP has increased by 3.7 contrasted with the baseline, and the number of
parameters has decreased from 6.33 million to 0.67 million compared with the
standard GRU.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07225" title="Abstract">arXiv:2106.07225</a> [<a href="/pdf/2106.07225" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> English to Bangla Machine Translation Using Recurrent Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siddique%2C+S">Shaykh Siddique</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+T">Tahmid Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Talukder%2C+M+R+A">Md. Rifayet Azam Talukder</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M+M">Md. Mohsin Uddin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures, Published with International Journal of Future Computer and Communication (IJFCC)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Siddique, Shaykh, et al. "English to Bangla Machine Translation
  Using Recurrent Neural Network." International Journal of Future Computer and
  Communication 9.2 (2020)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The applications of recurrent neural networks in machine translation are
increasing in natural language processing. Besides other languages, Bangla
language contains a large amount of vocabulary. Improvement of English to
Bangla machine translation would be a significant contribution to Bangla
Language processing. This paper describes an architecture of English to Bangla
machine translation system. The system has been implemented with the
encoder-decoder recurrent neural network. The model uses a knowledge-based
context vector for the mapping of English and Bangla words. Performances of the
model based on activation functions are measured here. The best performance is
achieved for the linear activation function in encoder layer and the tanh
activation function in decoder layer. From the execution of GRU and LSTM layer,
GRU performed better than LSTM. The attention layers are enacted with softmax
and sigmoid activation function. The approach of the model outperforms the
previous state-of-the-art systems in terms of cross-entropy loss metrics. The
reader can easily find out the structure of the machine translation of English
to Bangla and the efficient activation functions from the paper.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07226" title="Abstract">arXiv:2106.07226</a> [<a href="/pdf/2106.07226" title="Download PDF">pdf</a>, <a href="/format/2106.07226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More Real than Real: A Study on Human Visual Perception of Synthetic  Faces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lago%2C+F">Federica Lago</a>, 
<a href="/search/cs?searchtype=author&query=Pasquini%2C+C">Cecilia Pasquini</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hme%2C+R">Rainer B&#xf6;hme</a>, 
<a href="/search/cs?searchtype=author&query=Dumont%2C+H">H&#xe9;l&#xe8;ne Dumont</a>, 
<a href="/search/cs?searchtype=author&query=Goffaux%2C+V">Val&#xe9;rie Goffaux</a>, 
<a href="/search/cs?searchtype=author&query=Boato%2C+G">Giulia Boato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Deep fakes became extremely popular in the last years, also thanks to their
increasing realism. Therefore, there is the need to measures human's ability to
distinguish between real and synthetic face images when confronted with
cutting-edge creation technologies. We describe the design and results of a
perceptual experiment we have conducted, where a wide and diverse group of
volunteers has been exposed to synthetic face images produced by
state-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN,
StyleGAN2). The experiment outcomes reveal how strongly we should call into
question our human ability to discriminate real faces from synthetic ones
generated through modern AI.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07228" title="Abstract">arXiv:2106.07228</a> [<a href="/pdf/2106.07228" title="Download PDF">pdf</a>, <a href="/format/2106.07228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Parking Space Detection Using Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nyambal%2C+J">Julien Nyambal</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+R">Richard Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Finding a parking space nowadays becomes an issue that is not to be
neglected, it consumes time and energy. We have used computer vision techniques
to infer the state of the parking lot given the data collected from the
University of The Witwatersrand. This paper presents an approach for a
real-time parking space classification based on Convolutional Neural Networks
(CNN) using Caffe and Nvidia DiGITS framework. The training process has been
done using DiGITS and the output is a caffemodel used for predictions to detect
vacant and occupied parking spots. The system checks a defined area whether a
parking spot (bounding boxes defined at initialization of the system) is
containing a car or not (occupied or vacant). Those bounding box coordinates
are saved from a frame of the video of the parking lot in a JSON format, to be
later used by the system for sequential prediction on each parking spot. The
system has been trained using the LeNet network with the Nesterov Accelerated
Gradient as solver and the AlexNet network with the Stochastic Gradient Descent
as solver. We were able to get an accuracy on the validation set of 99\% for
both networks. The accuracy on a foreign dataset(PKLot) returned as well 99\%.
Those are experimental results based on the training set shows how robust the
system can be when the prediction has to take place in a different parking
space.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07229" title="Abstract">arXiv:2106.07229</a> [<a href="/pdf/2106.07229" title="Download PDF">pdf</a>, <a href="/format/2106.07229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Machine Learning with Fully Homomorphic Encryption  for Deep Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joon-Woo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">HyungChul Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yongwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+W">Woosuk Choi</a>, 
<a href="/search/cs?searchtype=author&query=Eom%2C+J">Jieun Eom</a>, 
<a href="/search/cs?searchtype=author&query=Deryabin%2C+M">Maxim Deryabin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E">Eunsang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+D">Donghoon Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Sik Kim</a>, 
<a href="/search/cs?searchtype=author&query=No%2C+J">Jong-Seon No</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fully homomorphic encryption (FHE) is one of the prospective tools for
privacypreserving machine learning (PPML), and several PPML models have been
proposed based on various FHE schemes and approaches. Although the FHE schemes
are known as suitable tools to implement PPML models, previous PPML models on
FHE encrypted data are limited to only simple and non-standard types of machine
learning models. These non-standard machine learning models are not proven
efficient and accurate with more practical and advanced datasets. Previous PPML
schemes replace non-arithmetic activation functions with simple arithmetic
functions instead of adopting approximation methods and do not use
bootstrapping, which enables continuous homomorphic evaluations. Thus, they
could not use standard activation functions and could not employ a large number
of layers. The maximum classification accuracy of the existing PPML model with
the FHE for the CIFAR-10 dataset was only 77% until now. In this work, we
firstly implement the standard ResNet-20 model with the RNS-CKKS FHE with
bootstrapping and verify the implemented model with the CIFAR-10 dataset and
the plaintext model parameters. Instead of replacing the non-arithmetic
functions with the simple arithmetic function, we use state-of-the-art
approximation methods to evaluate these non-arithmetic functions, such as the
ReLU, with sufficient precision [1]. Further, for the first time, we use the
bootstrapping technique of the RNS-CKKS scheme in the proposed model, which
enables us to evaluate a deep learning model on the encrypted data. We
numerically verify that the proposed model with the CIFAR-10 dataset shows
98.67% identical results to the original ResNet-20 model with non-encrypted
data. The classification accuracy of the proposed model is 90.67%, which is
pretty close to that of the original ResNet-20 CNN model...
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07233" title="Abstract">arXiv:2106.07233</a> [<a href="/pdf/2106.07233" title="Download PDF">pdf</a>, <a href="/format/2106.07233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimality Notions via Factorization Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wi%C3%9Fmann%2C+T">Thorsten Wi&#xdf;mann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Discrete Mathematics (cs.DM); Category Theory (math.CT)

</div>
<p class="mathjax">For the minimization of state-based systems (i.e. the reduction of the number
of states while retaining the system's semantics), there are two obvious
aspects: removing unnecessary states of the system and merging redundant states
in the system.
<br />In the present article, we relate the two aspects on coalgebras by defining
an abstract notion of minimality using factorization systems. We will find
criteria on the category that ensure uniqueness, existence, and functoriality
of the minimization aspects, where the proofs instantiate to those for
reachability and bisimilarity minimization in the standard coalgebra
literature. Finally, we will see how the two aspects of minimization interact
and under which criteria they can be sequenced in any order, like in automata
minimization.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07234" title="Abstract">arXiv:2106.07234</a> [<a href="/pdf/2106.07234" title="Download PDF">pdf</a>, <a href="/format/2106.07234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-World Evaluation of the Impact of Automated Driving System  Technology on Driver Gaze Behavior, Reaction Time and Trust
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morales-Alvarez%2C+W">Walter Morales-Alvarez</a>, 
<a href="/search/cs?searchtype=author&query=Marouf%2C+M">Mohamed Marouf</a>, 
<a href="/search/cs?searchtype=author&query=Tadjine%2C+H+H">Hadj. Hamma Tadjine</a>, 
<a href="/search/cs?searchtype=author&query=Olaverri-Monreal%2C+C">Cristina Olaverri-Monreal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted and accepted in the IEEE Intelligent Vehicle Symposium 2021 Conference (IV2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Recent developments in advanced driving assistance systems (ADAS) that rely
on some level of autonomy have led the automobile industry and research
community to investigate the impact they might have on driving performance.
However, most of the research performed so far is based on simulated
environments. In this study, we investigated the behavior of drivers in a
vehicle with automated driving system (ADS) capabilities in a real-life driving
scenario. We analyzed their response to a take over request (TOR) at two
different driving speeds while being engaged in non-driving-related tasks
(NDRT). Results from the performed experiments showed that driver reaction time
to a TOR, gaze behavior and self-reported trust in automation were affected by
the type of NDRT being concurrently performed and driver reaction time and gaze
behavior additionally depended on the driving or vehicle speed at the time of
TOR.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07237" title="Abstract">arXiv:2106.07237</a> [<a href="/pdf/2106.07237" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Einstein more agreeable and less neurotic than Hitler? A  computational exploration of the emotional and personality profiles of  historical persons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+A+M">Arthur M. Jacobs</a>, 
<a href="/search/cs?searchtype=author&query=Kinder%2C+A">Annette Kinder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent progress in distributed semantic models (DSM) offers new ways to
estimate personality traits of both fictive and real people. In this
exploratory study we applied an extended version of the algorithm developed in
Jacobs (2019) to compute the likeability scores, emotional figure profiles and
BIG5 personality traits for 100 historical persons from the arts, politics or
science domains whose names are rather unique (e.g., Einstein, Kahlo, Picasso).
We compared the results produced by static (word2vec) and dynamic (BERT)
language model representations in four studies. The results show both the
potential and limitations of such DSM-based computations of personality
profiles and point ways to further develop this approach to become a useful
tool in data science, psychology or computational and neurocognitive poetics
(Jacobs, 2015).
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07239" title="Abstract">arXiv:2106.07239</a> [<a href="/pdf/2106.07239" title="Download PDF">pdf</a>, <a href="/format/2106.07239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Clustering Under a Bounded Cost
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esmaeili%2C+S+A">Seyed A. Esmaeili</a>, 
<a href="/search/cs?searchtype=author&query=Brubach%2C+B">Brian Brubach</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+A">Aravind Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Dickerson%2C+J+P">John P. Dickerson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Clustering is a fundamental unsupervised learning problem where a dataset is
partitioned into clusters that consist of nearby points in a metric space. A
recent variant, fair clustering, associates a color with each point
representing its group membership and requires that each color has
(approximately) equal representation in each cluster to satisfy group fairness.
In this model, the cost of the clustering objective increases due to enforcing
fairness in the algorithm. The relative increase in the cost, the ''price of
fairness,'' can indeed be unbounded. Therefore, in this paper we propose to
treat an upper bound on the clustering objective as a constraint on the
clustering problem, and to maximize equality of representation subject to it.
We consider two fairness objectives: the group utilitarian objective and the
group egalitarian objective, as well as the group leximin objective which
generalizes the group egalitarian objective. We derive fundamental lower bounds
on the approximation of the utilitarian and egalitarian objectives and
introduce algorithms with provable guarantees for them. For the leximin
objective we introduce an effective heuristic algorithm. We further derive
impossibility results for other natural fairness objectives. We conclude with
experimental results on real-world datasets that demonstrate the validity of
our algorithms.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07240" title="Abstract">arXiv:2106.07240</a> [<a href="/pdf/2106.07240" title="Download PDF">pdf</a>, <a href="/ps/2106.07240" title="Download PostScript">ps</a>, <a href="/format/2106.07240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Biases in Toxic Language Detection through Invariant  Rationalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yung-Sung Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mingye Gao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Hongyin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Glass%2C+J">James Glass</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yun-Nung Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shang-Wen Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 5th Workshop on Online Abuse and Harms at ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic detection of toxic language plays an essential role in protecting
social media users, especially minority groups, from verbal abuse. However,
biases toward some attributes, including gender, race, and dialect, exist in
most training datasets for toxicity detection. The biases make the learned
models unfair and can even exacerbate the marginalization of people.
Considering that current debiasing methods for general natural language
understanding tasks cannot effectively mitigate the biases in the toxicity
detectors, we propose to use invariant rationalization (InvRat), a
game-theoretic framework consisting of a rationale generator and a predictor,
to rule out the spurious correlation of certain syntactic patterns (e.g.,
identity mentions, dialect) to toxicity labels. We empirically show that our
method yields lower false positive rate in both lexical and dialectal
attributes than previous debiasing methods.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07241" title="Abstract">arXiv:2106.07241</a> [<a href="/pdf/2106.07241" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contemporary Amharic Corpus: Automatically Morpho-Syntactically Tagged  Amharic Corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gezmu%2C+A+M">Andargachew Mekonnen Gezmu</a>, 
<a href="/search/cs?searchtype=author&query=Seyoum%2C+B+E">Binyam Ephrem Seyoum</a>, 
<a href="/search/cs?searchtype=author&query=Gasser%2C+M">Michael Gasser</a>, 
<a href="/search/cs?searchtype=author&query=N%C3%BCrnberger%2C+A">Andreas N&#xfc;rnberger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing at COLING 2018
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the First Workshop on Linguistic Resources for
  Natural Language Processing, pp. 65-70. 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduced the contemporary Amharic corpus, which is automatically tagged
for morpho-syntactic information. Texts are collected from 25,199 documents
from different domains and about 24 million orthographic words are tokenized.
Since it is partly a web corpus, we made some automatic spelling error
correction. We have also modified the existing morphological analyzer,
HornMorpho, to use it for the automatic tagging.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07242" title="Abstract">arXiv:2106.07242</a> [<a href="/pdf/2106.07242" title="Download PDF">pdf</a>, <a href="/format/2106.07242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The asymmetric particle population density method for simulation of  coupled noisy oscillators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+N">Ningyuan Wang</a>, 
<a href="/search/math?searchtype=author&query=Forger%2C+D+B">Daniel B Forger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A wide variety of biological phenomena can be modeled by the collective
activity of a population of individual units. A common strategy for simulating
such a system, the population density approach, is to take the macroscopic
limit and update its population density function. However, in many cases, the
coupling between the units and noise gives rise to complex behaviors
challenging to existing population density approach methods. To address these
challenges, we develop the asymmetric particle population density (APPD) method
that efficiently and accurately simulates such populations consist of coupled
elements. The APPD is well-suited for a parallel implementation. We compare the
performance of the method against direct Monte-Carlo simulation and verify its
accuracy by applying it to the well-studied Hodgkin-Huxley model, with a range
of challenging scenarios. We find that our method can accurately reproduce
complex macroscopic behaviors such as inhibitory coupling-induced clustering
and noise-induced firing while being faster than the direct simulation.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07247" title="Abstract">arXiv:2106.07247</a> [<a href="/pdf/2106.07247" title="Download PDF">pdf</a>, <a href="/format/2106.07247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age of Information for Multiple-Source Multiple-Server Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javani%2C+A">Alireza Javani</a>, 
<a href="/search/cs?searchtype=author&query=Zorgui%2C+M">Marwen Zorgui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiying Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Having timely and fresh knowledge about the current state of information
sources is critical in a variety of applications. In particular, a status
update may arrive at the destination later than its generation time due to
processing and communication delays. The freshness of the status update at the
destination is captured by the notion of age of information. In this study, we
analyze a multiple sensing network with multiple sources, multiple servers, and
a monitor (destination). Each source corresponds to an independent piece of
information and its age is measured individually. Given a particular source,
the servers independently sense the source of information and send the status
update to the monitor. We assume that updates arrive at the servers according
to Poisson random processes. Each server sends its updates to the monitor
through a direct link, which is modeled as a queue. The service time to
transmit an update is considered to be an exponential random variable. We
examine both homogeneous and heterogeneous service and arrival rates for the
single-source case, and only homogeneous arrival and service rates for the
multiple-source case. We derive a closed-form expression for the average age of
information under a last-come-first-serve (LCFS) queue for a single source and
arbitrary number of homogeneous servers. Using a recursive method, we derive
the explicit average age of information for any number of sources and
homogeneous servers. We also investigate heterogeneous servers and a single
source, and present algorithms for finding the average age of information.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07249" title="Abstract">arXiv:2106.07249</a> [<a href="/pdf/2106.07249" title="Download PDF">pdf</a>, <a href="/format/2106.07249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic winning shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peltom%C3%A4ki%2C+J">Jarkko Peltom&#xe4;ki</a>, 
<a href="/search/cs?searchtype=author&query=Salo%2C+V">Ville Salo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">To each one-dimensional subshift $X$, we may associate a winning shift $W(X)$
which arises from a combinatorial game played on the language of $X$.
Previously it has been studied what properties of $X$ does $W(X)$ inherit. For
example, $X$ and $W(X)$ have the same factor complexity and if $X$ is a sofic
subshift, then $W(X)$ is also sofic. In this paper, we develop a notion of
automaticity for $W(X)$, that is, we propose what it means that a vector
representation of $W(X)$ is accepted by a finite automaton.
<br />Let $S$ be an abstract numeration system such that addition with respect to
$S$ is a rational relation. Let $X$ be a subshift generated by an $S$-automatic
word. We prove that as long as there is a bound on the number of nonzero
symbols in configurations of $W(X)$ (which follows from $X$ having sublinear
factor complexity), then $W(X)$ is accepted by a finite automaton, which can be
effectively constructed from the description of $X$. We provide an explicit
automaton when $X$ is generated by certain automatic words such as the
Thue-Morse word.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07250" title="Abstract">arXiv:2106.07250</a> [<a href="/pdf/2106.07250" title="Download PDF">pdf</a>, <a href="/ps/2106.07250" title="Download PostScript">ps</a>, <a href="/format/2106.07250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Interpretation of Softmax Cross-Entropy and Negative Sampling:  With Case Study for Knowledge Graph Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamigaito%2C+H">Hidetaka Kamigaito</a>, 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+K">Katsuhiko Hayashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACL-IJCNLP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">In knowledge graph embedding, the theoretical relationship between the
softmax cross-entropy and negative sampling loss functions has not been
investigated. This makes it difficult to fairly compare the results of the two
different loss functions. We attempted to solve this problem by using the
Bregman divergence to provide a unified interpretation of the softmax
cross-entropy and negative sampling loss functions. Under this interpretation,
we can derive theoretical findings for fair comparison. Experimental results on
the FB15k-237 and WN18RR datasets show that the theoretical findings are valid
in practical settings.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07252" title="Abstract">arXiv:2106.07252</a> [<a href="/pdf/2106.07252" title="Download PDF">pdf</a>, <a href="/ps/2106.07252" title="Download PostScript">ps</a>, <a href="/format/2106.07252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Robust Clustering Over Time for Temporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">In many clustering scenes, data samples' attribute values change over time.
For such data, we are often interested in obtaining a partition for each time
step and tracking the dynamic change of partitions. Normally, a smooth change
is assumed for data to have a temporal smooth nature. Existing algorithms
consider the temporal smoothness as an a priori preference and bias the search
towards the preferred direction. This a priori manner leads to a risk of
converging to an unexpected region because it is not always the case that a
reasonable preference can be elicited given the little prior knowledge about
the data. To address this issue, this paper proposes a new clustering framework
called evolutionary robust clustering over time. One significant innovation of
the proposed framework is processing the temporal smoothness in an a posteriori
manner, which avoids unexpected convergence that occurs in existing algorithms.
Furthermore, the proposed framework automatically tunes the weight of
smoothness without data's affinity matrix and predefined parameters, which
holds better applicability and scalability. The effectiveness and efficiency of
the proposed framework are confirmed by comparing with state-of-the-art
algorithms on both synthetic and real datasets.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07255" title="Abstract">arXiv:2106.07255</a> [<a href="/pdf/2106.07255" title="Download PDF">pdf</a>, <a href="/format/2106.07255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Myopic Community Detection with One-shot Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ke%2C+C">Chuyang Ke</a>, 
<a href="/search/cs?searchtype=author&query=Honorio%2C+J">Jean Honorio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we study the problem of recovering the community structure of
a network under federated myopic learning. Under this paradigm, we have several
clients, each of them having a myopic view, i.e., observing a small subgraph of
the network. Each client sends a censored evidence graph to a central server.
We provide an efficient algorithm, which computes a consensus signed weighted
graph from clients evidence, and recovers the underlying network structure in
the central server. We analyze the topological structure conditions of the
network, as well as the signal and noise levels of the clients that allow for
recovery of the network structure. Our analysis shows that exact recovery is
possible and can be achieved in polynomial time. We also provide
information-theoretic limits for the central server to recover the network
structure from any single client evidence. Finally, as a byproduct of our
analysis, we provide a novel Cheeger-type inequality for general signed
weighted graphs.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07256" title="Abstract">arXiv:2106.07256</a> [<a href="/pdf/2106.07256" title="Download PDF">pdf</a>, <a href="/format/2106.07256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Guided LiDAR Depth Map Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krauss%2C+B">Bryan Krauss</a>, 
<a href="/search/cs?searchtype=author&query=Schroeder%2C+G">Gregory Schroeder</a>, 
<a href="/search/cs?searchtype=author&query=Gustke%2C+M">Marko Gustke</a>, 
<a href="/search/cs?searchtype=author&query=Hussein%2C+A">Ahmed Hussein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2021 IEEE Intelligent Vehicles Symposium (IV21). This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Accurate dense depth estimation is crucial for autonomous vehicles to analyze
their environment. This paper presents a non-deep learning-based approach to
densify a sparse LiDAR-based depth map using a guidance RGB image. To achieve
this goal the RGB image is at first cleared from most of the camera-LiDAR
misalignment artifacts. Afterward, it is over segmented and a plane for each
superpixel is approximated. In the case a superpixel is not well represented by
a plane, a plane is approximated for a convex hull of the most inlier. Finally,
the pinhole camera model is used for the interpolation process and the
remaining areas are interpolated. The evaluation of this work is executed using
the KITTI depth completion benchmark, which validates the proposed work and
shows that it outperforms the state-of-the-art non-deep learning-based methods,
in addition to several deep learning-based methods.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07257" title="Abstract">arXiv:2106.07257</a> [<a href="/pdf/2106.07257" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication is the universal solvent: atreya bot -- an interactive bot  for chemical scientists
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+M">Mahak Sharma</a> (1), 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+A">Abhishek Kaushik</a> (2), 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Rajesh Kumar</a> (3), 
<a href="/search/cs?searchtype=author&query=Rai%2C+S+K">Sushant Kumar Rai</a> (3), 
<a href="/search/cs?searchtype=author&query=Desai%2C+H+H">Harshada Hanumant Desai</a> (3), 
<a href="/search/cs?searchtype=author&query=Yadav%2C+S">Sargam Yadav</a> (3) ((1) Vidhya Bhawan Gandhiyan Institute of Educational Studies,(2) Dublin City University, Ireland,(3) Dublin Business School, Dublin, Ireland)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IFIP 9.4 2021 1st Virtual Conference Conference Theme: Resilient ICT4D May 25th 28th, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Conversational agents are a recent trend in human-computer interaction,
deployed in multidisciplinary applications to assist the users. In this paper,
we introduce "Atreya", an interactive bot for chemistry enthusiasts,
researchers, and students to study the ChEMBL database. Atreya is hosted by
Telegram, a popular cloud-based instant messaging application. This
user-friendly bot queries the ChEMBL database, retrieves the drug details for a
particular disease, targets associated with that drug, etc. This paper explores
the potential of using a conversational agent to assist chemistry students and
chemical scientist in complex information seeking process.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07258" title="Abstract">arXiv:2106.07258</a> [<a href="/pdf/2106.07258" title="Download PDF">pdf</a>, <a href="/format/2106.07258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GitTables: A Large-Scale Corpus of Relational Tables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hulsebos%2C+M">Madelon Hulsebos</a>, 
<a href="/search/cs?searchtype=author&query=Demiralp%2C+%C3%87">&#xc7;a&#x11f;atay Demiralp</a>, 
<a href="/search/cs?searchtype=author&query=Groth%2C+P">Paul Groth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The practical success of deep learning has sparked interest in improving
relational table tasks, like data search, with models trained on large table
corpora. Existing corpora primarily contain tables extracted from HTML pages,
limiting the capability to represent offline database tables. To train and
evaluate high-capacity models for applications beyond the Web, we need
additional resources with tables that resemble relational database tables.
<br />Here we introduce GitTables, a corpus of currently 1.7M relational tables
extracted from GitHub. Our continuing curation aims at growing the corpus to at
least 20M tables. We annotate table columns in GitTables with more than 2K
different semantic types from Schema.org and DBpedia. Our column annotations
consist of semantic types, hierarchical relations, range types and
descriptions.
<br />The corpus is available at https://gittables.github.io. Our analysis of
GitTables shows that its structure, content, and topical coverage differ
significantly from existing table corpora. We evaluate our annotation pipeline
on hand-labeled tables from the T2Dv2 benchmark and find that our approach
provides results on par with human annotations. We demonstrate a use case of
GitTables by training a semantic type detection model on it and obtain high
prediction accuracy. We also show that the same model trained on tables from
theWeb generalizes poorly.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07260" title="Abstract">arXiv:2106.07260</a> [<a href="/pdf/2106.07260" title="Download PDF">pdf</a>, <a href="/format/2106.07260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAPTOR: End-to-end Risk-Aware MDP Planning and Policy Learning by  Backpropagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patton%2C+N">Noah Patton</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jihwan Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Gimelfarb%2C+M">Michael Gimelfarb</a>, 
<a href="/search/cs?searchtype=author&query=Sanner%2C+S">Scott Sanner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Planning provides a framework for optimizing sequential decisions in complex
environments. Recent advances in efficient planning in deterministic or
stochastic high-dimensional domains with continuous action spaces leverage
backpropagation through a model of the environment to directly optimize
actions. However, existing methods typically not take risk into account when
optimizing in stochastic domains, which can be incorporated efficiently in MDPs
by optimizing the entropic utility of returns. We bridge this gap by
introducing Risk-Aware Planning using PyTorch (RAPTOR), a novel framework for
risk-sensitive planning through end-to-end optimization of the entropic utility
objective. A key technical difficulty of our approach lies in that direct
optimization of the entropic utility by backpropagation is impossible due to
the presence of environment stochasticity. The novelty of RAPTOR lies in the
reparameterization of the state distribution, which makes it possible to apply
stochastic backpropagatation through sufficient statistics of the entropic
utility computed from forward-sampled trajectories. The direct optimization of
this empirical objective in an end-to-end manner is called the risk-averse
straight-line plan, which commits to a sequence of actions in advance and can
be sub-optimal in highly stochastic domains. We address this shortcoming by
optimizing for risk-aware Deep Reactive Policies (RaDRP) in our framework. We
evaluate and compare these two forms of RAPTOR on three highly stochastic
do-mains, including nonlinear navigation, HVAC control, and linear reservoir
control, demonstrating the ability to manage risk in complex MDPs.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07268" title="Abstract">arXiv:2106.07268</a> [<a href="/pdf/2106.07268" title="Download PDF">pdf</a>, <a href="/format/2106.07268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FastICARL: Fast Incremental Classifier and Representation Learning with  Efficient Budget Allocation in Audio Sensing Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+Y+D">Young D. Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+J">Jagmohan Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Mascolo%2C+C">Cecilia Mascolo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at INTERSPEECH 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Various incremental learning (IL) approaches have been proposed to help deep
learning models learn new tasks/classes continuously without forgetting what
was learned previously (i.e., avoid catastrophic forgetting). With the growing
number of deployed audio sensing applications that need to dynamically
incorporate new tasks and changing input distribution from users, the ability
of IL on-device becomes essential for both efficiency and user privacy.
<br />However, prior works suffer from high computational costs and storage demands
which hinders the deployment of IL on-device. In this work, to overcome these
limitations, we develop an end-to-end and on-device IL framework, FastICARL,
that incorporates an exemplar-based IL and quantization in the context of
audio-based applications. We first employ k-nearest-neighbor to reduce the
latency of IL. Then, we jointly utilize a quantization technique to decrease
the storage requirements of IL. We implement FastICARL on two types of mobile
devices and demonstrate that FastICARL remarkably decreases the IL time up to
78-92% and the storage requirements by 2-4 times without sacrificing its
performance. FastICARL enables complete on-device IL, ensuring user privacy as
the user data does not need to leave the device.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07270" title="Abstract">arXiv:2106.07270</a> [<a href="/pdf/2106.07270" title="Download PDF">pdf</a>, <a href="/format/2106.07270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Industry 4.0 and Prospects of Circular Economy: A Survey of Robotic  Assembly and Disassembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daneshmand%2C+M">Morteza Daneshmand</a>, 
<a href="/search/cs?searchtype=author&query=Noroozi%2C+F">Fatemeh Noroozi</a>, 
<a href="/search/cs?searchtype=author&query=Corneanu%2C+C">Ciprian Corneanu</a>, 
<a href="/search/cs?searchtype=author&query=Mafakheri%2C+F">Fereshteh Mafakheri</a>, 
<a href="/search/cs?searchtype=author&query=Fiorini%2C+P">Paolo Fiorini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Despite their contributions to the financial efficiency and environmental
sustainability of industrial processes, robotic assembly and disassembly have
been understudied in the existing literature. This is in contradiction to their
importance in realizing the Fourth Industrial Revolution. More specifically,
although most of the literature has extensively discussed how to optimally
assemble or disassemble given products, the role of other factors has been
overlooked. For example, the types of robots involved in implementing the
sequence plans, which should ideally be taken into account throughout the whole
chain consisting of design, assembly, disassembly and reassembly. Isolating the
foregoing operations from the rest of the components of the relevant ecosystems
may lead to erroneous inferences toward both the necessity and efficiency of
the underlying procedures. In this paper we try to alleviate these shortcomings
by comprehensively investigating the state-of-the-art in robotic assembly and
disassembly. We consider and review various aspects of manufacturing and
remanufacturing frameworks while particularly focusing on their desirability
for supporting a circular economy.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07271" title="Abstract">arXiv:2106.07271</a> [<a href="/pdf/2106.07271" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optical Fault Injection Attacks against Radiation-Hard Registers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petryk%2C+D">Dmytro Petryk</a>, 
<a href="/search/cs?searchtype=author&query=Dyka%2C+Z">Zoya Dyka</a>, 
<a href="/search/cs?searchtype=author&query=Sorge%2C+R">Roland Sorge</a>, 
<a href="/search/cs?searchtype=author&query=Schaeffner%2C+J">Jan Schaeffner</a>, 
<a href="/search/cs?searchtype=author&query=Langendoerfer%2C+P">Peter Langendoerfer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">If devices are physically accessible optical fault injection attacks pose a
great threat since the data processed as well as the operation flow can be
manipulated. Successful physical attacks may lead not only to leakage of secret
information such as cryptographic private keys, but can also cause economic
damage especially if as a result of such a manipulation a critical
infrastructure is successfully attacked. Laser based attacks exploit the
sensitivity of CMOS technologies to electromagnetic radiation in the visible or
the infrared spectrum. It can be expected that radiation-hard designs,
specially crafted for space applications, are more robust not only against
high-energy particles and short electromagnetic waves but also against optical
fault injection attacks. In this work we investigated the sensitivity of
radiation-hard JICG shift registers to optical fault injection attacks. In our
experiments, we were able to repeatable trigger bit-set and bit-reset
operations changing the data stored in single JICG flip-flops despite their
high-radiation fault tolerance.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07273" title="Abstract">arXiv:2106.07273</a> [<a href="/pdf/2106.07273" title="Download PDF">pdf</a>, <a href="/format/2106.07273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible dual-branched message passing neural network for quantum  mechanical property prediction with molecular conformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jo%2C+J">Jeonghee Jo</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+B">Bumju Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byunghan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sungroh Yoon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">A molecule is a complex of heterogeneous components, and the spatial
arrangements of these components determine the whole molecular properties and
characteristics. With the advent of deep learning in computational chemistry,
several studies have focused on how to predict molecular properties based on
molecular configurations. Message passing neural network provides an effective
framework for capturing molecular geometric features with the perspective of a
molecule as a graph. However, most of these studies assumed that all
heterogeneous molecular features, such as atomic charge, bond length, or other
geometric features always contribute equivalently to the target prediction,
regardless of the task type. In this study, we propose a dual-branched neural
network for molecular property prediction based on message-passing framework.
Our model learns heterogeneous molecular features with different scales, which
are trained flexibly according to each prediction target. In addition, we
introduce a discrete branch to learn single atom features without local
aggregation, apart from message-passing steps. We verify that this novel
structure can improve the model performance with faster convergence in most
targets. The proposed model outperforms other recent models with sparser
representations. Our experimental results indicate that in the chemical
property prediction tasks, the diverse chemical nature of targets should be
carefully considered for both model performance and generalizability.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07275" title="Abstract">arXiv:2106.07275</a> [<a href="/pdf/2106.07275" title="Download PDF">pdf</a>, <a href="/format/2106.07275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascaded Span Extraction and Response Generation for Document-Grounded  Dialog
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daheim%2C+N">Nico Daheim</a>, 
<a href="/search/cs?searchtype=author&query=Thulke%2C+D">David Thulke</a>, 
<a href="/search/cs?searchtype=author&query=Dugast%2C+C">Christian Dugast</a>, 
<a href="/search/cs?searchtype=author&query=Ney%2C+H">Hermann Ney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 1st DialDoc Workshop at ACL-IJCNLP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper summarizes our entries to both subtasks of the first DialDoc
shared task which focuses on the agent response prediction task in
goal-oriented document-grounded dialogs. The task is split into two subtasks:
predicting a span in a document that grounds an agent turn and generating an
agent response based on a dialog and grounding document. In the first subtask,
we restrict the set of valid spans to the ones defined in the dataset, use a
biaffine classifier to model spans, and finally use an ensemble of different
models. For the second subtask, we use a cascaded model which grounds the
response prediction on the predicted span instead of the full document. With
these approaches, we obtain significant improvements in both subtasks compared
to the baseline.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07277" title="Abstract">arXiv:2106.07277</a> [<a href="/pdf/2106.07277" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ontological Entities for Planning and Describing Cultural Heritage 3D  Models Creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amico%2C+N">Nicola Amico</a>, 
<a href="/search/cs?searchtype=author&query=Felicetti%2C+A">Achille Felicetti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACM Journal on Computing and Cultural Heritage (JOCCH) 2021. 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">In the last decades the rapid development of technologies and methodologies
in the field of digitization and 3D modelling has led to an increasing
proliferation of 3D technologies in the Cultural Heritage domain. Despite the
great potential of 3D digital heritage, the "special effects" of 3D may often
overwhelm its importance in research. Projects and consortia of scholars have
tried to put order in the different fields of application of these
technologies, providing guidelines and proposing workflows. The use of computer
graphics as an effective methodology for CH research and communication
highlighted the need of transparent provenance data to properly document
digital assets and understand the degree of scientific quality and reliability
of their outcomes. The building and release of provenance knowledge, consisting
in the complete formal documentation of each phase of the process, is therefore
of fundamental importance to ensure its repeatability and to guarantee the
integration and interoperability of the generated metadata on the Semantic Web.
This paper proposes a methodology for documenting the planning and creation of
3D models used in archaeology and Cultural Heritage, by means of an application
profile based on the CIDOC CRM ecosystem and other international standards.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07278" title="Abstract">arXiv:2106.07278</a> [<a href="/pdf/2106.07278" title="Download PDF">pdf</a>, <a href="/format/2106.07278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Which Mutual-Information Representation Learning Objectives are  Sufficient for Control?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rakelly%2C+K">Kate Rakelly</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Florensa%2C+C">Carlos Florensa</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mutual information maximization provides an appealing formalism for learning
representations of data. In the context of reinforcement learning (RL), such
representations can accelerate learning by discarding irrelevant and redundant
information, while retaining the information necessary for control. Much of the
prior work on these methods has addressed the practical difficulties of
estimating mutual information from samples of high-dimensional observations,
while comparatively less is understood about which mutual information
objectives yield representations that are sufficient for RL from a theoretical
perspective. In this paper, we formalize the sufficiency of a state
representation for learning and representing the optimal policy, and study
several popular mutual-information based objectives through this lens.
Surprisingly, we find that two of these objectives can yield insufficient
representations given mild and common assumptions on the structure of the MDP.
We corroborate our theoretical results with empirical experiments on a
simulated game environment with visual observations.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07283" title="Abstract">arXiv:2106.07283</a> [<a href="/pdf/2106.07283" title="Download PDF">pdf</a>, <a href="/format/2106.07283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-based Domain Adaptation for Single Stage Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidit">Vidit</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While domain adaptation has been used to improve the performance of object
detectors when the training and test data follow different distributions,
previous work has mostly focused on two-stage detectors. This is because their
use of region proposals makes it possible to perform local adaptation, which
has been shown to significantly improve the adaptation effectiveness. Here, by
contrast, we target single-stage architectures, which are better suited to
resource-constrained detection than two-stage ones but do not provide region
proposals. To nonetheless benefit from the strength of local adaptation, we
introduce an attention mechanism that lets us identify the important regions on
which adaptation should focus. Our approach is generic and can be integrated
into any single-stage detector. We demonstrate this on standard benchmark
datasets by applying it to both SSD and YOLO. Furthermore, for an equivalent
single-stage architecture, our method outperforms the state-of-the-art domain
adaptation technique even though it was designed specifically for this
particular detector.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07285" title="Abstract">arXiv:2106.07285</a> [<a href="/pdf/2106.07285" title="Download PDF">pdf</a>, <a href="/format/2106.07285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing Pre-Trained Language Models for Disease Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alghanmi%2C+I">Israa Alghanmi</a>, 
<a href="/search/cs?searchtype=author&query=Espinosa-Anke%2C+L">Luis Espinosa-Anke</a>, 
<a href="/search/cs?searchtype=author&query=Schockaert%2C+S">Steven Schockaert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACL 2021 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pre-trained language models such as ClinicalBERT have achieved impressive
results on tasks such as medical Natural Language Inference. At first glance,
this may suggest that these models are able to perform medical reasoning tasks,
such as mapping symptoms to diseases. However, we find that standard benchmarks
such as MedNLI contain relatively few examples that require such forms of
reasoning. To better understand the medical reasoning capabilities of existing
language models, in this paper we introduce DisKnE, a new benchmark for Disease
Knowledge Evaluation. To construct this benchmark, we annotated each positive
MedNLI example with the types of medical reasoning that are needed. We then
created negative examples by corrupting these positive examples in an
adversarial way. Furthermore, we define training-test splits per disease,
ensuring that no knowledge about test diseases can be learned from the training
data, and we canonicalize the formulation of the hypotheses to avoid the
presence of artefacts. This leads to a number of binary classification
problems, one for each type of reasoning and each disease. When analysing
pre-trained models for the clinical/biomedical domain on the proposed
benchmark, we find that their performance drops considerably.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07286" title="Abstract">arXiv:2106.07286</a> [<a href="/pdf/2106.07286" title="Download PDF">pdf</a>, <a href="/format/2106.07286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TimeLens: Event-based Video Frame Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Stepan Tulyakov</a>, 
<a href="/search/cs?searchtype=author&query=Gehrig%2C+D">Daniel Gehrig</a>, 
<a href="/search/cs?searchtype=author&query=Georgoulis%2C+S">Stamatios Georgoulis</a>, 
<a href="/search/cs?searchtype=author&query=Erbach%2C+J">Julius Erbach</a>, 
<a href="/search/cs?searchtype=author&query=Gehrig%2C+M">Mathias Gehrig</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanyou Li</a>, 
<a href="/search/cs?searchtype=author&query=Scaramuzza%2C+D">Davide Scaramuzza</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">State-of-the-art frame interpolation methods generate intermediate frames by
inferring object motions in the image from consecutive key-frames. In the
absence of additional information, first-order approximations, i.e. optical
flow, must be used, but this choice restricts the types of motions that can be
modeled, leading to errors in highly dynamic scenarios. Event cameras are novel
sensors that address this limitation by providing auxiliary visual information
in the blind-time between frames. They asynchronously measure per-pixel
brightness changes and do this with high temporal resolution and low latency.
Event-based frame interpolation methods typically adopt a synthesis-based
approach, where predicted frame residuals are directly applied to the
key-frames. However, while these approaches can capture non-linear motions they
suffer from ghosting and perform poorly in low-texture regions with few events.
Thus, synthesis-based and flow-based approaches are complementary. In this
work, we introduce Time Lens, a novel indicates equal contribution method that
leverages the advantages of both. We extensively evaluate our method on three
synthetic and two real benchmarks where we show an up to 5.21 dB improvement in
terms of PSNR over state-of-the-art frame-based and event-based methods.
Finally, we release a new large-scale dataset in highly dynamic scenarios,
aimed at pushing the limits of existing methods.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07287" title="Abstract">arXiv:2106.07287</a> [<a href="/pdf/2106.07287" title="Download PDF">pdf</a>, <a href="/format/2106.07287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Science Methodologies: Current Challenges and Future Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinez%2C+I">I&#xf1;igo Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Viles%2C+E">Elisabeth Viles</a>, 
<a href="/search/cs?searchtype=author&query=Olaizola%2C+I+G">Igor G. Olaizola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 23 figures, 5 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Big Data Research, Vol. 24, January 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Data science has employed great research efforts in developing advanced
analytics, improving data models and cultivating new algorithms. However, not
many authors have come across the organizational and socio-technical challenges
that arise when executing a data science project: lack of vision and clear
objectives, a biased emphasis on technical issues, a low level of maturity for
ad-hoc projects and the ambiguity of roles in data science are among these
challenges. Few methodologies have been proposed on the literature that tackle
these type of challenges, some of them date back to the mid-1990, and
consequently they are not updated to the current paradigm and the latest
developments in big data and machine learning technologies. In addition, fewer
methodologies offer a complete guideline across team, project and data &amp;
information management. In this article we would like to explore the necessity
of developing a more holistic approach for carrying out data science projects.
We first review methodologies that have been presented on the literature to
work on data science projects and classify them according to the their focus:
project, team, data and information management. Finally, we propose a
conceptual framework containing general characteristics that a methodology for
managing data science projects with a holistic point of view should have. This
framework can be used by other researchers as a roadmap for the design of new
data science methodologies or the updating of existing ones.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07288" title="Abstract">arXiv:2106.07288</a> [<a href="/pdf/2106.07288" title="Download PDF">pdf</a>, <a href="/format/2106.07288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-Aided Heuristics Design for Storage System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yingtian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Han Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mingxuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Jia Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Computer systems such as storage systems normally require transparent
white-box algorithms that are interpretable for human experts. In this work, we
propose a learning-aided heuristic design method, which automatically generates
human-readable strategies from Deep Reinforcement Learning (DRL) agents. This
method benefits from the power of deep learning but avoids the shortcoming of
its black-box property. Besides the white-box advantage, experiments in our
storage productions resource allocation scenario also show that this solution
outperforms the systems default settings and the elaborately handcrafted
strategy by human experts.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07289" title="Abstract">arXiv:2106.07289</a> [<a href="/pdf/2106.07289" title="Download PDF">pdf</a>, <a href="/format/2106.07289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Personalized Federated Min-Max Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beznosikov%2C+A">Aleksandr Beznosikov</a>, 
<a href="/search/cs?searchtype=author&query=Sushko%2C+V">Vadim Sushko</a>, 
<a href="/search/cs?searchtype=author&query=Sadiev%2C+A">Abdurakhmon Sadiev</a>, 
<a href="/search/cs?searchtype=author&query=Gasnikov%2C+A">Alexander Gasnikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
<p class="mathjax">Personalized Federated Learning has recently seen tremendous progress,
allowing the design of novel machine learning applications preserving privacy
of the data used for training. Existing theoretical results in this field
mainly focus on distributed optimization under minimization problems. This
paper is the first to study PFL for saddle point problems, which cover a
broader class of optimization tasks and are thus of more relevance for
applications than the minimization. In this work, we consider a recently
proposed PFL setting with the mixing objective function, an approach combining
the learning of a global model together with local distributed learners. Unlike
most of the previous papers, which considered only the centralized setting, we
work in a more general and decentralized setup. This allows to design and to
analyze more practical and federated ways to connect devices to the network. We
present two new algorithms for our problem. A theoretical analysis of the
methods is presented for smooth (strongly-)convex-(strongly-)concave saddle
point problems. We also demonstrate the effectiveness of our problem
formulation and the proposed algorithms on experiments with neural networks
with adversarial noise.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07296" title="Abstract">arXiv:2106.07296</a> [<a href="/pdf/2106.07296" title="Download PDF">pdf</a>, <a href="/ps/2106.07296" title="Download PostScript">ps</a>, <a href="/format/2106.07296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RRULES: An improvement of the RULES rule-based classifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palliser-Sans%2C+R">Rafel Palliser-Sans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">RRULES is presented as an improvement and optimization over RULES, a simple
inductive learning algorithm for extracting IF-THEN rules from a set of
training examples. RRULES optimizes the algorithm by implementing a more
effective mechanism to detect irrelevant rules, at the same time that checks
the stopping conditions more often. This results in a more compact rule set
containing more general rules which prevent overfitting the training set and
obtain a higher test accuracy. Moreover, the results show that RRULES
outperforms the original algorithm by reducing the coverage rate up to a factor
of 7 while running twice or three times faster consistently over several
datasets.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07297" title="Abstract">arXiv:2106.07297</a> [<a href="/pdf/2106.07297" title="Download PDF">pdf</a>, <a href="/format/2106.07297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Node Classification Meets Link Prediction on Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abboud%2C+R">Ralph Abboud</a>, 
<a href="/search/cs?searchtype=author&query=Ceylan%2C+%C4%B0+%C4%B0">&#x130;smail &#x130;lkan Ceylan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Node classification and link prediction are widely studied tasks in graph
representation learning. While both transductive node classification and link
prediction operate over a single input graph, they are studied in isolation so
far, which leads to discrepancies. Node classification models take as input a
graph with node features and incomplete node labels, and implicitly assume that
the input graph is relationally complete, i.e., no edges are missing from the
input graph. This is in sharp contrast with link prediction models that are
solely motivated by the relational incompleteness of the input graph which does
not have any node features. We propose a unifying perspective and study the
problems of (i) transductive node classification over incomplete graphs and
(ii) link prediction over graphs with node features. We propose an extension to
an existing box embedding model, and show that this model is fully expressive,
and can solve both of these tasks in an end-to-end fashion. To empirically
evaluate our model, we construct a knowledge graph with node features, which is
challenging both for node classification and link prediction. Our model
performs very strongly when compared to the respective state-of-the-art models
for node classification and link prediction on this dataset and shows the
importance of a unified perspective for node classification and link prediction
on knowledge graphs.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07299" title="Abstract">arXiv:2106.07299</a> [<a href="/pdf/2106.07299" title="Download PDF">pdf</a>, <a href="/format/2106.07299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Based Estimator for UAVs with Real-time Identification Using DNN  and the Modified Relay Feedback Test
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wahbah%2C+M">Mohamad Wahbah</a>, 
<a href="/search/cs?searchtype=author&query=Chehadeh%2C+M">Mohamad Chehadeh</a>, 
<a href="/search/cs?searchtype=author&query=Zweiri%2C+Y">Yahya Zweiri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Control performance of Unmanned Aerial Vehicles (UAVs) is directly affected
by their ability to estimate their states accurately. With the increasing
popularity of autonomous UAV solutions in real world applications, it is
imperative to develop robust adaptive estimators that can ameliorate sensor
noises in low-cost UAVs. Utilizing the knowledge of UAV dynamics in estimation
can provide significant advantages, but remains challenging due to the complex
and expensive pre-flight experiments required to obtain UAV dynamic parameters.
In this paper, we propose two decoupled dynamic model based Extended Kalman
Filters for UAVs, that provide high rate estimates for position, and velocity
of rotational and translational states, as well as filtered inertial
acceleration. The dynamic model parameters are estimated online using the Deep
Neural Network and Modified Relay Feedback Test (DNN-MRFT) framework, without
requiring any prior knowledge of the UAV physical parameters. The designed
filters with real-time identified process model parameters are tested
experimentally and showed two advantages. Firstly, smooth and lag-free
estimates of the UAV rotational speed and inertial acceleration are obtained,
and used to improve the closed loop system performance, reducing the controller
action by over 6 %. Secondly, the proposed approach enabled the UAV to track
aggressive trajectories with low rate position measurements, a task usually
infeasible under those conditions. The experimental data shows that we achieved
estimation performance matching other methods that requires full knowledge of
the UAV parameters.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07300" title="Abstract">arXiv:2106.07300</a> [<a href="/pdf/2106.07300" title="Download PDF">pdf</a>, <a href="/format/2106.07300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guaranteeing Half-Maximin Shares Under Cardinality Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hummel%2C+H">Halvard Hummel</a>, 
<a href="/search/cs?searchtype=author&query=Hetland%2C+M+L">Magnus Lie Hetland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study the problem of fair allocation of a set of indivisible items among
agents with additive valuations, under cardinality constraints. In this setting
the items are partitioned into categories, each with its own limit on the
number of items it may contribute to any bundle. One example of such a problem
is allocating seats in a multitrack conference. We consider the fairness
measure known as the maximin share (MMS) guarantee, and propose a novel
polynomial-time algorithm for finding $1/2$-approximate MMS allocations. We
extend the notions and algorithms related to ordered and reduced instances to
work with cardinality constraints, and combine these with a bag filling style
procedure. Our algorithm improves on that of Biswas and Barman (IJCAI-18), with
its approximation ratio of $1/3$. We also present an optimizing algorithm,
which for each instance, instead of fixing $\alpha = 1/2$, uses bisection to
find the largest $\alpha$ for which our algorithm obtains a valid
$\alpha$-approximate MMS allocation. Numerical tests show that our algorithm
finds strictly better approximations than the guarantee of $1/2$ for most
instances, in many cases surpassing $3/5$. The optimizing version of the
algorithm produces MMS allocations in a comparable number of instances to that
of Biswas and Barman's algorithm, on average achieving a better approximation
when MMS is not obtained.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07303" title="Abstract">arXiv:2106.07303</a> [<a href="/pdf/2106.07303" title="Download PDF">pdf</a>, <a href="/format/2106.07303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iNNformant: Boundary Samples as Telltale Watermarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schl%C3%B6gl%2C+A">Alexander Schl&#xf6;gl</a>, 
<a href="/search/cs?searchtype=author&query=Kupek%2C+T">Tobias Kupek</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hme%2C+R">Rainer B&#xf6;hme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Will be presented at IH&amp;MMSEC '21
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Multimedia (cs.MM)

</div>
<p class="mathjax">Boundary samples are special inputs to artificial neural networks crafted to
identify the execution environment used for inference by the resulting output
label. The paper presents and evaluates algorithms to generate transparent
boundary samples. Transparency refers to a small perceptual distortion of the
host signal (i.e., a natural input sample). For two established image
classifiers, ResNet on FMNIST and CIFAR10, we show that it is possible to
generate sets of boundary samples which can identify any of four tested
microarchitectures. These sets can be built to not contain any sample with a
worse peak signal-to-noise ratio than 70dB. We analyze the relationship between
search complexity and resulting transparency.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07306" title="Abstract">arXiv:2106.07306</a> [<a href="/pdf/2106.07306" title="Download PDF">pdf</a>, <a href="/ps/2106.07306" title="Download PostScript">ps</a>, <a href="/format/2106.07306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constraining Linear-chain CRFs to Regular Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papay%2C+S">Sean Papay</a>, 
<a href="/search/cs?searchtype=author&query=Klinger%2C+R">Roman Klinger</a>, 
<a href="/search/cs?searchtype=author&query=Pad%C3%B3%2C+S">Sebastian Pad&#xf3;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In structured prediction, a major challenge for models is to represent the
interdependencies within their output structures. For the common case where
outputs are structured as a sequence, linear-chain conditional random fields
(CRFs) are a widely used model class which can learn local dependencies in
output sequences. However, the CRF's Markov assumption makes it impossible for
these models to capture nonlocal dependencies, and standard CRFs are unable to
respect nonlocal constraints of the data (such as global arity constraints on
output labels). We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding. We prove that
constrained training is never worse than constrained decoding, and show using
synthetic data that it can be substantially better in practice. Additionally,
we demonstrate a practical benefit on downstream tasks by incorporating a
RegCCRF into a deep neural model for semantic role labeling, exceeding
state-of-the-art results on a standard dataset.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07307" title="Abstract">arXiv:2106.07307</a> [<a href="/pdf/2106.07307" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Recipe for Social Media Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Shahid Alam</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+J">Juvariya Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The Ubiquitous nature of smartphones has significantly increased the use of
social media platforms, such as Facebook, Twitter, TikTok, and LinkedIn, etc.,
among the public, government, and businesses. Facebook generated ~70 billion
USD in 2019 in advertisement revenues alone, a ~27% increase from the previous
year. Social media has also played a strong role in outbreaks of social
protests responsible for political changes in different countries. As we can
see from the above examples, social media plays a big role in business
intelligence and international politics. In this paper, we present and discuss
a high-level functional intelligence model (recipe) of Social Media Analysis
(SMA). This model synthesizes the input data and uses operational intelligence
to provide actionable recommendations. In addition, it also matches the
synthesized function of the experiences and learning gained from the
environment. The SMA model presented is independent of the application domain,
and can be applied to different domains, such as Education, Healthcare and
Government, etc. Finally, we also present some of the challenges faced by SMA
and how the SMA model presented in this paper solves them.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07310" title="Abstract">arXiv:2106.07310</a> [<a href="/pdf/2106.07310" title="Download PDF">pdf</a>, <a href="/format/2106.07310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pixel Sampling for Style Preserving Face Pose Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xiangnan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Di Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zehua Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liming Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The existing auto-encoder based face pose editing methods primarily focus on
modeling the identity preserving ability during pose synthesis, but are less
able to preserve the image style properly, which refers to the color,
brightness, saturation, etc. In this paper, we take advantage of the well-known
frontal/profile optical illusion and present a novel two-stage approach to
solve the aforementioned dilemma, where the task of face pose manipulation is
cast into face inpainting. By selectively sampling pixels from the input face
and slightly adjust their relative locations with the proposed ``Pixel
Attention Sampling" module, the face editing result faithfully keeps the
identity information as well as the image style unchanged. By leveraging
high-dimensional embedding at the inpainting stage, finer details are
generated. Further, with the 3D facial landmarks as guidance, our method is
able to manipulate face pose in three degrees of freedom, i.e., yaw, pitch, and
roll, resulting in more flexible face pose editing than merely controlling the
yaw angle as usually achieved by the current state-of-the-art. Both the
qualitative and quantitative evaluations validate the superiority of the
proposed approach.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07313" title="Abstract">arXiv:2106.07313</a> [<a href="/pdf/2106.07313" title="Download PDF">pdf</a>, <a href="/format/2106.07313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Gradient -- An Adaptive Technique for Improving Gradient  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fattah%2C+E+A">Esmail Abdul Fattah</a>, 
<a href="/search/math?searchtype=author&query=Van+Niekerk%2C+J">Janet Van Niekerk</a>, 
<a href="/search/math?searchtype=author&query=Rue%2C+H">Haavard Rue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Computing the gradient of a function provides fundamental information about
its behavior. This information is essential for several applications and
algorithms across various fields. One common application that require gradients
are optimization techniques such as stochastic gradient descent, Newton's
method and trust region methods. However, these methods usually requires a
numerical computation of the gradient at every iteration of the method which is
prone to numerical errors. We propose a simple limited-memory technique for
improving the accuracy of a numerically computed gradient in this
gradient-based optimization framework by exploiting (1) a coordinate
transformation of the gradient and (2) the history of previously taken descent
directions. The method is verified empirically by extensive experimentation on
both test functions and on real data applications. The proposed method is
implemented in the R package smartGrad and in C++.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07314" title="Abstract">arXiv:2106.07314</a> [<a href="/pdf/2106.07314" title="Download PDF">pdf</a>, <a href="/format/2106.07314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computer Vision Tool for Detection, Mapping and Fault Classification of  PV Modules in Aerial IR Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bommes%2C+L">Lukas Bommes</a>, 
<a href="/search/cs?searchtype=author&query=Pickel%2C+T">Tobias Pickel</a>, 
<a href="/search/cs?searchtype=author&query=Buerhop-Lutz%2C+C">Claudia Buerhop-Lutz</a>, 
<a href="/search/cs?searchtype=author&query=Hauch%2C+J">Jens Hauch</a>, 
<a href="/search/cs?searchtype=author&query=Brabec%2C+C">Christoph Brabec</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+I+M">Ian Marius Peters</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Increasing deployment of photovoltaics (PV) plants demands for cheap and fast
inspection. A viable tool for this task is thermographic imaging by unmanned
aerial vehicles (UAV). In this work, we develop a computer vision tool for the
semi-automatic extraction of PV modules from thermographic UAV videos. We use
it to curate a dataset containing 4.3 million IR images of 107842 PV modules
from thermographic videos of seven different PV plants. To demonstrate its use
for automated PV plant inspection, we train a ResNet-50 to classify ten common
module anomalies with more than 90 % test accuracy. Experiments show that our
tool generalizes well to different PV plants. It successfully extracts PV
modules from 512 out of 561 plant rows. Failures are mostly due to an
inappropriate UAV trajectory and erroneous module segmentation. Including all
manual steps our tool enables inspection of 3.5 MW p to 9 MW p of PV
installations per day, potentially scaling to multi-gigawatt plants due to its
parallel nature. While we present an effective method for automated PV plant
inspection, we are also confident that our approach helps to meet the growing
demand for large thermographic datasets for machine learning tasks, such as
power prediction or unsupervised defect identification.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07316" title="Abstract">arXiv:2106.07316</a> [<a href="/pdf/2106.07316" title="Download PDF">pdf</a>, <a href="/format/2106.07316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Sentence-Level Representations for Passage Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leonhardt%2C+J">Jurek Leonhardt</a>, 
<a href="/search/cs?searchtype=author&query=Beringer%2C+F">Fabian Beringer</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+A">Avishek Anand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recently, pre-trained contextual models, such as BERT, have shown to perform
well in language related tasks. We revisit the design decisions that govern the
applicability of these models for the passage re-ranking task in open-domain
question answering. We find that common approaches in the literature rely on
fine-tuning a pre-trained BERT model and using a single, global representation
of the input, discarding useful fine-grained relevance signals in token- or
sentence-level representations. We argue that these discarded tokens hold
useful information that can be leveraged. In this paper, we explicitly model
the sentence-level representations by using Dynamic Memory Networks (DMNs) and
conduct empirical evaluation to show improvements in passage re-ranking over
fine-tuned vanilla BERT models by memory-enhanced explicit sentence modelling
on a diverse set of open-domain QA datasets. We further show that freezing the
BERT model and only training the DMN layer still comes close to the original
performance, while improving training efficiency drastically. This indicates
that the usual fine-tuning step mostly helps to aggregate the inherent
information in a single output token, as opposed to adapting the whole model to
the new task, and only achieves rather small gains.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07317" title="Abstract">arXiv:2106.07317</a> [<a href="/pdf/2106.07317" title="Download PDF">pdf</a>, <a href="/format/2106.07317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Machine Learning Techniques for Data Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imbrea%2C+A">Alexandru-Ionut Imbrea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 14 figures, Originally published as <a href="https://essay.utwente.nl/80548">this https URL</a> at the 32nd Twente Student Conference on IT Jan. 31st, 2019, Enschede, The Netherlands, Supervised by: dr. Doina Bucur, dr. Claudio Pinho Rebelo de S\'a
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automated machine learning techniques benefited from tremendous research
progress in recently. These developments and the continuous-growing demand for
machine learning experts led to the development of numerous AutoML tools.
However, these tools assume that the entire training dataset is available
upfront and that the underlying distribution does not change over time. These
assumptions do not hold in a data stream mining setting where an unbounded
stream of data cannot be stored and is likely to manifest concept drift.
Industry applications of machine learning on streaming data become more popular
due to the increasing adoption of real-time streaming patterns in IoT,
microservices architectures, web analytics, and other fields. The research
summarized in this paper surveys the state-of-the-art open-source AutoML tools,
applies them to data collected from streams, and measures how their performance
changes over time. For comparative purposes, batch, batch incremental and
instance incremental estimators are applied and compared. Moreover, a
meta-learning technique for online algorithm selection based on meta-feature
extraction is proposed and compared while model replacement and continual
AutoML techniques are discussed. The results show that off-the-shelf AutoML
tools can provide satisfactory results but in the presence of concept drift,
detection or adaptation techniques have to be applied to maintain the
predictive accuracy over time.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07318" title="Abstract">arXiv:2106.07318</a> [<a href="/pdf/2106.07318" title="Download PDF">pdf</a>, <a href="/ps/2106.07318" title="Download PostScript">ps</a>, <a href="/format/2106.07318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiobjective Bilevel Evolutionary Approach for Off-Grid  Direction-of-Arrival Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+A">J. Andrew Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xin Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to an Elsevier journal for peer-review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">The source number identification is an essential step in direction-of-arrival
(DOA) estimation. Existing methods may provide a wrong source number due to
inferior statistical properties (in low SNR or limited snapshots) or modeling
errors (caused by relaxing sparse penalties), especially in impulsive noise. To
address this issue, we propose a novel idea of simultaneous source number
identification and DOA estimation. We formulate a multiobjective off-grid DOA
estimation model to realize this idea, by which the source number can be
automatically identified together with DOA estimation. In particular, the
source number is properly exploited by the $l_0$ norm of impinging signals
without relaxations, guaranteeing accuracy. Furthermore, we design a
multiobjective bilevel evolutionary algorithm to solve the proposed model. The
source number identification and sparse recovery are simultaneously optimized
at the on-grid (lower) level. A forward search strategy is developed to further
refine the grid at the off-grid (upper) level. This strategy does not need
linear approximations and can eliminate the off-grid gap with low computational
complexity. Simulation results demonstrate the outperformance of our method in
terms of source number and root mean square error.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07319" title="Abstract">arXiv:2106.07319</a> [<a href="/pdf/2106.07319" title="Download PDF">pdf</a>, <a href="/ps/2106.07319" title="Download PostScript">ps</a>, <a href="/format/2106.07319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coresets for constrained k-median and k-means clustering in low  dimensional Euclidean space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+M">Melanie Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Wargalla%2C+J">Julian Wargalla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study (Euclidean) $k$-median and $k$-means with constraints in the
streaming model.
<br />There have been recent efforts to design unified algorithms to solve
constrained $k$-means problems without using knowledge of the specific
constraint at hand aside from mild assumptions like the polynomial
computability of feasibility under the constraint (compute if a clustering
satisfies the constraint) or the presence of an efficient assignment oracle
(given a set of centers, produce an optimal assignment of points to the centers
which satisfies the constraint). These algorithms have a running time
exponential in $k$, but can be applied to a wide range of constraints.
<br />We demonstrate that a technique proposed in 2019 for solving a specific
constrained streaming $k$-means problem, namely fair $k$-means clustering,
actually implies streaming algorithms for all these constraints. These work for
low dimensional Euclidean space. [Note that there are more algorithms for
streaming fair $k$-means today, in particular they exist for high dimensional
spaces now as well.]
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07321" title="Abstract">arXiv:2106.07321</a> [<a href="/pdf/2106.07321" title="Download PDF">pdf</a>, <a href="/format/2106.07321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Free Lunch: Microservice Practices Reconsidered in Industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Q">Qilin Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xin Peng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chuan He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanzhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dewei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yuanfang Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Microservice architecture advocates a number of technologies and practices
such as lightweight container, container orchestration, and DevOps, with the
promised benefits of faster delivery, improved scalability, and greater
autonomy. However, microservice systems implemented in industry vary a lot in
terms of adopted practices and achieved benefits, drastically different from
what is advocated in the literature. In this article, we conduct an empirical
study, including an online survey with 51 responses and 14 interviews for
experienced microservice experts to advance our understanding regarding to
microservice practices in industry. As a part of our findings, the empirical
study clearly revealed three levels of maturity of microservice systems (from
basic to advanced): independent development and deployment, high scalability
and availability, and service ecosystem, categorized by the fulfilled benefits
of microservices. We also identify 11 practical issues that constrain the
microservice capabilities of organizations. For each issue, we summarize the
practices that have been explored and adopted in industry, along with the
remaining challenges. Our study can help practitioners better position their
microservice systems and determine what infrastructures and capabilities are
worth investing. Our study can also help researchers better understand
industrial microservice practices and identify useful research problems.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07324" title="Abstract">arXiv:2106.07324</a> [<a href="/pdf/2106.07324" title="Download PDF">pdf</a>, <a href="/ps/2106.07324" title="Download PostScript">ps</a>, <a href="/format/2106.07324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Computations for Bifurcations and Spectral Stability of  Solitary Waves in Coupled Nonlinear Schr&#xf6;dinger Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yagasaki%2C+K">Kazuyuki Yagasaki</a>, 
<a href="/search/math?searchtype=author&query=Yamazoe%2C+S">Shotaro Yamazoe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Dynamical Systems (math.DS)

</div>
<p class="mathjax">We numerically study solitary waves in the coupled nonlinear Schr\"odinger
equations. We detect pitchfork bifurcations of the fundamental solitary wave
and compute eigenvalues and eigenfunctions of the corresponding eigenvalue
problems to determine the spectral stability of solitary waves born at the
pitchfork bifurcations. Our numerical results demonstrate the theoretical ones
which the authors obtained recently. We also compute generalized eigenfunctions
associated with the zero eigenvalue for the bifurcated solitary wave exhibiting
a saddle-node bifurcation, and show that it does not change its stability type
at the saddle-node bifurcation point.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07327" title="Abstract">arXiv:2106.07327</a> [<a href="/pdf/2106.07327" title="Download PDF">pdf</a>, <a href="/format/2106.07327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Quanvolutional Neural Networks with enhanced image encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mattern%2C+D">Denny Mattern</a>, 
<a href="/search/cs?searchtype=author&query=Martyniuk%2C+D">Darya Martyniuk</a>, 
<a href="/search/cs?searchtype=author&query=Willems%2C+H">Henri Willems</a>, 
<a href="/search/cs?searchtype=author&query=Bergmann%2C+F">Fabian Bergmann</a>, 
<a href="/search/cs?searchtype=author&query=Paschke%2C+A">Adrian Paschke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image classification is an important task in various machine learning
applications. In recent years, a number of classification methods based on
quantum machine learning and different quantum image encoding techniques have
been proposed. In this paper, we study the effect of three different quantum
image encoding approaches on the performance of a convolution-inspired hybrid
quantum-classical image classification algorithm called quanvolutional neural
network (QNN). We furthermore examine the effect of variational - i.e.
trainable - quantum circuits on the classification results. Our experiments
indicate that some image encodings are better suited for variational circuits.
However, our experiments show as well that there is not one best image
encoding, but that the choice of the encoding depends on the specific
constraints of the application.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07329" title="Abstract">arXiv:2106.07329</a> [<a href="/pdf/2106.07329" title="Download PDF">pdf</a>, <a href="/format/2106.07329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-Policy Deep Reinforcement Learning for the Average-Reward Criterion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+K+W">Keith W. Ross</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Machine Learning (ICML) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We develop theory and algorithms for average-reward on-policy Reinforcement
Learning (RL). We first consider bounding the difference of the long-term
average reward for two policies. We show that previous work based on the
discounted return (Schulman et al., 2015; Achiam et al., 2017) results in a
non-meaningful bound in the average-reward setting. By addressing the
average-reward criterion directly, we then derive a novel bound which depends
on the average divergence between the two policies and Kemeny's constant. Based
on this bound, we develop an iterative procedure which produces a sequence of
monotonically improved policies for the average reward criterion. This
iterative procedure can then be combined with classic DRL (Deep Reinforcement
Learning) methods, resulting in practical DRL algorithms that target the
long-run average reward criterion. In particular, we demonstrate that
Average-Reward TRPO (ATRPO), which adapts the on-policy TRPO algorithm to the
average-reward criterion, significantly outperforms TRPO in the most
challenging MuJuCo environments.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07333" title="Abstract">arXiv:2106.07333</a> [<a href="/pdf/2106.07333" title="Download PDF">pdf</a>, <a href="/format/2106.07333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brima%2C+Y">Yusuf Brima</a>, 
<a href="/search/cs?searchtype=author&query=Tushar%2C+M+H+K">Mossadek Hossain Kamal Tushar</a>, 
<a href="/search/cs?searchtype=author&query=Kabir%2C+U">Upama Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+T">Tariqul Islam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was carried out as a collaboration between the Department of Computer Science and Engineering -- the University of Dhaka and the National Institute of Neuroscience (NINS), Bangladesh. We created a novel neurological discord dataset of 37 disease categories
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in
the field of radiology to create images of the anatomical and physiological
structure of patients. MRI is the prevalent medical imaging practice to find
abnormalities in soft tissues. Traditionally they are analyzed by a radiologist
to detect abnormalities in soft tissues, especially the brain. The process of
interpreting a massive volume of patient's MRI is laborious. Hence, the use of
Machine Learning methodologies can aid in detecting abnormalities in soft
tissues with considerable accuracy. In this research, we have curated a novel
dataset and developed a framework that uses Deep Transfer Learning to perform a
multi-classification of tumors in the brain MRI images. In this paper, we
adopted the Deep Residual Convolutional Neural Network (ResNet50) architecture
for the experiments along with discriminative learning techniques to train the
model. Using the novel dataset and two publicly available MRI brain datasets,
this proposed approach attained a classification accuracy of 86.40\% on the
curated dataset, 93.80\% on the Harvard Whole Brain Atlas dataset, and 97.05\%
accuracy on the School of Biomedical Engineering dataset. Results of our
experiments significantly demonstrate our proposed framework for transfer
learning is a potential and effective method for brain tumor
multi-classification tasks.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07336" title="Abstract">arXiv:2106.07336</a> [<a href="/pdf/2106.07336" title="Download PDF">pdf</a>, <a href="/ps/2106.07336" title="Download PostScript">ps</a>, <a href="/format/2106.07336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-Based Proofs of Combinatorial Results on Bipartite Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sason%2C+I">Igal Sason</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Proceedings of 2021 IEEE International Symposium on Information Theory, July 12-20, 2021 (virtual symposium). arXiv admin note: text overlap with <a href="/abs/2012.12107">arXiv:2012.12107</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">This work considers new entropy-based proofs of some known, or otherwise
refined, combinatorial bounds for bipartite graphs. These include upper bounds
on the number of the independent sets, lower bounds on the minimal number of
colors in constrained edge coloring, and lower bounds on the number of walks of
a given length in bipartite graphs. The proofs of these combinatorial results
rely on basic properties of the Shannon entropy.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07338" title="Abstract">arXiv:2106.07338</a> [<a href="/pdf/2106.07338" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neighborhood Rough Set based Multi-document Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yadav%2C+N">Nidhika Yadav</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, original paper not submitted anywhere else
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This research paper proposes a novel Neighbourhood Rough Set based approach
for supervised Multi-document Text Summarization (MDTS) with analysis and
impact on the summarization results for MDTS. Here, Rough Set based LERS
algorithm is improved using Neighborhood Rough Set which is itself a novel
combination called Neighborhood-LERS to be experimented for evaluations of
efficacy and efficiency. In this paper, we shall apply and evaluate the
proposed Neighborhood-LERS for Multi-document Summarization which here is
proved experimentally to be superior to the base LERS technique for MDTS.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07340" title="Abstract">arXiv:2106.07340</a> [<a href="/pdf/2106.07340" title="Download PDF">pdf</a>, <a href="/format/2106.07340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MathBERT: A Pre-trained Language Model for General NLP Tasks in  Mathematics Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J+T">Jia Tracy Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yamashita%2C+M">Michiharu Yamashita</a>, 
<a href="/search/cs?searchtype=author&query=Prihar%2C+E">Ethan Prihar</a>, 
<a href="/search/cs?searchtype=author&query=Heffernan%2C+N">Neil Heffernan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xintao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Due to the transfer learning nature of BERT model, researchers have achieved
better performance than base BERT by further pre-training the original BERT on
a huge domain-specific corpus. Due to the special nature of mathematical texts
which often contain math equations and symbols, the original BERT model
pre-trained on general English context will not fit Natural Language Processing
(NLP) tasks in mathematical education well. Therefore, we propose MathBERT, a
BERT pre-trained on large mathematical corpus including pre-k to graduate level
mathematical content to tackle math-specific tasks. In addition, We generate a
customized mathematical vocabulary to pre-train with MathBERT and compare the
performance to the MathBERT pre-trained with the original BERT vocabulary. We
select three important tasks in mathematical education such as knowledge
component, auto-grading, and knowledge tracing prediction to evaluate the
performance of MathBERT. Our experiments show that MathBERT outperforms the
base BERT by 2-9\% margin. In some cases, MathBERT pre-trained with
mathematical vocabulary is better than MathBERT trained with original
vocabulary.To our best knowledge, MathBERT is the first pre-trained model for
general purpose mathematics education tasks.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07341" title="Abstract">arXiv:2106.07341</a> [<a href="/pdf/2106.07341" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> i-Pulse: A NLP based novel approach for employee engagement in logistics  organization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+R">Rachit Garg</a>, 
<a href="/search/cs?searchtype=author&query=Kiwelekar%2C+A+W">Arvind W Kiwelekar</a>, 
<a href="/search/cs?searchtype=author&query=Netak%2C+L+D">Laxman D Netak</a>, 
<a href="/search/cs?searchtype=author&query=Ghodake%2C+A">Akshay Ghodake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 Pages 7 Figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Information Management Data Insights
  (Elsevier) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Although most logistics and freight forwarding organizations, in one way or
another, claim to have core values. The engagement of employees is a vast
structure that affects almost every part of the company's core environmental
values. There is little theoretical knowledge about the relationship between
firms and the engagement of employees. Based on research literature, this paper
aims to provide a novel approach for insight around employee engagement in a
logistics organization by implementing deep natural language processing
concepts. The artificial intelligence-enabled solution named Intelligent Pulse
(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and
provides the actionable insights and gist of employee feedback. I-Pulse allows
the stakeholders to think in new ways in their organization, helping them to
have a powerful influence on employee engagement, retention, and efficiency.
This study is of corresponding interest to researchers and practitioners.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07343" title="Abstract">arXiv:2106.07343</a> [<a href="/pdf/2106.07343" title="Download PDF">pdf</a>, <a href="/format/2106.07343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent  Detection and Slot Filling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yutai Hou</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yongkui Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACL 2021 findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we investigate few-shot joint learning for dialogue language
understanding. Most existing few-shot models learn a single task each time with
only a few examples. However, dialogue language understanding contains two
closely related tasks, i.e., intent detection and slot filling, and often
benefits from jointly learning the two tasks. This calls for new few-shot
learning techniques that are able to capture task relations from only a few
examples and jointly learn multiple tasks. To achieve this, we propose a
similarity-based few-shot learning scheme, named Contrastive Prototype Merging
network (ConProm), that learns to bridge metric spaces of intent and slot on
data-rich domains, and then adapt the bridged metric space to the specific
few-shot domain. Experiments on two public datasets, Snips and FewJoint, show
that our model significantly outperforms the strong baselines in one and five
shots settings.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07344" title="Abstract">arXiv:2106.07344</a> [<a href="/pdf/2106.07344" title="Download PDF">pdf</a>, <a href="/format/2106.07344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Information Spreading Mechanisms During COVID-19 Pandemic  by Analyzing the Impact of Tweet Text and User Features for Retweet  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+P+I">Pervaiz Iqbal Khan</a>, 
<a href="/search/cs?searchtype=author&query=Razzak%2C+I">Imran Razzak</a>, 
<a href="/search/cs?searchtype=author&query=Dengel%2C+A">Andreas Dengel</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+S">Sheraz Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">COVID-19 has affected the world economy and the daily life routine of almost
everyone. It has been a hot topic on social media platforms such as Twitter,
Facebook, etc. These social media platforms enable users to share information
with other users who can reshare this information, thus causing this
information to spread. Twitter's retweet functionality allows users to share
the existing content with other users without altering the original content.
Analysis of social media platforms can help in detecting emergencies during
pandemics that lead to taking preventive measures. One such type of analysis is
predicting the number of retweets for a given COVID-19 related tweet. Recently,
CIKM organized a retweet prediction challenge for COVID-19 tweets focusing on
using numeric features only. However, our hypothesis is, tweet text may play a
vital role in an accurate retweet prediction. In this paper, we combine numeric
and text features for COVID-19 related retweet predictions. For this purpose,
we propose two CNN and RNN based models and evaluate the performance of these
models on a publicly available TweetsCOV19 dataset using seven different
evaluation metrics. Our evaluation results show that combining tweet text with
numeric features improves the performance of retweet prediction significantly.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07345" title="Abstract">arXiv:2106.07345</a> [<a href="/pdf/2106.07345" title="Download PDF">pdf</a>, <a href="/format/2106.07345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Guided Contrastive Learning for BERT Sentence Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taeuk Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K+M">Kang Min Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sang-goo Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although BERT and its variants have reshaped the NLP landscape, it still
remains unclear how best to derive sentence embeddings from such pre-trained
Transformers. In this work, we propose a contrastive learning method that
utilizes self-guidance for improving the quality of BERT sentence
representations. Our method fine-tunes BERT in a self-supervised fashion, does
not rely on data augmentation, and enables the usual [CLS] token embeddings to
function as sentence vectors. Moreover, we redesign the contrastive learning
objective (NT-Xent) and apply it to sentence representation learning. We
demonstrate with extensive experiments that our approach is more effective than
competitive baselines on diverse sentence-related tasks. We also show it is
efficient at inference and robust to domain shifts.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07346" title="Abstract">arXiv:2106.07346</a> [<a href="/pdf/2106.07346" title="Download PDF">pdf</a>, <a href="/format/2106.07346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-Driven Topic Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zheng Fang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>, 
<a href="/search/cs?searchtype=author&query=Procter%2C+R">Rob Procter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL2021 finding paper. For source code, see <a href="https://github.com/Fitz-like-coding/QDTM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Topic modeling is an unsupervised method for revealing the hidden semantic
structure of a corpus. It has been increasingly widely adopted as a tool in the
social sciences, including political science, digital humanities and
sociological research in general. One desirable property of topic models is to
allow users to find topics describing a specific aspect of the corpus. A
possible solution is to incorporate domain-specific knowledge into topic
modeling, but this requires a specification from domain experts. We propose a
novel query-driven topic model that allows users to specify a simple query in
words or phrases and return query-related topics, thus avoiding tedious work
from domain experts. Our proposed approach is particularly attractive when the
user-specified query has a low occurrence in a text corpus, making it difficult
for traditional topic models built on word cooccurrence patterns to identify
relevant topics. Experimental results demonstrate the effectiveness of our
model in comparison with both classical topic models and neural topic models.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07347" title="Abstract">arXiv:2106.07347</a> [<a href="/pdf/2106.07347" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zipf Matrix Factorization : Matrix Factorization with Matthew Effect  Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICAIBD 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommender system recommends interesting items to users based on users' past
information history. Researchers have been paying attention to improvement of
algorithmic performance such as MAE and precision@K. Major techniques such as
matrix factorization and learning to rank are optimized based on such
evaluation metrics. However, the intrinsic Matthew Effect problem poses great
threat to the fairness of the recommender system, and the unfairness problem
cannot be resolved by optimization of traditional metrics. In this paper, we
propose a novel algorithm that incorporates Matthew Effect reduction with the
matrix factorization framework. We demonstrate that our approach can boost the
fairness of the algorithm and enhances performance evaluated by traditional
metrics.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07348" title="Abstract">arXiv:2106.07348</a> [<a href="/pdf/2106.07348" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is it a click bait? Let&#x27;s predict using Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Sohom Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> M.Tech Thesis defended at BITS, Pilani
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">In this era of digitisation, news reader tend to read news online. This is
because, online media instantly provides access to a wide variety of content.
Thus, people don't have to wait for tomorrow's newspaper to know what's
happening today. Along with these virtues, online news have some vices as well.
One such vice is presence of social media posts (tweets) relating to news
articles whose sole purpose is to draw attention of the users rather than
directing them to read the actual content. Such posts are referred to as
clickbaits. The objective of this project is to develop a system which would be
capable of predicting how likely are the social media posts (tweets) relating
to new articles tend to be clickbait.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07349" title="Abstract">arXiv:2106.07349</a> [<a href="/pdf/2106.07349" title="Download PDF">pdf</a>, <a href="/format/2106.07349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Integrated Gradients to explain Linguistic Acceptability learnt by  BERT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nayak%2C+A">Anmol Nayak</a>, 
<a href="/search/cs?searchtype=author&query=Timmapathini%2C+H+P">Hari Prasad Timmapathini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">BERT has been a breakthrough in language understanding by leveraging the
multi-head self-attention mechanism in its architecture. To the best of our
knowledge this work is the first to leverage Layer Integrated Gradients
Attribution Scores (LIGAS) to explain the Linguistic Acceptability criteria
that are learnt by BERT on the Corpus of Linguistic Acceptability (CoLA)
benchmark dataset. Our experiments on 5 different categories of sentences lead
to the following interesting findings: 1) LIGAS for Linguistically Acceptable
(LA) sentences are significantly smaller in comparison to Linguistically
Unacceptable (LUA) sentences, 2) There are specific subtrees of the
Constituency Parse Tree (CPT) for LA and LUA sentences which contribute larger
LIGAS, 3) Across the different categories of sentences we observed around 88%
to 100% of the Correctly classified sentences had positive LIGAS, indicating a
strong positive relationship to the prediction confidence of the model, and 4)
Around 57% of the Misclassified sentences had positive LIGAS, which we believe
can become correctly classified sentences if the LIGAS are parameterized in the
loss function of the model.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07350" title="Abstract">arXiv:2106.07350</a> [<a href="/pdf/2106.07350" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> THG: Transformer with Hyperbolic Geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yibin Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages,3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformer model architectures have become an indispensable staple in deep
learning lately for their effectiveness across a range of tasks. Recently, a
surge of "X-former" models have been proposed which improve upon the original
Transformer architecture. However, most of these variants make changes only
around the quadratic time and memory complexity of self-attention, i.e. the dot
product between the query and the key. What's more, they are calculate solely
in Euclidean space. In this work, we propose a novel Transformer with
Hyperbolic Geometry (THG) model, which take the advantage of both Euclidean
space and Hyperbolic space. THG makes improvements in linear transformations of
self-attention, which are applied on the input sequence to get the query and
the key, with the proposed hyperbolic linear. Extensive experiments on sequence
labeling task, machine reading comprehension task and classification task
demonstrate the effectiveness and generalizability of our model. It also
demonstrates THG could alleviate overfitting.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07351" title="Abstract">arXiv:2106.07351</a> [<a href="/pdf/2106.07351" title="Download PDF">pdf</a>, <a href="/format/2106.07351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Single Lane to Highways: Analyzing the Adoption of Multipath TCP in  the Internet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aschenbrenner%2C+F">Florian Aschenbrenner</a>, 
<a href="/search/cs?searchtype=author&query=Shreedhar%2C+T">Tanya Shreedhar</a>, 
<a href="/search/cs?searchtype=author&query=Gasser%2C+O">Oliver Gasser</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+N">Nitinder Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Ott%2C+J">J&#xf6;rg Ott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 2021 IFIP Networking Conference (Networking '21). Visit <a href="https://mptcp.io">this https URL</a> for up-to-date MPTCP measurement results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Multipath TCP (MPTCP) extends traditional TCP to enable simultaneous use of
multiple connection endpoints at the source and destination. MPTCP has been
under active development since its standardization in 2013, and more recently
in February 2020, MPTCP was upstreamed to the Linux kernel.
<br />In this paper, we provide the first broad analysis of MPTCPv0 in the
Internet. We probe the entire IPv4 address space and an IPv6 hitlist to detect
MPTCP-enabled systems operational on port 80 and 443. Our scans reveal a steady
increase in MPTCP-capable IPs, reaching 9k+ on IPv4 and a few dozen on IPv6. We
also discover a significant share of seemingly MPTCP-capable hosts, an artifact
of middleboxes mirroring TCP options. We conduct targeted HTTP(S) measurements
towards select hosts and find that middleboxes can aggressively impact the
perceived quality of applications utilizing MPTCP. Finally, we analyze two
complementary traffic traces from CAIDA and MAWI to shed light on the
real-world usage of MPTCP. We find that while MPTCP usage has increased by a
factor of 20 over the past few years, its traffic share is still quite low.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07352" title="Abstract">arXiv:2106.07352</a> [<a href="/pdf/2106.07352" title="Download PDF">pdf</a>, <a href="/format/2106.07352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=FitzGerald%2C+N">Nicholas FitzGerald</a>, 
<a href="/search/cs?searchtype=author&query=Botha%2C+J+A">Jan A. Botha</a>, 
<a href="/search/cs?searchtype=author&query=Gillick%2C+D">Daniel Gillick</a>, 
<a href="/search/cs?searchtype=author&query=Bikel%2C+D+M">Daniel M. Bikel</a>, 
<a href="/search/cs?searchtype=author&query=Kwiatkowski%2C+T">Tom Kwiatkowski</a>, 
<a href="/search/cs?searchtype=author&query=McCallum%2C+A">Andrew McCallum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We present an instance-based nearest neighbor approach to entity linking. In
contrast to most prior entity retrieval systems which represent each entity
with a single vector, we build a contextualized mention-encoder that learns to
place similar mentions of the same entity closer in vector space than mentions
of different entities. This approach allows all mentions of an entity to serve
as "class prototypes" as inference involves retrieving from the full set of
labeled entity mentions in the training set and applying the nearest mention
neighbor's entity label. Our model is trained on a large multilingual corpus of
mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor
inference on an index of 700 million mentions. It is simpler to train, gives
more interpretable predictions, and outperforms all other systems on two
multilingual entity linking benchmarks.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07353" title="Abstract">arXiv:2106.07353</a> [<a href="/pdf/2106.07353" title="Download PDF">pdf</a>, <a href="/format/2106.07353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posthoc Verification and the Fallibility of the Ground Truth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yifan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Botzer%2C+N">Nicholas Botzer</a>, 
<a href="/search/cs?searchtype=author&query=Weninger%2C+T">Tim Weninger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Classifiers commonly make use of pre-annotated datasets, wherein a model is
evaluated by pre-defined metrics on a held-out test set typically made of
human-annotated labels. Metrics used in these evaluations are tied to the
availability of well-defined ground truth labels, and these metrics typically
do not allow for inexact matches. These noisy ground truth labels and strict
evaluation metrics may compromise the validity and realism of evaluation
results. In the present work, we discuss these concerns and conduct a
systematic posthoc verification experiment on the entity linking (EL) task.
Unlike traditional methodologies, which asks annotators to provide free-form
annotations, we ask annotators to verify the correctness of annotations after
the fact (i.e., posthoc). Compared to pre-annotation evaluation,
state-of-the-art EL models performed extremely well according to the posthoc
evaluation methodology. Posthoc validation also permits the validation of the
ground truth dataset. Surprisingly, we find predictions from EL models had a
similar or higher verification rate than the ground truth. We conclude with a
discussion on these findings and recommendations for future evaluations.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07356" title="Abstract">arXiv:2106.07356</a> [<a href="/pdf/2106.07356" title="Download PDF">pdf</a>, <a href="/format/2106.07356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture of Virtual-Kernel Experts for Multi-Objective User Profile  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenhui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Meng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liqun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bifeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In many industrial applications like online advertising and recommendation
systems, diverse and accurate user profiles can greatly help improve
personalization. For building user profiles, deep learning is widely used to
mine expressive tags to describe users' preferences from their historical
actions. For example, tags mined from users' click-action history can represent
the categories of ads that users are interested in, and they are likely to
continue being clicked in the future. Traditional solutions usually introduce
multiple independent Two-Tower models to mine tags from different actions,
e.g., click, conversion. However, the models cannot learn complementarily and
support effective training for data-sparse actions. Besides, limited by the
lack of information fusion between the two towers, the model learning is
insufficient to represent users' preferences on various topics well. This paper
introduces a novel multi-task model called Mixture of Virtual-Kernel Experts
(MVKE) to learn multiple topic-related user preferences based on different
actions unitedly. In MVKE, we propose a concept of Virtual-Kernel Expert, which
focuses on modeling one particular facet of the user's preference, and all of
them learn coordinately. Besides, the gate-based structure used in MVKE builds
an information fusion bridge between two towers, improving the model's
capability much and maintaining high efficiency. We apply the model in Tencent
Advertising System, where both online and offline evaluations show that our
method has a significant improvement compared with the existing ones and brings
about an obvious lift to actual advertising revenue.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07357" title="Abstract">arXiv:2106.07357</a> [<a href="/pdf/2106.07357" title="Download PDF">pdf</a>, <a href="/ps/2106.07357" title="Download PostScript">ps</a>, <a href="/format/2106.07357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conforming and Nonconforming Finite Element Methods for Biharmonic  Inverse Source Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shylaja%2C+D">Devika Shylaja</a>, 
<a href="/search/math?searchtype=author&query=Nair%2C+M+T">M. T. Nair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Functional Analysis (math.FA)

</div>
<p class="mathjax">This paper deals with the numerical approximation of the biharmonic inverse
source problem in an abstract setting in which the measurement data is
finite-dimensional. This unified framework in particular covers the conforming
and nonconforming finite element methods (FEMs). The inverse problem is
analysed through the forward problem. Error estimate for the forward solution
is derived in an abstract set-up that applies to conforming and Morley
nonconforming FEMs. Since the inverse problem is ill-posed, Tikhonov
regularisation is considered to obtain a stable approximate solution. Error
estimate is established for the regularised solution for different
regularisation schemes. Numerical results that confirm the theoretical results
are also presented.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07359" title="Abstract">arXiv:2106.07359</a> [<a href="/pdf/2106.07359" title="Download PDF">pdf</a>, <a href="/format/2106.07359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MexPub: Deep Transfer Learning for Metadata Extraction from German  Publications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boukhers%2C+Z">Zeyd Boukhers</a>, 
<a href="/search/cs?searchtype=author&query=Beili%2C+N">Nada Beili</a>, 
<a href="/search/cs?searchtype=author&query=Hartmann%2C+T">Timo Hartmann</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+P">Prantik Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Zafar%2C+M+A">Muhammad Arslan Zafar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A long version of an accepted paper @ JCDL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Digital Libraries (cs.DL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Extracting metadata from scientific papers can be considered a solved problem
in NLP due to the high accuracy of state-of-the-art methods. However, this does
not apply to German scientific publications, which have a variety of styles and
layouts. In contrast to most of the English scientific publications that follow
standard and simple layouts, the order, content, position and size of metadata
in German publications vary greatly among publications. This variety makes
traditional NLP methods fail to accurately extract metadata from these
publications. In this paper, we present a method that extracts metadata from
PDF documents with different layouts and styles by viewing the document as an
image. We used Mask R-CNN that is trained on COCO dataset and finetuned with
PubLayNet dataset that consists of ~200K PDF snapshots with five basic classes
(e.g. text, figure, etc). We refine-tuned the model on our proposed synthetic
dataset consisting of ~30K article snapshots to extract nine patterns (i.e.
author, title, etc). Our synthetic dataset is generated using contents in both
languages German and English and a finite set of challenging templates obtained
from German publications. Our method achieved an average accuracy of around
$90\%$ which validates its capability to accurately extract metadata from a
variety of PDF documents with challenging templates.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07360" title="Abstract">arXiv:2106.07360</a> [<a href="/pdf/2106.07360" title="Download PDF">pdf</a>, <a href="/format/2106.07360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Rank Projections of GCNs Laplacian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grinsztajn%2C+N">Nathan Grinsztajn</a> (Scool), 
<a href="/search/cs?searchtype=author&query=Preux%2C+P">Philippe Preux</a> (Scool), 
<a href="/search/cs?searchtype=author&query=Oyallon%2C+E">Edouard Oyallon</a> (MLIA)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2021 Workshop GTRL, 2021, Online, France
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we study the behavior of standard models for community
detection under spectral manipulations. Through various ablation experiments,
we evaluate the impact of bandpass filtering on the performance of a GCN: we
empirically show that most of the necessary and used information for nodes
classification is contained in the low-frequency domain, and thus contrary to
images, high frequencies are less crucial to community detection. In
particular, it is sometimes possible to obtain accuracies at a state-of-the-art
level with simple classifiers that rely only on a few low frequencies.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07363" title="Abstract">arXiv:2106.07363</a> [<a href="/pdf/2106.07363" title="Download PDF">pdf</a>, <a href="/format/2106.07363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cognitive-aware Short-text Understanding for Inferring Professions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esmailzadeh%2C+S">Sayna Esmailzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+S">Saeid Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Kangavari%2C+M+R">Mohammad Reza Kangavari</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Wen Hua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Leveraging short-text contents to estimate the occupation of microblog
authors has significant gains in many applications. Yet challenges abound.
Firstly brief textual contents come with excessive lexical noise that makes the
inference problem challenging. Secondly, cognitive-semantics are not evident,
and important linguistic features are latent in short-text contents. Thirdly,
it is hard to measure the correlation between the cognitive short-text
semantics and the features pertaining various occupations. We argue that the
multi-aspect cognitive features are needed to correctly associate short-text
contents to a particular job and discover suitable people for the careers. To
this end, we devise a novel framework that on the one hand, can infer
short-text contents and exploit cognitive features, and on the other hand,
fuses various adopted novel algorithms, such as curve fitting, support vector,
and boosting modules to better predict the occupation of the authors. The final
estimation module manufactures the $R^w$-tree via coherence weight to tune the
best outcome in the inferring process. We conduct comprehensive experiments on
real-life Twitter data. The experimental results show that compared to other
rivals, our cognitive multi-aspect model can achieve a higher performance in
the career estimation procedure, where it is inevitable to neglect the
contextual semantics of users.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07364" title="Abstract">arXiv:2106.07364</a> [<a href="/pdf/2106.07364" title="Download PDF">pdf</a>, <a href="/ps/2106.07364" title="Download PostScript">ps</a>, <a href="/format/2106.07364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meaning Representation of Numeric Fused-Heads in UCCA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+R">Ruixiang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Hershcovich%2C+D">Daniel Hershcovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> UnImplicit Workshop at ACL 2021 (abstract)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We exhibit that the implicit UCCA parser does not address numeric fused-heads
(NFHs) consistently, which could result either from inconsistent annotation,
insufficient training data or a modelling limitation. and show which factors
are involved. We consider this phenomenon important, as it is pervasive in text
and critical for correct inference. Careful design and fine-grained annotation
of NFHs in meaning representation frameworks would benefit downstream tasks
such as machine translation, natural language inference and question answering,
particularly when they require numeric reasoning, as recovering and
categorizing them. We are investigating the treatment of this phenomenon by
other meaning representations, such as AMR. We encourage researchers in meaning
representations, and computational linguistics in general, to address this
phenomenon in future research.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07368" title="Abstract">arXiv:2106.07368</a> [<a href="/pdf/2106.07368" title="Download PDF">pdf</a>, <a href="/format/2106.07368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality-Aware Network for Face Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Q">Qing Song</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+X">Xueshi Xin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2nd place in Short-video Face Parsing Track of The 3rd Person in Context (PIC) Workshop and Challenge at CVPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This is a very short technical report, which introduces the solution of the
Team BUPT-CASIA for Short-video Face Parsing Track of The 3rd Person in Context
(PIC) Workshop and Challenge at CVPR 2021.
<br />Face parsing has recently attracted increasing interest due to its numerous
application potentials. Generally speaking, it has a lot in common with human
parsing, such as task setting, data characteristics, number of categories and
so on. Therefore, this work applies state-of-the-art human parsing method to
face parsing task to explore the similarities and differences between them. Our
submission achieves 86.84% score and wins the 2nd place in the challenge.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07369" title="Abstract">arXiv:2106.07369</a> [<a href="/pdf/2106.07369" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Self-Supervised Framework for Function Learning and Extrapolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Segert%2C+S+N">Simon N. Segert</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+J+D">Jonathan D. Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Understanding how agents learn to generalize -- and, in particular, to
extrapolate -- in high-dimensional, naturalistic environments remains a
challenge for both machine learning and the study of biological agents. One
approach to this has been the use of function learning paradigms, which allow
peoples' empirical patterns of generalization for smooth scalar functions to be
described precisely. However, to date, such work has not succeeded in
identifying mechanisms that acquire the kinds of general purpose
representations over which function learning can operate to exhibit the
patterns of generalization observed in human empirical studies. Here, we
present a framework for how a learner may acquire such representations, that
then support generalization -- and extrapolation in particular -- in a few-shot
fashion. Taking inspiration from a classic theory of visual processing, we
construct a self-supervised encoder that implements the basic inductive bias of
invariance under topological distortions. We show the resulting representations
outperform those from other models for unsupervised time series learning in
several downstream function learning tasks, including extrapolation.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07371" title="Abstract">arXiv:2106.07371</a> [<a href="/pdf/2106.07371" title="Download PDF">pdf</a>, <a href="/format/2106.07371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A2MM: Mitigating Frontrunning, Transaction Reordering and Consensus  Instability in Decentralized Exchanges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Liyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+K">Kaihua Qin</a>, 
<a href="/search/cs?searchtype=author&query=Gervais%2C+A">Arthur Gervais</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The asset trading volume on blockchain-based exchanges (DEX) increased
substantially since the advent of Automated Market Makers (AMM). Yet, AMMs and
their forks compete on the same blockchain, incurring unnecessary network and
block-space overhead, by attracting sandwich attackers and arbitrage
competitions. Moreover, conceptually speaking, a blockchain is one database,
and we find little reason to partition this database into multiple competing
exchanges, which then necessarily require price synchronization through
arbitrage.
<br />This paper shows that DEX arbitrage and trade routing among similar AMMs can
be performed efficiently and atomically on-chain within smart contracts. These
insights lead us to create a new AMM design, an Automated Arbitrage Market
Maker, short A2MM DEX. A2MM aims to unite multiple AMMs to reduce overheads,
costs and increase blockchain security. With respect to Miner Extractable Value
(MEV), A2MM serves as a decentralized design for users to atomically collect
MEV, mitigating the dangers of centralized MEV relay services.
<br />We show that A2MM offers essential security benefits. First, A2MM strengthens
the blockchain consensus security by mitigating the competitive exploitation of
MEV, therefore reducing the risks of consensus forks. A2MM reduces the network
layer overhead of competitive transactions, improves network propagation,
leading to less stale blocks and better blockchain security. Through trade
routing, A2MM reduces the predatory risks of sandwich attacks by taking
advantage of the minimum profitable victim input. A2MM also offers financial
benefits to traders. Failed swap transactions from competitive trading occupy
valuable block space, implying an upward pressure on transaction fees. Our
evaluations shows that A2MM frees up 32.8% block-space of AMM-related
transactions. In expectation, A2MM's revenue allows to reduce swap fees by 90%.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07374" title="Abstract">arXiv:2106.07374</a> [<a href="/pdf/2106.07374" title="Download PDF">pdf</a>, <a href="/format/2106.07374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based Trajectory Visualization for Text Mining of COVID-19  Biomedical Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+Y">Yeseul Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+D">Dongjun Chung</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jina Park</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+I+H">Ick Hoon Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Since the emergence of the worldwide pandemic of COVID-19, relevant research
has been published at a dazzling pace, which makes it hard to follow the
research in this area without dedicated efforts. It is practically impossible
to implement this task manually due to the high volume of the relevant
literature. Text mining has been considered to be a powerful approach to
address this challenge, especially the topic modeling, a well-known
unsupervised method that aims to reveal latent topics from the literature.
However, in spite of its potential utility, the results generated from this
approach are often investigated manually. Hence, its application to the
COVID-19 literature is not straightforward and expert knowledge is needed to
make meaningful interpretations. In order to address these challenges, we
propose a novel analytical framework for effective visualization and mining of
topic modeling results. Here we assumed that topics constituting a paper can be
positioned on an interaction map, which belongs to a high-dimensional Euclidean
space. Based on this assumption, after summarizing topics with their topic-word
distributions using the biterm topic model, we mapped these latent topics on
networks to visualize relationships among the topics. Moreover, in the proposed
approach, the change of relationships among topics can be traced using a
trajectory plot generated with different levels of word richness. These results
together provide a deeply mined and intuitive representation of relationships
among topics related to a specific research area. The application of this
proposed framework to the PubMed literature shows that our approach facilitates
understanding of the topics constituting the COVID-19 knowledge.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07378" title="Abstract">arXiv:2106.07378</a> [<a href="/pdf/2106.07378" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recommending Multiple Criteria Decision Analysis Methods with A New  Taxonomy-based Decision Support System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cinelli%2C+M">Marco Cinelli</a>, 
<a href="/search/cs?searchtype=author&query=Kadzi%C5%84ski%2C+M">Mi&#x142;osz Kadzi&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Miebs%2C+G">Grzegorz Miebs</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+M">Michael Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=S%C5%82owi%C5%84ski%2C+R">Roman S&#x142;owi&#x144;ski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Other Computer Science (cs.OH)

</div>
<p class="mathjax">We present the Multiple Criteria Decision Analysis Methods Selection Software
(MCDA-MSS). This decision support system helps analysts answering a recurring
question in decision science: Which is the most suitable Multiple Criteria
Decision Analysis method (or a subset of MCDA methods) that should be used for
a given Decision-Making Problem (DMP)?. The MCDA-MSS includes guidance to lead
decision-making processes and choose among an extensive collection (over 200)
of MCDA methods. These are assessed according to an original comprehensive set
of problem characteristics. The accounted features concern problem formulation,
preference elicitation and types of preference information, desired features of
a preference model, and construction of the decision recommendation. The
applicability of the MCDA-MSS has been tested on several case studies. The
MCDA-MSS includes the capabilities of (i) covering from very simple to very
complex DMPs, (ii) offering recommendations for DMPs that do not match any
method from the collection, (iii) helping analysts prioritize efforts for
reducing gaps in the description of the DMPs, and (iv) unveiling methodological
mistakes that occur in the selection of the methods. A community-wide
initiative involving experts in MCDA methodology, analysts using these methods,
and decision-makers receiving decision recommendations will contribute to
expansion of the MCDA-MSS.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07380" title="Abstract">arXiv:2106.07380</a> [<a href="/pdf/2106.07380" title="Download PDF">pdf</a>, <a href="/format/2106.07380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting the Popularity of Reddit Posts with AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juno Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Social media creates crucial mass changes, as popular posts and opinions cast
a significant influence on users' decisions and thought processes. For example,
the recent Reddit uprising inspired by r/wallstreetbets which had remarkable
economic impact was started with a series of posts on the thread. The
prediction of posts that may have a notable impact will allow for the
preparation of possible following trends. This study aims to develop a machine
learning model capable of accurately predicting the popularity of a Reddit
post. Specifically, the model is predicting the number of upvotes a post will
receive based on its textual content. I experimented with three different
models: a baseline linear regression model, a random forest regression model,
and a neural network. I collected Reddit post data from an online data set and
analyzed the model's performance when trained on a single subreddit and a
collection of subreddits. The results showed that the neural network model
performed the best when the loss of the models were compared. With the use of a
machine learning model to predict social trends through the reaction users have
to post, a better picture of the near future can be envisioned.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07381" title="Abstract">arXiv:2106.07381</a> [<a href="/pdf/2106.07381" title="Download PDF">pdf</a>, <a href="/format/2106.07381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Semi-supervised Multi-task Learning Approach to Classify Customer  Contact Intents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Li Dong</a>, 
<a href="/search/cs?searchtype=author&query=Spencer%2C+M+C">Matthew C. Spencer</a>, 
<a href="/search/cs?searchtype=author&query=Biagi%2C+A">Amir Biagi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in ACL-IJCNLP 2021 workshop ECNLP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the area of customer support, understanding customers' intents is a
crucial step. Machine learning plays a vital role in this type of intent
classification. In reality, it is typical to collect confirmation from customer
support representatives (CSRs) regarding the intent prediction, though it can
unnecessarily incur prohibitive cost to ask CSRs to assign existing or new
intents to the mis-classified cases. Apart from the confirmed cases with and
without intent labels, there can be a number of cases with no human curation.
This data composition (Positives + Unlabeled + multiclass Negatives) creates
unique challenges for model development. In response to that, we propose a
semi-supervised multi-task learning paradigm. In this manuscript, we share our
experience in building text-based intent classification models for a customer
support service on an E-commerce website. We improve the performance
significantly by evolving the model from multiclass classification to
semi-supervised multi-task learning by leveraging the negative cases, domain-
and task-adaptively pretrained ALBERT on customer contact texts, and a number
of un-curated data with no labels. In the evaluation, the final model boosts
the average AUC ROC by almost 20 points compared to the baseline finetuned
multiclass classification ALBERT model.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07384" title="Abstract">arXiv:2106.07384</a> [<a href="/pdf/2106.07384" title="Download PDF">pdf</a>, <a href="/format/2106.07384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoParkeR : Multi-objective Parking Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahaman%2C+M+S">Mohammad Saiedur Rahaman</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+F+D">Flora D. Salim</a>, 
<a href="/search/cs?searchtype=author&query=Turky%2C+A">Ayad Turky</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+A">Andy Song</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+J">Jeffrey Chan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junliang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Bradbrook%2C+D">Doug Bradbrook</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing parking recommendation solutions mainly focus on finding and
suggesting parking spaces based on the unoccupied options only. However, there
are other factors associated with parking spaces that can influence someone's
choice of parking such as fare, parking rule, walking distance to destination,
travel time, likelihood to be unoccupied at a given time. More importantly,
these factors may change over time and conflict with each other which makes the
recommendations produced by current parking recommender systems ineffective. In
this paper, we propose a novel problem called multi-objective parking
recommendation. We present a solution by designing a multi-objective parking
recommendation engine called MoParkeR that considers various conflicting
factors together. Specifically, we utilise a non-dominated sorting technique to
calculate a set of Pareto-optimal solutions, consisting of recommended
trade-off parking spots. We conduct extensive experiments using two real-world
datasets to show the applicability of our multi-objective recommendation
methodology.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07385" title="Abstract">arXiv:2106.07385</a> [<a href="/pdf/2106.07385" title="Download PDF">pdf</a>, <a href="/format/2106.07385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemEval-2021 Task 11: NLPContributionGraph -- Structuring Scholarly NLP  Contributions for a Research Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Souza%2C+J">Jennifer D&#x27;Souza</a>, 
<a href="/search/cs?searchtype=author&query=Auer%2C+S">S&#xf6;ren Auer</a>, 
<a href="/search/cs?searchtype=author&query=Pedersen%2C+T">Ted Pedersen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, 8 tables, In Proceedings of the Fifteenth Workshop on Semantic Evaluation SemEval-2021 (to appear)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Digital Libraries (cs.DL); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">There is currently a gap between the natural language expression of scholarly
publications and their structured semantic content modeling to enable
intelligent content search. With the volume of research growing exponentially
every year, a search feature operating over semantically structured content is
compelling. The SemEval-2021 Shared Task NLPContributionGraph (a.k.a. 'the NCG
task') tasks participants to develop automated systems that structure
contributions from NLP scholarly articles in the English language. Being the
first-of-its-kind in the SemEval series, the task released structured data from
NLP scholarly articles at three levels of information granularity, i.e. at
sentence-level, phrase-level, and phrases organized as triples toward Knowledge
Graph (KG) building. The sentence-level annotations comprised the few sentences
about the article's contribution. The phrase-level annotations were scientific
term and predicate phrases from the contribution sentences. Finally, the
triples constituted the research overview KG. For the Shared Task,
participating systems were then expected to automatically classify contribution
sentences, extract scientific terms and relations from the sentences, and
organize them as KG triples.
<br />Overall, the task drew a strong participation demographic of seven teams and
27 participants. The best end-to-end task system classified contribution
sentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While
the absolute performance to generate triples remains low, in the conclusion of
this article, the difficulty of producing such data and as a consequence of
modeling it is highlighted.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07386" title="Abstract">arXiv:2106.07386</a> [<a href="/pdf/2106.07386" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do peers share the same criteria for assessing grant applications?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hug%2C+S+E">Sven E. Hug</a>, 
<a href="/search/cs?searchtype=author&query=Ochsner%2C+M">Michael Ochsner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1, draft
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">This study examines a basic assumption of peer review, namely, the idea that
there is a consensus on evaluation criteria among peers, which is a necessary
condition for the reliability of peer judgements. Empirical evidence indicating
that there is no consensus or more than one consensus would offer an
explanation for the disagreement effect, the low inter-rater reliability
consistently observed in peer review. To investigate this basic assumption, we
have surveyed all humanities scholars in Switzerland on 23 grant review
criteria. We have employed latent class tree modelling to identify subgroups in
which scholars rated criteria similarly (i.e. latent classes) and to explore
covariates predicting class membership. We have identified two consensus
classes, two consensus-close classes, and a consensus-far class. The consensus
classes contain a core consensus (ten criteria related to knowledge gaps,
feasibility, rigour, comprehensibility and argumentation, and academic
relevance, as well as to the competence and experience of the applicant) and a
broad consensus that includes the core consensus plus eight
contribution-related criteria, such as originality. These results provide a
possible explanation for the disagreement effect. Moreover, the results are
consistent with the notion of conservatism, which holds that original research
is undervalued in peer review, while other aspects, such as methodology and
feasibility, are overweighted. The covariate analysis indicated that age and
having tenure increases from the consensus-far to the consensus-close to the
consensus classes. This suggests that the more academic experience scholars
accumulate, the more their understanding of review criteria conforms to the
social norm.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07387" title="Abstract">arXiv:2106.07387</a> [<a href="/pdf/2106.07387" title="Download PDF">pdf</a>, <a href="/format/2106.07387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An SMT Based Compositional Model to Solve a Conflict-Free Electric  Vehicle Routing Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roselli%2C+S+F">Sabino Francesco Roselli</a>, 
<a href="/search/cs?searchtype=author&query=Fabian%2C+M">Martin Fabian</a>, 
<a href="/search/cs?searchtype=author&query=%C3%85kesson%2C+K">Knut &#xc5;kesson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The Vehicle Routing Problem (VRP) is the combinatorial optimization problem
of designing routes for vehicles to visit customers in such a fashion that a
cost function, typically the number of vehicles, or the total travelled
distance is minimized. The problem finds applications in industrial scenarios,
for example where Automated Guided Vehicles run through the plant to deliver
components from the warehouse. This specific problem, henceforth called the
Electric Conflict-Free Vehicle Routing Problem (CF-EVRP), involves constraints
such as limited operating range of the vehicles, time windows on the delivery
to the customers, and limited capacity on the number of vehicles the road
segments can accommodate at the same time. Such a complex system results in a
large model that cannot easily be solved to optimality in reasonable time. We
therefore developed a compositional model that breaks down the problem into
smaller and simpler sub-problems and provides sub-optimal, feasible solutions
to the original problem. The algorithm exploits the strengths of SMT solvers,
which proved in our previous work to be an efficient approach to deal with
scheduling problems. Compared to a monolithic model for the CF-EVRP, written in
the SMT standard language and solved using a state-of-the-art SMT solver the
compositional model was found to be significantly faster.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07395" title="Abstract">arXiv:2106.07395</a> [<a href="/pdf/2106.07395" title="Download PDF">pdf</a>, <a href="/format/2106.07395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dilated filters for edge detection algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Orhei%2C+C">Ciprian Orhei</a>, 
<a href="/search/cs?searchtype=author&query=Bogdan%2C+V">Victor Bogdan</a>, 
<a href="/search/cs?searchtype=author&query=Bonchis%2C+C">Cosmin Bonchis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Edges are a basic and fundamental feature in image processing, that are used
directly or indirectly in huge amount of applications. Inspired by the
expansion of image resolution and processing power dilated convolution
techniques appeared. Dilated convolution have impressive results in machine
learning, we discuss here the idea of dilating the standard filters which are
used in edge detection algorithms. In this work we try to put together all our
previous and current results by using instead of the classical convolution
filters a dilated one. We compare the results of the edge detection algorithms
using the proposed dilation filters with original filters or custom variants.
Experimental results confirm our statement that dilation of filters have
positive impact for edge detection algorithms form simple to rather complex
algorithms.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07400" title="Abstract">arXiv:2106.07400</a> [<a href="/pdf/2106.07400" title="Download PDF">pdf</a>, <a href="/format/2106.07400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Determinantal Beam Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meister%2C+C">Clara Meister</a>, 
<a href="/search/cs?searchtype=author&query=Forster%2C+M">Martina Forster</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Beam search is a go-to strategy for decoding neural sequence models. The
algorithm can naturally be viewed as a subset optimization problem, albeit one
where the corresponding set function does not reflect interactions between
candidates. Empirically, this leads to sets often exhibiting high overlap,
e.g., strings may differ by only a single word. Yet in use-cases that call for
multiple solutions, a diverse or representative set is often desired. To
address this issue, we propose a reformulation of beam search, which we call
determinantal beam search. Determinantal beam search has a natural relationship
to determinantal point processes (DPPs), models over sets that inherently
encode intra-set interactions. By posing iterations in beam search as a series
of subdeterminant maximization problems, we can turn the algorithm into a
diverse subset selection process. In a case study, we use the string
subsequence kernel to explicitly encourage n-gram coverage in text generated
from a sequence model. We observe that our algorithm offers competitive
performance against other diverse set generation strategies in the context of
language generation, while providing a more general approach to optimizing for
diversity.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07405" title="Abstract">arXiv:2106.07405</a> [<a href="/pdf/2106.07405" title="Download PDF">pdf</a>, <a href="/format/2106.07405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An adaptive high-order surface finite element method for the  self-consistent field theory on general curved surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jiang%2C+K">Kai Jiang</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+J">Jianggang Liu</a>, 
<a href="/search/math?searchtype=author&query=Wei%2C+H">Huayi Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we develop an adaptive high-order surface finite element
method (FEM) to solve self-consistent field equations of polymers on general
curved surfaces. It is an improvement of the existing algorithm of [J. Comp.
Phys. 387: 230-244 (2019)] in which a linear surface FEM was presented to
address this problem. The high-order surface FEM is obtained by the high-order
surface geometrical approximation and high-order function space approximation.
In order to describe the sharp interface in the strong segregation system more
accurately, an adaptive FEM equipped with a novel Log marking strategy is
proposed. Compared with the traditional strategy, this new marking strategy can
not only label the elements that need to be refined or coarsened, but also give
the refined or coarsened times, which can make full use of the information of a
posterior error estimator and improve the efficiency of the adaptive algorithm.
To demonstrate the power of our approach, we investigate the self-assembled
patterns of diblock copolymers on several distinct curved surfaces. Numerical
results illustrate the efficiency of the proposed method, especially for strong
segregation systems.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07408" title="Abstract">arXiv:2106.07408</a> [<a href="/pdf/2106.07408" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Eye Tracker To Evaluate Cockpit Design -- A Flight Simulation  Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hebbar%2C+A">Archana Hebbar</a>, 
<a href="/search/cs?searchtype=author&query=Pashilkar%2C+A">Abhay Pashilkar</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+P">Pradipta Biswas</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Aviation 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper investigates applications of eye tracking in transport aircraft
design evaluations. Piloted simulations were conducted for a complete flight
profile including take off, cruise and landing flight scenario using the
transport aircraft flight simulator at CSIR National Aerospace Laboratories.
Thirty-one simulation experiments were carried out with three pilots and
engineers while recording the ocular parameters and the flight data.
Simulations were repeated for high workload conditions like flying with
degraded visibility and during stall. Pilots visual scan behaviour and workload
levels were analysed using ocular parameters; while comparing with the
statistical deviations from the desired flight path. Conditions for fatigue
were also recreated through long duration simulations and signatures for the
same from the ocular parameters were assessed. Results from the study found
correlation between the statistical inferences obtained from the ocular
parameters with those obtained from the flight path deviations. The paper also
demonstrates an evaluators console that assists the designers or evaluators for
better understanding of pilots attentional resource allocation.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07409" title="Abstract">arXiv:2106.07409</a> [<a href="/pdf/2106.07409" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3rd Place Solution for Short-video Face Parsing Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+X">XiaoFei Si</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">JiangTao Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3pages tech report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Short videos have many applications on fashion trends, hot spots, street
interviews, public education, and creative advertising. We propose an
Edge-Aware Network(EANet) that uses edge information to refine the segmentation
edge. And experiments show our proposed EANet boots up the facial parsing
results. We also use post-process like grab cut to refine and merge the parsing
results.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07410" title="Abstract">arXiv:2106.07410</a> [<a href="/pdf/2106.07410" title="Download PDF">pdf</a>, <a href="/format/2106.07410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Explainability in Deep Learning Based Natural Language Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholizadeh%2C+S">Shafie Gholizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+N">Nengfeng Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Machine learning (ML) model explainability has received growing attention,
especially in the area related to model risk and regulations. In this paper, we
reviewed and compared some popular ML model explainability methodologies,
especially those related to Natural Language Processing (NLP) models. We then
applied one of the NLP explainability methods Layer-wise Relevance Propagation
(LRP) to a NLP classification model. We used the LRP method to derive a
relevance score for each word in an instance, which is a local explainability.
The relevance scores are then aggregated together to achieve global variable
importance of the model. Through the case study, we also demonstrated how to
apply the local explainability method to false positive and false negative
instances to discover the weakness of a NLP model. These analysis can help us
to understand NLP models better and reduce the risk due to the black-box nature
of NLP models. We also identified some common issues due to the special natures
of NLP models and discussed how explainability analysis can act as a control to
detect these issues after the model has been trained.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07411" title="Abstract">arXiv:2106.07411</a> [<a href="/pdf/2106.07411" title="Download PDF">pdf</a>, <a href="/format/2106.07411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial success in closing the gap between human and machine vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geirhos%2C+R">Robert Geirhos</a>, 
<a href="/search/cs?searchtype=author&query=Narayanappa%2C+K">Kantharaju Narayanappa</a>, 
<a href="/search/cs?searchtype=author&query=Mitzkus%2C+B">Benjamin Mitzkus</a>, 
<a href="/search/cs?searchtype=author&query=Thieringer%2C+T">Tizian Thieringer</a>, 
<a href="/search/cs?searchtype=author&query=Bethge%2C+M">Matthias Bethge</a>, 
<a href="/search/cs?searchtype=author&query=Wichmann%2C+F+A">Felix A. Wichmann</a>, 
<a href="/search/cs?searchtype=author&query=Brendel%2C+W">Wieland Brendel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preliminary version of this work was presented as Oral at the 2020 NeurIPS workshop on "Shared Visual Representations in Human &amp; Machine Intelligence" (<a href="/abs/2010.08377">arXiv:2010.08377</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">A few years ago, the first CNN surpassed human performance on ImageNet.
However, it soon became clear that machines lack robustness on more challenging
test cases, a major obstacle towards deploying machines "in the wild" and
towards obtaining better computational models of human visual perception. Here
we ask: Are we making progress in closing the gap between human and machine
vision? To answer this question, we tested human observers on a broad range of
out-of-distribution (OOD) datasets, adding the "missing human baseline" by
recording 85,120 psychophysical trials across 90 participants. We then
investigated a range of promising machine learning developments that crucially
deviate from standard supervised CNNs along three axes: objective function
(self-supervised, adversarially trained, CLIP language-image training),
architecture (e.g. vision transformers), and dataset size (ranging from 1M to
1B). Our findings are threefold. (1.) The longstanding robustness gap between
humans and CNNs is closing, with the best models now matching or exceeding
human performance on most OOD datasets. (2.) There is still a substantial
image-level consistency gap, meaning that humans make different errors than
models. In contrast, most models systematically agree in their categorisation
errors, even substantially different ones like contrastive self-supervised vs.
standard supervised models. (3.) In many cases, human-to-model consistency
improves when training dataset size is increased by one to three orders of
magnitude. Our results give reason for cautious optimism: While there is still
much room for improvement, the behavioural difference between human and machine
vision is narrowing. In order to measure future progress, 17 OOD datasets with
image-level human behavioural data are provided as a benchmark here:
https://github.com/bethgelab/model-vs-human/
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07412" title="Abstract">arXiv:2106.07412</a> [<a href="/pdf/2106.07412" title="Download PDF">pdf</a>, <a href="/format/2106.07412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Counting and Sampling of Optima for the Knapsack Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bossek%2C+J">Jakob Bossek</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+A">Aneta Neumann</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+F">Frank Neumann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Computing sets of high quality solutions has gained increasing interest in
recent years. In this paper, we investigate how to obtain sets of optimal
solutions for the classical knapsack problem. We present an algorithm to count
exactly the number of optima to a zero-one knapsack problem instance. In
addition, we show how to efficiently sample uniformly at random from the set of
all global optima. In our experimental study, we investigate how the number of
optima develops for classical random benchmark instances dependent on their
generator parameters. We find that the number of global optima can increase
exponentially for practically relevant classes of instances with correlated
weights and profits which poses a justification for the considered exact
counting problem.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07413" title="Abstract">arXiv:2106.07413</a> [<a href="/pdf/2106.07413" title="Download PDF">pdf</a>, <a href="/format/2106.07413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IncBL: Incremental Bug Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhou Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jieke Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Numerous efforts have been invested in improving the effectiveness of bug
localization techniques, whereas little attention is paid to making these tools
run more efficiently in continuously evolving software repositories. This paper
first analyzes the information retrieval model behind a classic bug
localization tool, BugLocator, and builds a mathematical foundation that the
model can be updated incrementally when codebase or bug reports evolve. Then,
we present IncBL, a tool for Incremental Bug Localization in evolving software
repositories. IncBL is evaluated on the Bugzbook dataset, and the results show
that IncBL can significantly reduce the running time by 77.79% on average
compared with re-computing the model, while maintaining the same level of
accuracy. We also implement IncBL as a Github App that can be easily integrated
into open-source projects on Github, and users can also deploy and use IncBL
locally. The demo video for IncBL can be viewed at
https://youtu.be/G4gMuvlJSb0, and the source code can be found at
https://github.com/soarsmu/IncBL
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07415" title="Abstract">arXiv:2106.07415</a> [<a href="/pdf/2106.07415" title="Download PDF">pdf</a>, <a href="/format/2106.07415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accumulative Iterative Codes Based on Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perotti%2C+A+G">Alberto G. Perotti</a>, 
<a href="/search/cs?searchtype=author&query=Popovic%2C+B+M">Branislav M. Popovic</a>, 
<a href="/search/cs?searchtype=author&query=Safavi%2C+A+R">Anahid R. Safavi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 8 figures, 4 tables; submitted to IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The Accumulative Iterative Code (AIC) proposed in this work is a new error
correcting code for channels with feedback. AIC sends the information message
to the receiver in a number of transmissions, where the initial transmission
contains the uncoded message and each subsequent transmission informs the
receiver about the locations of the errors that corrupted the previous
transmission. Error locations are determined based on the forward channel
output, which is made available to the transmitter through the feedback
channel.
<br />AIC achieves arbitrarily low error rates, thereby being suitablefor
applications demanding extremely high reliability. In the same time, AIC
achieves spectral efficiencies very close to the channel capacity in a wide
range of signal-to-noise ratios even for transmission of short information
messages.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07417" title="Abstract">arXiv:2106.07417</a> [<a href="/pdf/2106.07417" title="Download PDF">pdf</a>, <a href="/format/2106.07417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Estimation of Resource Overload Risk in 5G Multi-Tenancy Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamad%2C+Y+S">Yasameen Shihab Hamad</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bin Han</a>, 
<a href="/search/cs?searchtype=author&query=ucan%2C+O+N">Osman Nuri ucan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at ESREL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The technology of network slicing, as the most characteristic feature of the
fifth generation (5G) wireless networks, manages the resources and network
functions in heterogeneous and logically isolated slices on the top of a shared
physical infrastructure, where every slice can be independently customized to
fulfill the specific requirements of its devoted service type. It enables a new
paradigm of multi-tenancy networking, where the network slices can be leased by
the mobile network operator (MNO) to tenants in form of public cloud computing
service, known as Slice-asa- Service (SlaaS). Similar to classical cloud
computing scenarios, SlaaS benefits from overbooking its resources to numerous
tenants, taking advantage of the resource elasticity and diversity, at a price
of risking overloading network resources and violating the service-level
agreements (SLAs), which stipulate the quality of service (QoS) that shall be
guaranteed to the network slices. Thus, it becomes a critical challenge to the
MNOs, accurately estimating the resource overload risk - especially under the
sophisticated network dynamics - for monitoring and enhancing the reliability
of SlaaS business.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07419" title="Abstract">arXiv:2106.07419</a> [<a href="/pdf/2106.07419" title="Download PDF">pdf</a>, <a href="/format/2106.07419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low cost cloud based remote microscopy for biological sciences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baudin%2C+P+V">Pierre V Baudin</a>, 
<a href="/search/cs?searchtype=author&query=Ly%2C+V+T">Victoria T Ly</a>, 
<a href="/search/cs?searchtype=author&query=Pansodtee%2C+P">Pattawong Pansodtee</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+E+A">Erik A Jung</a>, 
<a href="/search/cs?searchtype=author&query=Currie%2C+R">Robert Currie</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+R">Ryan Hoffman</a>, 
<a href="/search/cs?searchtype=author&query=Willsey%2C+H+R">Helen Rankin Willsey</a>, 
<a href="/search/cs?searchtype=author&query=Pollen%2C+A+A">Alex A Pollen</a>, 
<a href="/search/cs?searchtype=author&query=Nowakowski%2C+T+J">Tomasz J Nowakowski</a>, 
<a href="/search/cs?searchtype=author&query=Haussler%2C+D">David Haussler</a>, 
<a href="/search/cs?searchtype=author&query=Mostajo-Radji%2C+M+A">Mohammed Andres Mostajo-Radji</a>, 
<a href="/search/cs?searchtype=author&query=Salama%2C+S">Sofie Salama</a>, 
<a href="/search/cs?searchtype=author&query=Teodorescu%2C+M">Mircea Teodorescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The authors Pierre V Baudin and Victoria T Ly contributed equally to this work. 21 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">A low cost remote imaging platform for biological applications was developed.
The "Picroscope" is a device that allows the user to perform longitudinal
imaging studies on multi-well cell culture plates. Here we present the network
architecture and software used to facilitate communication between modules
within the device as well as external cloud services. A web based console was
created to control the device and view experiment results. Post processing
tools were developed to analyze captured data in the cloud. The result is a
platform for controlling biological experiments from outside the lab.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07423" title="Abstract">arXiv:2106.07423</a> [<a href="/pdf/2106.07423" title="Download PDF">pdf</a>, <a href="/format/2106.07423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ETICA: Efficient Two-Level I/O Caching Architecture for Virtualized  Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmadian%2C+S">Saba Ahmadian</a>, 
<a href="/search/cs?searchtype=author&query=Salkhordeh%2C+R">Reza Salkhordeh</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>, 
<a href="/search/cs?searchtype=author&query=Asadi%2C+H">Hossein Asadi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Parallel and Distributed Systems (Volume: 32,
  Issue: 10, Oct. 1 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In this paper, we propose an Efficient Two-Level I/O Caching Architecture
(ETICA) for virtualized platforms that can significantly improve I/O latency,
endurance, and cost (in terms of cache size) while preserving the reliability
of write-pending data blocks. As opposed to previous one-level I/O caching
schemes in virtualized platforms, our proposed architecture 1) provides two
levels of cache by employing both Dynamic Random-Access Memory (DRAM) and SSD
in the I/O caching layer of virtualized platforms and 2) effectively partitions
the cache space between running VMs to achieve maximum performance and minimum
cache size. To manage the two-level cache, unlike the previous reuse distance
calculation schemes such as Useful Reuse Distance (URD), which only consider
the request type and neglect the impact of cache write policy, we propose a new
metric, Policy Optimized reuse Distance (POD). The key idea of POD is to
effectively calculate the reuse distance and estimate the amount of two-level
DRAM+SSD cache space to allocate by considering both 1) the request type and 2)
the cache write policy. Doing so results in enhanced performance and reduced
cache size due to the allocation of cache blocks only for the requests that
would be served by the I/O cache. ETICA maintains the reliability of
write-pending data blocks and improves performance by 1) assigning an effective
and fixed write policy at each level of the I/O cache hierarchy and 2)
employing effective promotion and eviction methods between cache levels. Our
extensive experiments conducted with a real implementation of the proposed
two-level storage caching architecture show that ETICA provides 45% higher
performance, compared to the state-of-the-art caching schemes in virtualized
platforms, while improving both cache size and SSD endurance by 51.7% and
33.8%, respectively.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07428" title="Abstract">arXiv:2106.07428</a> [<a href="/pdf/2106.07428" title="Download PDF">pdf</a>, <a href="/format/2106.07428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio Attacks and Defenses against AED Systems - A Practical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+R+d">Rodrigo dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Nilizadeh%2C+S">Shirin Nilizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Audio Event Detection (AED) Systems capture audio from the environment and
employ some deep learning algorithms for detecting the presence of a specific
sound of interest. In this paper, we evaluate deep learning-based AED systems
against evasion attacks through adversarial examples. We run multiple security
critical AED tasks, implemented as CNNs classifiers, and then generate audio
adversarial examples using two different types of noise, namely background and
white noise, that can be used by the adversary to evade detection. We also
examine the robustness of existing third-party AED capable devices, such as
Nest devices manufactured by Google, which run their own black-box deep
learning models.
<br />We show that an adversary can focus on audio adversarial inputs to cause AED
systems to misclassify, similarly to what has been previously done by works
focusing on adversarial examples from the image domain. We then, seek to
improve classifiers' robustness through countermeasures to the attacks. We
employ adversarial training and a custom denoising technique. We show that
these countermeasures, when applied to audio input, can be successful, either
in isolation or in combination, generating relevant increases of nearly fifty
percent in the performance of the classifiers when these are under attack.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07431" title="Abstract">arXiv:2106.07431</a> [<a href="/pdf/2106.07431" title="Download PDF">pdf</a>, <a href="/format/2106.07431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRASH: Raw Audio Score-based Generative Modeling for Controllable  High-resolution Drum Sound Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rouard%2C+S">Simon Rouard</a>, 
<a href="/search/cs?searchtype=author&query=Hadjeres%2C+G">Ga&#xeb;tan Hadjeres</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we propose a novel score-base generative model for
unconditional raw audio synthesis. Our proposal builds upon the latest
developments on diffusion process modeling with stochastic differential
equations, which already demonstrated promising results on image generation. We
motivate novel heuristics for the choice of the diffusion processes better
suited for audio generation, and consider the use of a conditional U-Net to
approximate the score function. While previous approaches on diffusion models
on audio were mainly designed as speech vocoders in medium resolution, our
method termed CRASH (Controllable Raw Audio Synthesis with High-resolution)
allows us to generate short percussive sounds in 44.1kHz in a controllable way.
Through extensive experiments, we showcase on a drum sound generation task the
numerous sampling schemes offered by our method (unconditional generation,
deterministic generation, inpainting, interpolation, variations,
class-conditional sampling) and propose the class-mixing sampling, a novel way
to generate "hybrid" sounds. Our proposed method closes the gap with GAN-based
methods on raw audio, while offering more flexible generation capabilities with
lighter and easier-to-train models.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07432" title="Abstract">arXiv:2106.07432</a> [<a href="/pdf/2106.07432" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information exchange, meaning and redundancy generation in anticipatory  systems: self-organization of expectations -- the case of Covid-19
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ivanova%2C+I+A">Inga A. Ivanova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">When studying the evolution of complex systems one refers to model
representations comprising various descriptive parameters. There is hardly
research where system evolution is described on the base of information flows
in the system. The paper focuses on the link between the dynamics of
information and system evolution. Information, exchanged between different
system's parts, before being processed is first provided with meaning by the
system. Meanings are generated from the perspective of hindsight, i.e. against
the arrow of time. The same information can be differently interpreted by
different system's parts (i,e,provided with different meanings) so that the
number of options for possible system development is proliferated. Some options
eventually turn into observable system states. So that system evolutionary
dynamics can be considered as due to information processing within the system.
This process is considered here in a model representation. The model under
study is Triple Helix (TH) model, which was earlier used to describe
interactions between university, industry and government to foster innovations.
In TH model the system is comprised of three interacting parts where each part
process information ina different way. The model is not limited to the sphere
of innovation and can be used in a broader perspective. Here TH is
conceptualized in the framework of three compertment model used to describe
infectious disease. The paper demonstrates how the dynamics of information and
meaning can be incorporated in the description of Covid-19 infectious
propagation. The results show correspondence of model predictions with
observable infection dynamics.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07435" title="Abstract">arXiv:2106.07435</a> [<a href="/pdf/2106.07435" title="Download PDF">pdf</a>, <a href="/format/2106.07435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Both Rates of Fake News and Fact-based News on Twitter Negatively  Correlate with the State-level COVID-19 Vaccine Uptake
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+H">Hanjia Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zihe Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiebo Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">There is evidence of misinformation in the online discourses and discussions
about the COVID-19 vaccines. Using a sample of 1.6 million geotagged English
tweets and the data from the CDC COVID Data Tracker, we conduct a quantitative
study to understand the influence of both misinformation and fact-based news on
Twitter on the COVID-19 vaccine uptake in the U.S. from April 19 when U.S.
adults were vaccine eligible to May 7, 2021, after controlling state-level
factors such as demographics, education, and the pandemic severity. We identify
the tweets related to either misinformation or fact-based news by analyzing the
URLs. By analyzing the content of the most frequent tweets of these two groups,
we find that their structures are similar, making it difficult for Twitter
users to distinguish one from another by reading the text alone. The users who
spread both fake news and fact-based news tend to show a negative attitude
towards the vaccines. We further conduct the Fama-MacBeth regression with the
Newey-West adjustment to examine the effect of fake-news-related and
fact-related tweets on the vaccination rate, and find marginally negative
correlations.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07441" title="Abstract">arXiv:2106.07441</a> [<a href="/pdf/2106.07441" title="Download PDF">pdf</a>, <a href="/format/2106.07441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatically eliminating seam lines with Poisson editing in complex  relative radiometric normalization mosaicking scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+J">Jie Lian</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+X">Xuchen Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuze Tian</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Hongwei Duan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Relative radiometric normalization (RRN) mosaicking among multiple remote
sensing images is crucial for the downstream tasks, including map-making, image
recognition, semantic segmentation, and change detection. However, there are
often seam lines on the mosaic boundary and radiometric contrast left,
especially in complex scenarios, making the appearance of mosaic images
unsightly and reducing the accuracy of the latter classification/recognition
algorithms. This paper renders a novel automatical approach to eliminate seam
lines in complex RRN mosaicking scenarios. It utilizes the histogram matching
on the overlap area to alleviate radiometric contrast, Poisson editing to
remove the seam lines, and merging procedure to determine the normalization
transfer order. Our method can handle the mosaicking seam lines with arbitrary
shapes and images with extreme topological relationships (with a small
intersection area). These conditions make the main feathering or blending
methods, e.g., linear weighted blending and Laplacian pyramid blending,
unavailable. In the experiment, our approach visually surpasses the automatic
methods without Poisson editing and the manual blurring and feathering method
using GIMP software.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07442" title="Abstract">arXiv:2106.07442</a> [<a href="/pdf/2106.07442" title="Download PDF">pdf</a>, <a href="/format/2106.07442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latency-Constrained Prediction of mmWave/THz Link Blockages through  Meta-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kal%C3%B8r%2C+A+E">Anders E. Kal&#xf8;r</a>, 
<a href="/search/cs?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>, 
<a href="/search/cs?searchtype=author&query=Popovski%2C+P">Petar Popovski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Wireless applications that use high-reliability low-latency links depend
critically on the capability of the system to predict link quality. This
dependence is especially acute at the high carrier frequencies used by mmWave
and THz systems, where the links are susceptible to blockages. Predicting
blockages with high reliability requires a large number of data samples to
train effective machine learning modules. With the aim of mitigating data
requirements, we introduce a framework based on meta-learning, whereby data
from distinct deployments are leveraged to optimize a shared initialization
that decreases the data set size necessary for any new deployment. Predictors
of two different events are studied: (1) at least one blockage occurs in a time
window, and (2) the link is blocked for the entire time window. The results
show that an RNN-based predictor trained using meta-learning is able to predict
blockages after observing fewer samples than predictors trained using standard
methods.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07445" title="Abstract">arXiv:2106.07445</a> [<a href="/pdf/2106.07445" title="Download PDF">pdf</a>, <a href="/format/2106.07445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PopSkipJump: Decision-Based Attack for Probabilistic Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simon-Gabriel%2C+C">Carl-Johann Simon-Gabriel</a>, 
<a href="/search/cs?searchtype=author&query=Sheikh%2C+N+A">Noman Ahmed Sheikh</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML'21. Code available at <a href="https://github.com/cjsg/PopSkipJump">this https URL</a> . 9 pages &amp; 7 figures in main part, 14 pages &amp; 10 figures in appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Most current classifiers are vulnerable to adversarial examples, small input
perturbations that change the classification output. Many existing attack
algorithms cover various settings, from white-box to black-box classifiers, but
typically assume that the answers are deterministic and often fail when they
are not. We therefore propose a new adversarial decision-based attack
specifically designed for classifiers with probabilistic outputs. It is based
on the HopSkipJump attack by Chen et al. (2019, <a href="/abs/1904.02144">arXiv:1904.02144v5</a> ), a strong
and query efficient decision-based attack originally designed for deterministic
classifiers. Our P(robabilisticH)opSkipJump attack adapts its amount of queries
to maintain HopSkipJump's original output quality across various noise levels,
while converging to its query efficiency as the noise level decreases. We test
our attack on various noise models, including state-of-the-art off-the-shelf
randomized defenses, and show that they offer almost no extra robustness to
decision-based attacks. Code is available at
https://github.com/cjsg/PopSkipJump .
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07447" title="Abstract">arXiv:2106.07447</a> [<a href="/pdf/2106.07447" title="Download PDF">pdf</a>, <a href="/format/2106.07447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HuBERT: Self-Supervised Speech Representation Learning by Masked  Prediction of Hidden Units
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+W">Wei-Ning Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Bolte%2C+B">Benjamin Bolte</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y+H">Yao-Hung Hubert Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Lakhotia%2C+K">Kushal Lakhotia</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+A">Abdelrahman Mohamed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Self-supervised approaches for speech representation learning are challenged
by three unique problems: (1) there are multiple sound units in each input
utterance, (2) there is no lexicon of input sound units during the pre-training
phase, and (3) sound units have variable lengths with no explicit segmentation.
To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)
approach for self-supervised speech representation learning, which utilizes an
offline clustering step to provide aligned target labels for a BERT-like
prediction loss. A key ingredient of our approach is applying the prediction
loss over the masked regions only, which forces the model to learn a combined
acoustic and language model over the continuous inputs. HuBERT relies primarily
on the consistency of the unsupervised clustering step rather than the
intrinsic quality of the assigned cluster labels. Starting with a simple
k-means teacher of 100 clusters, and using two iterations of clustering, the
HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0
performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with
10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,
HuBERT shows up to 19% and 13% relative WER reduction on the more challenging
dev-other and test-other evaluation subsets.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07448" title="Abstract">arXiv:2106.07448</a> [<a href="/pdf/2106.07448" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel mapping for visual to auditory sensory substitution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehrbani%2C+E">Ezsan Mehrbani</a>, 
<a href="/search/cs?searchtype=author&query=Mirhoseini%2C+S+F">Sezedeh Fatemeh Mirhoseini</a>, 
<a href="/search/cs?searchtype=author&query=Riahi%2C+N">Noushin Riahi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">visual information can be converted into audio stream via sensory
substitution devices in order to give visually impaired people the chance of
perception of their surrounding easily and simultaneous to performing everyday
tasks. In this study, visual environmental features namely, coordinate, type of
objects and their size are assigned to audio features related to music tones
such as frequency, time duration and note permutations. Results demonstrated
that this new method has more training time efficiency in comparison with our
previous method named VBTones which sinusoidal tones were applied. Moreover,
results in blind object recognition for real objects was achieved 88.05 on
average.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07449" title="Abstract">arXiv:2106.07449</a> [<a href="/pdf/2106.07449" title="Download PDF">pdf</a>, <a href="/format/2106.07449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Methodology For Creating Information Flow Specifications of Hardware  Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deutschbein%2C+C">Calvin Deutschbein</a>, 
<a href="/search/cs?searchtype=author&query=Meza%2C+A">Andres Meza</a>, 
<a href="/search/cs?searchtype=author&query=Restuccia%2C+F">Francesco Restuccia</a>, 
<a href="/search/cs?searchtype=author&query=Kastner%2C+R">Ryan Kastner</a>, 
<a href="/search/cs?searchtype=author&query=Sturton%2C+C">Cynthia Sturton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, submitted to ICCAD 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">We present a methodology for creating information flow specifications of
hardware designs. Such specifications can help designers better understand
their design and are necessary for security validation processes. By combining
information flow tracking and specification mining, we are able to produce
information flow properties of a design without prior knowledge of security
agreements or specifications. We develop a tool, Isadora, to evaluate our
methodology. We demonstrate Isadora may define the information flows within an
access control module in isolation and within an SoC and over a RISC-V design.
Over the access control module, Isadora mined output completely covers an
assertion based security specification of the design provided by the designers.
For both the access control module and RISC-V, we sample Isadora output
properties and find 10 out of 10 and 8 out of 10 properties, respectively,
define the design behavior to relevant to a Common Weakness Enumeration (CWE).
We find our methodology may independently mine security properties manually
developed by hardware designers, automatically generate properties describing
CWEs over a design, and scale to SoC and CPU designs.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07451" title="Abstract">arXiv:2106.07451</a> [<a href="/pdf/2106.07451" title="Download PDF">pdf</a>, <a href="/format/2106.07451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PI-GNN: A Novel Perspective on Semi-Supervised Node Classification  against Noisy Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xuefeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+T">Tian Bian</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+Y">Yu Rong</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tingyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenbing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junzhou Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Semi-supervised node classification, as a fundamental problem in graph
learning, leverages unlabeled nodes along with a small portion of labeled nodes
for training. Existing methods rely heavily on high-quality labels, which,
however, are expensive to obtain in real-world applications since certain
noises are inevitably involved during the labeling process. It hence poses an
unavoidable challenge for the learning algorithm to generalize well. In this
paper, we propose a novel robust learning objective dubbed pairwise
interactions (PI) for the model, such as Graph Neural Network (GNN) to combat
noisy labels. Unlike classic robust training approaches that operate on the
pointwise interactions between node and class label pairs, PI explicitly forces
the embeddings for node pairs that hold a positive PI label to be close to each
other, which can be applied to both labeled and unlabeled nodes. We design
several instantiations for PI labels based on the graph structure and the node
class labels, and further propose a new uncertainty-aware training technique to
mitigate the negative effect of the sub-optimal PI labels. Extensive
experiments on different datasets and GNN architectures demonstrate the
effectiveness of PI, yielding a promising improvement over the state-of-the-art
methods.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07453" title="Abstract">arXiv:2106.07453</a> [<a href="/pdf/2106.07453" title="Download PDF">pdf</a>, <a href="/format/2106.07453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Data-specific Model Search for Collaborative Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Q">Quanming Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Depeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> KDD 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Collaborative filtering (CF), as a fundamental approach for recommender
systems, is usually built on the latent factor model with learnable parameters
to predict users' preferences towards items. However, designing a proper CF
model for a given data is not easy, since the properties of datasets are highly
diverse. In this paper, motivated by the recent advances in automated machine
learning (AutoML), we propose to design a data-specific CF model by AutoML
techniques. The key here is a new framework that unifies state-of-the-art
(SOTA) CF methods and splits them into disjoint stages of input encoding,
embedding function, interaction function, and prediction function. We further
develop an easy-to-use, robust, and efficient search strategy, which utilizes
random search and a performance predictor for efficient searching within the
above framework. In this way, we can combinatorially generalize data-specific
CF models, which have not been visited in the literature, from SOTA ones.
Extensive experiments on five real-world datasets demonstrate that our method
can consistently outperform SOTA ones for various CF tasks. Further experiments
verify the rationality of the proposed framework and the efficiency of the
search strategy. The searched CF models can also provide insights for exploring
more effective methods in the future
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07455" title="Abstract">arXiv:2106.07455</a> [<a href="/pdf/2106.07455" title="Download PDF">pdf</a>, <a href="/format/2106.07455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilient and Distributed Discrete Optimal Transport with Deceptive  Adversary: A Game-Theoretic Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hughes%2C+J">Jason Hughes</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Juntao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long version of paper of the same title in Control System Letters (L-CSS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Optimal transport (OT) is a framework that can be used to guide the optimal
allocation of a limited amount of resources. The classical OT paradigm does not
consider malicious attacks in its formulation and thus the designed transport
plan lacks resiliency to an adversary. To address this concern, we establish an
OT framework that explicitly accounts for the adversarial and stealthy
manipulation of participating nodes in the network during the transport
strategy design. Specifically, we propose a game-theoretic approach to capture
the strategic interactions between the transport planner and the deceptive
attacker. We analyze the properties of the established two-person zero-sum game
thoroughly. We further develop a fully distributed algorithm to compute the
optimal resilient transport strategies, and show the convergence of the
algorithm to a saddle-point equilibrium. Finally, we demonstrate the
effectiveness of the designed algorithm using case studies.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07456" title="Abstract">arXiv:2106.07456</a> [<a href="/pdf/2106.07456" title="Download PDF">pdf</a>, <a href="/format/2106.07456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending the RISC-V ISA for exploring advanced reconfigurable SIMD  instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papaphilippou%2C+P">Philippos Papaphilippou</a>, 
<a href="/search/cs?searchtype=author&query=Kelly%2C+P+H+J">Paul H. J. Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Luk%2C+W">Wayne Luk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Fifth Workshop on Computer Architecture Research with RISC-V (CARRV 2021), co-located with ISCA 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">This paper presents a novel, non-standard set of vector instruction types for
exploring custom SIMD instructions in a softcore. The new types allow
simultaneous access to a relatively high number of operands, reducing the
instruction count where applicable. Additionally, a high-performance
open-source RISC-V (RV32 IM) softcore is introduced, optimised for exploring
custom SIMD instructions and streaming performance. By providing instruction
templates for instruction development in HDL/Verilog, efficient FPGA-based
instructions can be developed with few low-level lines of code. In order to
improve custom SIMD instruction performance, the softcore's cache hierarchy is
optimised for bandwidth, such as with very wide blocks for the last-level
cache. The approach is demonstrated on example memory-intensive applications on
an FPGA. Although the exploration is based on the softcore, the goal is to
provide a means to experiment with advanced SIMD instructions which could be
loaded in future CPUs that feature reconfigurable regions as custom
instructions. Finally, we provide some insights on the challenges and
effectiveness of such future micro-architectures.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07459" title="Abstract">arXiv:2106.07459</a> [<a href="/pdf/2106.07459" title="Download PDF">pdf</a>, <a href="/format/2106.07459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Piercing All Translates of a Set of Axis-Parallel Rectangles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dumitrescu%2C+A">Adrian Dumitrescu</a>, 
<a href="/search/cs?searchtype=author&query=Tkadlec%2C+J">Josef Tkadlec</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, to appear in Proceedings of IWOCA 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">For a given shape $S$ in the plane, one can ask what is the lowest possible
density of a point set $P$ that pierces (``intersects'', ``hits'') all
translates of $S$. This is equivalent to determining the covering density of
$S$ and as such is well studied. Here we study the analogous question for
families of shapes where the connection to covering no longer exists. That is,
we require that a single point set $P$ simultaneously pierces each translate of
each shape from some family $\mathcal F$. We denote the lowest possible density
of such an $\mathcal F$-piercing point set by $\pi_T(\mathcal F)$.
Specifically, we focus on families $\mathcal F$ consisting of axis-parallel
rectangles. When $|\mathcal F|=2$ we exactly solve the case when one rectangle
is more squarish than $2\times 1$, and give bounds (within $10\,\%$ of each
other) for the remaining case when one rectangle is wide and the other one is
tall. When $|\mathcal F|\ge 2$ we present a linear-time constant-factor
approximation algorithm for computing $\pi_T(\mathcal F)$ (with ratio $1.895$).
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07464" title="Abstract">arXiv:2106.07464</a> [<a href="/pdf/2106.07464" title="Download PDF">pdf</a>, <a href="/format/2106.07464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Interpretive Learning as Metarule Specialisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patsantzis%2C+S">Stassa Patsantzis</a>, 
<a href="/search/cs?searchtype=author&query=Muggleton%2C+S+H">Stephen H. Muggleton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages. Submitted to the Machine Learning Journal Special Issue on Learning and Reasoning on June 1st, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In Meta-Interpretive Learning (MIL) the metarules, second-order datalog
clauses acting as inductive bias, are manually defined by the user. In this
work we show that second-order metarules for MIL can be learned by MIL. We
define a generality ordering of metarules by $\theta$-subsumption and show that
user-defined sort metarules are derivable by specialisation of the most-general
matrix metarules in a language class; and that these matrix metarules are in
turn derivable by specialisation of third-order punch metarules with variables
that range over the set of second-order literals and for which only an upper
bound on their number of literals need be user-defined. We show that the
cardinality of a metarule language is polynomial in the number of literals in
punch metarules. We re-frame MIL as metarule specialisation by resolution. We
modify the MIL metarule specialisation operator to return new metarules rather
than first-order clauses and prove the correctness of the new operator. We
implement the new operator as TOIL, a sub-system of the MIL system Louise. Our
experiments show that as user-defined sort metarules are progressively replaced
by sort metarules learned by TOIL, Louise's predictive accuracy is maintained
at the cost of a small increase in training times. We conclude that
automatically derived metarules can replace user-defined metarules.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07468" title="Abstract">arXiv:2106.07468</a> [<a href="/pdf/2106.07468" title="Download PDF">pdf</a>, <a href="/format/2106.07468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the stability of conservative discontinuous Galerkin/Hermite spectral  methods for the Vlasov-Poisson system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bessemoulin-Chatard%2C+M">Marianne Bessemoulin-Chatard</a> (LMJL), 
<a href="/search/math?searchtype=author&query=Filbet%2C+F">Francis Filbet</a> (IMT)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2004.02685">arXiv:2004.02685</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We study a class of spatial discretizations for the Vlasov-Poisson system
written as an hyperbolic system using Hermite polynomials. In particular, we
focus on spectral methods and discontinuous Galerkin approximations. To obtain
L 2 stability properties, we introduce a new L 2 weighted space, with a time
dependent weight. For the Hermite spectral form of the Vlasov-Poisson system,
we prove conservation of mass, momentum and total energy, as well as global
stability for the weighted L 2 norm. These properties are then discussed for
several spatial discretizations. Finally, numerical simulations are performed
with the proposed DG/Hermite spectral method to highlight its stability and
conservation features.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07470" title="Abstract">arXiv:2106.07470</a> [<a href="/pdf/2106.07470" title="Download PDF">pdf</a>, <a href="/format/2106.07470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing vector fields across surfaces: interest for characterizing the  orientations of cortical folds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bohi%2C+A">Amine Bohi</a>, 
<a href="/search/cs?searchtype=author&query=Auzias%2C+G">Guillaume Auzias</a>, 
<a href="/search/cs?searchtype=author&query=Lef%C3%A8vre%2C+J">Julien Lef&#xe8;vre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Differential Geometry (math.DG); Biological Physics (physics.bio-ph); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Vectors fields defined on surfaces constitute relevant and useful
representations but are rarely used. One reason might be that comparing vector
fields across two surfaces of the same genus is not trivial: it requires to
transport the vector fields from the original surfaces onto a common domain. In
this paper, we propose a framework to achieve this task by mapping the vector
fields onto a common space, using some notions of differential geometry. The
proposed framework enables the computation of statistics on vector fields. We
demonstrate its interest in practice with an application on real data with a
quantitative assessment of the reproducibility of curvature directions that
describe the complex geometry of cortical folding patterns. The proposed
framework is general and can be applied to different types of vector fields and
surfaces, allowing for a large number of high potential applications in medical
imaging.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07471" title="Abstract">arXiv:2106.07471</a> [<a href="/pdf/2106.07471" title="Download PDF">pdf</a>, <a href="/format/2106.07471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signal processing on simplicial complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schaub%2C+M+T">Michael T. Schaub</a>, 
<a href="/search/cs?searchtype=author&query=Seby%2C+J">Jean-Baptiste Seby</a>, 
<a href="/search/cs?searchtype=author&query=Frantzen%2C+F">Florian Frantzen</a>, 
<a href="/search/cs?searchtype=author&query=Roddenberry%2C+T+M">T. Mitchell Roddenberry</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Segarra%2C+S">Santiago Segarra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages; 5 figures. arXiv admin note: text overlap with <a href="/abs/2101.05510">arXiv:2101.05510</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Discrete Mathematics (cs.DM); Machine Learning (cs.LG); Systems and Control (eess.SY); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Higher-order networks have so far been considered primarily in the context of
studying the structure of complex systems, i.e., the higher-order or multi-way
relations connecting the constituent entities. More recently, a number of
studies have considered dynamical processes that explicitly ac- count for such
higher-order dependencies, e.g., in the context of epidemic spreading processes
or opinion formation. In this chapter, we focus on a closely related, but
distinct third perspective: how can we use higher-order relationships to
process signals and data supported on higher-order network structures. In
particular, we survey how ideas from signal processing of data supported on
regular domains, such as time series or images, can be extended to graphs and
simplicial complexes. We discuss Fourier analysis, signal denois- ing, signal
interpolation, and nonlinear processing through neural networks based on
simplicial complexes. Key to our developments is the Hodge Laplacian matrix, a
multi-relational operator that leverages the special structure of simplicial
complexes and generalizes desirable properties of the Laplacian matrix in graph
signal processing.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07472" title="Abstract">arXiv:2106.07472</a> [<a href="/pdf/2106.07472" title="Download PDF">pdf</a>, <a href="/ps/2106.07472" title="Download PostScript">ps</a>, <a href="/format/2106.07472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of a Target-Based Actor-Critic Algorithm with Linear Function  Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barakat%2C+A">Anas Barakat</a>, 
<a href="/search/cs?searchtype=author&query=Bianchi%2C+P">Pascal Bianchi</a>, 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+J">Julien Lehmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Actor-critic methods integrating target networks have exhibited a stupendous
empirical success in deep reinforcement learning. However, a theoretical
understanding of the use of target networks in actor-critic methods is largely
missing in the literature. In this paper, we bridge this gap between theory and
practice by proposing the first theoretical analysis of an online target-based
actor-critic algorithm with linear function approximation in the discounted
reward setting. Our algorithm uses three different timescales: one for the
actor and two for the critic. Instead of using the standard single timescale
temporal difference (TD) learning algorithm as a critic, we use a two
timescales target-based version of TD learning closely inspired from practical
actor-critic algorithms implementing target networks. First, we establish
asymptotic convergence results for both the critic and the actor under
Markovian sampling. Then, we provide a finite-time analysis showing the impact
of incorporating a target network into actor-critic methods.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07473" title="Abstract">arXiv:2106.07473</a> [<a href="/pdf/2106.07473" title="Download PDF">pdf</a>, <a href="/format/2106.07473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Series Anomaly Detection with label-free Model Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+D">Deokwoo Jung</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+N">Nandini Ramanan</a>, 
<a href="/search/cs?searchtype=author&query=Amjadi%2C+M">Mehrnaz Amjadi</a>, 
<a href="/search/cs?searchtype=author&query=Karingula%2C+S+R">Sankeerth Rao Karingula</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+J">Jake Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Coelho%2C+C+N">Claudionor Nunes Coelho Jr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 Figure, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Anomaly detection for time-series data becomes an essential task for many
data-driven applications fueled with an abundance of data and out-of-the-box
machine-learning algorithms. In many real-world settings, developing a reliable
anomaly model is highly challenging due to insufficient anomaly labels and the
prohibitively expensive cost of obtaining anomaly examples. It imposes a
significant bottleneck to evaluate model quality for model selection and
parameter tuning reliably. As a result, many existing anomaly detection
algorithms fail to show their promised performance after deployment.
<br />In this paper, we propose LaF-AD, a novel anomaly detection algorithm with
label-free model selection for unlabeled times-series data. Our proposed
algorithm performs a fully unsupervised ensemble learning across a large number
of candidate parametric models. We develop a model variance metric that
quantifies the sensitivity of anomaly probability with a bootstrapping method.
Then it makes a collective decision for anomaly events by model learners using
the model variance. Our algorithm is easily parallelizable, more robust for
ill-conditioned and seasonal data, and highly scalable for a large number of
anomaly models. We evaluate our algorithm against other state-of-the-art
methods on a synthetic domain and a benchmark public data set.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07474" title="Abstract">arXiv:2106.07474</a> [<a href="/pdf/2106.07474" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Interpretable Machine Learning Models in Parallel  Coordinates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kovalerchuk%2C+B">Boris Kovalerchuk</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+D">Dustin Hayes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper contributes to interpretable machine learning via visual knowledge
discovery in parallel coordinates. The concepts of hypercubes and hyper-blocks
are used as easily understandable by end-users in the visual form in parallel
coordinates. The Hyper algorithm for classification with mixed and pure
hyper-blocks (HBs) is proposed to discover hyper-blocks interactively and
automatically in individual, multiple, overlapping, and non-overlapping
setting. The combination of hyper-blocks with linguistic description of visual
patterns is presented too. It is shown that Hyper models generalize decision
trees. The Hyper algorithm was tested on the benchmark data from UCI ML
repository. It allowed discovering pure and mixed HBs with all data and then
with 10-fold cross validation. The links between hyper-blocks, dimension
reduction and visualization are established. Major benefits of hyper-block
technology and the Hyper algorithm are in their ability to discover and observe
hyper-blocks by end-users including side by side visualizations making patterns
visible for all classes. Another advantage of sets of HBs relative to the
decision trees is the ability to avoid both data overgeneralization and
overfitting.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07475" title="Abstract">arXiv:2106.07475</a> [<a href="/pdf/2106.07475" title="Download PDF">pdf</a>, <a href="/format/2106.07475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating sanity checks for saliency maps with image and text  classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kokhlikyan%2C+N">Narine Kokhlikyan</a>, 
<a href="/search/cs?searchtype=author&query=Miglani%2C+V">Vivek Miglani</a>, 
<a href="/search/cs?searchtype=author&query=Alsallakh%2C+B">Bilal Alsallakh</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+M">Miguel Martin</a>, 
<a href="/search/cs?searchtype=author&query=Reblitz-Richardson%2C+O">Orion Reblitz-Richardson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Saliency maps have shown to be both useful and misleading for explaining
model predictions especially in the context of images. In this paper, we
perform sanity checks for text modality and show that the conclusions made for
image do not directly transfer to text. We also analyze the effects of the
input multiplier in certain saliency maps using similarity scores,
max-sensitivity and infidelity evaluation metrics. Our observations reveal that
the input multiplier carries input's structural patterns in explanation maps,
thus leading to similar results regardless of the choice of model parameters.
We also show that the smoothness of a Neural Network (NN) function can affect
the quality of saliency-based explanations. Our investigations reveal that
replacing ReLUs with Softplus and MaxPool with smoother variants such as
LogSumExp (LSE) can lead to explanations that are more reliable based on the
infidelity evaluation metric.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07476" title="Abstract">arXiv:2106.07476</a> [<a href="/pdf/2106.07476" title="Download PDF">pdf</a>, <a href="/format/2106.07476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Graph Neural Networks with 1000 Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guohao Li</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M">Matthias M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=Koltun%2C+V">Vladlen Koltun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICML'2021. Code available at <a href="https://www.deepgcns.org/arch/gnn1000.">this https URL</a> Work done during Guohao Li's internship at Intel Intelligent Systems Lab
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Deep graph neural networks (GNNs) have achieved excellent results on various
tasks on increasingly large graph datasets with millions of nodes and edges.
However, memory complexity has become a major obstacle when training deep GNNs
for practical applications due to the immense number of nodes, edges, and
intermediate activations. To improve the scalability of GNNs, prior works
propose smart graph sampling or partitioning strategies to train GNNs with a
smaller set of nodes or sub-graphs. In this work, we study reversible
connections, group convolutions, weight tying, and equilibrium models to
advance the memory and parameter efficiency of GNNs. We find that reversible
connections in combination with deep network architectures enable the training
of overparameterized GNNs that significantly outperform existing methods on
multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each)
and RevGNN-Wide (448 layers with 224 channels each) were both trained on a
single commodity GPU and achieve an ROC-AUC of $87.74 \pm 0.13$ and $88.14 \pm
0.15$ on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep
is the deepest GNN in the literature by one order of magnitude. Please visit
our project website https://www.deepgcns.org/arch/gnn1000 for more information.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07477" title="Abstract">arXiv:2106.07477</a> [<a href="/pdf/2106.07477" title="Download PDF">pdf</a>, <a href="/format/2106.07477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S$^2$-MLP: Spatial-Shift MLP Architecture for Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xu Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yunfeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Ping Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, visual Transformer (ViT) and its following works abandon the
convolution and exploit the self-attention operation, attaining a comparable or
even higher accuracy than CNN. More recently, MLP-Mixer abandons both the
convolution and the self-attention operation, proposing an architecture
containing only MLP layers. To achieve cross-patch communications, it devises
an additional token-mixing MLP besides the channel-mixing MLP. It achieves
promising results when training on an extremely large-scale dataset. But it
cannot achieve as outstanding performance as its CNN and ViT counterparts when
training on medium-scale datasets such as ImageNet1K and ImageNet21K. The
performance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We
discover that token-mixing operation in MLP-Mixer is a variant of depthwise
convolution with a global reception field and spatial-specific configuration.
But the global reception field and the spatial-specific property make
token-mixing MLP prone to over-fitting. In this paper, we propose a novel pure
MLP architecture, spatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our
S$^2$-MLP only contains channel-mixing MLP. We devise a spatial-shift operation
for achieving the communication between patches. It has a local reception field
and is spatial-agnostic. Meanwhile, it is parameter-free and efficient for
computation. The proposed S$^2$-MLP attains higher recognition accuracy than
MLP-Mixer when training on ImageNet-1K dataset. Meanwhile, S$^2$-MLP
accomplishes as excellent performance as ViT on ImageNet-1K dataset with
considerably simpler architecture and fewer FLOPs and parameters.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07479" title="Abstract">arXiv:2106.07479</a> [<a href="/pdf/2106.07479" title="Download PDF">pdf</a>, <a href="/format/2106.07479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Online Riemannian PCA for Stochastic Canonical Correlation Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zihang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+R">Rudrasis Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+V">Vikas Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We present an efficient stochastic algorithm (RSG+) for canonical correlation
analysis (CCA) using a reparametrization of the projection matrices. We show
how this reparametrization (into structured matrices), simple in hindsight,
directly presents an opportunity to repurpose/adjust mature techniques for
numerical optimization on Riemannian manifolds. Our developments nicely
complement existing methods for this problem which either require $O(d^3)$ time
complexity per iteration with $O(\frac{1}{\sqrt{t}})$ convergence rate (where
$d$ is the dimensionality) or only extract the top $1$ component with
$O(\frac{1}{t})$ convergence rate. In contrast, our algorithm offers a strict
improvement for this classical problem: it achieves $O(d^2k)$ runtime
complexity per iteration for extracting the top $k$ canonical components with
$O(\frac{1}{t})$ convergence rate. While the paper primarily focuses on the
formulation and technical analysis of its properties, our experiments show that
the empirical behavior on common datasets is quite promising. We also explore a
potential application in training fair models where the label of protected
attribute is missing or otherwise unavailable.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07482" title="Abstract">arXiv:2106.07482</a> [<a href="/pdf/2106.07482" title="Download PDF">pdf</a>, <a href="/format/2106.07482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Domain Adaptation: A Generative View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Ruichu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fengzhu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijian Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+P">Pengfei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Lingling Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent years have witnessed tremendous interest in deep learning on
graph-structured data. Due to the high cost of collecting labeled
graph-structured data, domain adaptation is important to supervised graph
learning tasks with limited samples. However, current graph domain adaptation
methods are generally adopted from traditional domain adaptation tasks, and the
properties of graph-structured data are not well utilized. For example, the
observed social networks on different platforms are controlled not only by the
different crowd or communities but also by the domain-specific policies and the
background noise. Based on these properties in graph-structured data, we first
assume that the graph-structured data generation process is controlled by three
independent types of latent variables, i.e., the semantic latent variables, the
domain latent variables, and the random latent variables. Based on this
assumption, we propose a disentanglement-based unsupervised domain adaptation
method for the graph-structured data, which applies variational graph
auto-encoders to recover these latent variables and disentangles them via three
supervised learning modules. Extensive experimental results on two real-world
datasets in the graph classification task reveal that our method not only
significantly outperforms the traditional domain adaptation methods and the
disentangled-based domain adaptation methods but also outperforms the
state-of-the-art graph domain adaptation algorithms.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07483" title="Abstract">arXiv:2106.07483</a> [<a href="/pdf/2106.07483" title="Download PDF">pdf</a>, <a href="/format/2106.07483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Explainable AI Explain Unfairness? A Framework for Evaluating  Explainable AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alikhademi%2C+K">Kiana Alikhademi</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+B">Brianna Richardson</a>, 
<a href="/search/cs?searchtype=author&query=Drobina%2C+E">Emma Drobina</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+J+E">Juan E. Gilbert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Many ML models are opaque to humans, producing decisions too complex for
humans to easily understand. In response, explainable artificial intelligence
(XAI) tools that analyze the inner workings of a model have been created.
Despite these tools' strength in translating model behavior, critiques have
raised concerns about the impact of XAI tools as a tool for `fairwashing` by
misleading users into trusting biased or incorrect models. In this paper, we
created a framework for evaluating explainable AI tools with respect to their
capabilities for detecting and addressing issues of bias and fairness as well
as their capacity to communicate these results to their users clearly. We found
that despite their capabilities in simplifying and explaining model behavior,
many prominent XAI tools lack features that could be critical in detecting
bias. Developers can use our framework to suggest modifications needed in their
toolkits to reduce issues likes fairwashing.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07484" title="Abstract">arXiv:2106.07484</a> [<a href="/pdf/2106.07484" title="Download PDF">pdf</a>, <a href="/format/2106.07484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conservative Integrators for Piecewise Smooth Systems with Transversal  Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hirani%2C+A+N">Anil N. Hirani</a>, 
<a href="/search/math?searchtype=author&query=Wan%2C+A+T+S">Andy T.S. Wan</a>, 
<a href="/search/math?searchtype=author&query=Wojtalewicz%2C+N">Nikolas Wojtalewicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">We introduce conservative integrators for long term integration of piecewise
smooth systems with transversal dynamics and piecewise smooth conserved
quantities. In essence, for a piecewise dynamical system with piecewise defined
conserved quantities such that its trajectories cross transversally to its
interface, we combine Mannshardt's transition scheme and the Discrete
Multiplier Method to obtain conservative integrators capable of preserving
conserved quantities up to machine precision and accuracy order. We prove that
the order of accuracy of the integrators is preserved after crossing the
discontinuity in the case of codimension one number of conserved quantities.
Numerical examples illustrate the preservation of accuracy order.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07485" title="Abstract">arXiv:2106.07485</a> [<a href="/pdf/2106.07485" title="Download PDF">pdf</a>, <a href="/ps/2106.07485" title="Download PostScript">ps</a>, <a href="/format/2106.07485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grammar Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coecke%2C+B">Bob Coecke</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+V">Vincent Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, many pictures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Category Theory (math.CT)

</div>
<p class="mathjax">Diagrammatically speaking, grammatical calculi such as pregroups provide
wires between words in order to elucidate their interactions, and this enables
one to verify grammatical correctness of phrases and sentences. In this paper
we also provide wirings within words. This will enable us to identify
grammatical constructs that we expect to be either equal or closely related.
Hence, our work paves the way for a new theory of grammar, that provides novel
`grammatical truths'. We give a nogo-theorem for the fact that our wirings for
words make no sense for preordered monoids, the form which grammatical calculi
usually take. Instead, they require diagrams -- or equivalently, (free)
monoidal categories.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07487" title="Abstract">arXiv:2106.07487</a> [<a href="/pdf/2106.07487" title="Download PDF">pdf</a>, <a href="/format/2106.07487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> pix2rule: End-to-end Neuro-symbolic Rule Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cingillioglu%2C+N">Nuri Cingillioglu</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+A">Alessandra Russo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Humans have the ability to seamlessly combine low-level visual input with
high-level symbolic reasoning often in the form of recognising objects,
learning relations between them and applying rules. Neuro-symbolic systems aim
to bring a unifying approach to connectionist and logic-based principles for
visual processing and abstract reasoning respectively. This paper presents a
complete neuro-symbolic method for processing images into objects, learning
relations and logical rules in an end-to-end fashion. The main contribution is
a differentiable layer in a deep learning architecture from which symbolic
relations and rules can be extracted by pruning and thresholding. We evaluate
our model using two datasets: subgraph isomorphism task for symbolic rule
learning and an image classification domain with compound relations for
learning objects, relations and rules. We demonstrate that our model scales
beyond state-of-the-art symbolic learners and outperforms deep relational
neural network architectures.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07488" title="Abstract">arXiv:2106.07488</a> [<a href="/pdf/2106.07488" title="Download PDF">pdf</a>, <a href="/format/2106.07488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User-Guided Personalized Image Aesthetic Assessment based on Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+P">Pei Lv</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jianqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+X">Xixi Nie</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Weiming Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bing Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingliang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changsheng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Personalized image aesthetic assessment (PIAA) has recently become a hot
topic due to its usefulness in a wide variety of applications such as
photography, film and television, e-commerce, fashion design and so on. This
task is more seriously affected by subjective factors and samples provided by
users. In order to acquire precise personalized aesthetic distribution by small
amount of samples, we propose a novel user-guided personalized image aesthetic
assessment framework. This framework leverages user interactions to retouch and
rank images for aesthetic assessment based on deep reinforcement learning
(DRL), and generates personalized aesthetic distribution that is more in line
with the aesthetic preferences of different users. It mainly consists of two
stages. In the first stage, personalized aesthetic ranking is generated by
interactive image enhancement and manual ranking, meanwhile two policy networks
will be trained. The images will be pushed to the user for manual retouching
and simultaneously to the enhancement policy network. The enhancement network
utilizes the manual retouching results as the optimization goals of DRL. After
that, the ranking process performs the similar operations like the retouching
mentioned before. These two networks will be trained iteratively and
alternatively to help to complete the final personalized aesthetic assessment
automatically. In the second stage, these modified images are labeled with
aesthetic attributes by one style-specific classifier, and then the
personalized aesthetic distribution is generated based on the multiple
aesthetic attributes of these images, which conforms to the aesthetic
preference of users better.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07497" title="Abstract">arXiv:2106.07497</a> [<a href="/pdf/2106.07497" title="Download PDF">pdf</a>, <a href="/ps/2106.07497" title="Download PostScript">ps</a>, <a href="/format/2106.07497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security testing using JUnit and Perl scripts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harty%2C+J">Julian Harty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CAST '06, June 5-7, 2006, Indianapolis, Indiana, USA. Pages 56 -
  58
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In this paper, I describe a recent practical experience where JUnit was used
for testing security bugs in addition to functional bugs. Perl scripts were
also used during the exploration phase. The application being tested was
mature, but insecure.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07499" title="Abstract">arXiv:2106.07499</a> [<a href="/pdf/2106.07499" title="Download PDF">pdf</a>, <a href="/format/2106.07499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Survey of Data Augmentation for Limited Data Learning in  NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tam%2C+D">Derek Tam</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">NLP has achieved great progress in the past decade through the use of neural
models and large labeled datasets. The dependence on abundant data prevents NLP
models from being applied to low-resource settings or novel tasks where
significant time, money, or expertise is required to label massive amounts of
textual data. Recently, data augmentation methods have been explored as a means
of improving data efficiency in NLP. To date, there has been no systematic
empirical overview of data augmentation for NLP in the limited labeled data
setting, making it difficult to understand which methods work in which
settings. In this paper, we provide an empirical survey of recent progress on
data augmentation for NLP in the limited labeled data setting, summarizing the
landscape of methods (including token-level augmentations, sentence-level
augmentations, adversarial augmentations, and hidden-space augmentations) and
carrying out experiments on 11 datasets covering topics/news classification,
inference tasks, paraphrasing tasks, and single-sentence tasks. Based on the
results, we draw several conclusions to help practitioners choose appropriate
augmentations in different settings and discuss the current challenges and
future directions for limited data learning in NLP.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07501" title="Abstract">arXiv:2106.07501</a> [<a href="/pdf/2106.07501" title="Download PDF">pdf</a>, <a href="/ps/2106.07501" title="Download PostScript">ps</a>, <a href="/format/2106.07501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balanced Coarsening for Multilevel Hypergraph Partitioning via  Wasserstein Discrepancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhicheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiaxuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+L">Licheng Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We propose a balanced coarsening scheme for multilevel hypergraph
partitioning. In addition, an initial partitioning algorithm is designed to
improve the quality of k-way hypergraph partitioning. By assigning vertex
weights through the LPT algorithm, we generate a prior hypergraph under a
relaxed balance constraint. With the prior hypergraph, we have defined the
Wasserstein discrepancy to coordinate the optimal transport of coarsening
process. And the optimal transport matrix is solved by Sinkhorn algorithm. Our
coarsening scheme fully takes into account the minimization of connectivity
metric (objective function). For the initial partitioning stage, we define a
normalized cut function induced by Fiedler vector, which is theoretically
proved to be a concave function. Thereby, a three-point algorithm is designed
to find the best cut under the balance constraint.
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07502" title="Abstract">arXiv:2106.07502</a> [<a href="/pdf/2106.07502" title="Download PDF">pdf</a>, <a href="/format/2106.07502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training like Playing: A Reinforcement Learning And Knowledge  Graph-based framework for building Automatic Consultation System in Medical  Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yining Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Meilian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Keke Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce a framework for AI-based medical consultation system with
knowledge graph embedding and reinforcement learning components and its
implement. Our implement of this framework leverages knowledge organized as a
graph to have diagnosis according to evidence collected from patients
recurrently and dynamically. According to experiment we designed for evaluating
its performance, it archives a good result. More importantly, for getting
better performance, researchers can implement it on this framework based on
their innovative ideas, well designed experiments and even clinical trials.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07504" title="Abstract">arXiv:2106.07504</a> [<a href="/pdf/2106.07504" title="Download PDF">pdf</a>, <a href="/format/2106.07504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing the risk of fairwashing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=A%C3%AFvodji%2C+U">Ulrich A&#xef;vodji</a>, 
<a href="/search/cs?searchtype=author&query=Arai%2C+H">Hiromi Arai</a>, 
<a href="/search/cs?searchtype=author&query=Gambs%2C+S">S&#xe9;bastien Gambs</a>, 
<a href="/search/cs?searchtype=author&query=Hara%2C+S">Satoshi Hara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fairwashing refers to the risk that an unfair black-box model can be
explained by a fairer model through post-hoc explanations' manipulation.
However, to realize this, the post-hoc explanation model must produce different
predictions than the original black-box on some inputs, leading to a decrease
in the fidelity imposed by the difference in unfairness. In this paper, our
main objective is to characterize the risk of fairwashing attacks, in
particular by investigating the fidelity-unfairness trade-off. First, we
demonstrate through an in-depth empirical study on black-box models trained on
several real-world datasets and for several statistical notions of fairness
that it is possible to build high-fidelity explanation models with low
unfairness. For instance, we find that fairwashed explanation models can
exhibit up to $99.20\%$ fidelity to the black-box models they explain while
being $50\%$ less unfair. These results suggest that fidelity alone should not
be used as a proxy for the quality of black-box explanations. Second, we show
that fairwashed explanation models can generalize beyond the suing group
(\emph{i.e.}, data points that are being explained), which will only worsen as
more stable fairness methods get developed. Finally, we demonstrate that
fairwashing attacks can transfer across black-box models, meaning that other
black-box models can perform fairwashing without explicitly using their
predictions.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07505" title="Abstract">arXiv:2106.07505</a> [<a href="/pdf/2106.07505" title="Download PDF">pdf</a>, <a href="/format/2106.07505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Profanity and Hate Speech in Social Media with Semantic  Subspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahn%2C+V">Vanessa Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Ruiter%2C+D">Dana Ruiter</a>, 
<a href="/search/cs?searchtype=author&query=Kleinbauer%2C+T">Thomas Kleinbauer</a>, 
<a href="/search/cs?searchtype=author&query=Klakow%2C+D">Dietrich Klakow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, accepted as a long paper at Workshop on Online Abuse and Harms 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Hate speech and profanity detection suffer from data sparsity, especially for
languages other than English, due to the subjective nature of the tasks and the
resulting annotation incompatibility of existing corpora. In this study, we
identify profane subspaces in word and sentence representations and explore
their generalization capability on a variety of similar and distant target
tasks in a zero-shot setting. This is done monolingually (German) and
cross-lingually to closely-related (English), distantly-related (French) and
non-related (Arabic) tasks. We observe that, on both similar and distant target
tasks and across all languages, the subspace-based representations transfer
more effectively than standard BERT representations in the zero-shot setting,
with improvements between F1 +10.9 and F1 +42.9 over the baselines across all
tested monolingual and cross-lingual scenarios.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07510" title="Abstract">arXiv:2106.07510</a> [<a href="/pdf/2106.07510" title="Download PDF">pdf</a>, <a href="/format/2106.07510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the Cut Locus of a Riemannian Manifold via Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Facca%2C+E">Enrico Facca</a>, 
<a href="/search/math?searchtype=author&query=Berti%2C+L">Luca Berti</a>, 
<a href="/search/math?searchtype=author&query=Fass%C3%B3%2C+F">Francesco Fass&#xf3;</a>, 
<a href="/search/math?searchtype=author&query=Putti%2C+M">Mario Putti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Differential Geometry (math.DG); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this paper, we give a new characterization of the cut locus of a point on
a compact Riemannian manifold as the zero set of the optimal transport density
solution of the Monge-Kantorovich equations, a PDE formulation of the optimal
transport problem with cost equal to the geodesic distance. Combining this
result with an optimal transport numerical solver based on the so-called
dynamical Monge-Kantorovich approach, we propose a novel framework for the
numerical approximation of the cut locus of a point in a manifold. We show the
applicability of the proposed method on a few examples settled on 2d-surfaces
embedded in $R^{3}$ and discuss advantages and limitations.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07513" title="Abstract">arXiv:2106.07513</a> [<a href="/pdf/2106.07513" title="Download PDF">pdf</a>, <a href="/format/2106.07513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeLabeller: A Web-based Code Annotation Tool for Java Design Patterns  and Summaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Norman Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nazar%2C+N">Najam Nazar</a>, 
<a href="/search/cs?searchtype=author&query=Chong%2C+C+Y">Chun Yong Chong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The appropriate use of design patterns in code is a vital measurement of good
software quality in object-oriented software applications. There exist tools to
detect design pattern usage in Java source files, where their detection
mechanisms have been honed through the use of supervised machine learning
techniques that require large datasets of labelled files. However, manually
labelling these files leads to issues such as tediousness if the team of
labellers is small, and conflicting opinions between labellers, if large. Thus,
we present CodeLabeller, a web-based tool which aims to provide a more
efficient approach in handling the process of labelling Java source files at
scale by improving the data collection process throughout, and improving the
degree of reliability of responses by requiring each labeller to attach a
confidence rating to each of their responses. We test CodeLabeller by
constructing a corpus of over a thousand source files obtained from a large
collection of open-source Java projects, and labelling each Java source file
with their respective design patterns (if any), and summaries. This paper
discusses the motivation behind thecreation of CodeLabeller, a demonstration of
the tool and its UI, its implementation, benefits and lastly, some ideas for
future improvements. A demo version of CodeLabeller can be found at:
https://codelabeller.org.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07520" title="Abstract">arXiv:2106.07520</a> [<a href="/pdf/2106.07520" title="Download PDF">pdf</a>, <a href="/format/2106.07520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JUGE: An Infrastructure for Benchmarking Java Unit Test Generators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Devroey%2C+X">Xavier Devroey</a>, 
<a href="/search/cs?searchtype=author&query=Gambi%2C+A">Alessio Gambi</a>, 
<a href="/search/cs?searchtype=author&query=Galeotti%2C+J+P">Juan Pablo Galeotti</a>, 
<a href="/search/cs?searchtype=author&query=Just%2C+R">Ren&#xe9; Just</a>, 
<a href="/search/cs?searchtype=author&query=Kifetew%2C+F">Fitsum Kifetew</a>, 
<a href="/search/cs?searchtype=author&query=Panichella%2C+A">Annibale Panichella</a>, 
<a href="/search/cs?searchtype=author&query=Panichella%2C+S">Sebastiano Panichella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Researchers and practitioners have designed and implemented various automated
test case generators to support effective software testing. Such generators
exist for various languages (e.g., Java, C#, or Python) and for various
platforms (e.g., desktop, web, or mobile applications). Such generators exhibit
varying effectiveness and efficiency, depending on the testing goals they aim
to satisfy (e.g., unit-testing of libraries vs. system-testing of entire
applications) and the underlying techniques they implement. In this context,
practitioners need to be able to compare different generators to identify the
most suited one for their requirements, while researchers seek to identify
future research directions. This can be achieved through the systematic
execution of large-scale evaluations of different generators. However, the
execution of such empirical evaluations is not trivial and requires a
substantial effort to collect benchmarks, setup the evaluation infrastructure,
and collect and analyse the results. In this paper, we present our JUnit
Generation benchmarking infrastructure (JUGE) supporting generators (e.g.,
search-based, random-based, symbolic execution, etc.) seeking to automate the
production of unit tests for various purposes (e.g., validation, regression
testing, fault localization, etc.). The primary goal is to reduce the overall
effort, ease the comparison of several generators, and enhance the knowledge
transfer between academia and industry by standardizing the evaluation and
comparison process. Since 2013, eight editions of a unit testing tool
competition, co-located with the Search-Based Software Testing Workshop, have
taken place and used and updated JUGE. As a result, an increasing amount of
tools (over ten) from both academia and industry have been evaluated on JUGE,
matured over the years, and allowed the identification of future research
directions.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07527" title="Abstract">arXiv:2106.07527</a> [<a href="/pdf/2106.07527" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting 3D RNA Folding Patterns via Quadratic Binary Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M+W">Mark W. Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+A">Amit Verma</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+R">Rick Hennig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Summary results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The structure of an RNA molecule plays a significant role in its biological
function. Predicting structure given a one dimensional sequence of RNA
nucleotide bases is a difficult and important problem. Many computer programs
(known as in silico) are available for predicting 2-dimensional (secondary)
structures however 3-dimensional (tertiary) structure prediction is much more
difficult mainly due to the far greater number of feasible solutions and fewer
experimental data on the thermodynamic energies of 3D structures. It is also
challenging to verify the most likely three dimensional structure even with the
availability of sophisticated x-ray crystallography and nuclear magnetic
resonance imaging technologies. In this paper we develop three dimensional RNA
folding predictions by adding penalty and reward parameters to a previous two
dimensional approach based on Quadratic Unconstrained Binary Optimization
(QUBO) models. These parameters provide flexibility in the amount of three
dimensional folding allowed. We address the problem of multiple near-optimal
structures via a new weighted similarity structure measure and illustrate
folding pathways via progressively improving local optimal solutions. The
problems are solved via a new commercial QUBO solver AlphaQUBO (Meta-Analytics,
2020) that solves problems having hundreds of thousands of binary variables.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07528" title="Abstract">arXiv:2106.07528</a> [<a href="/pdf/2106.07528" title="Download PDF">pdf</a>, <a href="/format/2106.07528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Trust and Trust Modelling for the Future Fully-Connected Digital  World: A Comprehensive Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ting%2C+H+L+J">Hannah Lim Jing Ting</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+X">Xin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tieyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+C">Cheng-Kang Chu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">With the fast development of digital technologies, we are running into a
digital world. The relationship among people and the connections among things
become more and more complex, and new challenges arise. To tackle these
challenges, trust-a soft security mechanism-is considered as a promising
technology. Thus, in this survey, we do a comprehensive study on the trust and
trust modelling for the future digital world. We revisit the definitions and
properties of trust, and analysis the trust theories and discuss their impact
on digital trust modelling. We analyze the digital world and its corresponding
environment where people, things, and infrastructure connect with each other.
We detail the challenges that require trust in these digital scenarios. Under
our analysis of trust and the digital world, we define different types of trust
relationships and find out the factors that are needed to ensure a fully
representative model. Next, to meet the challenges of digital trust modelling,
comprehensive trust model evaluation criteria are proposed, and potential
securities and privacy issues of trust modelling are analyzed. Finally, we
provide a wide-ranging analysis of different methodologies, mathematical
theories, and how they can be applied to trust modelling.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07534" title="Abstract">arXiv:2106.07534</a> [<a href="/pdf/2106.07534" title="Download PDF">pdf</a>, <a href="/format/2106.07534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> z-anonymity: Zero-Delay Anonymization for Data Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jha%2C+N">Nikhil Jha</a>, 
<a href="/search/cs?searchtype=author&query=Favale%2C+T">Thomas Favale</a>, 
<a href="/search/cs?searchtype=author&query=Vassio%2C+L">Luca Vassio</a>, 
<a href="/search/cs?searchtype=author&query=Trevisan%2C+M">Martino Trevisan</a>, 
<a href="/search/cs?searchtype=author&query=Mellia%2C+M">Marco Mellia</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Jha, Nikhil, Thomas Favale, Luca Vassio, Martino Trevisan, and
  Marco Mellia. "z-anonymity: Zero-Delay Anonymization for Data Streams." In
  2020 IEEE International Conference on Big Data (Big Data), pp. 3996-4005.
  IEEE, 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">With the advent of big data and the birth of the data markets that sell
personal information, individuals' privacy is of utmost importance. The
classical response is anonymization, i.e., sanitizing the information that can
directly or indirectly allow users' re-identification. The most popular
solution in the literature is the k-anonymity. However, it is hard to achieve
k-anonymity on a continuous stream of data, as well as when the number of
dimensions becomes high.In this paper, we propose a novel anonymization
property called z-anonymity. Differently from k-anonymity, it can be achieved
with zero-delay on data streams and it is well suited for high dimensional
data. The idea at the base of z-anonymity is to release an attribute (an atomic
information) about a user only if at least z - 1 other users have presented the
same attribute in a past time window. z-anonymity is weaker than k-anonymity
since it does not work on the combinations of attributes, but treats them
individually. In this paper, we present a probabilistic framework to map the
z-anonymity into the k-anonymity property. Our results show that a proper
choice of the z-anonymity parameters allows the data curator to likely obtain a
k-anonymized dataset, with a precisely measurable probability. We also evaluate
a real use case, in which we consider the website visits of a population of
users and show that z-anonymity can work in practice for obtaining the
k-anonymity too.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07536" title="Abstract">arXiv:2106.07536</a> [<a href="/pdf/2106.07536" title="Download PDF">pdf</a>, <a href="/format/2106.07536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Throughput Maximization Leveraging Just-Enough SNR Margin and Channel  Spacing Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shilin Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Flexible optical network is a promising technology to accommodate
high-capacity demands in next-generation networks. To ensure uninterrupted
communication, existing lightpath provisioning schemes are mainly done with the
assumption of worst-case resource under-provisioning and fixed channel spacing,
which preserves an excessive signal-to-noise ratio (SNR) margin. However, under
a resource over-provisioning scenario, the excessive SNR margin restricts the
transmission bit-rate, leading to physical layer resource waste and stranded
transmission capacity. To tackle this challenging problem, we leverage an
iterative feedback tuning algorithm to provide a just-enough SNR margin, so as
to maximize the network throughput. Specifically, the proposed algorithm is
implemented in three steps. First, starting from the high SNR margin setup, we
establish an integer linear programming model as well as a heuristic algorithm
to maximize the network throughput by solving the problem of routing,
modulation format, forward error correction, baud-rate selection, and spectrum
assignment. Second, we optimize the channel spacing of the lightpaths obtained
from the previous step, thereby increasing the available physical layer
resources. Finally, we iteratively reduce the SNR margin of each lightpath
until the network throughput cannot be increased. Through numerical
simulations, we confirm the throughput improvement in different networks and
with different baud-rates. In particular, we find that our algorithm enables
over 20\% relative gain when network resource is over-provisioned, compared to
the traditional method preserving an excessive SNR margin.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07539" title="Abstract">arXiv:2106.07539</a> [<a href="/pdf/2106.07539" title="Download PDF">pdf</a>, <a href="/ps/2106.07539" title="Download PostScript">ps</a>, <a href="/format/2106.07539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Representation of Solutions to Elliptic PDEs in Barron Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+Z">Ziang Chen</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+J">Jianfeng Lu</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+Y">Yulong Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG); Analysis of PDEs (math.AP)

</div>
<p class="mathjax">Numerical solutions to high-dimensional partial differential equations (PDEs)
based on neural networks have seen exciting developments. This paper derives
complexity estimates of the solutions of $d$-dimensional second-order elliptic
PDEs in the Barron space, that is a set of functions admitting the integral of
certain parametric ridge function against a probability measure on the
parameters. We prove under some appropriate assumptions that if the
coefficients and the source term of the elliptic PDE lie in Barron spaces, then
the solution of the PDE is $\epsilon$-close with respect to the $H^1$ norm to a
Barron function. Moreover, we prove dimension-explicit bounds for the Barron
norm of this approximate solution, depending at most polynomially on the
dimension $d$ of the PDE. As a direct consequence of the complexity estimates,
the solution of the PDE can be approximated on any bounded domain by a
two-layer neural network with respect to the $H^1$ norm with a
dimension-explicit convergence rate.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07540" title="Abstract">arXiv:2106.07540</a> [<a href="/pdf/2106.07540" title="Download PDF">pdf</a>, <a href="/format/2106.07540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Various Tokenizers for Arabic Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alyafeai%2C+Z">Zaid Alyafeai</a>, 
<a href="/search/cs?searchtype=author&query=Al-shaibani%2C+M+S">Maged S. Al-shaibani</a>, 
<a href="/search/cs?searchtype=author&query=Ghaleb%2C+M">Mustafa Ghaleb</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+I">Irfan Ahmad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The first step in any NLP pipeline is learning word vector representations.
However, given a large text corpus, representing all the words is not
efficient. In the literature, many tokenization algorithms have emerged to
tackle this problem by creating subwords which in turn limits the vocabulary
size in any text corpus. However such algorithms are mostly language-agnostic
and lack a proper way of capturing meaningful tokens. Not to mention the
difficulty of evaluating such techniques in practice. In this paper, we
introduce three new tokenization algorithms for Arabic and compare them to
three other baselines using unsupervised evaluations. In addition to that, we
compare all the six algorithms by evaluating them on three tasks which are
sentiment analysis, news classification and poetry classification. Our
experiments show that the performance of such tokenization algorithms depends
on the size of the dataset, type of the task, and the amount of morphology that
exists in the dataset.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07542" title="Abstract">arXiv:2106.07542</a> [<a href="/pdf/2106.07542" title="Download PDF">pdf</a>, <a href="/format/2106.07542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Based Prediction of Future Stress Events in a Driving  Scenario
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clark%2C+J">Joseph Clark</a>, 
<a href="/search/cs?searchtype=author&query=Nath%2C+R+K">Rajdeep Kumar Nath</a>, 
<a href="/search/cs?searchtype=author&query=Thapliyal%2C+H">Himanshu Thapliyal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 Pages, IEEE 7th World Forum on Internet of Things 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper presents a model for predicting a driver's stress level up to one
minute in advance. Successfully predicting future stress would allow stress
mitigation to begin before the subject becomes stressed, reducing or possibly
avoiding the performance penalties of stress. The proposed model takes features
extracted from Galvanic Skin Response (GSR) signals on the foot and hand and
Respiration and Electrocardiogram (ECG) signals from the chest of the driver.
The data used to train the model was retrieved from an existing database and
then processed to create statistical and frequency features. A total of 42
features were extracted from the data and then expanded into a total of 252
features by grouping the data and taking six statistical measurements of each
group for each feature. A Random Forest Classifier was trained and evaluated
using a leave-one-subject-out testing approach. The model achieved 94% average
accuracy on the test data. Results indicate that the model performs well and
could be used as part of a vehicle stress prevention system.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07544" title="Abstract">arXiv:2106.07544</a> [<a href="/pdf/2106.07544" title="Download PDF">pdf</a>, <a href="/format/2106.07544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset of Propaganda Techniques of the State-Sponsored Information  Operation of the People&#x27;s Republic of China
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+R">Rong-Ching Chang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+C">Chun-Ming Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Lai Chang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chu-Hsing Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The digital media, identified as computational propaganda provides a pathway
for propaganda to expand its reach without limit. State-backed propaganda aims
to shape the audiences' cognition toward entities in favor of a certain
political party or authority. Furthermore, it has become part of modern
information warfare used in order to gain an advantage over opponents. Most of
the current studies focus on using machine learning, quantitative, and
qualitative methods to distinguish if a certain piece of information on social
media is propaganda. Mainly conducted on English content, but very little
research addresses Chinese Mandarin content. From propaganda detection, we want
to go one step further to provide more fine-grained information on propaganda
techniques that are applied. In this research, we aim to bridge the information
gap by providing a multi-labeled propaganda techniques dataset in Mandarin
based on a state-backed information operation dataset provided by Twitter. In
addition to presenting the dataset, we apply a multi-label text classification
using fine-tuned BERT. Potentially this could help future research in detecting
state-backed propaganda online especially in a cross-lingual context and cross
platforms identity consolidation.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07545" title="Abstract">arXiv:2106.07545</a> [<a href="/pdf/2106.07545" title="Download PDF">pdf</a>, <a href="/format/2106.07545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolarStream: Streaming Lidar Object Detection and Segmentation with  Polar Pillars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Vora%2C+S">Sourabh Vora</a>, 
<a href="/search/cs?searchtype=author&query=Beijbom%2C+O">Oscar Beijbom</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Recent works recognized lidars as an inherently streaming data source and
showed that the end-to-end latency of lidar perception models can be reduced
significantly by operating on wedge-shaped point cloud sectors rather then the
full point cloud. However, due to use of cartesian coordinate systems these
methods represent the sectors as rectangular regions, wasting memory and
compute. In this work we propose using a polar coordinate system and make two
key improvements on this design. First, we increase the spatial context by
using multi-scale padding from neighboring sectors: preceding sector from the
current scan and/or the following sector from the past scan. Second, we improve
the core polar convolutional architecture by introducing feature undistortion
and range stratified convolutions. Experimental results on the nuScenes dataset
show significant improvements over other streaming based methods. We also
achieve comparable results to existing non-streaming methods but with lower
latencies.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07548" title="Abstract">arXiv:2106.07548</a> [<a href="/pdf/2106.07548" title="Download PDF">pdf</a>, <a href="/format/2106.07548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A scalable multi-step least squares method for network identification  with unknown disturbance topology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fonken%2C+S+J+M">Stefanie J.M. Fonken</a>, 
<a href="/search/eess?searchtype=author&query=Ramaswamy%2C+K+R">Karthik R. Ramaswamy</a>, 
<a href="/search/eess?searchtype=author&query=Van+den+Hof%2C+P+M+J">Paul M.J. Van den Hof</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures, Submitted to Automatica on 14th June 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">Identification methods for dynamic networks typically require prior knowledge
of the network and disturbance topology, and often rely on solving poorly
scalable non-convex optimization problems. While methods for estimating network
topology are available in the literature, less attention has been paid to
estimating the disturbance topology, i.e., the (spatial) noise correlation
structure and the noise rank. In this work we present an identification method
for dynamic networks, in which an estimation of the disturbance topology
precedes the identification of the full dynamic network with known network
topology. To this end we extend the multi-step Sequential Linear Regression and
Weighted Null Space Fitting methods to deal with reduced rank noise, and use
these methods to estimate the disturbance topology and the network dynamics. As
a result, we provide a multi-step least squares algorithm with parallel
computation capabilities and that rely only on explicit analytical solutions,
thereby avoiding the usual non-convex optimizations involved. Consequently we
consistently estimate dynamic networks of Box Jenkins model structure, while
keeping the computational burden low. We provide a consistency proof that
includes path-based data informativity conditions for allocation of excitation
signals in the experimental design. Numerical simulations performed on a
dynamic network with reduced rank noise clearly illustrate the potential of
this method.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07549" title="Abstract">arXiv:2106.07549</a> [<a href="/pdf/2106.07549" title="Download PDF">pdf</a>, <a href="/format/2106.07549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Named Entity Normalization Model Using Edge Weight Updating Neural  Network: Assimilation Between Knowledge-Driven Graph and Data-Driven Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+S+H">Sung Hwan Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sungzoon Cho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Discriminating the matched named entity pairs or identifying the entities'
canonical forms are critical in text mining tasks. More precise named entity
normalization in text mining will benefit other subsequent text analytic
applications. We built the named entity normalization model with a novel Edge
Weight Updating Neural Network. Our proposed model when tested on four
different datasets achieved state-of-the-art results. We, next, verify our
model's performance on NCBI Disease, BC5CDR Disease, and BC5CDR Chemical
databases, which are widely used named entity normalization datasets in the
bioinformatics field. We also tested our model with our own financial named
entity normalization dataset to validate the efficacy for more general
applications. Using the constructed dataset, we differentiate named entity
pairs. Our model achieved the highest named entity normalization performances
in terms of various evaluation metrics.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07550" title="Abstract">arXiv:2106.07550</a> [<a href="/pdf/2106.07550" title="Download PDF">pdf</a>, <a href="/format/2106.07550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention mechanisms and deep learning for machine vision: A survey of  the state of the art
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hafiz%2C+A+M">Abdul Mueed Hafiz</a>, 
<a href="/search/cs?searchtype=author&query=Parah%2C+S+A">Shabir Ahmad Parah</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+R+U+A">Rouf Ul Alam Bhat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the advent of state of the art nature-inspired pure attention based
models i.e. transformers, and their success in natural language processing
(NLP), their extension to machine vision (MV) tasks was inevitable and much
felt. Subsequently, vision transformers (ViTs) were introduced which are giving
quite a challenge to the established deep learning based machine vision
techniques. However, pure attention based models/architectures like
transformers require huge data, large training times and large computational
resources. Some recent works suggest that combinations of these two varied
fields can prove to build systems which have the advantages of both these
fields. Accordingly, this state of the art survey paper is introduced which
hopefully will help readers get useful information about this interesting and
potential research area. A gentle introduction to attention mechanisms is
given, followed by a discussion of the popular attention based deep
architectures. Subsequently, the major categories of the intersection of
attention mechanisms and deep learning for machine vision (MV) based are
discussed. Afterwards, the major algorithms, issues and trends within the scope
of the paper are discussed.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07551" title="Abstract">arXiv:2106.07551</a> [<a href="/pdf/2106.07551" title="Download PDF">pdf</a>, <a href="/format/2106.07551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MALib: A Parallel Framework for Population-based Multi-agent  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Ming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Ziyu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanjing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+M">Muning Wen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Runzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 17 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Population-based multi-agent reinforcement learning (PB-MARL) refers to the
series of methods nested with reinforcement learning (RL) algorithms, which
produces a self-generated sequence of tasks arising from the coupled population
dynamics. By leveraging auto-curricula to induce a population of distinct
emergent strategies, PB-MARL has achieved impressive success in tackling
multi-agent tasks. Despite remarkable prior arts of distributed RL frameworks,
PB-MARL poses new challenges for parallelizing the training frameworks due to
the additional complexity of multiple nested workloads between sampling,
training and evaluation involved with heterogeneous policy interactions. To
solve these problems, we present MALib, a scalable and efficient computing
framework for PB-MARL. Our framework is comprised of three key components: (1)
a centralized task dispatching model, which supports the self-generated tasks
and scalable training with heterogeneous policy combinations; (2) a programming
architecture named Actor-Evaluator-Learner, which achieves high parallelism for
both training and sampling, and meets the evaluation requirement of
auto-curriculum learning; (3) a higher-level abstraction of MARL training
paradigms, which enables efficient code reuse and flexible deployments on
different distributed computing paradigms. Experiments on a series of complex
tasks such as multi-agent Atari Games show that MALib achieves throughput
higher than 40K FPS on a single machine with $32$ CPU cores; 5x speedup than
RLlib and at least 3x speedup than OpenSpiel in multi-agent training tasks.
MALib is publicly available at https://github.com/sjtu-marl/malib.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07552" title="Abstract">arXiv:2106.07552</a> [<a href="/pdf/2106.07552" title="Download PDF">pdf</a>, <a href="/format/2106.07552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PC-DAN: Point Cloud based Deep Affinity Network for 3D Multi-Object  Tracking (Accepted as an extended abstract in JRDB-ACT Workshop at CVPR21)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aakash Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Kini%2C+J">Jyoti Kini</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+M">Mubarak Shah</a>, 
<a href="/search/cs?searchtype=author&query=Mian%2C+A">Ajmal Mian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent times, the scope of LIDAR (Light Detection and Ranging)
sensor-based technology has spread across numerous fields. It is popularly used
to map terrain and navigation information into reliable 3D point cloud data,
potentially revolutionizing the autonomous vehicles and assistive robotic
industry. A point cloud is a dense compilation of spatial data in 3D
coordinates. It plays a vital role in modeling complex real-world scenes since
it preserves structural information and avoids perspective distortion, unlike
image data, which is the projection of a 3D structure on a 2D plane. In order
to leverage the intrinsic capabilities of the LIDAR data, we propose a
PointNet-based approach for 3D Multi-Object Tracking (MOT).
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07553" title="Abstract">arXiv:2106.07553</a> [<a href="/pdf/2106.07553" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Cognitive Science perspective for learning how to design meaningful  user experiences and human-centered technology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kingsley%2C+S">Sara Kingsley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This paper reviews literature in cognitive science, human-computer
interaction (HCI) and natural-language processing (NLP) to consider how
analogical reasoning (AR) could help inform the design of communication and
learning technologies, as well as online communities and digital platforms.
First, analogical reasoning (AR) is defined, and use-cases of AR in the
computing sciences are presented. The concept of schema is introduced, along
with use-cases in computing. Finally, recommendations are offered for future
work on using analogical reasoning and schema methods in the computing
sciences.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07554" title="Abstract">arXiv:2106.07554</a> [<a href="/pdf/2106.07554" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset for eye-tracking tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ildar%2C+R">R. Ildar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In recent years many different deep neural networks were developed, but due
to a large number of layers in deep networks, their training requires a long
time and a large number of datasets. Today is popular to use trained deep
neural networks for various tasks, even for simple ones in which such deep
networks are not required. The well-known deep networks such as YoloV3, SSD,
etc. are intended for tracking and monitoring various objects, therefore their
weights are heavy and the overall accuracy for a specific task is low.
Eye-tracking tasks need to detect only one object - an iris in a given area.
Therefore, it is logical to use a neural network only for this task. But the
problem is the lack of suitable datasets for training the model. In the
manuscript, we presented a dataset that is suitable for training custom models
of convolutional neural networks for eye-tracking tasks. Using data set data,
each user can independently pre-train the convolutional neural network models
for eye-tracking tasks. This dataset contains annotated 10,000 eye images in an
extension of 416 by 416 pixels. The table with annotation information shows the
coordinates and radius of the eye for each image. This manuscript can be
considered as a guide for the preparation of datasets for eye-tracking devices
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07555" title="Abstract">arXiv:2106.07555</a> [<a href="/pdf/2106.07555" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework to Counteract Suboptimal User-Behaviors in Exploratory  Learning Environments: an Application to MOOCs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lall%C3%A9%2C+S">S&#xe9;bastien Lall&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Conati%2C+C">Cristina Conati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The AAAI 2019 Workshop on Plan, Activity, and Intent Recognition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">While there is evidence that user-adaptive support can greatly enhance the
effectiveness of educational systems, designing such support for exploratory
learning environments (e.g., simulations) is still challenging due to the
open-ended nature of their interaction. In particular, there is little a priori
knowledge of which student's behaviors can be detrimental to learning in such
environments. To address this problem, we focus on a data-driven user-modeling
framework that uses logged interaction data to learn which behavioral or
activity patterns should trigger help during interaction with a specific
learning environment. This framework has been successfully used to provide
adaptive support in interactive learning simulations. Here we present a novel
application of this framework we are working on, namely to Massive Open Online
Courses (MOOCs), a form of exploratory environment that could greatly benefit
from adaptive support due to the large diversity of their users, but typically
lack of such adaptation. We describe an experiment aimed at investigating the
value of our framework to identify student's behaviors that can justify
adapting to, and report some preliminary results.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07556" title="Abstract">arXiv:2106.07556</a> [<a href="/pdf/2106.07556" title="Download PDF">pdf</a>, <a href="/format/2106.07556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long Term Object Detection and Tracking in Collaborative Learning  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teeparthi%2C+S">Sravani Teeparthi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Master's thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Human activity recognition in videos is a challenging problem that has drawn
a lot of interest, particularly when the goal requires the analysis of a large
video database. AOLME project provides a collaborative learning environment for
middle school students to explore mathematics, computer science, and
engineering by processing digital images and videos. As part of this project,
around 2200 hours of video data was collected for analysis. Because of the size
of the dataset, it is hard to analyze all the videos of the dataset manually.
Thus, there is a huge need for reliable computer-based methods that can detect
activities of interest. My thesis is focused on the development of accurate
methods for detecting and tracking objects in long videos. All the models are
validated on videos from 7 different sessions, ranging from 45 minutes to 90
minutes. The keyboard detector achieved a very high average precision (AP) of
92% at 0.5 intersection over union (IoU). Furthermore, a combined system of the
detector with a fast tracker KCF (159fps) was developed so that the algorithm
runs significantly faster without sacrificing accuracy. For a video of 23
minutes having resolution 858X480 @ 30 fps, the detection alone runs at 4.7Xthe
real-time, and the combined algorithm runs at 21Xthe real-time for an average
IoU of 0.84 and 0.82, respectively. The hand detector achieved average
precision (AP) of 72% at 0.5 IoU. The detection results were improved to 81%
using optimal data augmentation parameters. The hand detector runs at 4.7Xthe
real-time with AP of 81% at 0.5 IoU. The hand detection method was integrated
with projections and clustering for accurate proposal generation. This approach
reduced the number of false-positive hand detections by 80%. The overall hand
detection system runs at 4Xthe real-time, capturing all the activity regions of
the current collaborative group.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07557" title="Abstract">arXiv:2106.07557</a> [<a href="/pdf/2106.07557" title="Download PDF">pdf</a>, <a href="/format/2106.07557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Branch Hybrid Transformer Networkfor Corneal Endothelial Cell  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Higashita%2C+R">Risa Higashita</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Huazhu Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yanwu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haofeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Corneal endothelial cell segmentation plays a vital role inquantifying
clinical indicators such as cell density, coefficient of variation,and
hexagonality. However, the corneal endothelium's uneven reflectionand the
subject's tremor and movement cause blurred cell edges in theimage, which is
difficult to segment, and need more details and contextinformation to release
this problem. Due to the limited receptive field oflocal convolution and
continuous downsampling, the existing deep learn-ing segmentation methods
cannot make full use of global context andmiss many details. This paper
proposes a Multi-Branch hybrid Trans-former Network (MBT-Net) based on the
transformer and body-edgebranch. Firstly, We use the convolutional block to
focus on local tex-ture feature extraction and establish long-range
dependencies over space,channel, and layer by the transformer and residual
connection. Besides,We use the body-edge branch to promote local consistency
and to provideedge position information. On the self-collected dataset
TM-EM3000 andpublic Alisarine dataset, compared with other State-Of-The-Art
(SOTA)methods, the proposed method achieves an improvement.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07558" title="Abstract">arXiv:2106.07558</a> [<a href="/pdf/2106.07558" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transparent Model of Unabridged Data (TMUD)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Min Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 figures and 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advancements in computational power and algorithms have enabled
unabridged data (e.g., raw images or audio) to be used as input in some models
(e.g., deep learning). However, the black box nature of such models reduces
their likelihood of adoption by marketing scholars. Our paradigm of analysis,
the Transparent Model of Unabridged Data (TMUD), enables researchers to
investigate the inner workings of such black box models by incorporating an ex
ante filtration module and an ex post experimentation module. We empirically
demonstrate the TMUD by investigating the role of facial components and sexual
dimorphism in face perceptions, which have implications for four marketing
contexts: advertisement (perceptions of approachability, trustworthiness, and
competence), brand (perceptions of whether a face represents a brand's typical
customer), category (perceptions of whether a face represents a category's
typical customer), and customer persona (perceptions of whether a face
represents the persona of a brand's customer segment). Our results reveal new
and useful findings that enrich the existing literature on face perception,
most of which is based on abridged attributes (e.g., width of mouth). The TMUD
has great potential to be a useful paradigm for generating theoretical insights
and may encourage more marketing researchers and practitioners to use
unabridged data.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07559" title="Abstract">arXiv:2106.07559</a> [<a href="/pdf/2106.07559" title="Download PDF">pdf</a>, <a href="/format/2106.07559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Perceptual Learning: Image Categorization with Weak  Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chengliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Uriarte%2C+M">Mar&#xed;a Uriarte</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Helen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Morton%2C+D+C">Douglas C. Morton</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tian Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning has achieved much success on supervised learning tasks with
large sets of well-annotated training samples. However, in many practical
situations, such strong and high-quality supervision provided by training data
is unavailable due to the expensive and labor-intensive labeling process.
Automatically identifying and recognizing object categories in a large volume
of unlabeled images with weak supervision remains an important, yet unsolved
challenge in computer vision. In this paper, we propose a novel machine
learning framework, artificial perceptual learning (APL), to tackle the problem
of weakly supervised image categorization. The proposed APL framework is
constructed using state-of-the-art machine learning algorithms as building
blocks to mimic the cognitive development process known as infant
categorization. We develop and illustrate the proposed framework by
implementing a wide-field fine-grain ecological survey of tree species over an
8,000-hectare area of the El Yunque rainforest in Puerto Rico. It is based on
unlabeled high-resolution aerial images of the tree canopy. Misplaced
ground-based labels were available for less than 1% of these images, which
serve as the only weak supervision for this learning framework. We validate the
proposed framework using a small set of images with high quality human
annotations and show that the proposed framework attains human-level cognitive
economy.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07560" title="Abstract">arXiv:2106.07560</a> [<a href="/pdf/2106.07560" title="Download PDF">pdf</a>, <a href="/format/2106.07560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Allocating Stimulus Checks in Times of Crisis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papachristou%2C+M">Marios Papachristou</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study the problem of allocating bailouts (stimulus, subsidy allocations)
to people participating in a financial network subject to income shocks. We
build on the financial clearing framework of Eisenberg and Noe that allows the
incorporation of a bailout policy that is based on discrete bailouts motivated
by the types of stimulus checks people receive around the world as part of
COVID-19 economical relief plans. We show that optimally allocating such
bailouts on a financial network in order to maximize a variety of social
welfare objectives of this form is a computationally intractable problem. We
develop approximation algorithms to optimize these objectives and establish
guarantees for their approximation rations. Then, we incorporate multiple
fairness constraints in the optimization problems and establish relative bounds
on the solutions with versus without these constraints. Finally, we apply our
methodology to a variety of data, both in the context of a system of large
financial institutions with real-world data, as well as in a realistic societal
context with financial interactions between people and businesses for which we
use semi-artificial data derived from mobility patterns. Our results suggest
that the algorithms we develop and study have reasonable results in practice
and outperform other network-based heuristics. We argue that the presented
problem through the societal-level lens could assist policymakers in making
informed decisions on issuing subsidies.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07561" title="Abstract">arXiv:2106.07561</a> [<a href="/pdf/2106.07561" title="Download PDF">pdf</a>, <a href="/format/2106.07561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Servo Control from In-Sensor CNN Inference with A Pixel Processor  Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+L">Laurie Bose</a>, 
<a href="/search/cs?searchtype=author&query=Dudek%2C+P">Piotr Dudek</a>, 
<a href="/search/cs?searchtype=author&query=Mayol-Cuevas%2C+W">Walterio Mayol-Cuevas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This work demonstrates direct visual sensory-motor control using high-speed
CNN inference via a SCAMP-5 Pixel Processor Array (PPA). We demonstrate how
PPAs are able to efficiently bridge the gap between perception and action. A
binary Convolutional Neural Network (CNN) is used for a classic rock, paper,
scissors classification problem at over 8000 FPS. Control instructions are
directly sent to a servo motor from the PPA according to the CNN's
classification result without any other intermediate hardware.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07562" title="Abstract">arXiv:2106.07562</a> [<a href="/pdf/2106.07562" title="Download PDF">pdf</a>, <a href="/format/2106.07562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Network Structure Design based on N-Gauss Activation Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xiangri Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hongbin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingcheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work has shown that the activation function of the convolutional
neural network can meet the Lipschitz condition, then the corresponding
convolutional neural network structure can be constructed according to the
scale of the data set, and the data set can be trained more deeply, more
accurately and more effectively. In this article, we have accepted the
experimental results and introduced the core block N-Gauss, N-Gauss, and Swish
(Conv1, Conv2, FC1) neural network structure design to train MNIST, CIFAR10,
and CIFAR100 respectively. Experiments show that N-Gauss gives full play to the
main role of nonlinear modeling of activation functions, so that deep
convolutional neural networks have hierarchical nonlinear mapping learning
capabilities. At the same time, the training ability of N-Gauss on simple
one-dimensional channel small data sets is equivalent to the performance of
ReLU and Swish.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07563" title="Abstract">arXiv:2106.07563</a> [<a href="/pdf/2106.07563" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BPLF: A Bi-Parallel Linear Flow Model for Facial Expression Generation  from Emotion Set Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Gao Xu</a> (1), 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yuanpeng Long</a> (2), 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siwei Liu</a> (1), 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lijia Yang</a> (1), 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shimei Xu</a> (3), 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xiaoming Yao</a> (1,3), 
<a href="/search/cs?searchtype=author&query=Shu%2C+K">Kunxian Shu</a> (1) ((1) School of Computer Science and Technology, Chongqing Key Laboratory on Big Data for Bio Intelligence, Chongqing University of Posts and Telecommunications, Chongqing, China, (2) School of Economic Information Engineering, Southwestern University of Finance and Economics, Chengdu, China (3) 51yunjian.com, Hetie International Square, Chengdu, Sichuan, China)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The flow-based generative model is a deep learning generative model, which
obtains the ability to generate data by explicitly learning the data
distribution. Theoretically its ability to restore data is stronger than other
generative models. However, its implementation has many limitations, including
limited model design, too many model parameters and tedious calculation. In
this paper, a bi-parallel linear flow model for facial emotion generation from
emotion set images is constructed, and a series of improvements have been made
in terms of the expression ability of the model and the convergence speed in
training. The model is mainly composed of several coupling layers superimposed
to form a multi-scale structure, in which each coupling layer contains 1*1
reversible convolution and linear operation modules. Furthermore, this paper
sorted out the current public data set of facial emotion images, made a new
emotion data, and verified the model through this data set. The experimental
results show that, under the traditional convolutional neural network, the
3-layer 3*3 convolution kernel is more conducive to extracte the features of
the face images. The introduction of principal component decomposition can
improve the convergence speed of the model.
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07564" title="Abstract">arXiv:2106.07564</a> [<a href="/pdf/2106.07564" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An optimized Capsule-LSTM model for facial expression recognition with  video sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siwei Liu</a> (1), 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yuanpeng Long</a> (2), 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Gao Xu</a> (1), 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lijia Yang</a> (1), 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shimei Xu</a> (3), 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xiaoming Yao</a> (1,3), 
<a href="/search/cs?searchtype=author&query=Shu%2C+K">Kunxian Shu</a> (1) ((1) School of Computer Science and Technology, Chongqing Key Laboratory on Big Data for Bio Intelligence, Chongqing University of Posts and Telecommunications, Chongqing, China, (2) School of Economic Information Engineering, Southwestern University of Finance and Economics, Chengdu, China, (3) 51yunjian.com, Hetie International Square, Chengdu, Sichuan, China)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14pages,4 figurews
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">To overcome the limitations of convolutional neural network in the process of
facial expression recognition, a facial expression recognition model
Capsule-LSTM based on video frame sequence is proposed. This model is composed
of three networks includingcapsule encoders, capsule decoders and LSTM network.
The capsule encoder extracts the spatial information of facial expressions in
video frames. Capsule decoder reconstructs the images to optimize the network.
LSTM extracts the temporal information between video frames and analyzes the
differences in expression changes between frames. The experimental results from
the MMI dataset show that the Capsule-LSTM model proposed in this paper can
effectively improve the accuracy of video expression recognition.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07565" title="Abstract">arXiv:2106.07565</a> [<a href="/pdf/2106.07565" title="Download PDF">pdf</a>, <a href="/format/2106.07565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-Based Inpatient Fall Risk Assessment: A Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Armin%2C+M+A">Mohammad Ali Armin</a>, 
<a href="/search/cs?searchtype=author&query=Denman%2C+S">Simon Denman</a>, 
<a href="/search/cs?searchtype=author&query=Petersson%2C+L">Lars Petersson</a>, 
<a href="/search/cs?searchtype=author&query=Ahmedt-Aristizabal%2C+D">David Ahmedt-Aristizabal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Inpatient falls are a serious safety issue in hospitals and healthcare
facilities. Recent advances in video analytics for patient monitoring provide a
non-intrusive avenue to reduce this risk through continuous activity
monitoring. However, in-bed fall risk assessment systems have received less
attention in the literature. The majority of prior studies have focused on fall
event detection, and do not consider the circumstances that may indicate an
imminent inpatient fall. Here, we propose a video-based system that can monitor
the risk of a patient falling, and alert staff of unsafe behaviour to help
prevent falls before they occur. We propose an approach that leverages recent
advances in human localisation and skeleton pose estimation to extract spatial
features from video frames recorded in a simulated environment. We demonstrate
that body positions can be effectively recognised and provide useful evidence
for fall risk assessment. This work highlights the benefits of video-based
models for analysing behaviours of interest, and demonstrates how such a system
could enable sufficient lead time for healthcare professionals to respond and
address patient needs, which is necessary for the development of fall
intervention programs.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07568" title="Abstract">arXiv:2106.07568</a> [<a href="/pdf/2106.07568" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Full interpretable machine learning in 2D with inline coordinates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kovalerchuk%2C+B">Boris Kovalerchuk</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Hoang Phan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper proposed a new methodology for machine learning in 2-dimensional
space (2-D ML) in inline coordinates. It is a full machine learning approach
that does not require to deal with n-dimensional data in n-dimensional space.
It allows discovering n-D patterns in 2-D space without loss of n-D information
using graph representations of n-D data in 2-D. Specifically, it can be done
with the inline based coordinates in different modifications, including static
and dynamic ones. The classification and regression algorithms based on these
inline coordinates were introduced. A successful case study based on a
benchmark data demonstrated the feasibility of the approach. This approach
helps to consolidate further a whole new area of full 2-D machine learning as a
promising ML methodology. It has advantages of abilities to involve actively
the end-users into the discovering of models and their justification. Another
advantage is providing interpretable ML models.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07575" title="Abstract">arXiv:2106.07575</a> [<a href="/pdf/2106.07575" title="Download PDF">pdf</a>, <a href="/format/2106.07575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable and accurate multi-GPU based image reconstruction of  large-scale ptychography data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaodong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Nikitin%2C+V">Viktor Nikitin</a>, 
<a href="/search/cs?searchtype=author&query=Ching%2C+D+J">Daniel J. Ching</a>, 
<a href="/search/cs?searchtype=author&query=Aslan%2C+S">Selin Aslan</a>, 
<a href="/search/cs?searchtype=author&query=Gursoy%2C+D">Doga Gursoy</a>, 
<a href="/search/cs?searchtype=author&query=Bicer%2C+T">Tekin Bicer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Image and Video Processing (eess.IV); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">While the advances in synchrotron light sources, together with the
development of focusing optics and detectors, allow nanoscale ptychographic
imaging of materials and biological specimens, the corresponding experiments
can yield terabyte-scale large volumes of data that can impose a heavy burden
on the computing platform. While Graphical Processing Units (GPUs) provide high
performance for such large-scale ptychography datasets, a single GPU is
typically insufficient for analysis and reconstruction. Several existing works
have considered leveraging multiple GPUs to accelerate the ptychographic
reconstruction. However, they utilize only Message Passing Interface (MPI) to
handle the communications between GPUs. It poses inefficiency for the
configuration that has multiple GPUs in a single node, especially while
processing a single large projection, since it provides no optimizations to
handle the heterogeneous GPU interconnections containing both low-speed links,
e.g., PCIe, and high-speed links, e.g., NVLink. In this paper, we provide a
multi-GPU implementation that can effectively solve large-scale ptychographic
reconstruction problem with optimized performance on intra-node multi-GPU. We
focus on the conventional maximum-likelihood reconstruction problem using
conjugate-gradient (CG) for the solution and propose a novel hybrid
parallelization model to address the performance bottlenecks in CG solver.
Accordingly, we develop a tool called PtyGer (Ptychographic GPU(multiple)-based
reconstruction), implementing our hybrid parallelization model design. The
comprehensive evaluation verifies that PtyGer can fully preserve the original
algorithm's accuracy while achieving outstanding intra-node GPU scalability.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07577" title="Abstract">arXiv:2106.07577</a> [<a href="/pdf/2106.07577" title="Download PDF">pdf</a>, <a href="/format/2106.07577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> F-T-LSTM based Complex Network for Joint Acoustic Echo Cancellation and  Speech Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shimin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Yuxiang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+S">Shubo Lv</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yanxin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>

</div>
<p class="mathjax">With the increasing demand for audio communication and online conference,
ensuring the robustness of Acoustic Echo Cancellation (AEC) under the
complicated acoustic scenario including noise, reverberation and nonlinear
distortion has become a top issue. Although there have been some traditional
methods that consider nonlinear distortion, they are still inefficient for echo
suppression and the performance will be attenuated when noise is present. In
this paper, we present a real-time AEC approach using complex neural network to
better modeling the important phase information and frequency-time-LSTMs
(F-T-LSTM), which scan both frequency and time axis, for better temporal
modeling. Moreover, we utilize modified SI-SNR as cost function to make the
model to have better echo cancellation and noise suppression (NS) performance.
With only 1.4M parameters, the proposed approach outperforms the AEC-challenge
baseline by 0.27 in terms of Mean Opinion Score (MOS).
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07578" title="Abstract">arXiv:2106.07578</a> [<a href="/pdf/2106.07578" title="Download PDF">pdf</a>, <a href="/format/2106.07578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Gradient Aggregation for Federated Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dimitriadis%2C+D">Dimitrios Dimitriadis</a>, 
<a href="/search/cs?searchtype=author&query=Kumatani%2C+K">Kenichi Kumatani</a>, 
<a href="/search/cs?searchtype=author&query=Gmyr%2C+R">Robert Gmyr</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+Y">Yashesh Gaur</a>, 
<a href="/search/cs?searchtype=author&query=Eskimez%2C+S+E">Sefik Emre Eskimez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2008.02452">arXiv:2008.02452</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, a new learning algorithm for Federated Learning (FL) is
introduced. The proposed scheme is based on a weighted gradient aggregation
using two-step optimization to offer a flexible training pipeline. Herein, two
different flavors of the aggregation method are presented, leading to an order
of magnitude improvement in convergence speed compared to other distributed or
FL training algorithms like BMUF and FedAvg. Further, the aggregation algorithm
acts as a regularizer of the gradient quality. We investigate the effect of our
FL algorithm in supervised and unsupervised Speech Recognition (SR) scenarios.
The experimental validation is performed based on three tasks: first, the
LibriSpeech task showing a speed-up of 7x and 6% word error rate reduction
(WERR) compared to the baseline results. The second task is based on session
adaptation providing 20% WERR over a powerful LAS model. Finally, our
unsupervised pipeline is applied to the conversational SR task. The proposed FL
system outperforms the baseline systems in both convergence speed and overall
model performance.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07582" title="Abstract">arXiv:2106.07582</a> [<a href="/pdf/2106.07582" title="Download PDF">pdf</a>, <a href="/format/2106.07582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non Gaussian Denoising Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nachmani%2C+E">Eliya Nachmani</a>, 
<a href="/search/cs?searchtype=author&query=Roman%2C+R+S">Robin San Roman</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+L">Lior Wolf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Sound (cs.SD)

</div>
<p class="mathjax">Generative diffusion processes are an emerging and effective tool for image
and speech generation. In the existing methods, the underline noise
distribution of the diffusion process is Gaussian noise. However, fitting
distributions with more degrees of freedom, could help the performance of such
generative models. In this work, we investigate other types of noise
distribution for the diffusion process. Specifically, we show that noise from
Gamma distribution provides improved results for image and speech generation.
Moreover, we show that using a mixture of Gaussian noise variables in the
diffusion process improves the performance over a diffusion process that is
based on a single distribution. Our approach preserves the ability to
efficiently sample state in the training diffusion process while using Gamma
noise and a mixture of noise.
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07583" title="Abstract">arXiv:2106.07583</a> [<a href="/pdf/2106.07583" title="Download PDF">pdf</a>, <a href="/ps/2106.07583" title="Download PostScript">ps</a>, <a href="/format/2106.07583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Biomedical Entity Linking via Contrastive Context Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ujiie%2C+S">Shogo Ujiie</a>, 
<a href="/search/cs?searchtype=author&query=Iso%2C+H">Hayate Iso</a>, 
<a href="/search/cs?searchtype=author&query=Aramaki%2C+E">Eiji Aramaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce BioCoM, a contrastive learning framework for biomedical entity
linking that uses only two resources: a small-sized dictionary and a large
number of raw biomedical articles. Specifically, we build the training
instances from raw PubMed articles by dictionary matching and use them to train
a context-aware entity linking model with contrastive learning. We predict the
normalized biomedical entity at inference time through a nearest-neighbor
search. Results found that BioCoM substantially outperforms state-of-the-art
models, especially in low-resource settings, by effectively using the context
of the entities.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07588" title="Abstract">arXiv:2106.07588</a> [<a href="/pdf/2106.07588" title="Download PDF">pdf</a>, <a href="/ps/2106.07588" title="Download PostScript">ps</a>, <a href="/format/2106.07588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scenarios of future Indian electricity demand accounting for space  cooling and electric vehicle adoption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Barbar%2C+M">Marc Barbar</a>, 
<a href="/search/eess?searchtype=author&query=Mallapragada%2C+D">Dharik Mallapragada</a>, 
<a href="/search/eess?searchtype=author&query=Alsup%2C+M">Meia Alsup</a>, 
<a href="/search/eess?searchtype=author&query=Stoner%2C+R">Robert Stoner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">India is expected to witness rapid growth in electricity use over the next
two decades. Here, we introduce a custom regression model to project
electricity consumption in India over the coming decades, which includes a
bottom-up estimate of electricity consumption for two major growth drivers, air
conditioning, and vehicle electrification. The model projections are available
at a customizable level of spatial aggregation at an hourly temporal
resolution, which makes them useful as inputs to long-term electricity
infrastructure planning studies. The approach is used to develop electricity
consumption data sets spanning various technology adoption and growth scenarios
up to the year 2050 in five-year increments. The aim of the data is to provide
a range of scenarios for India's demand growth given new technology adoption.
With long-term hourly demand projections serving as an essential input for
electricity infrastructure modeling, this data publication enables further work
on energy efficiency, generation, and transmission expansion planning for a
fast-growing and increasingly important region from a global climate mitigation
perspective.
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07590" title="Abstract">arXiv:2106.07590</a> [<a href="/pdf/2106.07590" title="Download PDF">pdf</a>, <a href="/ps/2106.07590" title="Download PostScript">ps</a>, <a href="/format/2106.07590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution level battery storage valuation framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Barbar%2C+M">Marc Barbar</a>, 
<a href="/search/eess?searchtype=author&query=Mallapragada%2C+D+S">Dharik S. Mallapragada</a>, 
<a href="/search/eess?searchtype=author&query=Stoner%2C+R">Robert Stoner</a>, 
<a href="/search/eess?searchtype=author&query=Perez-Arriaga%2C+I">Ignacio Perez-Arriaga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The growing demand for electricity in emerging markets and developing
economies such as India is causing loading and congestion problems on
distribution networks, particularly in urban locations. Electric utilities in
these regions face unique constraints regarding raising capital required to
upgrade their congested networks. Battery storage has emerged as a non-wire
alternative to feeder-level upgrades. This article presents a valuation
framework by optimally sizing and placing battery storage on the distribution
network. We evaluate the value of storage using a real options analysis through
a Markov Chain Monte Carlo to identify the least-cost network upgrade strategy,
given demand growth uncertainty. When applied to urban distribution network
feeders typical of those found in congested cities in India, the approach
highlights the economic value of network investment deferrals by making use of
battery storage. We find that storage costs below 261 USD/kWh justify
investments in distribution level storage and storage as a non-wire alternative
only makes sense on moderately loaded feeders where storage charging is still
feasible without violating network thermal capacity limits.
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07594" title="Abstract">arXiv:2106.07594</a> [<a href="/pdf/2106.07594" title="Download PDF">pdf</a>, <a href="/format/2106.07594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Contrastive Learning Automated
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yuning You</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary materials are available at <a href="https://yyou1996.github.io/files/icml2021_graphcl_automated_supplement.pdf.">this https URL</a> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Self-supervised learning on graph-structured data has drawn recent interest
for learning generalizable, transferable and robust representations from
unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged
with promising representation learning performance. Unfortunately, unlike its
counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data
augmentations, which have to be manually picked per dataset, by either rules of
thumb or trial-and-errors, owing to the diverse nature of graph data. That
significantly limits the more general applicability of GraphCL. Aiming to fill
in this crucial gap, this paper proposes a unified bi-level optimization
framework to automatically, adaptively and dynamically select data
augmentations when performing GraphCL on specific graph data. The general
framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as
min-max optimization. The selections of augmentations made by JOAO are shown to
be in general aligned with previous "best practices" observed from handcrafted
tuning: yet now being automated, more flexible and versatile. Moreover, we
propose a new augmentation-aware projection head mechanism, which will route
output features through different projection heads corresponding to different
augmentations chosen at each training step. Extensive experiments demonstrate
that JOAO performs on par with or sometimes better than the state-of-the-art
competitors including GraphCL, on multiple graph datasets of various scales and
types, yet without resorting to any laborious dataset-specific tuning on
augmentation selection. We release the code at
https://github.com/Shen-Lab/GraphCL_Automated.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07596" title="Abstract">arXiv:2106.07596</a> [<a href="/pdf/2106.07596" title="Download PDF">pdf</a>, <a href="/format/2106.07596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximizing Revenue with Adaptive Modulation and Multiple FECs in  Flexible Optical Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tornatore%2C+M">Massimo Tornatore</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shilin Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Flexible optical networks (FONs) are being adopted to accommodate the
increasingly heterogeneous traffic in today's Internet. However, in presence of
high traffic load, not all offered traffic can be satisfied at all time. As
carried traffic load brings revenues to operators, traffic blocking due to
limited spectrum resource leads to revenue losses. In this study, given a set
of traffic requests to be provisioned, we consider the problem of maximizing
operator's revenue, subject to limited spectrum resource and physical layer
impairments (PLIs), namely amplified spontaneous emission noise (ASE),
self-channel interference (SCI), cross-channel interference (XCI), and node
crosstalk. In FONs, adaptive modulation, multiple FEC, and the tuning of power
spectrum density (PSD) can be effectively employed to mitigate the impact of
PLIs. Hence, in our study, we propose a universal bandwidth-related impairment
evaluation model based on channel bandwidth, which allows a performance
analysis for different PSD, FEC and modulations. Leveraging this PLI model and
a piecewise linear fitting function, we succeed to formulate the revenue
maximization problem as a mixed integer linear program. Then, to solve the
problem on larger network instances, a fast two-phase heuristic algorithm is
also proposed, which is shown to be near-optimal for revenue maximization.
Through simulations, we demonstrate that using adaptive modulation enables to
significantly increase revenues in the scenario of high signal-to-noise ratio
(SNR), where the revenue can even be doubled for high traffic load, while using
multiple FECs is more profitable for scenarios with low SNR.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07597" title="Abstract">arXiv:2106.07597</a> [<a href="/pdf/2106.07597" title="Download PDF">pdf</a>, <a href="/format/2106.07597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLPerf Tiny Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banbury%2C+C">Colby Banbury</a>, 
<a href="/search/cs?searchtype=author&query=Reddi%2C+V+J">Vijay Janapa Reddi</a>, 
<a href="/search/cs?searchtype=author&query=Torelli%2C+P">Peter Torelli</a>, 
<a href="/search/cs?searchtype=author&query=Holleman%2C+J">Jeremy Holleman</a>, 
<a href="/search/cs?searchtype=author&query=Jeffries%2C+N">Nat Jeffries</a>, 
<a href="/search/cs?searchtype=author&query=Kiraly%2C+C">Csaba Kiraly</a>, 
<a href="/search/cs?searchtype=author&query=Montino%2C+P">Pietro Montino</a>, 
<a href="/search/cs?searchtype=author&query=Kanter%2C+D">David Kanter</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+S">Sebastian Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Pau%2C+D">Danilo Pau</a>, 
<a href="/search/cs?searchtype=author&query=Thakker%2C+U">Urmish Thakker</a>, 
<a href="/search/cs?searchtype=author&query=Torrini%2C+A">Antonio Torrini</a>, 
<a href="/search/cs?searchtype=author&query=Warden%2C+P">Peter Warden</a>, 
<a href="/search/cs?searchtype=author&query=Cordaro%2C+J">Jay Cordaro</a>, 
<a href="/search/cs?searchtype=author&query=Di+Guglielmo%2C+G">Giuseppe Di Guglielmo</a>, 
<a href="/search/cs?searchtype=author&query=Duarte%2C+J">Javier Duarte</a>, 
<a href="/search/cs?searchtype=author&query=Gibellini%2C+S">Stephen Gibellini</a>, 
<a href="/search/cs?searchtype=author&query=Parekh%2C+V">Videet Parekh</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+H">Honson Tran</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+N">Nhan Tran</a>, 
<a href="/search/cs?searchtype=author&query=Wenxu%2C+N">Niu Wenxu</a>, 
<a href="/search/cs?searchtype=author&query=Xuesong%2C+X">Xu Xuesong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TinyML Benchmark
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Advancements in ultra-low-power tiny machine learning (TinyML) systems
promise to unlock an entirely new class of smart applications. However,
continued progress is limited by the lack of a widely accepted and easily
reproducible benchmark for these systems. To meet this need, we present MLPerf
Tiny, the first industry-standard benchmark suite for ultra-low-power tiny
machine learning systems. The benchmark suite is the collaborative effort of
more than 50 organizations from industry and academia and reflects the needs of
the community. MLPerf Tiny measures the accuracy, latency, and energy of
machine learning inference to properly evaluate the tradeoffs between systems.
Additionally, MLPerf Tiny implements a modular design that enables benchmark
submitters to show the benefits of their product, regardless of where it falls
on the ML deployment stack, in a fair and reproducible manner. The suite
features four benchmarks: keyword spotting, visual wake words, image
classification, and anomaly detection.
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07603" title="Abstract">arXiv:2106.07603</a> [<a href="/pdf/2106.07603" title="Download PDF">pdf</a>, <a href="/format/2106.07603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Adimensional Scale Invariant Steffensen (ASIS) Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Candela%2C+V+F">Vicente F. Candela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 4 figures. Submitted to AMC. Comments and suggestions are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Dimensionality of parameters and variables is a fundamental issue in physics
but mostly ignored from a mathematical point of view. Diffculties arising from
dimensional inconsistence are overcome by scaling analysis and, often, both
concepts, dimensionality and scaling, are confused. In the particular case of
iterative methods for solving non-linear equations, dimensionality and scaling
affects their robutness: while some classical methods, such as Newton, are
adimensional and scale independent, some other iterations as Steffensen's are
not; their convergence depends on the scaling, and their evaluation needs a
dimensional congruence. In this paper we introduce the concept of adimensional
form of a function in order to study the behavior of iterative methods, thus
correcting, if possible, some pathological features. From this adimensional
form we will devise an adimensional and scale invariant method based on
Steffensen's which we will call ASIS method.
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07606" title="Abstract">arXiv:2106.07606</a> [<a href="/pdf/2106.07606" title="Download PDF">pdf</a>, <a href="/format/2106.07606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Physics Informed Neural Network for Time-Dependent Nonlinear and  Higher Order Partial Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mattey%2C+R">Revanth Mattey</a>, 
<a href="/search/math?searchtype=author&query=Ghosh%2C+S">Susanta Ghosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">A physics informed neural network (PINN) incorporates the physics of a system
by satisfying its boundary value problem through a neural network's loss
function. The PINN approach has shown great success in approximating the map
between the solution of a partial differential equation (PDE) and its
spatio-temporal input. However, for strongly non-linear and higher order
partial differential equations PINN's accuracy reduces significantly. To
resolve this problem, we propose a novel PINN scheme that solves the PDE
sequentially over successive time segments using a single neural network. The
key idea is to re-train the same neural network for solving the PDE over
successive time segments while satisfying the already obtained solution for all
previous time segments. Thus it is named as backward compatible PINN (bc-PINN).
To illustrate the advantages of bc-PINN, we have used the Cahn Hilliard and
Allen Cahn equations, which are widely used to describe phase separation and
reaction diffusion systems. Our results show significant improvement in
accuracy over the PINN method while using a smaller number of collocation
points. Additionally, we have shown that using the phase space technique for a
higher order PDE could further improve the accuracy and efficiency of the
bc-PINN scheme.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07609" title="Abstract">arXiv:2106.07609</a> [<a href="/pdf/2106.07609" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Exact Spectral Derivative Discretization Finite Difference (ESDDFD)  Method for Wave Models: A Universal Wave View Through Natural Fractional /  Fractal Derivative Representations (or View Lens Shops for The Exponential  Wave Universe)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Clemence-Mkhope%2C+D+P">D.P. Clemence-Mkhope</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A wave view of the universe is proposed in which each natural phenomenon is
equipped with its own unique natural viewing lens. A self-sameness modeling
principle and its systematic application in Fourier-Laplace transform space is
proposed as a novel, universal discrete modeling paradigm for
advection-diffusion-reaction equations (ADREs) across non-integer derivatives,
time scales, and wave spectral signatures. Its implementation is a novel exact
spectral derivative discretization finite difference method (ESDDFD), a way for
crafting wave viewing lenses by obtaining discrete wave models from ADRE
models. The template for building these lenses come in the form of natural
derivative representations obtained from the wave signature probability
distribution function and its harmonic oscillation in FL transform space; use
of the ESDDFD method in the discrete numerical modeling of wave equations
requires no a-priori theory of any mathematical derivative. A major
mathematical consequence of this viewpoint is that all notions of the
mathematical integer or non-integer derivatives have representation as limits
of such natural derivative representations; this and other consequences are
discussed and a discretization of a simple integer derivative
diffusion-reaction equation is presented to illustrate the method. The
resulting view lenses, in the form of ESDDFD models, work well in detecting
both local and non-local Debye or Kohlrausch-Williams-Watts exponential
patterns; only Brownian motion and sub-diffusion are discussed in the present
article.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07611" title="Abstract">arXiv:2106.07611</a> [<a href="/pdf/2106.07611" title="Download PDF">pdf</a>, <a href="/format/2106.07611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuroevolution-Enhanced Multi-Objective Optimization for Mixed-Precision  Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miret%2C+S">Santiago Miret</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+V+S">Vui Seng Chua</a>, 
<a href="/search/cs?searchtype=author&query=Marder%2C+M">Mattias Marder</a>, 
<a href="/search/cs?searchtype=author&query=Phielipp%2C+M">Mariano Phielipp</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+N">Nilesh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+S">Somdeb Majumdar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mixed-precision quantization is a powerful tool to enable memory and compute
savings of neural network workloads by deploying different sets of bit-width
precisions on separate compute operations. Recent research has shown
significant progress in applying mixed-precision quantization techniques to
reduce the memory footprint of various workloads, while also preserving task
performance. Prior work, however, has often ignored additional objectives, such
as bit-operations, that are important for deployment of workloads on hardware.
Here we present a flexible and scalable framework for automated mixed-precision
quantization that optimizes multiple objectives. Our framework relies on
Neuroevolution-Enhanced Multi-Objective Optimization (NEMO), a novel search
method, to find Pareto optimal mixed-precision configurations for memory and
bit-operations objectives. Within NEMO, a population is divided into
structurally distinct sub-populations (species) which jointly form the Pareto
frontier of solutions for the multi-objective problem. At each generation,
species are re-sized in proportion to the goodness of their contribution to the
Pareto frontier. This allows NEMO to leverage established search techniques and
neuroevolution methods to continually improve the goodness of the Pareto
frontier. In our experiments we apply a graph-based representation to describe
the underlying workload, enabling us to deploy graph neural networks trained by
NEMO to find Pareto optimal configurations for various workloads trained on
ImageNet. Compared to the state-of-the-art, we achieve competitive results on
memory compression and superior results for compute compression for
MobileNet-V2, ResNet50 and ResNeXt-101-32x8d. A deeper analysis of the results
obtained by NEMO also shows that both the graph representation and the
species-based approach are critical in finding effective configurations for all
workloads.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07613" title="Abstract">arXiv:2106.07613</a> [<a href="/pdf/2106.07613" title="Download PDF">pdf</a>, <a href="/format/2106.07613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Metric Dimensionality Reduction with Distributed Topology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wagner%2C+A">Alexander Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+E">Elchanan Solomon</a>, 
<a href="/search/cs?searchtype=author&query=Bendich%2C+P">Paul Bendich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Topology (math.AT)

</div>
<p class="mathjax">We propose a novel approach to dimensionality reduction combining techniques
of metric geometry and distributed persistent homology, in the form of a
gradient-descent based method called DIPOLE. DIPOLE is a
dimensionality-reduction post-processing step that corrects an initial
embedding by minimizing a loss functional with both a local, metric term and a
global, topological term. By fixing an initial embedding method (we use
Isomap), DIPOLE can also be viewed as a full dimensionality-reduction pipeline.
This framework is based on the strong theoretical and computational properties
of distributed persistent homology and comes with the guarantee of almost sure
convergence. We observe that DIPOLE outperforms popular methods like UMAP,
t-SNE, and Isomap on a number of popular datasets, both visually and in terms
of precise quantitative metrics.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07615" title="Abstract">arXiv:2106.07615</a> [<a href="/pdf/2106.07615" title="Download PDF">pdf</a>, <a href="/format/2106.07615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magic Layouts: Structural Prior for Component Detection in User  Interface Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manandhar%2C+D">Dipu Manandhar</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hailin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Collomosse%2C+J">John Collomosse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present Magic Layouts; a method for parsing screenshots or hand-drawn
sketches of user interface (UI) layouts. Our core contribution is to extend
existing detectors to exploit a learned structural prior for UI designs,
enabling robust detection of UI components; buttons, text boxes and similar.
Specifically we learn a prior over mobile UI layouts, encoding common spatial
co-occurrence relationships between different UI components. Conditioning
region proposals using this prior leads to performance gains on UI layout
parsing for both hand-drawn UIs and app screenshots, which we demonstrate
within the context an interactive application for rapidly acquiring digital
prototypes of user experience (UX) designs.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07616" title="Abstract">arXiv:2106.07616</a> [<a href="/pdf/2106.07616" title="Download PDF">pdf</a>, <a href="/format/2106.07616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Semi-Implicit Meshless Method for Incompressible Flows in Complex  Geometries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shahane%2C+S">Shantanu Shahane</a>, 
<a href="/search/math?searchtype=author&query=Vanka%2C+S+P">Surya Pratap Vanka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We present an exponentially convergent semi-implicit meshless algorithm for
the solution of Navier-Stokes equations in complex domains. The algorithm
discretizes partial derivatives at scattered points using radial basis
functions as interpolants. Higher-order polynomials are appended to
polyharmonic splines (PHS-RBF) and a collocation method is used to derive the
interpolation coefficients. The interpolating kernels are then differentiated
and the partial-differential equations are satisfied by collocation at the
scattered points. The PHS-RBF interpolation is shown to be exponentially
convergent with discretization errors decreasing as a high power of a
representative distance between points. We present here a semi-implicit
algorithm for time-dependent and steady state fluid flows in complex domains.
At each time step, several iterations are performed to converge the momentum
and continuity equations. A Poisson equation for pressure corrections is
formulated by imposing divergence free condition on the iterated velocity
field. At each time step, the momentum and pressure correction equations are
repeatedly solved until the velocities and pressure converge to a pre-specified
tolerance. We have demonstrated the convergence and discretization accuracy of
the algorithm for two model problems and simulated three other complex
problems. In all cases, the algorithm is stable for Courant numbers in excess
of ten. The algorithm has the potential to accurately and efficiently solve
many fluid flow and heat transfer problems in complex domains. An open source
code Meshless Multi-Physics Software (MeMPhyS) is available for interested
users of the algorithm.
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07617" title="Abstract">arXiv:2106.07617</a> [<a href="/pdf/2106.07617" title="Download PDF">pdf</a>, <a href="/format/2106.07617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delving Deep into the Generalization of Vision Transformers under  Distribution Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shanghang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Daisheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhongang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haiyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+S">Shuai Yi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, Vision Transformers (ViTs) have achieved impressive results on
various vision tasks. Yet, their generalization ability under different
distribution shifts is rarely understood. In this work, we provide a
comprehensive study on the out-of-distribution generalization of ViTs. To
support a systematic investigation, we first present a taxonomy of distribution
shifts by categorizing them into five conceptual groups: corruption shift,
background shift, texture shift, destruction shift, and style shift. Then we
perform extensive evaluations of ViT variants under different groups of
distribution shifts and compare their generalization ability with CNNs. Several
important observations are obtained: 1) ViTs generalize better than CNNs under
multiple distribution shifts. With the same or fewer parameters, ViTs are ahead
of corresponding CNNs by more than 5% in top-1 accuracy under most distribution
shifts. 2) Larger ViTs gradually narrow the in-distribution and
out-of-distribution performance gap. To further improve the generalization of
ViTs, we design the Generalization-Enhanced ViTs by integrating adversarial
learning, information theory, and self-supervised learning. By investigating
three types of generalization-enhanced ViTs, we observe their
gradient-sensitivity and design a smoother learning strategy to achieve a
stable training process. With modified training schemes, we achieve
improvements on performance towards out-of-distribution data by 4% from vanilla
ViTs. We comprehensively compare three generalization-enhanced ViTs with their
corresponding CNNs, and observe that: 1) For the enhanced model, larger ViTs
still benefit more for the out-of-distribution generalization. 2)
generalization-enhanced ViTs are more sensitive to the hyper-parameters than
corresponding CNNs. We hope our comprehensive study could shed light on the
design of more generalizable learning architectures.
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07620" title="Abstract">arXiv:2106.07620</a> [<a href="/pdf/2106.07620" title="Download PDF">pdf</a>, <a href="/format/2106.07620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast symplectic integrator for Nesterov-type acceleration method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Goto%2C+S">Shin-itiro Goto</a>, 
<a href="/search/math?searchtype=author&query=Hino%2C+H">Hideitsu Hino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, explicit stable integrators based on symplectic and contact
geometries are proposed for a non-autonomous ordinarily differential equation
(ODE) found in improving convergence rate of Nesterov's accelerated gradient
method. Symplectic geometry is known to be suitable for describing Hamiltonian
mechanics, and contact geometry is known as an odd-dimensional counterpart of
symplectic geometry. Moreover, a procedure, called symplectization, is a known
way to construct a symplectic manifold from a contact manifold, yielding
Hamiltonian systems from contact ones. It is found in this paper that a
previously investigated non-autonomous ODE can be written as a contact
Hamiltonian system. Then, by symplectization of a non-autonomous contact
Hamiltonian vector field expressing the non-autonomous ODE, novel symplectic
integrators are derived. Because the proposed symplectic integrators preserve
hidden symplectic and contact structures in the ODE, they should be more stable
than the Runge-Kutta method. Numerical experiments demonstrate that, as
expected, the second-order symplectic integrator is stable and high convergence
rates are achieved.
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07621" title="Abstract">arXiv:2106.07621</a> [<a href="/pdf/2106.07621" title="Download PDF">pdf</a>, <a href="/format/2106.07621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generalization of Classical Formulas in Numerical Integration and  Series Convergence Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alabdulmohsin%2C+I">Ibrahim Alabdulmohsin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Classical Analysis and ODEs (math.CA)

</div>
<p class="mathjax">Summation formulas, such as the Euler-Maclaurin expansion or Gregory's
quadrature, have found many applications in mathematics, ranging from
accelerating series, to evaluating fractional sums and analyzing asymptotics,
among others. We show that these summation formulas actually arise as
particular instances of a single series expansion, including Euler's method for
alternating series. This new summation formula gives rise to a family of
polynomials, which contain both the Bernoulli and Gregory numbers in their
coefficients. We prove some properties of those polynomials, such as recurrence
identities and symmetries. Lastly, we present one case study, which illustrates
one potential application of the new expansion for finite impulse response
(FIR) filters.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07625" title="Abstract">arXiv:2106.07625</a> [<a href="/pdf/2106.07625" title="Download PDF">pdf</a>, <a href="/format/2106.07625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On numerical aspects of parameter identification for the  Landau-Lifshitz-Gilbert equation in Magnetic Particle Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nguyen%2C+T+T+N">Tram Thi Ngoc Nguyen</a>, 
<a href="/search/math?searchtype=author&query=Wald%2C+A">Anne Wald</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Inverse Problems and Imaging, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The Landau-Lifshitz-Gilbert equation yields a mathematical model to describe
the evolution of the magnetization of a magnetic material, particularly in
response to an external applied magnetic field. It allows one to take into
account various physical effects, such as the exchange within the magnetic
material itself. In particular, the Landau-Lifshitz-Gilbert equation encodes
relaxation effects, i.e., it describes the time-delayed alignment of the
magnetization field with an external magnetic field. These relaxation effects
are an important aspect in magnetic particle imaging, particularly in the
calibration process. In this article, we address the data-driven modeling of
the system function in magnetic particle imaging, where the
Landau-Lifshitz-Gilbert equation serves as the basic tool to include relaxation
effects in the model. We formulate the respective parameter identification
problem both in the all-at-once and the reduced setting, present reconstruction
algorithms that yield a regularized solution and discuss numerical experiments.
Apart from that, we propose a practical numerical solver to the nonlinear
Landau-Lifshitz-Gilbert equation, not via the classical finite element method,
but through solving only linear PDEs in an inverse problem framework.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07627" title="Abstract">arXiv:2106.07627</a> [<a href="/pdf/2106.07627" title="Download PDF">pdf</a>, <a href="/format/2106.07627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Automatic Interpretation of 3D Plots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brandt%2C+L+E">Laura E. Brandt</a>, 
<a href="/search/cs?searchtype=author&query=Freeman%2C+W+T">William T. Freeman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 12 figures, accepted to the 16th International Conference on Document Analysis and Recognition (ICDAR21)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper explores the challenge of teaching a machine how to
reverse-engineer the grid-marked surfaces used to represent data in 3D surface
plots of two-variable functions. These are common in scientific and economic
publications; and humans can often interpret them with ease, quickly gleaning
general shape and curvature information from the simple collection of curves.
While machines have no such visual intuition, they do have the potential to
accurately extract the more detailed quantitative data that guided the
surface's construction. We approach this problem by synthesizing a new dataset
of 3D grid-marked surfaces (SurfaceGrid) and training a deep neural net to
estimate their shape. Our algorithm successfully recovers shape information
from synthetic 3D surface plots that have had axes and shading information
removed, been rendered with a variety of grid types, and viewed from a range of
viewpoints.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07628" title="Abstract">arXiv:2106.07628</a> [<a href="/pdf/2106.07628" title="Download PDF">pdf</a>, <a href="/format/2106.07628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multiresolution adaptive wavelet method for nonlinear partial  differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Harnish%2C+C">Cale Harnish</a>, 
<a href="/search/math?searchtype=author&query=Dalessandro%2C+L">Luke Dalessandro</a>, 
<a href="/search/math?searchtype=author&query=Matous%2C+K">Karel Matous</a>, 
<a href="/search/math?searchtype=author&query=Livescu%2C+D">Daniel Livescu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">The multiscale complexity of modern problems in computational science and
engineering can prohibit the use of traditional numerical methods in
multi-dimensional simulations. Therefore, novel algorithms are required in
these situations to solve partial differential equations (PDEs) with features
evolving on a wide range of spatial and temporal scales. To meet these
challenges, we present a multiresolution wavelet algorithm to solve PDEs with
significant data compression and explicit error control. We discretize in space
by projecting fields and spatial derivative operators onto wavelet basis
functions. We provide error estimates for the wavelet representation of fields
and their derivatives. Then, our estimates are used to construct a sparse
multiresolution discretization which guarantees the prescribed accuracy.
Additionally, we embed a predictor-corrector procedure within the temporal
integration to dynamically adapt the computational grid and maintain the
accuracy of the solution of the PDE as it evolves. We present examples to
highlight the accuracy and adaptivity of our approach.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07630" title="Abstract">arXiv:2106.07630</a> [<a href="/pdf/2106.07630" title="Download PDF">pdf</a>, <a href="/format/2106.07630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchically Regularized Deep Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paria%2C+B">Biswajit Paria</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+R">Rajat Sen</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+A">Amr Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Abhimanyu Das</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Hierarchical forecasting is a key problem in many practical multivariate
forecasting applications - the goal is to simultaneously predict a large number
of correlated time series that are arranged in a pre-specified aggregation
hierarchy. The challenge is to exploit the hierarchical correlations to
simultaneously obtain good prediction accuracy for time series at different
levels of the hierarchy. In this paper, we propose a new approach for
hierarchical forecasting based on decomposing the time series along a global
set of basis time series and modeling hierarchical constraints using the
coefficients of the basis decomposition for each time series. Unlike past
methods, our approach is scalable at inference-time (forecasting for a specific
time series only needs access to its own data) while (approximately) preserving
coherence among the time series forecasts. We experiment on several publicly
available datasets and demonstrate significantly improved overall performance
on forecasts at different levels of the hierarchy, compared to existing
state-of-the-art hierarchical reconciliation methods.
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07631" title="Abstract">arXiv:2106.07631</a> [<a href="/pdf/2106.07631" title="Download PDF">pdf</a>, <a href="/format/2106.07631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Transformer for High-Resolution GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Long Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zizhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Ting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Metaxas%2C+D+N">Dimitris N. Metaxas</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Attention-based models, exemplified by the Transformer, can effectively model
long range dependency, but suffer from the quadratic complexity of
self-attention operation, making them difficult to be adopted for
high-resolution image generation based on Generative Adversarial Networks
(GANs). In this paper, we introduce two key ingredients to Transformer to
address this challenge. First, in low-resolution stages of the generative
process, standard global self-attention is replaced with the proposed
multi-axis blocked self-attention which allows efficient mixing of local and
global attention. Second, in high-resolution stages, we drop self-attention
while only keeping multi-layer perceptrons reminiscent of the implicit neural
function. To further improve the performance, we introduce an additional
self-modulation component based on cross-attention. The resulting model,
denoted as HiT, has a linear computational complexity with respect to the image
size and thus directly scales to synthesizing high definition images. We show
in the experiments that the proposed HiT achieves state-of-the-art FID scores
of 31.87 and 2.95 on unconditional ImageNet $128 \times 128$ and FFHQ $256
\times 256$, respectively, with a reasonable throughput. We believe the
proposed HiT is an important milestone for generators in GANs which are
completely free of convolutions.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07635" title="Abstract">arXiv:2106.07635</a> [<a href="/pdf/2106.07635" title="Download PDF">pdf</a>, <a href="/format/2106.07635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Causal Networks: Approximate Bayesian Inference over Causal  Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Annadani%2C+Y">Yashas Annadani</a>, 
<a href="/search/cs?searchtype=author&query=Rothfuss%2C+J">Jonas Rothfuss</a>, 
<a href="/search/cs?searchtype=author&query=Lacoste%2C+A">Alexandre Lacoste</a>, 
<a href="/search/cs?searchtype=author&query=Scherrer%2C+N">Nino Scherrer</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+A">Anirudh Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+S">Stefan Bauer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning the causal structure that underlies data is a crucial step towards
robust real-world decision making. The majority of existing work in causal
inference focuses on determining a single directed acyclic graph (DAG) or a
Markov equivalence class thereof. However, a crucial aspect to acting
intelligently upon the knowledge about causal structure which has been inferred
from finite data demands reasoning about its uncertainty. For instance,
planning interventions to find out more about the causal mechanisms that govern
our data requires quantifying epistemic uncertainty over DAGs. While Bayesian
causal inference allows to do so, the posterior over DAGs becomes intractable
even for a small number of variables. Aiming to overcome this issue, we propose
a form of variational inference over the graphs of Structural Causal Models
(SCMs). To this end, we introduce a parametric variational family modelled by
an autoregressive distribution over the space of discrete DAGs. Its number of
parameters does not grow exponentially with the number of variables and can be
tractably learned by maximising an Evidence Lower Bound (ELBO). In our
experiments, we demonstrate that the proposed variational posterior is able to
provide a good approximation of the true posterior.
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07643" title="Abstract">arXiv:2106.07643</a> [<a href="/pdf/2106.07643" title="Download PDF">pdf</a>, <a href="/format/2106.07643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Learning of Visual 3D Keypoints for Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+D">Deepak Pathak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICML 2021. Videos and code at <a href="https://buoyancy99.github.io/unsup-3d-keypoints/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">Learning sensorimotor control policies from high-dimensional images crucially
relies on the quality of the underlying visual representations. Prior works
show that structured latent space such as visual keypoints often outperforms
unstructured representations for robotic control. However, most of these
representations, whether structured or unstructured are learned in a 2D space
even though the control tasks are usually performed in a 3D environment. In
this work, we propose a framework to learn such a 3D geometric structure
directly from images in an end-to-end unsupervised manner. The input images are
embedded into latent 3D keypoints via a differentiable encoder which is trained
to optimize both a multi-view consistency loss and downstream task objective.
These discovered 3D keypoints tend to meaningfully capture robot joints as well
as object movements in a consistent manner across both time and 3D space. The
proposed approach outperforms prior state-of-art methods across a variety of
reinforcement learning benchmarks. Code and videos at
https://buoyancy99.github.io/unsup-3d-keypoints/
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue, 15 Jun 21</h3>
<dl>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.03121" title="Abstract">arXiv:1912.03121</a> (cross-list from physics.flu-dyn) [<a href="/pdf/1912.03121" title="Download PDF">pdf</a>, <a href="/format/1912.03121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical methods for hydraulic transients in visco-elastic pipes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Bertaglia%2C+G">Giulia Bertaglia</a>, 
<a href="/search/physics?searchtype=author&query=Ioriatti%2C+M">Matteo Ioriatti</a>, 
<a href="/search/physics?searchtype=author&query=Valiani%2C+A">Alessandro Valiani</a>, 
<a href="/search/physics?searchtype=author&query=Dumbser%2C+M">Michael Dumbser</a>, 
<a href="/search/physics?searchtype=author&query=Caleffi%2C+V">Valerio Caleffi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Fluids Struct. 81 (2018) 230-254
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">A wide and critical comparison of the capability of Method of
Characteristics, Explicit Path-Conservative Finite Volume Method and
Semi-Implicit Staggered Finite Volume Method is presented and discussed, in
terms of accuracy and efficiency. The viscoelastic behaviour of the pipe wall,
the effects of the unsteadiness of the flow on the friction losses, cavitation
and cross-sectional changes are considered. The analyses are performed
comparing numerical solutions obtained using the three models against
experimental data and analytical solutions. Water hammer studies in high
density polyethylene pipes, for which laboratory data have been provided, are
used as test cases. Considering the viscoelastic mechanical behaviour of
plastic materials, a 3-parameter and a multi-parameter linear viscoelastic
rheological model are adopted and implemented in each numerical scheme.
Original extensions of existing techniques for the numerical treatment of such
viscoelastic models are introduced in this work for the first time. After a
focused calibration of the viscoelastic parameters, the different performance
of the numerical models is investigated. A comparison of the results is
presented considering the unsteady wall-shear stress, with a new approach
proposed for turbulent flows, or simply considering a quasi-steady friction
model. A predominance of the damping effect due to viscoelasticity with respect
to the damping effect related to the unsteady friction is confirmed in these
contexts. All the numerical methods show a good agreement with the experimental
data and a high efficiency of the Method of Characteristics in standard
configuration is observed. Three Riemann Problems are chosen and run to stress
the numerical methods, considering cross-sectional changes, more flexible
materials and cavitation cases. In these demanding scenarios, the weak spots of
the Method of Characteristics are depicted.
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.03167" title="Abstract">arXiv:1912.03167</a> (cross-list from physics.flu-dyn) [<a href="/pdf/1912.03167" title="Download PDF">pdf</a>, <a href="/format/1912.03167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling blood flow in viscoelastic vessels: the 1D augmented  fluid-structure interaction system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Bertaglia%2C+G">Giulia Bertaglia</a>, 
<a href="/search/physics?searchtype=author&query=Caleffi%2C+V">Valerio Caleffi</a>, 
<a href="/search/physics?searchtype=author&query=Valiani%2C+A">Alessandro Valiani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in Computer Methods in Applied Mechanics and Engineering
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Comput. Methods Appl. Mech. Eng. 360 (2020) 112772
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Mathematical models and numerical simulations are widely used in the field of
hemodynamics, representing a valuable resource to better understand
physiological and pathological processes. The theory behind the phenomenon is
closely related to the study of incompressible flow through compliant
thin-walled tubes. The mechanical interaction between blood flow and vessel
wall must be properly described by the model. Recent works show the benefits of
characterizing the rheology of the vessel wall through a viscoelastic law.
Considering the viscous contribution of the wall material and not simply the
elastic one leads to a more realistic representation of the vessel behavior,
which manifests not only an instantaneous elastic strain but also a viscous
damping effect on pulse pressure waves, coupled to energy losses. The aim of
this work is to propose an easily extensible 1D mathematical model able to
accurately capture the FSI. The originality lies in the introduction of a
viscoelastic tube law in PDE form, valid for both arterial and venous networks,
leading to an augmented FSI system. In contrast to well established
mathematical models, the proposed one is natively hyperbolic. The model is
solved with a 2nd-order numerical scheme based on an IMEX RK scheme conceived
for applications to hyperbolic systems with stiff relaxation terms. The
validation of the model is performed on different tests. Results obtained in
Riemann problems, adopting a simple elastic tube law for the characterization
of the vessel wall, are compared with available exact solutions. To validate
the contribution given by the viscoelastic term, the Method of Manufactured
Solutions is applied. Specific tests are designed to verify the well-balancing
and the AP property of the scheme. A specific test with an inlet pulse pressure
wave is designed to assess the effects of viscoelasticity.
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.03285" title="Abstract">arXiv:1912.03285</a> (cross-list from physics.flu-dyn) [<a href="/pdf/1912.03285" title="Download PDF">pdf</a>, <a href="/format/1912.03285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational hemodynamics in arteries with the one-dimensional  augmented fluid-structure interaction system: viscoelastic parameters  estimation and comparison with in-vivo data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Bertaglia%2C+G">Giulia Bertaglia</a>, 
<a href="/search/physics?searchtype=author&query=Navas-Montilla%2C+A">Adri&#xe1;n Navas-Montilla</a>, 
<a href="/search/physics?searchtype=author&query=Valiani%2C+A">Alessandro Valiani</a>, 
<a href="/search/physics?searchtype=author&query=Garc%C3%ADa%2C+M+I+M">Manuel Ignacio Monge Garc&#xed;a</a>, 
<a href="/search/physics?searchtype=author&query=Murillo%2C+J">Javier Murillo</a>, 
<a href="/search/physics?searchtype=author&query=Caleffi%2C+V">Valerio Caleffi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to Journal of Biomechanics
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Biomech. 100 (2020) 109595
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Mathematical models are widely recognized as a valuable tool for
cardiovascular diagnosis and the study of circulatory diseases, especially to
obtain data that require otherwise invasive measurements. To correctly simulate
body hemodynamics, the viscoelastic properties of vessel walls are a key aspect
to be taken into account as they play an essential role in cardiovascular
behavior. The present work aims to apply the augmented fluid-structure
interaction system of blood flow to real case studies to assess the validity of
the model as a valuable resource to improve cardiovascular diagnostics and the
treatment of pathologies. First, the ability of the model to correctly simulate
pulse waveforms in single arterial segments is verified using literature
benchmark test cases. Such cases are designed taking into account a simple
elastic behavior of the wall in the upper thoracic aorta and in the common
carotid artery. Furthermore, in-vivo pressure waveforms, extracted from
tonometric measurements performed on four human common carotid arteries and two
common femoral arteries, are compared to numerical solutions. It is highlighted
that the viscoelastic damping effect of arterial walls is required to avoid an
overestimation of pressure peaks. An effective procedure to estimate the
viscoelastic parameters of the model is herein proposed, which returns
hysteresis curves of the common carotid arteries dissipating energy fractions
in line with values calculated from literature hysteresis loops in the same
vessel.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06573" title="Abstract">arXiv:2106.06573</a> (cross-list from stat.ML) [<a href="/pdf/2106.06573" title="Download PDF">pdf</a>, <a href="/ps/2106.06573" title="Download PostScript">ps</a>, <a href="/format/2106.06573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Deflation Process in Over-parametrized Tensor  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ge%2C+R">Rong Ge</a>, 
<a href="/search/stat?searchtype=author&query=Ren%2C+Y">Yunwei Ren</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/stat?searchtype=author&query=Zhou%2C+M">Mo Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper we study the training dynamics for gradient flow on
over-parametrized tensor decomposition problems. Empirically, such training
process often first fits larger components and then discovers smaller
components, which is similar to a tensor deflation process that is commonly
used in tensor decomposition algorithms. We prove that for orthogonally
decomposable tensor, a slightly modified version of gradient flow would follow
a tensor deflation process and recover all the tensor components. Our proof
suggests that for orthogonal tensors, gradient flow dynamics works similarly as
greedy low-rank learning in the matrix setting, which is a first step towards
understanding the implicit regularization effect of over-parametrized models
for low-rank tensors.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06574" title="Abstract">arXiv:2106.06574</a> (cross-list from math.OC) [<a href="/pdf/2106.06574" title="Download PDF">pdf</a>, <a href="/format/2106.06574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Landscape Correspondence of Empirical and Population Risks in the  Eigendecomposition Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+S">Shuang Li</a>, 
<a href="/search/math?searchtype=author&query=Tang%2C+G">Gongguo Tang</a>, 
<a href="/search/math?searchtype=author&query=Wakin%2C+M+B">Michael B. Wakin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Spectral methods include a family of algorithms related to the eigenvectors
of certain data-generated matrices. In this work, we are interested in studying
the geometric landscape of the eigendecomposition problem in various spectral
methods. In particular, we first extend known results regarding the landscape
at critical points to larger regions near the critical points in a special case
of finding the leading eigenvector of a symmetric matrix. For a more general
eigendecomposition problem, inspired by recent findings on the connection
between the landscapes of empirical risk and population risk, we then build a
novel connection between the landscape of an eigendecomposition problem that
uses random measurements and the one that uses the true data matrix. We also
apply our theory to a variety of low-rank matrix optimization problems and
conduct a series of simulations to illustrate our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06608" title="Abstract">arXiv:2106.06608</a> (cross-list from stat.ME) [<a href="/pdf/2106.06608" title="Download PDF">pdf</a>, <a href="/format/2106.06608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Analysis from the Fourier Integral Theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>, 
<a href="/search/stat?searchtype=author&query=Walker%2C+S+G">Stephen G. Walker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">Taking the Fourier integral theorem as our starting point, in this paper we
focus on natural Monte Carlo and fully nonparametric estimators of multivariate
distributions and conditional distribution functions. We do this without the
need for any estimated covariance matrix or dependence structure between
variables. These aspects arise immediately from the integral theorem. Being
able to model multivariate data sets using conditional distribution functions
we can study a number of problems, such as prediction for Markov processes,
estimation of mixing distribution functions which depend on covariates, and
general multivariate data. Estimators are explicit Monte Carlo based and
require no recursive or iterative algorithms.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06664" title="Abstract">arXiv:2106.06664</a> (cross-list from eess.IV) [<a href="/pdf/2106.06664" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rapid COVID-19 Risk Screening by Eye-region Manifestations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fu%2C+Y">Yanwei Fu</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+L">Lei Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+H">Haojie Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+Q">Qiang Sun</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+L">Li Yang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hong Li</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+J">Jiao Xie</a>, 
<a href="/search/eess?searchtype=author&query=Xue%2C+X">Xiangyang Xue</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+F">Feng Li</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Pei%2C+Y">Yantao Pei</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+X">Xiuqi Wu</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+Y">Yanhua Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Gu1%2C+H+T+M">Hongxia Tian Mengwei Gu1</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">It is still nontrivial to develop a new fast COVID-19 screening method with
the easier access and lower cost, due to the technical and cost limitations of
the current testing methods in the medical resource-poor districts. On the
other hand, there are more and more ocular manifestations that have been
reported in the COVID-19 patients as growing clinical evidence[1]. This
inspired this project. We have conducted the joint clinical research since
January 2021 at the ShiJiaZhuang City, Heibei province, China, which approved
by the ethics committee of The fifth hospital of ShiJiaZhuang of Hebei Medical
University. We undertake several blind tests of COVID-19 patients by Union
Hospital, Tongji Medical College, Huazhong University of Science and
Technology, Wuhan, China. Meantime as an important part of the ongoing globally
COVID-19 eye test program by AIMOMICS since February 2020, we propose a new
fast screening method of analyzing the eye-region images, captured by common
CCD and CMOS cameras. This could reliably make a rapid risk screening of
COVID-19 with the sustainable stable high performance in different countries
and races. Our model for COVID-19 rapid prescreening have the merits of the
lower cost, fully self-performed, non-invasive, importantly real-time, and thus
enables the continuous health surveillance. We further implement it as the open
accessible APIs, and provide public service to the world. Our pilot experiments
show that our model is ready to be usable to all kinds of surveillance
scenarios, such as infrared temperature measurement device at airports and
stations, or directly pushing to the target people groups smartphones as a
packaged application.
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06691" title="Abstract">arXiv:2106.06691</a> (cross-list from stat.ML) [<a href="/pdf/2106.06691" title="Download PDF">pdf</a>, <a href="/format/2106.06691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Doubly Non-Central Beta Matrix Factorization for DNA Methylation Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Schein%2C+A">Aaron Schein</a>, 
<a href="/search/stat?searchtype=author&query=Nagulpally%2C+A">Anjali Nagulpally</a>, 
<a href="/search/stat?searchtype=author&query=Wallach%2C+H">Hanna Wallach</a>, 
<a href="/search/stat?searchtype=author&query=Flaherty%2C+P">Patrick Flaherty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Genomics (q-bio.GN); Applications (stat.AP)

</div>
<p class="mathjax">We present a new non-negative matrix factorization model for $(0,1)$
bounded-support data based on the doubly non-central beta (DNCB) distribution,
a generalization of the beta distribution. The expressiveness of the DNCB
distribution is particularly useful for modeling DNA methylation datasets,
which are typically highly dispersed and multi-modal; however, the model
structure is sufficiently general that it can be adapted to many other domains
where latent representations of $(0,1)$ bounded-support data are of interest.
Although the DNCB distribution lacks a closed-form conjugate prior, several
augmentations let us derive an efficient posterior inference algorithm composed
entirely of analytic updates. Our model improves out-of-sample predictive
performance on both real and synthetic DNA methylation datasets over
state-of-the-art methods in bioinformatics. In addition, our model yields
meaningful latent representations that accord with existing biological
knowledge.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06718" title="Abstract">arXiv:2106.06718</a> (cross-list from astro-ph.HE) [<a href="/pdf/2106.06718" title="Download PDF">pdf</a>, <a href="/format/2106.06718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Convolutional Neural Networks for the Helicity Classification of  Magnetic Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Vago%2C+N+O+P">Nicol&#xf2; Oreste Pinciroli Vago</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hameed%2C+I+A">Ibrahim A. Hameed</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kachelriess%2C+M">Michael Kachelriess</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, extended version of a contribution to the proceedings of the 37.th ICRC 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Astrophysical Phenomena (astro-ph.HE)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); High Energy Physics - Phenomenology (hep-ph)

</div>
<p class="mathjax">The presence of non-zero helicity in intergalactic magnetic fields is a
smoking gun for their primordial origin since they have to be generated by
processes that break CP invariance. As an experimental signature for the
presence of helical magnetic fields, an estimator $Q$ based on the triple
scalar product of the wave-vectors of photons generated in electromagnetic
cascades from, e.g., TeV blazars, has been suggested previously. We propose to
apply deep learning to helicity classification employing Convolutional Neural
Networks and show that this method outperforms the $Q$ estimator.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06741" title="Abstract">arXiv:2106.06741</a> (cross-list from math.OC) [<a href="/pdf/2106.06741" title="Download PDF">pdf</a>, <a href="/format/2106.06741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Optimization with Markovian Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+M">Mengmeng Li</a>, 
<a href="/search/math?searchtype=author&query=Sutter%2C+T">Tobias Sutter</a>, 
<a href="/search/math?searchtype=author&query=Kuhn%2C+D">Daniel Kuhn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study a stochastic program where the probability distribution of the
uncertain problem parameters is unknown and only indirectly observed via
finitely many correlated samples generated by an unknown Markov chain with $d$
states. We propose a data-driven distributionally robust optimization model to
estimate the problem's objective function and optimal solution. By leveraging
results from large deviations theory, we derive statistical guarantees on the
quality of these estimators. The underlying worst-case expectation problem is
nonconvex and involves $\mathcal O(d^2)$ decision variables. Thus, it cannot be
solved efficiently for large $d$. By exploiting the structure of this problem,
we devise a customized Frank-Wolfe algorithm with convex direction-finding
subproblems of size $\mathcal O(d)$. We prove that this algorithm finds a
stationary point efficiently under mild conditions. The efficiency of the
method is predicated on a dimensionality reduction enabled by a dual
reformulation. Numerical experiments indicate that our approach has better
computational and statistical properties than the state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06743" title="Abstract">arXiv:2106.06743</a> (cross-list from eess.IV) [<a href="/pdf/2106.06743" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hippocampus segmentation in magnetic resonance images of Alzheimer&#x27;s  patients using Deep machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Varmazyar%2C+H">Hadi Varmazyar</a>, 
<a href="/search/eess?searchtype=author&query=Yousefi-Banaem%2C+H">Hossein Yousefi-Banaem</a>, 
<a href="/search/eess?searchtype=author&query=Malekzadeh%2C+S">Saber Malekzadeh</a>, 
<a href="/search/eess?searchtype=author&query=Gharehaghaji%2C+N">Nahideh Gharehaghaji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Background: Alzheimers disease is a progressive neurodegenerative disorder
and the main cause of dementia in aging. Hippocampus is prone to changes in the
early stages of Alzheimers disease. Detection and observation of the
hippocampus changes using magnetic resonance imaging (MRI) before the onset of
Alzheimers disease leads to the faster preventive and therapeutic measures.
Objective: The aim of this study was the segmentation of the hippocampus in
magnetic resonance (MR) images of Alzheimers patients using deep machine
learning method. Methods: U-Net architecture of convolutional neural network
was proposed to segment the hippocampus in the real MRI data. The MR images of
the 100 and 35 patients available in Alzheimers disease Neuroimaging Initiative
(ADNI) dataset, was used for the train and test of the model, respectively. The
performance of the proposed method was compared with manual segmentation by
measuring the similarity metrics. Results: The desired segmentation achieved
after 10 iterations. A Dice similarity coefficient (DSC) = 92.3%, sensitivity =
96.5%, positive predicted value (PPV) = 90.4%, and Intersection over Union
(IoU) value for the train 92.94 and test 92.93 sets were obtained which are
acceptable. Conclusion: The proposed approach is promising and can be extended
in the prognosis of Alzheimers disease by the prediction of the hippocampus
volume changes in the early stage of the disease.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06784" title="Abstract">arXiv:2106.06784</a> (cross-list from eess.IV) [<a href="/pdf/2106.06784" title="Download PDF">pdf</a>, <a href="/format/2106.06784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Residual Networks based Distortion Classification and Ranking for  Laparoscopic Image Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khan%2C+Z+A">Zohaib Amjad Khan</a>, 
<a href="/search/eess?searchtype=author&query=Beghdadi%2C+A">Azeddine Beghdadi</a>, 
<a href="/search/eess?searchtype=author&query=Kaaniche%2C+M">Mounir Kaaniche</a>, 
<a href="/search/eess?searchtype=author&query=Cheikh%2C+F+A">Faouzi Alaya Cheikh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 Pages, ICIP 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Laparoscopic images and videos are often affected by different types of
distortion like noise, smoke, blur and nonuniform illumination. Automatic
detection of these distortions, followed generally by application of
appropriate image quality enhancement methods, is critical to avoid errors
during surgery. In this context, a crucial step involves an objective
assessment of the image quality, which is a two-fold problem requiring both the
classification of the distortion type affecting the image and the estimation of
the severity level of that distortion. Unlike existing image quality measures
which focus mainly on estimating a quality score, we propose in this paper to
formulate the image quality assessment task as a multi-label classification
problem taking into account both the type as well as the severity level (or
rank) of distortions. Here, this problem is then solved by resorting to a deep
neural networks based approach. The obtained results on a laparoscopic image
dataset show the efficiency of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06805" title="Abstract">arXiv:2106.06805</a> (cross-list from stat.AP) [<a href="/pdf/2106.06805" title="Download PDF">pdf</a>, <a href="/format/2106.06805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Higher Education Throughput in South Africa Using a  Tree-Based Ensemble Technique
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mbuvha%2C+R">Rendani Mbuvha</a>, 
<a href="/search/stat?searchtype=author&query=Zondo%2C+P">Patience Zondo</a>, 
<a href="/search/stat?searchtype=author&query=Mauda%2C+A">Aluwani Mauda</a>, 
<a href="/search/stat?searchtype=author&query=Marwala%2C+T">Tshilidzi Marwala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We use gradient boosting machines and logistic regression to predict academic
throughput at a South African university. The results highlight the significant
influence of socio-economic factors and field of study as predictors of
throughput. We further find that socio-economic factors become less of a
predictor relative to the field of study as the time to completion increases.
We provide recommendations on interventions to counteract the identified
effects, which include academic, psychosocial and financial support.
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06806" title="Abstract">arXiv:2106.06806</a> (cross-list from math.AP) [<a href="/pdf/2106.06806" title="Download PDF">pdf</a>, <a href="/format/2106.06806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a parabolic sine-Gordon model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cheng%2C+X">Xinyu Cheng</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+D">Dong Li</a>, 
<a href="/search/math?searchtype=author&query=Quan%2C+C">Chaoyu Quan</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+W">Wen Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages; to appear in "Numerical Mathematics: Theory, Methods and Applications"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We consider a parabolic sine-Gordon model with periodic boundary conditions.
We prove a fundamental maximum principle which gives a priori uniform control
of the solution. In the one-dimensional case we classify all bounded steady
states and exhibit some explicit solutions. For the numerical discretization we
employ first order IMEX, and second order BDF2 discretization without any
additional stabilization term. We rigorously prove the energy stability of the
numerical schemes under nearly sharp and quite mild time step constraints. We
demonstrate the striking similarity of the parabolic sine-Gordon model with the
standard Allen-Cahn equations with double well potentials.
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06841" title="Abstract">arXiv:2106.06841</a> (cross-list from quant-ph) [<a href="/pdf/2106.06841" title="Download PDF">pdf</a>, <a href="/ps/2106.06841" title="Download PostScript">ps</a>, <a href="/format/2106.06841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Algorithms and Simulation for Parallel and Distributed Quantum  Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Parekh%2C+R">Rhea Parekh</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ricciardi%2C+A">Andrea Ricciardi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Darwish%2C+A">Ahmed Darwish</a>, 
<a href="/search/quant-ph?searchtype=author&query=DiAdamo%2C+S">Stephen DiAdamo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">A viable approach for building large-scale quantum computers is to interlink
small-scale quantum computers with a quantum network to create a larger
distributed quantum computer. When designing quantum algorithms for such a
distributed quantum computer, one can make use of the added parallelization and
distribution abilities inherent in such a system. An added difficulty to
consider for distributed quantum computing is that a complex control system to
orchestrate the various components is required. In this work, we present
distributed and parallel versions of quantum algorithms and discuss potential
benefits and we propose a general scheme for controlling the system. Further,
we present the Interlin-q simulation platform which aims to simplify designing
and simulating parallel and distributed quantum algorithms. Interlin-q's main
features are generating and executing control instructions across a simulated
quantum network of simulated quantum computers. We demonstrate a simulation of
a proposed parallelized algorithm using Interlin-q and discuss steps for
developing Interlin-q into a control system for distributed quantum computers.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06855" title="Abstract">arXiv:2106.06855</a> (cross-list from eess.SP) [<a href="/pdf/2106.06855" title="Download PDF">pdf</a>, <a href="/ps/2106.06855" title="Download PostScript">ps</a>, <a href="/format/2106.06855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Wideband Sliding Correlation Channel Sounder in 65 nm CMOS: Evaluation  Board Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shakya%2C+D">Dipankar Shakya</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+T">Ting Wu</a>, 
<a href="/search/eess?searchtype=author&query=Knox%2C+M+E">Michael E. Knox</a>, 
<a href="/search/eess?searchtype=author&query=Rappaport%2C+T+S">Theodore S. Rappaport</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 10 figures, Transactions on Circuits and Systems-II: Express Briefs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Emerging applications such as wireless sensing, position location, robotics,
and many more are driven by the ultra-wide bandwidths available at
millimeter-wave (mmWave) and Terahertz (THz) frequencies. The characterization
and efficient utilization of wireless channels at these extremely high
frequencies require detailed knowledge of the radio propagation characteristics
of the channels. Such knowledge is developed through empirical observations of
operating conditions using wireless transceivers that measure the impulse
response through channel sounding. Today, cutting-edge channel sounders rely on
several bulky RF hardware components with complicated interconnections, large
parasitics, and sub-GHz RF bandwidth. This paper presents a compact sliding
correlation-based channel sounder baseband built on a monolithic integrated
circuit (IC) using 65 nm CMOS, implemented as an evaluation board achieving a 2
GHz RF bandwidth. The IC is the worlds first gigabit-per-second channel sounder
baseband implemented in low-cost CMOS. The presented single-board system can be
employed at both the transmit and receive baseband to study multipath
characteristics and path loss. Thus, the singleboard implementation provides an
inexpensive and compact solution for sliding correlation-based channel sounding
with 1 ns multipath delay resolution.
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06858" title="Abstract">arXiv:2106.06858</a> (cross-list from eess.AS) [<a href="/pdf/2106.06858" title="Download PDF">pdf</a>, <a href="/format/2106.06858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving weakly supervised sound event detection with self-supervised  auxiliary tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Deshmukh%2C+S">Soham Deshmukh</a>, 
<a href="/search/eess?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+R">Rita Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at INTERSPEECH 21
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While multitask and transfer learning has shown to improve the performance of
neural networks in limited data settings, they require pretraining of the model
on large datasets beforehand. In this paper, we focus on improving the
performance of weakly supervised sound event detection in low data and noisy
settings simultaneously without requiring any pretraining task. To that extent,
we propose a shared encoder architecture with sound event detection as a
primary task and an additional secondary decoder for a self-supervised
auxiliary task. We empirically evaluate the proposed framework for weakly
supervised sound event detection on a remix dataset of the DCASE 2019 task 1
acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20
dB SNR. To ensure we retain the localisation information of multiple sound
events, we propose a two-step attention pooling mechanism that provides a
time-frequency localisation of multiple audio events in the clip. The proposed
framework with two-step attention outperforms existing benchmark models by
22.3%, 12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an
ablation study to determine the contribution of the auxiliary task and two-step
attention pooling to the SED performance improvement.
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06881" title="Abstract">arXiv:2106.06881</a> (cross-list from math.OC) [<a href="/pdf/2106.06881" title="Download PDF">pdf</a>, <a href="/format/2106.06881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A public transit network optimization model for equitable access to  social services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rumpf%2C+A">Adam Rumpf</a>, 
<a href="/search/math?searchtype=author&query=Kaul%2C+H">Hemanshu Kaul</a> (Illinois Institute of Technology)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We present a flexible public transit network design model which optimizes a
social access objective while guaranteeing that the system's costs and transit
times remain within a preset margin of their current levels. The purpose of the
model is to find a set of minor, immediate modifications to an existing bus
network that can give more communities access to the chosen services while
having a minimal impact on the current network's operator costs and user costs.
Design decisions consist of reallocation of existing resources in order to
adjust line frequencies and capacities. We present a hybrid tabu
search/simulated annealing algorithm for the solution of this
optimization-based model. As a case study we apply the model to the problem of
improving equity of access to primary health care facilities in the Chicago
metropolitan area. The results of the model suggest that it is possible to
achieve better primary care access equity through reassignment of existing
buses and implementation of express runs, while leaving overall service levels
relatively unaffected.
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06891" title="Abstract">arXiv:2106.06891</a> (cross-list from math.OC) [<a href="/pdf/2106.06891" title="Download PDF">pdf</a>, <a href="/ps/2106.06891" title="Download PostScript">ps</a>, <a href="/format/2106.06891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Alternating Direction Method of Multipliers for  Byzantine-Robust Distributed Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lin%2C+F">Feng Lin</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+W">Weiyu Li</a>, 
<a href="/search/math?searchtype=author&query=Ling%2C+Q">Qing Ling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper aims to solve a distributed learning problem under Byzantine
attacks. In the underlying distributed system, a number of unknown but
malicious workers (termed as Byzantine workers) can send arbitrary messages to
the master and bias the learning process, due to data corruptions, computation
errors or malicious attacks. Prior work has considered a total variation (TV)
norm-penalized approximation formulation to handle the Byzantine attacks, where
the TV norm penalty forces the regular workers' local variables to be close,
and meanwhile, tolerates the outliers sent by the Byzantine workers. To solve
the TV norm-penalized approximation formulation, we propose a Byzantine-robust
stochastic alternating direction method of multipliers (ADMM) that fully
utilizes the separable problem structure. Theoretically, we prove that the
proposed method converges to a bounded neighborhood of the optimal solution at
a rate of O(1/k) under mild assumptions, where k is the number of iterations
and the size of neighborhood is determined by the number of Byzantine workers.
Numerical experiments on the MNIST and COVERTYPE datasets demonstrate the
effectiveness of the proposed method to various Byzantine attacks.
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06975" title="Abstract">arXiv:2106.06975</a> (cross-list from q-bio.QM) [<a href="/pdf/2106.06975" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel fully 3D, microfluidic-oriented, gel-based and low cost  stretchable soft sensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Annabestani%2C+M">Mohsen Annabestani</a>, 
<a href="/search/q-bio?searchtype=author&query=Esmaili-Dokht%2C+P">Pouria Esmaili-Dokht</a>, 
<a href="/search/q-bio?searchtype=author&query=Olianasab%2C+S+A">Seyyed Ali Olianasab</a>, 
<a href="/search/q-bio?searchtype=author&query=Orouji%2C+N">Nooshin Orouji</a>, 
<a href="/search/q-bio?searchtype=author&query=Alipour%2C+Z">Zeinab Alipour</a>, 
<a href="/search/q-bio?searchtype=author&query=Sayad%2C+M+H">Mohammad Hossein Sayad</a>, 
<a href="/search/q-bio?searchtype=author&query=Rajabi%2C+K">Kimia Rajabi</a>, 
<a href="/search/q-bio?searchtype=author&query=Mazzolai%2C+B">Barbara Mazzolai</a>, 
<a href="/search/q-bio?searchtype=author&query=Fardmanesh%2C+M">Mehdi Fardmanesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Soft Condensed Matter (cond-mat.soft); Robotics (cs.RO); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">In this paper, a novel fully 3D, microfluidic-oriented, gel-based, and
low-cost highly stretchable resistive sensors have been presented. By the
proposed method we are able to measure and discriminate all of the stretch,
twist, and pressure features by a single sensor which is the potential that we
have obtained from the fully 3D structure of our sensor. Against previous
sensors which all have used EGaIn as the conductive material of their sensor,
we have used low-cost, safe, and ubiquitous glycol-based gel instead. To show
the functionality of the proposed sensor some FEM simulations, a set of the
designed experimental tests were done which showed the linear, accurate, and
durable operation of the proposed sensor. Finally, the sensor was put through
its paces on the knee, elbow, and wrist of a female test subject. Also, to
evaluate the pressure functionality of the sensor, a fully 3D active foot
insole was developed, fabricated, and evaluated. All of the results show
promising features for the proposed sensor to be used in real-world
applications like rehabilitation, wearable devices, soft robotics, smart
clothing, gait analysis, AR/VR, etc.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06980" title="Abstract">arXiv:2106.06980</a> (cross-list from eess.IV) [<a href="/pdf/2106.06980" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Approach Towards Physics Informed Lung Ultrasound Image Scoring  Neural Network for Diagnostic Assistance in COVID-19
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Panicker%2C+M+R">Mahesh Raveendranatha Panicker</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y+T">Yale Tung Chen</a>, 
<a href="/search/eess?searchtype=author&query=M%2C+G">Gayathri M</a>, 
<a href="/search/eess?searchtype=author&query=N%2C+M+A">Madhavanunni A N</a>, 
<a href="/search/eess?searchtype=author&query=Narayan%2C+K+V">Kiran Vishnu Narayan</a>, 
<a href="/search/eess?searchtype=author&query=Kesavadas%2C+C">C Kesavadas</a>, 
<a href="/search/eess?searchtype=author&query=Vinod%2C+A+P">A P Vinod</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures, 3 tables, submitted to Springer SIVP Special Issue for COVID19
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Ultrasound is fast becoming an inevitable diagnostic tool for regular and
continuous monitoring of the lung with the recent outbreak of COVID-19. In this
work, a novel approach is presented to extract acoustic propagation-based
features to automatically highlight the region below pleura, which is an
important landmark in lung ultrasound (LUS). Subsequently, a multichannel input
formed by using the acoustic physics-based feature maps is fused to train a
neural network, referred to as LUSNet, to classify the LUS images into five
classes of varying severity of lung infection to track the progression of
COVID-19. In order to ensure that the proposed approach is agnostic to the type
of acquisition, the LUSNet, which consists of a U-net architecture is trained
in an unsupervised manner with the acoustic feature maps to ensure that the
encoder-decoder architecture is learning features in the pleural region of
interest. A novel combination of the U-net output and the U-net encoder output
is employed for the classification of severity of infection in the lung. A
detailed analysis of the proposed approach on LUS images over the infection to
full recovery period of ten confirmed COVID-19 subjects shows an average
five-fold cross-validation accuracy, sensitivity, and specificity of 97%, 93%,
and 98% respectively over 5000 frames of COVID-19 videos. The analysis also
shows that, when the input dataset is limited and diverse as in the case of
COVID-19 pandemic, an aided effort of combining acoustic propagation-based
features along with the gray scale images, as proposed in this work, improves
the performance of the neural network significantly and also aids the labelling
and triaging process.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06987" title="Abstract">arXiv:2106.06987</a> (cross-list from eess.IV) [<a href="/pdf/2106.06987" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning the Imaging Landmarks: Unsupervised Key point Detection in Lung  Ultrasound Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tripathi%2C+A">Arpan Tripathi</a>, 
<a href="/search/eess?searchtype=author&query=Panicker%2C+M+R">Mahesh Raveendranatha Panicker</a>, 
<a href="/search/eess?searchtype=author&query=Hareendranathan%2C+A+R">Abhilash R Hareendranathan</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y+T">Yale Tung Chen</a>, 
<a href="/search/eess?searchtype=author&query=Jaremko%2C+J+L">Jacob L Jaremko</a>, 
<a href="/search/eess?searchtype=author&query=Narayan%2C+K+V">Kiran Vishnu Narayan</a>, 
<a href="/search/eess?searchtype=author&query=C%2C+K">Kesavadas C</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures, submitted to IEEE EMBC 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Lung ultrasound (LUS) is an increasingly popular diagnostic imaging modality
for continuous and periodic monitoring of lung infection, given its advantages
of non-invasiveness, non-ionizing nature, portability and easy disinfection.
The major landmarks assessed by clinicians for triaging using LUS are pleura, A
and B lines. There have been many efforts for the automatic detection of these
landmarks. However, restricting to a few pre-defined landmarks may not reveal
the actual imaging biomarkers particularly in case of new pathologies like
COVID-19. Rather, the identification of key landmarks should be driven by data
given the availability of a plethora of neural network algorithms. This work is
a first of its kind attempt towards unsupervised detection of the key LUS
landmarks in LUS videos of COVID-19 subjects during various stages of
infection. We adapted the relatively newer approach of transporter neural
networks to automatically mark and track pleura, A and B lines based on their
periodic motion and relatively stable appearance in the videos. Initial results
on unsupervised pleura detection show an accuracy of 91.8% employing 1081 LUS
video frames.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06999" title="Abstract">arXiv:2106.06999</a> (cross-list from eess.AS) [<a href="/pdf/2106.06999" title="Download PDF">pdf</a>, <a href="/format/2106.06999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dataset of Dynamic Reverberant Sound Scenes with Directional  Interferers for Sound Event Localization and Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Politis%2C+A">Archontis Politis</a>, 
<a href="/search/eess?searchtype=author&query=Adavanne%2C+S">Sharath Adavanne</a>, 
<a href="/search/eess?searchtype=author&query=Krause%2C+D">Daniel Krause</a>, 
<a href="/search/eess?searchtype=author&query=Deleforge%2C+A">Antoine Deleforge</a>, 
<a href="/search/eess?searchtype=author&query=Srivastava%2C+P">Prerak Srivastava</a>, 
<a href="/search/eess?searchtype=author&query=Virtanen%2C+T">Tuomas Virtanen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">This report presents the dataset and baseline of Task 3 of the DCASE2021
Challenge on Sound Event Localization and Detection (SELD). The dataset is
based on emulation of real recordings of static or moving sound events under
real conditions of reverberation and ambient noise, using spatial room impulse
responses captured in a variety of rooms and delivered in two spatial formats.
The acoustical synthesis remains the same as in the previous iteration of the
challenge, however the new dataset brings more challenging conditions of
polyphony and overlapping instances of the same class. The most important
difference of the new dataset is the introduction of directional interferers,
meaning sound events that are localized in space but do not belong to the
target classes to be detected and are not annotated. Since such interfering
events are expected in every real-world scenario of SELD, the new dataset aims
to promote systems that deal with this condition effectively. A modified
SELDnet baseline employing the recent ACCDOA representation for SELD problems
accompanies the dataset and is described herein. To investigate the individual
and combined effects of ambient noise, interferers, and reverberation, we study
the performance of the baseline on different versions of the dataset excluding
or including combinations of these factors. The results indicate that by far
the most detrimental effects are caused by directional interferers.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07016" title="Abstract">arXiv:2106.07016</a> (cross-list from eess.AS) [<a href="/pdf/2106.07016" title="Download PDF">pdf</a>, <a href="/format/2106.07016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WASE: Learning When to Attend for Speaker Extraction in Cocktail Party  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hao%2C+Y">Yunzhe Hao</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jiaming Xu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+B">Bo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICASSP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Signal Processing (eess.SP)

</div>
<p class="mathjax">In the speaker extraction problem, it is found that additional information
from the target speaker contributes to the tracking and extraction of the
target speaker, which includes voiceprint, lip movement, facial expression, and
spatial information. However, no one cares for the cue of sound onset, which
has been emphasized in the auditory scene analysis and psychology. Inspired by
it, we explicitly modeled the onset cue and verified the effectiveness in the
speaker extraction task. We further extended to the onset/offset cues and got
performance improvement. From the perspective of tasks, our onset/offset-based
model completes the composite task, a complementary combination of speaker
extraction and speaker-dependent voice activity detection. We also combined
voiceprint with onset/offset cues. Voiceprint models voice characteristics of
the target while onset/offset models the start/end information of the speech.
From the perspective of auditory scene analysis, the combination of two
perception cues can promote the integrity of the auditory object. The
experiment results are also close to state-of-the-art performance, using nearly
half of the parameters. We hope that this work will inspire communities of
speech processing and psychology, and contribute to communication between them.
Our code will be available in https://github.com/aispeech-lab/wase/.
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07044" title="Abstract">arXiv:2106.07044</a> (cross-list from math.ST) [<a href="/pdf/2106.07044" title="Download PDF">pdf</a>, <a href="/format/2106.07044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal detection of the feature matching map in presence of noise and  outliers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Galstyan%2C+T">Tigran Galstyan</a>, 
<a href="/search/math?searchtype=author&query=Minasyan%2C+A">Arshak Minasyan</a>, 
<a href="/search/math?searchtype=author&query=Dalalyan%2C+A">Arnak Dalalyan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of finding the matching map between two sets of $d$
dimensional vectors from noisy observations, where the second set contains
outliers. The matching map is then an injection, which can be consistently
estimated only if the vectors of the second set are well separated. The main
result shows that, in the high-dimensional setting, a detection region of
unknown injection can be characterized by the sets of vectors for which the
inlier-inlier distance is of order at least $d^{1/4}$ and the inlier-outlier
distance is of order at least $d^{1/2}$. These rates are achieved using the
estimated matching minimizing the sum of logarithms of distances between
matched pairs of points. We also prove lower bounds establishing optimality of
these rates. Finally, we report results of numerical experiments on both
synthetic and real world data that illustrate our theoretical results and
provide further insight into the properties of the estimators studied in this
work.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07066" title="Abstract">arXiv:2106.07066</a> (cross-list from eess.IV) [<a href="/pdf/2106.07066" title="Download PDF">pdf</a>, <a href="/format/2106.07066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Hyperspectral Image Super-Resolution via RGB Fusion and TV-TV  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Vella%2C+M">Marija Vella</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+B">Bowen Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/eess?searchtype=author&query=Mota%2C+J+F+C">Jo&#xe3;o F. C. Mota</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICIP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Hyperspectral (HS) images contain detailed spectral information that has
proven crucial in applications like remote sensing, surveillance, and
astronomy. However, because of hardware limitations of HS cameras, the captured
images have low spatial resolution. To improve them, the low-resolution
hyperspectral images are fused with conventional high-resolution RGB images via
a technique known as fusion based HS image super-resolution. Currently, the
best performance in this task is achieved by deep learning (DL) methods. Such
methods, however, cannot guarantee that the input measurements are satisfied in
the recovered image, since the learned parameters by the network are applied to
every test image. Conversely, model-based algorithms can typically guarantee
such measurement consistency. Inspired by these observations, we propose a
framework that integrates learning and model based methods. Experimental
results show that our method produces images of superior spatial and spectral
resolution compared to the current leading methods, whether model- or DL-based.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07071" title="Abstract">arXiv:2106.07071</a> (cross-list from math.OC) [<a href="/pdf/2106.07071" title="Download PDF">pdf</a>, <a href="/format/2106.07071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk Assessment of Stealthy Attacks on Uncertain Control Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Anand%2C+S+C">Sribalaji C. Anand</a>, 
<a href="/search/math?searchtype=author&query=Teixeira%2C+A+M+H">Andr&#xe9; M. H. Teixeira</a>, 
<a href="/search/math?searchtype=author&query=Ahl%C3%A9n%2C+A">Anders Ahl&#xe9;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this article, we address the problem of risk assessment of stealthy
attacks on uncertain control systems. Considering data injection attacks that
aim at maximizing impact while remaining undetected, we use the recently
proposed output-to-output gain to characterize the risk associated with the
impact of attacks in two setups: A full system knowledge attacker and a limited
system knowledge attacker. The risk in each setup is formulated using a
well-established risk metric, namely the Value-at-Risk and the maximum expected
loss, respectively. Under these setups, the risk assessment problem corresponds
to an untractable infinite non-convex optimization problem. To address this
limitation, we adopt the framework of scenario-based optimization to
approximate the infinite non-convex optimization problem by a sampled
non-convex optimization problem. Then, based on the framework of dissipative
system theory and S-procedure, the sampled non-convex risk assessment problem
is formulated as an equivalent convex semi-definite program. Additionally, we
derive the necessary and sufficient conditions for the risk to be bounded.
Finally, we illustrate the results through numerical simulation of a
hydro-turbine power system.
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07072" title="Abstract">arXiv:2106.07072</a> (cross-list from math.CO) [<a href="/pdf/2106.07072" title="Download PDF">pdf</a>, <a href="/ps/2106.07072" title="Download PostScript">ps</a>, <a href="/format/2106.07072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On rainbow-free colourings of uniform hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Koerkamp%2C+R+G">Ragnar Groot Koerkamp</a>, 
<a href="/search/math?searchtype=author&query=%C5%BDivn%C3%BD%2C+S">Stanislav &#x17d;ivn&#xfd;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We study rainbow-free colourings of $k$-uniform hypergraphs; that is,
colourings that use $k$ colours but with the property that no hyperedge attains
all colours. We show that $p^*=(k-1)(\ln n)/n$ is the threshold function for
the existence of a rainbow-free colouring in a random $k$-uniform hypergraph.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07079" title="Abstract">arXiv:2106.07079</a> (cross-list from math.OC) [<a href="/pdf/2106.07079" title="Download PDF">pdf</a>, <a href="/format/2106.07079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Inertial Best-Response with Voluntary and Limited  Communication in Random Communication Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ayd%C4%B1n%2C+S">Sarper Ayd&#x131;n</a>, 
<a href="/search/math?searchtype=author&query=Eksin%2C+C">Ceyhun Eksin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Systems and Control (eess.SY)

</div>
<p class="mathjax">Multiple autonomous agents interact over a random communication network to
maximize their individual utility functions which depend on the actions of
other agents. We consider decentralized best-response with inertia type
algorithms in which agents form beliefs about the future actions of other
players based on local information, and take an action that maximizes their
expected utility computed with respect to these beliefs or continue to take
their previous action. We show convergence of these types of algorithms to a
Nash equilibrium in weakly acyclic games under the condition that the belief
update and information exchange protocols successfully learn the actions of
other players with positive probability in finite time given a static
environment, i.e., when other agents' actions do not change. We design a
decentralized fictitious play algorithm with voluntary and limited
communication (DFP-VL) protocols that satisfy this condition. In the voluntary
communication protocol, each agent decides whom to exchange information with by
assessing the novelty of its information and the potential effect of its
information on others' assessments of their utility functions. The limited
communication protocol entails agents sending only their most frequent action
to agents that they decide to communicate with. Numerical experiments on a
target assignment game demonstrate that the voluntary and limited communication
protocol can more than halve the number of communication attempts while
retaining the same convergence rate as DFP in which agents constantly attempt
to communicate.
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07103" title="Abstract">arXiv:2106.07103</a> (cross-list from q-fin.ST) [<a href="/pdf/2106.07103" title="Download PDF">pdf</a>, <a href="/format/2106.07103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A News-based Machine Learning Model for Adaptive Asset Pricing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Zhu%2C+L">Liao Zhu</a>, 
<a href="/search/q-fin?searchtype=author&query=Wu%2C+H">Haoxuan Wu</a>, 
<a href="/search/q-fin?searchtype=author&query=Wells%2C+M+T">Martin T. Wells</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">The paper proposes a new asset pricing model -- the News Embedding UMAP
Selection (NEUS) model, to explain and predict the stock returns based on the
financial news. Using a combination of various machine learning algorithms, we
first derive a company embedding vector for each basis asset from the financial
news. Then we obtain a collection of the basis assets based on their company
embedding. After that for each stock, we select the basis assets to explain and
predict the stock return with high-dimensional statistical methods. The new
model is shown to have a significantly better fitting and prediction power than
the Fama-French 5-factor model.
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07138" title="Abstract">arXiv:2106.07138</a> (cross-list from stat.ML) [<a href="/pdf/2106.07138" title="Download PDF">pdf</a>, <a href="/format/2106.07138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Metric Learning in Multi-View Data: A Downstream Task  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+S">Shulei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)

</div>
<p class="mathjax">Self-supervised metric learning has been a successful approach for learning a
distance from an unlabeled dataset. The resulting distance is broadly useful
for improving various distance-based downstream tasks, even when no information
from downstream tasks is utilized in the metric learning stage. To gain
insights into this approach, we develop a statistical framework to
theoretically study how self-supervised metric learning can benefit downstream
tasks in the context of multi-view data. Under this framework, we show that the
target distance of metric learning satisfies several desired properties for the
downstream tasks. On the other hand, our investigation suggests the target
distance can be further improved by moderating each direction's weights. In
addition, our analysis precisely characterizes the improvement by
self-supervised metric learning on four commonly used downstream tasks: sample
identification, two-sample testing, $k$-means clustering, and $k$-nearest
neighbor classification. As a by-product, we propose a simple spectral method
for self-supervised metric learning, which is computationally efficient and
minimax optimal for estimating target distance. Finally, numerical experiments
are presented to support the theoretical results in the paper.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07144" title="Abstract">arXiv:2106.07144</a> (cross-list from eess.AS) [<a href="/pdf/2106.07144" title="Download PDF">pdf</a>, <a href="/format/2106.07144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-shot learning of new sound classes for target sound extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Delcroix%2C+M">Marc Delcroix</a>, 
<a href="/search/eess?searchtype=author&query=V%C3%A1zquez%2C+J+B">Jorge Bennasar V&#xe1;zquez</a>, 
<a href="/search/eess?searchtype=author&query=Ochiai%2C+T">Tsubasa Ochiai</a>, 
<a href="/search/eess?searchtype=author&query=Kinoshita%2C+K">Keisuke Kinoshita</a>, 
<a href="/search/eess?searchtype=author&query=Araki%2C+S">Shoko Araki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Target sound extraction consists of extracting the sound of a target acoustic
event (AE) class from a mixture of AE sounds. It can be realized using a neural
network that extracts the target sound conditioned on a 1-hot vector that
represents the desired AE class. With this approach, embedding vectors
associated with the AE classes are directly optimized for the extraction of
sound classes seen during training. However, it is not easy to extend this
framework to new AE classes, i.e. unseen during training. Recently, speech,
music, or AE sound extraction based on enrollment audio of the desired sound
offers the potential of extracting any target sound in a mixture given only a
short audio signal of a similar sound. In this work, we propose combining
1-hot- and enrollment-based target sound extraction, allowing optimal
performance for seen AE classes and simple extension to new classes. In
experiments with synthesized sound mixtures generated with the Freesound
Dataset (FSD) datasets, we demonstrate the benefit of the combined framework
for both seen and new AE classes. Besides, we also propose adapting the
embedding vectors obtained from a few enrollment audio samples (few-shot) to
further improve performance on new classes.
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07148" title="Abstract">arXiv:2106.07148</a> (cross-list from stat.ML) [<a href="/pdf/2106.07148" title="Download PDF">pdf</a>, <a href="/format/2106.07148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Sample Complexity of Learning with Geometric Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bietti%2C+A">Alberto Bietti</a>, 
<a href="/search/stat?searchtype=author&query=Venturi%2C+L">Luca Venturi</a>, 
<a href="/search/stat?searchtype=author&query=Bruna%2C+J">Joan Bruna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Many supervised learning problems involve high-dimensional data such as
images, text, or graphs. In order to make efficient use of data, it is often
useful to leverage certain geometric priors in the problem at hand, such as
invariance to translations, permutation subgroups, or stability to small
deformations. We study the sample complexity of learning problems where the
target function presents such invariance and stability properties, by
considering spherical harmonic decompositions of such functions on the sphere.
We provide non-parametric rates of convergence for kernel methods, and show
improvements in sample complexity by a factor equal to the size of the group
when using an invariant kernel over the group, compared to the corresponding
non-invariant kernel. These improvements are valid when the sample size is
large enough, with an asymptotic behavior that depends on spectral properties
of the group. Finally, these gains are extended beyond invariance groups to
also cover geometric stability to small deformations, modeled here as subsets
(not necessarily subgroups) of permutations.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07163" title="Abstract">arXiv:2106.07163</a> (cross-list from math.OC) [<a href="/pdf/2106.07163" title="Download PDF">pdf</a>, <a href="/ps/2106.07163" title="Download PostScript">ps</a>, <a href="/format/2106.07163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State-dependent Riccati equation feedback stabilization for nonlinear  PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alla%2C+A">Alessandro Alla</a>, 
<a href="/search/math?searchtype=author&query=Kalise%2C+D">Dante Kalise</a>, 
<a href="/search/math?searchtype=author&query=Simoncini%2C+V">Valeria Simoncini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The synthesis of suboptimal feedback laws for controlling nonlinear dynamics
arising from semi-discretized PDEs is studied. An approach based on the
State-dependent Riccati Equation (SDRE) is presented for H2 and Hinf control
problems. Depending on the nonlinearity and the dimension of the resulting
problem, offline, online, and hybrid offline-online alternatives to the SDRE
synthesis are proposed. The hybrid offline-online SDRE method reduces to the
sequential solution of Lyapunov equations, effectively enabling the computation
of suboptimal feedback controls for two-dimensional PDEs. Numerical tests for
the Sine-Gordon, degenerate Zeldovich, and viscous Burgers' PDEs are presented,
providing a thorough experimental assessment of the proposed methodology.
</p>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07199" title="Abstract">arXiv:2106.07199</a> (cross-list from eess.SP) [<a href="/pdf/2106.07199" title="Download PDF">pdf</a>, <a href="/format/2106.07199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Experimental Assessment of Detection Schemes for Air  Interface Attacks in Adverse Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Orlando%2C+D">Danilo Orlando</a>, 
<a href="/search/eess?searchtype=author&query=Palam%C3%A0%2C+I">Ivan Palam&#xe0;</a>, 
<a href="/search/eess?searchtype=author&query=Bartoletti%2C+S">Stefania Bartoletti</a>, 
<a href="/search/eess?searchtype=author&query=Bianchi%2C+G">Giuseppe Bianchi</a>, 
<a href="/search/eess?searchtype=author&query=Melazzi%2C+N+B">Nicola Blefari Melazzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">In this letter, we propose three schemes designed to detect attacks over the
air interface in cellular networks. These decision rules rely on the
generalized likelihood ratio test, and are fed by data that can be acquired
using common off-the-shelf receivers. In addition to more classical
(barrage/smart) noise jamming attacks, we further assess the capability of the
proposed schemes to detect the stealthy activation of a rogue base station. The
evaluation is carried out through an experimentation of a LTE system concretely
reproduced using Software-Defined Radios. Illustrative examples confirm that
the proposed schemes can effectively detect air interface threats with high
probability.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07202" title="Abstract">arXiv:2106.07202</a> (cross-list from physics.soc-ph) [<a href="/pdf/2106.07202" title="Download PDF">pdf</a>, <a href="/format/2106.07202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal transport in multilayer networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ibrahim%2C+A+A">Abdullahi Adinoyi Ibrahim</a>, 
<a href="/search/physics?searchtype=author&query=Lonardi%2C+A">Alessandro Lonardi</a>, 
<a href="/search/physics?searchtype=author&query=De+Bacco%2C+C">Caterina De Bacco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Modeling traffic distribution and extracting optimal flows in multilayer
networks is of utmost importance to design efficient multi-modal network
infrastructures. Recent results based on optimal transport theory provide
powerful and computationally efficient methods to address this problem, but
they are mainly focused on modeling single-layer networks. Here we adapt these
results to study how optimal flows distribute on multilayer networks. We
propose a model where optimal flows on different layers contribute differently
to the total cost to be minimized. This is done by means of a parameter that
varies with layers, which allows to flexibly tune the sensitivity to traffic
congestion of the various layers. As an application, we consider transportation
networks, where each layer is associated to a different transportation system
and show how the traffic distribution varies as we tune this parameter across
layers. We show an example of this result on the real 2-layer network of the
city of Bordeaux with bus and tram, where we find that in certain regimes the
presence of the tram network significantly unburdens the traffic on the road
network. Our model paves the way to further analysis of optimal flows and
navigability strategies in real multilayer networks.
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07243" title="Abstract">arXiv:2106.07243</a> (cross-list from math.OC) [<a href="/pdf/2106.07243" title="Download PDF">pdf</a>, <a href="/ps/2106.07243" title="Download PostScript">ps</a>, <a href="/format/2106.07243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressed Gradient Tracking for Decentralized Optimization Over General  Directed Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Song%2C+Z">Zhuoqing Song</a>, 
<a href="/search/math?searchtype=author&query=Shi%2C+L">Lei Shi</a>, 
<a href="/search/math?searchtype=author&query=Pu%2C+S">Shi Pu</a>, 
<a href="/search/math?searchtype=author&query=Yan%2C+M">Ming Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> working paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we propose two communication-efficient algorithms for
decentralized optimization over a multi-agent network with general directed
network topology. In the first part, we consider a novel
communication-efficient gradient tracking based method, termed Compressed
Push-Pull (CPP), which combines the Push-Pull method with communication
compression. We show that CPP is applicable to a general class of unbiased
compression operators and achieves linear convergence for strongly convex and
smooth objective functions. In the second part, we propose a broadcast-like
version of CPP (B-CPP), which also achieves linear convergence rate under the
same conditions for the objective functions. B-CPP can be applied in an
asynchronous broadcast setting and further reduce communication costs compared
to CPP. Numerical experiments complement the theoretical analysis and confirm
the effectiveness of the proposed methods.
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07262" title="Abstract">arXiv:2106.07262</a> (cross-list from q-bio.PE) [<a href="/pdf/2106.07262" title="Download PDF">pdf</a>, <a href="/format/2106.07262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial spread of COVID-19 outbreak in Italy using multiscale kinetic  transport equations with uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Bertaglia%2C+G">Giulia Bertaglia</a>, 
<a href="/search/q-bio?searchtype=author&query=Boscheri%2C+W">Walter Boscheri</a>, 
<a href="/search/q-bio?searchtype=author&query=Dimarco%2C+G">Giacomo Dimarco</a>, 
<a href="/search/q-bio?searchtype=author&query=Pareschi%2C+L">Lorenzo Pareschi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Numerical Analysis (math.NA); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">In this paper we introduce a space-dependent multiscale model to describe the
spatial spread of an infectious disease under uncertain data with particular
interest in simulating the onset of the COVID-19 epidemic in Italy. While virus
transmission is ruled by a SEIAR type compartmental model, within our approach
the population is given by a sum of commuters moving on a extra-urban scale and
non commuters interacting only on the smaller urban scale. A transport dynamic
of the commuter population at large spatial scales, based on kinetic equations,
is coupled with a diffusion model for non commuters at the urban scale. Thanks
to a suitable scaling limit, the kinetic transport model used to describe the
dynamics of commuters, within a given urban area coincides with the diffusion
equations that characterize the movement of non-commuting individuals. Because
of the high uncertainty in the data reported in the early phase of the
epidemic, the presence of random inputs in both the initial data and the
epidemic parameters is included in the model. A robust numerical method is
designed to deal with the presence of multiple scales and the uncertainty
quantification process. In our simulations, we considered a realistic
geographical domain, describing the Lombardy region, in which the size of the
cities, the number of infected individuals, the average number of daily
commuters moving from one city to another, and the epidemic aspects are taken
into account through a calibration of the model parameters based on the actual
available data. The results show that the model is able to describe correctly
the main features of the spatial expansion of the first wave of COVID-19 in
northern Italy.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07263" title="Abstract">arXiv:2106.07263</a> (cross-list from stat.ML) [<a href="/pdf/2106.07263" title="Download PDF">pdf</a>, <a href="/format/2106.07263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning for Variance Reduction in Online Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Guo%2C+Y">Yongyi Guo</a>, 
<a href="/search/stat?searchtype=author&query=Coey%2C+D">Dominic Coey</a>, 
<a href="/search/stat?searchtype=author&query=Konutgan%2C+M">Mikael Konutgan</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+W">Wenting Li</a>, 
<a href="/search/stat?searchtype=author&query=Schoener%2C+C">Chris Schoener</a>, 
<a href="/search/stat?searchtype=author&query=Goldman%2C+M">Matt Goldman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of variance reduction in randomized controlled
trials, through the use of covariates correlated with the outcome but
independent of the treatment. We propose a machine learning regression-adjusted
treatment effect estimator, which we call MLRATE. MLRATE uses machine learning
predictors of the outcome to reduce estimator variance. It employs
cross-fitting to avoid overfitting biases, and we prove consistency and
asymptotic normality under general conditions. MLRATE is robust to poor
predictions from the machine learning step: if the predictions are uncorrelated
with the outcomes, the estimator performs asymptotically no worse than the
standard difference-in-means estimator, while if predictions are highly
correlated with outcomes, the efficiency gains are large. In A/A tests, for a
set of 48 outcome metrics commonly monitored in Facebook experiments the
estimator has over 70\% lower variance than the simple difference-in-means
estimator, and about 19\% lower variance than the common univariate procedure
which adjusts only for pre-experiment values of the outcome.
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07291" title="Abstract">arXiv:2106.07291</a> (cross-list from eess.SP) [<a href="/pdf/2106.07291" title="Download PDF">pdf</a>, <a href="/format/2106.07291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi Polarization Square Patch Antenna with a Reconfigurable Feeding  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rezaeipour%2C+R">Roozbeh Rezaeipour</a>, 
<a href="/search/eess?searchtype=author&query=Sadeghzadeh%2C+R">Ramezanali Sadeghzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">A multi-polarization square patch antenna with a reconfigurable feeding
network is presented in this paper. The reconfigurable feeding network of this
antenna is implemented on an FR-4 substrate by a Wilkinson power divider and a
branch line coupler which perform amplitude distribution in the feeding
network. Besides, two switching circuits which consist of one PIN diode
(BAR63-02w) and its DC biasing circuit manage the RF signal flow on this
feeding network. These switching circuits control the phase of the RF signal
applied to the square patch, so it can provide linear polarization, left-hand
and right-hand circular polarization at 2.45 GHz which has many applications in
wireless networks. The simulated and measured results are presented which
illuminate acceptable axial ratio bandwidth (ARBW) for both right-hand and
left-hand circular polarization in (2.38-2.48 GHz) and minimum -10 dB return
loss at 2.45 GHz.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07292" title="Abstract">arXiv:2106.07292</a> (cross-list from q-bio.PE) [<a href="/pdf/2106.07292" title="Download PDF">pdf</a>, <a href="/format/2106.07292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology identifies emerging adaptive mutations in SARS-CoV-2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Bleher%2C+M">Michael Bleher</a>, 
<a href="/search/q-bio?searchtype=author&query=Hahn%2C+L">Lukas Hahn</a>, 
<a href="/search/q-bio?searchtype=author&query=Patino-Galindo%2C+J+A">Juan Angel Patino-Galindo</a>, 
<a href="/search/q-bio?searchtype=author&query=Carriere%2C+M">Mathieu Carriere</a>, 
<a href="/search/q-bio?searchtype=author&query=Bauer%2C+U">Ulrich Bauer</a>, 
<a href="/search/q-bio?searchtype=author&query=Rabadan%2C+R">Raul Rabadan</a>, 
<a href="/search/q-bio?searchtype=author&query=Ott%2C+A">Andreas Ott</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Computational Geometry (cs.CG); Genomics (q-bio.GN); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">The COVID-19 pandemic has lead to a worldwide effort to characterize its
evolution through the mapping of mutations in the genome of the coronavirus
SARS-CoV-2. Ideally, one would like to quickly identify new mutations that
could confer adaptive advantages (e.g. higher infectivity or immune evasion) by
leveraging the large number of genomes. One way of identifying adaptive
mutations is by looking at convergent mutations, mutations in the same genomic
position that occur independently. However, the large number of currently
available genomes precludes the efficient use of phylogeny-based techniques.
Here, we establish a fast and scalable Topological Data Analysis approach for
the early warning and surveillance of emerging adaptive mutations based on
persistent homology. It identifies convergent events merely by their
topological footprint and thus overcomes limitations of current phylogenetic
inference techniques. This allows for an unbiased and rapid analysis of large
viral datasets. We introduce a new topological measure for convergent evolution
and apply it to the GISAID dataset as of February 2021, comprising 303,651
high-quality SARS-CoV-2 isolates collected since the beginning of the pandemic.
We find that topologically salient mutations on the receptor-binding domain
appear in several variants of concern and are linked with an increase in
infectivity and immune escape, and for many adaptive mutations the topological
signal precedes an increase in prevalence. We show that our method effectively
identifies emerging adaptive mutations at an early stage. By localizing
topological signals in the dataset, we extract geo-temporal information about
the early occurrence of emerging adaptive mutations. The identification of
these mutations can help to develop an alert system to monitor mutations of
concern and guide experimentalists to focus the study of specific circulating
variants.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07302" title="Abstract">arXiv:2106.07302</a> (cross-list from quant-ph) [<a href="/pdf/2106.07302" title="Download PDF">pdf</a>, <a href="/format/2106.07302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum diffusion map for nonlinear dimensionality reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Sornsaeng%2C+A">Apimuk Sornsaeng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Dangniam%2C+N">Ninnat Dangniam</a>, 
<a href="/search/quant-ph?searchtype=author&query=Palittapongarnpim%2C+P">Pantita Palittapongarnpim</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chotibut%2C+T">Thiparat Chotibut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Inspired by random walk on graphs, diffusion map (DM) is a class of
unsupervised machine learning that offers automatic identification of
low-dimensional data structure hidden in a high-dimensional dataset. In recent
years, among its many applications, DM has been successfully applied to
discover relevant order parameters in many-body systems, enabling automatic
classification of quantum phases of matter. However, classical DM algorithm is
computationally prohibitive for a large dataset, and any reduction of the time
complexity would be desirable. With a quantum computational speedup in mind, we
propose a quantum algorithm for DM, termed quantum diffusion map (qDM). Our qDM
takes as an input N classical data vectors, performs an eigen-decomposition of
the Markov transition matrix in time $O(\log^3 N)$, and classically constructs
the diffusion map via the readout (tomography) of the eigenvectors, giving a
total runtime of $O(N^2 \text{polylog}\, N)$. Lastly, quantum subroutines in
qDM for constructing a Markov transition operator, and for analyzing its
spectral properties can also be useful for other random walk-based algorithms.
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07323" title="Abstract">arXiv:2106.07323</a> (cross-list from math.OC) [<a href="/pdf/2106.07323" title="Download PDF">pdf</a>, <a href="/ps/2106.07323" title="Download PostScript">ps</a>, <a href="/format/2106.07323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gridless Evolutionary Approach for Line Spectral Estimation with Unknown  Model Order
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yan%2C+B">Bai Yan</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jin Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+J+A">J. Andrew Zhang</a>, 
<a href="/search/math?searchtype=author&query=Yao%2C+X">Xin Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Gridless methods show great superiority in line spectral estimation. These
methods need to solve an atomic $l_0$ norm (i.e., the continuous analog of
$l_0$ norm) minimization problem to estimate frequencies and model order. Since
this problem is NP-hard to compute, relaxations of atomic $l_0$ norm, such as
nuclear norm and reweighted atomic norm, have been employed for promoting
sparsity. However, the relaxations give rise to a resolution limit,
subsequently leading to biased model order and convergence error. To overcome
the above shortcomings of relaxation, we propose a novel idea of simultaneously
estimating the frequencies and model order by means of the atomic $l_0$ norm.
To accomplish this idea, we build a multiobjective optimization model. The
measurment error and the atomic $l_0$ norm are taken as the two optimization
objectives. The proposed model directly exploits the model order via the atomic
$l_0$ norm, thus breaking the resolution limit. We further design a
variable-length evolutionary algorithm to solve the proposed model, which
includes two innovations. One is a variable-length coding and search strategy.
It flexibly codes and interactively searches diverse solutions with different
model orders. These solutions act as steppingstones that help fully exploring
the variable and open-ended frequency search space and provide extensive
potentials towards the optima. Another innovation is a model order pruning
mechanism, which heuristically prunes less contributive frequencies within the
solutions, thus significantly enhancing convergence and diversity. Simulation
results confirm the superiority of our approach in both frequency estimation
and model order selection.
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07355" title="Abstract">arXiv:2106.07355</a> (cross-list from q-bio.NC) [<a href="/pdf/2106.07355" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting the imagined contents using brain activation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Miyapuram%2C+K+P">Krishna Prasad Miyapuram</a>, 
<a href="/search/q-bio?searchtype=author&query=Schultz%2C+W">Wolfram Schultz</a>, 
<a href="/search/q-bio?searchtype=author&query=Tobler%2C+P+N">Philippe N. Tobler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published In 2013 Fourth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG) (pp. 1-3)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Mental imagery refers to percept-like experiences in the absence of sensory
input. Brain imaging studies suggest common, modality-specific, neural
correlates imagery and perception. We associated abstract visual stimuli with
either visually presented or imagined monetary rewards and scrambled pictures.
Brain images for a group of 12 participants were collected using functional
magnetic resonance imaging. Statistical analysis showed that human midbrain
regions were activated irrespective of the monetary rewards being imagined or
visually present. A support vector machine trained on the midbrain activation
patterns to the visually presented rewards predicted with 75% accuracy whether
the participants imagined the monetary reward or the scrambled picture during
imagination trials. Training samples were drawn from visually presented trials
and classification accuracy was assessed for imagination trials. These results
suggest the use of machine learning technique for classification of underlying
cognitive states from brain imaging data.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07358" title="Abstract">arXiv:2106.07358</a> (cross-list from q-fin.ST) [<a href="/pdf/2106.07358" title="Download PDF">pdf</a>, <a href="/ps/2106.07358" title="Download PostScript">ps</a>, <a href="/format/2106.07358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Credit spread approximation and improvement using random forest  regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Mercadier%2C+M">Mathieu Mercadier</a>, 
<a href="/search/q-fin?searchtype=author&query=Lardy%2C+J">Jean-Pierre Lardy</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> European Journal of Operational Research, Elsevier, 2019, 277 (1),
  pp.351-365
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG); Pricing of Securities (q-fin.PR)

</div>
<p class="mathjax">Credit Default Swap (CDS) levels provide a market appreciation of companies'
default risk. These derivatives are not always available, creating a need for
CDS approximations. This paper offers a simple, global and transparent CDS
structural approximation, which contrasts with more complex and proprietary
approximations currently in use. This Equity-to-Credit formula (E2C), inspired
by CreditGrades, obtains better CDS approximations, according to empirical
analyses based on a large sample spanning 2016-2018. A random forest regression
run with this E2C formula and selected additional financial data results in an
87.3% out-of-sample accuracy in CDS approximations. The transparency property
of this algorithm confirms the predominance of the E2C estimate, and the impact
of companies' debt rating and size, in predicting their CDS.
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07361" title="Abstract">arXiv:2106.07361</a> (cross-list from q-fin.ST) [<a href="/pdf/2106.07361" title="Download PDF">pdf</a>, <a href="/format/2106.07361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Forecasting of Imbalance Prices in the Belgian Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Dumas%2C+J">Jonathan Dumas</a>, 
<a href="/search/q-fin?searchtype=author&query=Boukas%2C+I">Ioannis Boukas</a>, 
<a href="/search/q-fin?searchtype=author&query=de+Villena%2C+M+M">Miguel Manuel de Villena</a>, 
<a href="/search/q-fin?searchtype=author&query=Mathieu%2C+S">S&#xe9;bastien Mathieu</a>, 
<a href="/search/q-fin?searchtype=author&query=Corn%C3%A9lusse%2C+B">Bertrand Corn&#xe9;lusse</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2019 16th International Conference on the European Energy Market
  (EEM). IEEE, 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Forecasting imbalance prices is essential for strategic participation in the
short-term energy markets. A novel two-step probabilistic approach is proposed,
with a particular focus on the Belgian case. The first step consists of
computing the net regulation volume state transition probabilities. It is
modeled as a matrix computed using historical data. This matrix is then used to
infer the imbalance prices since the net regulation volume can be related to
the level of reserves activated and the corresponding marginal prices for each
activation level are published by the Belgian Transmission System Operator one
day before electricity delivery. This approach is compared to a deterministic
model, a multi-layer perceptron, and a widely used probabilistic technique,
Gaussian Processes.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07379" title="Abstract">arXiv:2106.07379</a> (cross-list from eess.IV) [<a href="/pdf/2106.07379" title="Download PDF">pdf</a>, <a href="/format/2106.07379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Inference Machines as inverse problem solvers for MR  relaxometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sabidussi%2C+E+R">E. R. Sabidussi</a>, 
<a href="/search/eess?searchtype=author&query=Klein%2C+S">S. Klein</a>, 
<a href="/search/eess?searchtype=author&query=Caan%2C+M+W+A">M. W. A. Caan</a>, 
<a href="/search/eess?searchtype=author&query=Bazrafkan%2C+S">S. Bazrafkan</a>, 
<a href="/search/eess?searchtype=author&query=Dekker%2C+A+J+d">A. J. den Dekker</a>, 
<a href="/search/eess?searchtype=author&query=Sijbers%2C+J">J. Sijbers</a>, 
<a href="/search/eess?searchtype=author&query=Niessen%2C+W+J">W. J. Niessen</a>, 
<a href="/search/eess?searchtype=author&query=Poot%2C+D+H+J">D. H. J. Poot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we propose the use of Recurrent Inference Machines (RIMs) to
perform T1 and T2 mapping. The RIM is a neural network framework that learns an
iterative inference process based on the signal model, similar to conventional
statistical methods for quantitative MRI (QMRI), such as the Maximum Likelihood
Estimator (MLE). This framework combines the advantages of both data-driven and
model-based methods, and, we hypothesize, is a promising tool for QMRI.
Previously, RIMs were used to solve linear inverse reconstruction problems.
Here, we show that they can also be used to optimize non-linear problems and
estimate relaxometry maps with high precision and accuracy. The developed RIM
framework is evaluated in terms of accuracy and precision and compared to an
MLE method and an implementation of the ResNet. The results show that the RIM
improves the quality of estimates compared to the other techniques in Monte
Carlo experiments with simulated data, test-retest analysis of a system
phantom, and in-vivo scans. Additionally, inference with the RIM is 150 times
faster than the MLE, and robustness to (slight) variations of scanning
parameters is demonstrated. Hence, the RIM is a promising and flexible method
for QMRI. Coupled with an open-source training data generation tool, it
presents a compelling alternative to previous methods.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07382" title="Abstract">arXiv:2106.07382</a> (cross-list from physics.class-ph) [<a href="/pdf/2106.07382" title="Download PDF">pdf</a>, <a href="/format/2106.07382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation of viscoelastic Cosserat rods based on the geometrically  exact dynamics of special Euclidean strands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Giusteri%2C+G+G">G. G. Giusteri</a>, 
<a href="/search/physics?searchtype=author&query=Miglio%2C+E">E. Miglio</a>, 
<a href="/search/physics?searchtype=author&query=Parolini%2C+N">N. Parolini</a>, 
<a href="/search/physics?searchtype=author&query=Penati%2C+M">M. Penati</a>, 
<a href="/search/physics?searchtype=author&query=Zambetti%2C+R">R. Zambetti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Physics (physics.class-ph)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We propose a method for the description and simulation of the nonlinear
dynamics of slender structures modeled as Cosserat rods. It is based on
interpreting the strains and the generalized velocities of the cross sections
as basic variables and elements of the special Euclidean algebra. This
perspective emerges naturally from the evolution equations for strands, that
are one-dimensional submanifolds, of the special Euclidean group. The
discretization of the corresponding equations for the three-dimensional motion
of a Cosserat rod is performed, in space, by using a staggered grid. The time
evolution is then approximated with a semi-implicit method. Within this
approach we can easily include dissipative effects due to both the action of
external forces and the presence of internal mechanical dissipation. The
comparison with results obtained with different schemes shows the effectiveness
of the proposed method, which is able to provide very good predictions of
nonlinear dynamical effects and shows competitive computation times also as an
energy-minimizing method to treat static problems.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07393" title="Abstract">arXiv:2106.07393</a> (cross-list from stat.AP) [<a href="/pdf/2106.07393" title="Download PDF">pdf</a>, <a href="/format/2106.07393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-replication Reliability -- An Empirical Approach to Interpreting  Inter-rater Reliability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wong%2C+K">Ka Wong</a>, 
<a href="/search/stat?searchtype=author&query=Paritosh%2C+P">Praveen Paritosh</a>, 
<a href="/search/stat?searchtype=author&query=Aroyo%2C+L">Lora Aroyo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We present a new approach to interpreting IRR that is empirical and
contextualized. It is based upon benchmarking IRR against baseline measures in
a replication, one of which is a novel cross-replication reliability (xRR)
measure based on Cohen's kappa. We call this approach the xRR framework. We
opensource a replication dataset of 4 million human judgements of facial
expressions and analyze it with the proposed framework. We argue this framework
can be used to measure the quality of crowdsourced datasets.
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07452" title="Abstract">arXiv:2106.07452</a> (cross-list from stat.ML) [<a href="/pdf/2106.07452" title="Download PDF">pdf</a>, <a href="/format/2106.07452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Marginalising over Stationary Kernels with Bayesian Quadrature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hamid%2C+S">Saad Hamid</a>, 
<a href="/search/stat?searchtype=author&query=Schulze%2C+S">Sebastian Schulze</a>, 
<a href="/search/stat?searchtype=author&query=Osborne%2C+M+A">Michael A. Osborne</a>, 
<a href="/search/stat?searchtype=author&query=Roberts%2C+S+J">Stephen J. Roberts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Marginalising over families of Gaussian Process kernels produces flexible
model classes with well-calibrated uncertainty estimates. Existing approaches
require likelihood evaluations of many kernels, rendering them prohibitively
expensive for larger datasets. We propose a Bayesian Quadrature scheme to make
this marginalisation more efficient and thereby more practical. Through use of
the maximum mean discrepancies between distributions, we define a kernel over
kernels that captures invariances between Spectral Mixture (SM) Kernels. Kernel
samples are selected by generalising an information-theoretic acquisition
function for warped Bayesian Quadrature. We show that our framework achieves
more accurate predictions with better calibrated uncertainty than
state-of-the-art baselines, especially when given limited (wall-clock) time
budgets.
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07454" title="Abstract">arXiv:2106.07454</a> (cross-list from math.OC) [<a href="/pdf/2106.07454" title="Download PDF">pdf</a>, <a href="/format/2106.07454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NG+ : A Multi-Step Matrix-Product Natural Gradient Method for Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yang%2C+M">Minghan Yang</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+D">Dong Xu</a>, 
<a href="/search/math?searchtype=author&query=Cui%2C+Q">Qiwen Cui</a>, 
<a href="/search/math?searchtype=author&query=Wen%2C+Z">Zaiwen Wen</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+P">Pengxiang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, a novel second-order method called NG+ is proposed. By
following the rule ``the shape of the gradient equals the shape of the
parameter", we define a generalized fisher information matrix (GFIM) using the
products of gradients in the matrix form rather than the traditional
vectorization. Then, our generalized natural gradient direction is simply the
inverse of the GFIM multiplies the gradient in the matrix form. Moreover, the
GFIM and its inverse keeps the same for multiple steps so that the
computational cost can be controlled and is comparable with the first-order
methods. A global convergence is established under some mild conditions and a
regret bound is also given for the online learning setting. Numerical results
on image classification with ResNet50, quantum chemistry modeling with Schnet,
neural machine translation with Transformer and recommendation system with DLRM
illustrate that GN+ is competitive with the state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07512" title="Abstract">arXiv:2106.07512</a> (cross-list from stat.ML) [<a href="/pdf/2106.07512" title="Download PDF">pdf</a>, <a href="/format/2106.07512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Last Layer Marginal Likelihood for Invariance Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Schw%C3%B6bel%2C+P+E">Pola Elisabeth Schw&#xf6;bel</a>, 
<a href="/search/stat?searchtype=author&query=J%C3%B8rgensen%2C+M">Martin J&#xf8;rgensen</a>, 
<a href="/search/stat?searchtype=author&query=Ober%2C+S+W">Sebastian W. Ober</a>, 
<a href="/search/stat?searchtype=author&query=van+der+Wilk%2C+M">Mark van der Wilk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Data augmentation is often used to incorporate inductive biases into models.
Traditionally, these are hand-crafted and tuned with cross validation. The
Bayesian paradigm for model selection provides a path towards end-to-end
learning of invariances using only the training data, by optimising the
marginal likelihood. We work towards bringing this approach to neural networks
by using an architecture with a Gaussian process in the last layer, a model for
which the marginal likelihood can be computed. Experimentally, we improve
performance by learning appropriate invariances in standard benchmarks, the low
data regime and in a medical imaging task. Optimisation challenges for
invariant Deep Kernel Gaussian processes are identified, and a systematic
analysis is presented to arrive at a robust training scheme. We introduce a new
lower bound to the marginal likelihood, which allows us to perform inference
for a larger class of likelihood functions than before, thereby overcoming some
of the training challenges that existed with previous approaches.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07524" title="Abstract">arXiv:2106.07524</a> (cross-list from eess.IV) [<a href="/pdf/2106.07524" title="Download PDF">pdf</a>, <a href="/format/2106.07524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIA-COV19D: COVID-19 Detection through 3-D Chest CT Image Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kollias%2C+D">Dimitrios Kollias</a>, 
<a href="/search/eess?searchtype=author&query=Arsenos%2C+A">Anastasios Arsenos</a>, 
<a href="/search/eess?searchtype=author&query=Soukissian%2C+L">Levon Soukissian</a>, 
<a href="/search/eess?searchtype=author&query=Kollias%2C+S">Stefanos Kollias</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Early and reliable COVID-19 diagnosis based on chest 3-D CT scans can assist
medical specialists in vital circumstances. Deep learning methodologies
constitute a main approach for chest CT scan analysis and disease prediction.
However, large annotated databases are necessary for developing deep learning
models that are able to provide COVID-19 diagnosis across various medical
environments in different countries. Due to privacy issues, publicly available
COVID-19 CT datasets are highly difficult to obtain, which hinders the research
and development of AI-enabled diagnosis methods of COVID-19 based on CT scans.
In this paper we present the COV19-CT-DB database which is annotated for
COVID-19, consisting of about 5,000 3-D CT scans, We have split the database in
training, validation and test datasets. The former two datasets can be used for
training and validation of machine learning models, while the latter will be
used for evaluation of the developed models. We also present a deep learning
approach, based on a CNN-RNN network and report its performance on the
COVID19-CT-DB database.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07533" title="Abstract">arXiv:2106.07533</a> (cross-list from eess.IV) [<a href="/pdf/2106.07533" title="Download PDF">pdf</a>, <a href="/format/2106.07533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posterior Temperature Optimization in Variational Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Laves%2C+M">Max-Heinrich Laves</a>, 
<a href="/search/eess?searchtype=author&query=T%C3%B6lle%2C+M">Malte T&#xf6;lle</a>, 
<a href="/search/eess?searchtype=author&query=Schlaefer%2C+A">Alexander Schlaefer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Cold posteriors have been reported to perform better in practice in the
context of Bayesian deep learning (Wenzel2020 et al., 2020). In variational
inference, it is common to employ only a partially tempered posterior by
scaling the complexity term in the log-evidence lower bound (ELBO). In this
work, we first derive the ELBO for a fully tempered posterior in mean-field
variational inference and subsequently use Bayesian optimization to
automatically find the optimal posterior temperature. Choosing an appropriate
posterior temperature leads to better predictive performance and improved
uncertainty calibration, which we demonstrate for the task of denoising medical
X-ray images.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07537" title="Abstract">arXiv:2106.07537</a> (cross-list from stat.ML) [<a href="/pdf/2106.07537" title="Download PDF">pdf</a>, <a href="/format/2106.07537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Wasserstein Minimax Framework for Mixed Linear Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Diamandis%2C+T">Theo Diamandis</a>, 
<a href="/search/stat?searchtype=author&query=Eldar%2C+Y+C">Yonina C. Eldar</a>, 
<a href="/search/stat?searchtype=author&query=Fallah%2C+A">Alireza Fallah</a>, 
<a href="/search/stat?searchtype=author&query=Farnia%2C+F">Farzan Farnia</a>, 
<a href="/search/stat?searchtype=author&query=Ozdaglar%2C+A">Asuman Ozdaglar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in 38th International Conference on Machine Learning (ICML 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Multi-modal distributions are commonly used to model clustered data in
statistical learning tasks. In this paper, we consider the Mixed Linear
Regression (MLR) problem. We propose an optimal transport-based framework for
MLR problems, Wasserstein Mixed Linear Regression (WMLR), which minimizes the
Wasserstein distance between the learned and target mixture regression models.
Through a model-based duality analysis, WMLR reduces the underlying MLR task to
a nonconvex-concave minimax optimization problem, which can be provably solved
to find a minimax stationary point by the Gradient Descent Ascent (GDA)
algorithm. In the special case of mixtures of two linear regression models, we
show that WMLR enjoys global convergence and generalization guarantees. We
prove that WMLR's sample complexity grows linearly with the dimension of data.
Finally, we discuss the application of WMLR to the federated learning task
where the training samples are collected by multiple agents in a network.
Unlike the Expectation Maximization algorithm, WMLR directly extends to the
distributed, federated learning setting. We support our theoretical results
through several numerical experiments, which highlight our framework's ability
to handle the federated learning setting with mixture models.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07541" title="Abstract">arXiv:2106.07541</a> (cross-list from math.OC) [<a href="/pdf/2106.07541" title="Download PDF">pdf</a>, <a href="/format/2106.07541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilient Control of Platooning Networked Robitic Systems via Dynamic  Watermarking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Porter%2C+M">Matthew Porter</a>, 
<a href="/search/math?searchtype=author&query=Joshi%2C+A">Arnav Joshi</a>, 
<a href="/search/math?searchtype=author&query=Dey%2C+S">Sidhartha Dey</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+Q">Qirui Wu</a>, 
<a href="/search/math?searchtype=author&query=Hespanhol%2C+P">Pedro Hespanhol</a>, 
<a href="/search/math?searchtype=author&query=Aswani%2C+A">Anil Aswani</a>, 
<a href="/search/math?searchtype=author&query=Johnson-Roberson%2C+M">Matthew Johnson-Roberson</a>, 
<a href="/search/math?searchtype=author&query=Vasudevan%2C+R">Ram Vasudevan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Networked robotic systems, such as connected vehicle platoons, can improve
the safety and efficiency of transportation networks by allowing for high-speed
coordination. To enable such coordination, these systems rely on networked
communications. This can make them susceptible to cyber attacks. Though
security methods such as encryption or specially designed network topologies
can increase the difficulty of successfully executing such an attack, these
techniques are unable to guarantee secure communication against an attacker.
More troublingly, these security methods are unable to ensure that individual
agents are able to detect attacks that alter the content of specific messages.
To ensure resilient behavior under such attacks, this paper formulates a
networked linear time-varying version of dynamic watermarking in which each
agent generates and adds a private excitation to the input of its corresponding
robotic subsystem. This paper demonstrates that such a method can enable each
agent in a networked robotic system to detect cyber attacks. By altering
measurements sent between vehicles, this paper illustrates that an attacker can
create unstable behavior within a platoon. By utilizing the dynamic
watermarking method proposed in this paper, the attack is detected, allowing
the vehicles in the platoon to gracefully degrade to a non-communicative
control strategy that maintains safety across a variety of scenarios.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07585" title="Abstract">arXiv:2106.07585</a> (cross-list from math.OC) [<a href="/pdf/2106.07585" title="Download PDF">pdf</a>, <a href="/format/2106.07585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Controllability for Quasilinear Non-negative Definite System of  ODEs and SDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Djordjevic%2C+J">Jasmina Djordjevic</a>, 
<a href="/search/math?searchtype=author&query=Konjik%2C+S">Sanja Konjik</a>, 
<a href="/search/math?searchtype=author&query=Mitrovi%C4%87%2C+D">Darko Mitrovi&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Novak%2C+A">Andrej Novak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We consider exact and averaged control problem for a system of quasi-linear
ODEs and SDEs with a non-negative definite symmetric matrix of the system. The
strategy of the proof is the standard linearization of the system by fixing the
function appearing in the nonlinear part of the system, and then applying the
Leray-Schauder fixed point theorem. We shall also need the continuous induction
arguments to prolong the control to the final state which is a novel approach
in the field. This enables us to obtain controllability for arbitrarily large
initial data (so called global controllability).
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07592" title="Abstract">arXiv:2106.07592</a> (cross-list from physics.med-ph) [<a href="/pdf/2106.07592" title="Download PDF">pdf</a>, <a href="/format/2106.07592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No more glowing in the dark: How deep learning improves exposure date  estimation in thermoluminescence dosimetry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Mentzel%2C+F">F. Mentzel</a>, 
<a href="/search/physics?searchtype=author&query=Derugin%2C+E">E. Derugin</a>, 
<a href="/search/physics?searchtype=author&query=Jansen%2C+H">H. Jansen</a>, 
<a href="/search/physics?searchtype=author&query=Kr%C3%B6ninger%2C+K">K. Kr&#xf6;ninger</a>, 
<a href="/search/physics?searchtype=author&query=Nackenhorst%2C+O">O. Nackenhorst</a>, 
<a href="/search/physics?searchtype=author&query=Walbersloh%2C+J">J. Walbersloh</a>, 
<a href="/search/physics?searchtype=author&query=Weingarten%2C+J">J. Weingarten</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The time- or temperature-resolved detector signal from a thermoluminescence
dosimeter can reveal additional information about circumstances of an exposure
to ionizing irradiation. We present studies using deep neural networks to
estimate the date of a single irradiation with 12 mSv within a monitoring
interval of 42 days from glow curves of novel TL-DOS personal dosimeters
developed by the Materialpr\"ufungsamt NRW in cooperation with TU Dortmund
University. Using a deep convolutional network, the irradiation date can be
predicted from raw time-resolved glow curve data with an uncertainty of roughly
1-2 days on a 68% confidence level without the need for a prior transformation
into temperature space and a subsequent glow curve deconvolution. This
corresponds to a significant improvement in prediction accuracy compared to a
prior publication, which yielded a prediction uncertainty of 2-4 days using
features obtained from a glow curve deconvolution as input to a neural network.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07608" title="Abstract">arXiv:2106.07608</a> (cross-list from eess.IV) [<a href="/pdf/2106.07608" title="Download PDF">pdf</a>, <a href="/format/2106.07608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recursive Refinement Network for Deformable Lung Registration between  Exhale and Inhale CT Scans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=He%2C+X">Xinzi He</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+J">Jia Guo</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xuzhe Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Bi%2C+H">Hanwen Bi</a>, 
<a href="/search/eess?searchtype=author&query=Gerard%2C+S">Sarah Gerard</a>, 
<a href="/search/eess?searchtype=author&query=Kaczka%2C+D">David Kaczka</a>, 
<a href="/search/eess?searchtype=author&query=Motahari%2C+A">Amin Motahari</a>, 
<a href="/search/eess?searchtype=author&query=Hoffman%2C+E">Eric Hoffman</a>, 
<a href="/search/eess?searchtype=author&query=Reinhardt%2C+J">Joseph Reinhardt</a>, 
<a href="/search/eess?searchtype=author&query=Barr%2C+R+G">R. Graham Barr</a>, 
<a href="/search/eess?searchtype=author&query=Angelini%2C+E">Elsa Angelini</a>, 
<a href="/search/eess?searchtype=author&query=Laine%2C+A">Andrew Laine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Unsupervised learning-based medical image registration approaches have
witnessed rapid development in recent years. We propose to revisit a commonly
ignored while simple and well-established principle: recursive refinement of
deformation vector fields across scales. We introduce a recursive refinement
network (RRN) for unsupervised medical image registration, to extract
multi-scale features, construct normalized local cost correlation volume and
recursively refine volumetric deformation vector fields. RRN achieves state of
the art performance for 3D registration of expiratory-inspiratory pairs of CT
lung scans. On DirLab COPDGene dataset, RRN returns an average Target
Registration Error (TRE) of 0.83 mm, which corresponds to a 13% error reduction
from the best result presented in the leaderboard. In addition to comparison
with conventional methods, RRN leads to 89% error reduction compared to
deep-learning-based peer approaches.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.07636" title="Abstract">arXiv:2106.07636</a> (cross-list from stat.ML) [<a href="/pdf/2106.07636" title="Download PDF">pdf</a>, <a href="/format/2106.07636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta Two-Sample Testing: Learning Kernels for Testing with Limited Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/stat?searchtype=author&query=Xu%2C+W">Wenkai Xu</a>, 
<a href="/search/stat?searchtype=author&query=Lu%2C+J">Jie Lu</a>, 
<a href="/search/stat?searchtype=author&query=Sutherland%2C+D+J">Danica J. Sutherland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available from <a href="https://github.com/fengliu90/MetaTesting">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Modern kernel-based two-sample tests have shown great success in
distinguishing complex, high-dimensional distributions with appropriate learned
kernels. Previous work has demonstrated that this kernel learning procedure
succeeds, assuming a considerable number of observed samples from each
distribution. In realistic scenarios with very limited numbers of data samples,
however, it can be challenging to identify a kernel powerful enough to
distinguish complex distributions. We address this issue by introducing the
problem of meta two-sample testing (M2ST), which aims to exploit (abundant)
auxiliary data on related tasks to find an algorithm that can quickly identify
a powerful test on new target tasks. We propose two specific algorithms for
this task: a generic scheme which improves over baselines and amore tailored
approach which performs even better. We provide both theoretical justification
and empirical evidence that our proposed meta-testing schemes out-perform
learning kernel-based tests directly from scarce observations, and identify
when such schemes will be successful.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue, 15 Jun 21</h3>
<dl>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1510.07357" title="Abstract">arXiv:1510.07357</a> (replaced) [<a href="/pdf/1510.07357" title="Download PDF">pdf</a>, <a href="/format/1510.07357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Bare-Bones Communication in Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chlebus%2C+B+S">Bogdan S. Chlebus</a>, 
<a href="/search/cs?searchtype=author&query=Kowalski%2C+D+R">Dariusz R. Kowalski</a>, 
<a href="/search/cs?searchtype=author&query=Vaya%2C+S">Shailesh Vaya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1705.09543" title="Abstract">arXiv:1705.09543</a> (replaced) [<a href="/e-print/1705.09543" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Indoor Location for Smart Environments with Wireless Sensor and Actuator  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhongliang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kuendig%2C+S">Stephane Kuendig</a>, 
<a href="/search/cs?searchtype=author&query=Carrera%2C+J">Jose Carrera</a>, 
<a href="/search/cs?searchtype=author&query=Carron%2C+B">Blaise Carron</a>, 
<a href="/search/cs?searchtype=author&query=Braun%2C+T">Torsten Braun</a>, 
<a href="/search/cs?searchtype=author&query=Rolim%2C+J">Jose Rolim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> different version is ongoing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1805.09694" title="Abstract">arXiv:1805.09694</a> (replaced) [<a href="/pdf/1805.09694" title="Download PDF">pdf</a>, <a href="/format/1805.09694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A derived isometry theorem for constructible sheaves on $\mathbb{R}$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Berkouk%2C+N">Nicolas Berkouk</a>, 
<a href="/search/math?searchtype=author&query=Ginot%2C+G">Gr&#xe9;gory Ginot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1807.10926" title="Abstract">arXiv:1807.10926</a> (replaced) [<a href="/pdf/1807.10926" title="Download PDF">pdf</a>, <a href="/format/1807.10926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An upper bound for min-max angle of polygons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asaeedi%2C+S">Saeed Asaeedi</a>, 
<a href="/search/cs?searchtype=author&query=Didehvar%2C+F">Farzad Didehvar</a>, 
<a href="/search/cs?searchtype=author&query=Mohades%2C+A">Ali Mohades</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1807.11702" title="Abstract">arXiv:1807.11702</a> (replaced) [<a href="/pdf/1807.11702" title="Download PDF">pdf</a>, <a href="/ps/1807.11702" title="Download PostScript">ps</a>, <a href="/format/1807.11702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Computation of Sequence Mappability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alzamel%2C+M">Mai Alzamel</a>, 
<a href="/search/cs?searchtype=author&query=Charalampopoulos%2C+P">Panagiotis Charalampopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Iliopoulos%2C+C+S">Costas S. Iliopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Kociumaka%2C+T">Tomasz Kociumaka</a>, 
<a href="/search/cs?searchtype=author&query=Pissis%2C+S+P">Solon P. Pissis</a>, 
<a href="/search/cs?searchtype=author&query=Radoszewski%2C+J">Jakub Radoszewski</a>, 
<a href="/search/cs?searchtype=author&query=Straszy%C5%84ski%2C+J">Juliusz Straszy&#x144;ski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SPIRE 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1811.09999" title="Abstract">arXiv:1811.09999</a> (replaced) [<a href="/pdf/1811.09999" title="Download PDF">pdf</a>, <a href="/ps/1811.09999" title="Download PostScript">ps</a>, <a href="/format/1811.09999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conservative Galerkin methods for dispersive Hamiltonian problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jackaman%2C+J">James Jackaman</a>, 
<a href="/search/math?searchtype=author&query=Pryer%2C+T">Tristan Pryer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1901.05732" title="Abstract">arXiv:1901.05732</a> (replaced) [<a href="/pdf/1901.05732" title="Download PDF">pdf</a>, <a href="/format/1901.05732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Coded Caching with Correlated Files
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Tuninetti%2C+D">Daniela Tuninetti</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+M">Mingyue Ji</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A short version of this paper was presented at the 2019 IEEE International Symposium on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1901.05921" title="Abstract">arXiv:1901.05921</a> (replaced) [<a href="/pdf/1901.05921" title="Download PDF">pdf</a>, <a href="/ps/1901.05921" title="Download PostScript">ps</a>, <a href="/format/1901.05921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Optimality of D2D Coded Caching with Uncoded Cache Placement and  One-shot Delivery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yapar%2C+%C3%87">&#xc7;a&#x11f;kan Yapar</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Schaefer%2C+R+F">Rafael F. Schaefer</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1901.06547" title="Abstract">arXiv:1901.06547</a> (replaced) [<a href="/pdf/1901.06547" title="Download PDF">pdf</a>, <a href="/ps/1901.06547" title="Download PostScript">ps</a>, <a href="/format/1901.06547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moss&#x27; logic for ordered coalgebras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%ADlkov%C3%A1%2C+M">Marta B&#xed;lkov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Dost%C3%A1l%2C+M">Mat&#x11b;j Dost&#xe1;l</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1904.01636" title="Abstract">arXiv:1904.01636</a> (replaced) [<a href="/pdf/1904.01636" title="Download PDF">pdf</a>, <a href="/format/1904.01636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards annotation-efficient segmentation via image-to-image translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vorontsov%2C+E">Eugene Vorontsov</a>, 
<a href="/search/cs?searchtype=author&query=Molchanov%2C+P">Pavlo Molchanov</a>, 
<a href="/search/cs?searchtype=author&query=Beckham%2C+C">Christopher Beckham</a>, 
<a href="/search/cs?searchtype=author&query=Kautz%2C+J">Jan Kautz</a>, 
<a href="/search/cs?searchtype=author&query=Kadoury%2C+S">Samuel Kadoury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1904.12171" title="Abstract">arXiv:1904.12171</a> (replaced) [<a href="/pdf/1904.12171" title="Download PDF">pdf</a>, <a href="/format/1904.12171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction with Unpredictable Feature Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+B">Bo-Jian Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhi-Hua Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1904.13088" title="Abstract">arXiv:1904.13088</a> (replaced) [<a href="/pdf/1904.13088" title="Download PDF">pdf</a>, <a href="/ps/1904.13088" title="Download PostScript">ps</a>, <a href="/format/1904.13088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dependence-Aware, Unbounded Sound Predictive Race Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gen%C3%A7%2C+K">Kaan Gen&#xe7;</a>, 
<a href="/search/cs?searchtype=author&query=Roemer%2C+J">Jake Roemer</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yufan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Bond%2C+M+D">Michael D. Bond</a> (Ohio State University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1905.10696" title="Abstract">arXiv:1905.10696</a> (replaced) [<a href="/pdf/1905.10696" title="Download PDF">pdf</a>, <a href="/format/1905.10696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lifelong Neural Predictive Coding: Learning Cumulatively Online without  Forgetting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ororbia%2C+A">Alexander Ororbia</a>, 
<a href="/search/cs?searchtype=author&query=Mali%2C+A">Ankur Mali</a>, 
<a href="/search/cs?searchtype=author&query=Kifer%2C+D">Daniel Kifer</a>, 
<a href="/search/cs?searchtype=author&query=Giles%2C+C+L">C. Lee Giles</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Key updates including results on standard benchmarks, e.g., split mnist/fmnist/not-mnist. Task selection/basal ganglia model has been integrated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1905.12418" title="Abstract">arXiv:1905.12418</a> (replaced) [<a href="/pdf/1905.12418" title="Download PDF">pdf</a>, <a href="/format/1905.12418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expected Tight Bounds for Robust Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alsubaihi%2C+S">Salman Alsubaihi</a>, 
<a href="/search/cs?searchtype=author&query=Bibi%2C+A">Adel Bibi</a>, 
<a href="/search/cs?searchtype=author&query=Alfadly%2C+M">Modar Alfadly</a>, 
<a href="/search/cs?searchtype=author&query=Hamdi%2C+A">Abdullah Hamdi</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented as a RobustML workshop paper at ICLR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1905.13298" title="Abstract">arXiv:1905.13298</a> (replaced) [<a href="/pdf/1905.13298" title="Download PDF">pdf</a>, <a href="/format/1905.13298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepShift: Towards Multiplication-Less Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elhoushi%2C+M">Mostafa Elhoushi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zihao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shafiq%2C+F">Farhan Shafiq</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y+H">Ye Henry Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+Y">Joey Yiwei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> -Added results for 8-bit and 16-bit fixed point activations, as well as 5-bit, 4-bit, 3-bit, and 2-bit weights. - Added link to GitHub code - Updated and fixed the training algorithm - Introduced 2 approaches for backward and forward pases - Showed better results for training from scratch on CIFAR10 and Imagenet - Added implementation on NVIDIA's GPU -Accepted in CVPR Mobile AI 2021 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1906.06514" title="Abstract">arXiv:1906.06514</a> (replaced) [<a href="/pdf/1906.06514" title="Download PDF">pdf</a>, <a href="/format/1906.06514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PVRED: A Position-Velocity Recurrent Encoder-Decoder for Human Motion  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongsong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jian Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Bin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiashi Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1907.03963" title="Abstract">arXiv:1907.03963</a> (replaced) [<a href="/pdf/1907.03963" title="Download PDF">pdf</a>, <a href="/ps/1907.03963" title="Download PostScript">ps</a>, <a href="/format/1907.03963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Follow Your Star: New Frameworks for Online Stochastic Matching with  Known and Unknown Patience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brubach%2C+B">Brian Brubach</a>, 
<a href="/search/cs?searchtype=author&query=Grammel%2C+N">Nathaniel Grammel</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Will Ma</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+A">Aravind Srinivasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Multiagent Systems (cs.MA); Combinatorics (math.CO); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1907.04793" title="Abstract">arXiv:1907.04793</a> (replaced) [<a href="/pdf/1907.04793" title="Download PDF">pdf</a>, <a href="/format/1907.04793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform stability of a class of large-scale parallel server networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hmedi%2C+H">Hassan Hmedi</a>, 
<a href="/search/math?searchtype=author&query=Arapostathis%2C+A">Ari Arapostathis</a>, 
<a href="/search/math?searchtype=author&query=Pang%2C+G">Guodong Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1907.08738" title="Abstract">arXiv:1907.08738</a> (replaced) [<a href="/pdf/1907.08738" title="Download PDF">pdf</a>, <a href="/format/1907.08738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Inference Gaussian Process Multiproxy Alignment of Continuous  Signals (BIGMACS): Applications for Paleoceanography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lee%2C+T">Taehee Lee</a>, 
<a href="/search/stat?searchtype=author&query=Lisiecki%2C+L+E">Lorraine E. Lisiecki</a>, 
<a href="/search/stat?searchtype=author&query=Rand%2C+D">Devin Rand</a>, 
<a href="/search/stat?searchtype=author&query=Gebbie%2C+G">Geoffrey Gebbie</a>, 
<a href="/search/stat?searchtype=author&query=Lawrence%2C+C+E">Charles E. Lawrence</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article has been submitted to "Bayesian Analysis"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1907.09532" title="Abstract">arXiv:1907.09532</a> (replaced) [<a href="/pdf/1907.09532" title="Download PDF">pdf</a>, <a href="/ps/1907.09532" title="Download PostScript">ps</a>, <a href="/format/1907.09532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational p-Willmore Flow with Conformal Penalty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gruber%2C+A">Anthony Gruber</a>, 
<a href="/search/math?searchtype=author&query=Aulisa%2C+E">Eugenio Aulisa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted 6/2020 to ACM Trans. Graph
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1907.12972" title="Abstract">arXiv:1907.12972</a> (replaced) [<a href="/pdf/1907.12972" title="Download PDF">pdf</a>, <a href="/format/1907.12972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transferability of Spectral Graph Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levie%2C+R">Ron Levie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bucci%2C+L">Lorenzo Bucci</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M+M">Michael M. Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Kutyniok%2C+G">Gitta Kutyniok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1908.00337" title="Abstract">arXiv:1908.00337</a> (replaced) [<a href="/pdf/1908.00337" title="Download PDF">pdf</a>, <a href="/format/1908.00337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Greek language teaching platform for primary school pupils
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michailidi%2C+E">Eleni Michailidi</a>, 
<a href="/search/cs?searchtype=author&query=Skordas%2C+I">Ioannis Skordas</a>, 
<a href="/search/cs?searchtype=author&query=Papatsimouli%2C+M">Maria Papatsimouli</a>, 
<a href="/search/cs?searchtype=author&query=Lazaridis%2C+L">Lazaros Lazaridis</a>, 
<a href="/search/cs?searchtype=author&query=Michailidis%2C+H">Heracles Michailidis</a>, 
<a href="/search/cs?searchtype=author&query=Tavoultzidou%2C+S">Stavroula Tavoultzidou</a>, 
<a href="/search/cs?searchtype=author&query=Fragulis%2C+G+F">George F. Fragulis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1908.04556" title="Abstract">arXiv:1908.04556</a> (replaced) [<a href="/pdf/1908.04556" title="Download PDF">pdf</a>, <a href="/format/1908.04556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinterpretation and Extension of Entropy Correction Terms for Residual  Distribution and Discontinuous Galerkin Schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Abgrall%2C+R">R&#xe9;mi Abgrall</a>, 
<a href="/search/math?searchtype=author&query=%C3%96ffner%2C+P">Philipp &#xd6;ffner</a>, 
<a href="/search/math?searchtype=author&query=Ranocha%2C+H">Hendrik Ranocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.09297" title="Abstract">arXiv:1910.09297</a> (replaced) [<a href="/pdf/1910.09297" title="Download PDF">pdf</a>, <a href="/format/1910.09297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical methods for the mass-conserved Ohta-Kawasaki equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Juan Zhang</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+S">Shifeng Li</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+K">Kai Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.09417" title="Abstract">arXiv:1910.09417</a> (replaced) [<a href="/pdf/1910.09417" title="Download PDF">pdf</a>, <a href="/format/1910.09417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum Probability Theorem: A Framework for Probabilistic Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marvasti%2C+A+E">Amir Emad Marvasti</a>, 
<a href="/search/cs?searchtype=author&query=Marvasti%2C+E+E">Ehsan Emad Marvasti</a>, 
<a href="/search/cs?searchtype=author&query=Bagci%2C+U">Ulas Bagci</a>, 
<a href="/search/cs?searchtype=author&query=Foroosh%2C+H">Hassan Foroosh</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> A. Emad Marvasti, E. Emad Marvasti, U. Bagci and H. Foroosh,
  "Maximum Probability Theorem: A Framework for Probabilistic Machine
  Learning," in IEEE Transactions on Artificial Intelligence, doi:
  10.1109/TAI.2021.3086046
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1911.10258" title="Abstract">arXiv:1911.10258</a> (replaced) [<a href="/pdf/1911.10258" title="Download PDF">pdf</a>, <a href="/format/1911.10258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fantastic Four: Differentiable Bounds on Singular Values of Convolution  Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singla%2C+S">Sahil Singla</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1911.11090" title="Abstract">arXiv:1911.11090</a> (replaced) [<a href="/pdf/1911.11090" title="Download PDF">pdf</a>, <a href="/format/1911.11090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning of Neural Architectures for Few-Shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elsken%2C+T">Thomas Elsken</a>, 
<a href="/search/cs?searchtype=author&query=Staffler%2C+B">Benedikt Staffler</a>, 
<a href="/search/cs?searchtype=author&query=Metzen%2C+J+H">Jan Hendrik Metzen</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.02660" title="Abstract">arXiv:1912.02660</a> (replaced) [<a href="/pdf/1912.02660" title="Download PDF">pdf</a>, <a href="/ps/1912.02660" title="Download PostScript">ps</a>, <a href="/format/1912.02660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crisp-determinization of weighted tree automata over strong bimonoids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=F%C3%BCl%C3%B6p%2C+Z">Zolt&#xe1;n F&#xfc;l&#xf6;p</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B3sz%C3%B3%2C+D">D&#xe1;vid K&#xf3;sz&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Vogler%2C+H">Heiko Vogler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.04007" title="Abstract">arXiv:1912.04007</a> (replaced) [<a href="/pdf/1912.04007" title="Download PDF">pdf</a>, <a href="/format/1912.04007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subspace power method for symmetric tensor decomposition and generalized  PCA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kileel%2C+J">Joe Kileel</a>, 
<a href="/search/math?searchtype=author&query=Pereira%2C+J+M">Jo&#xe3;o M. Pereira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages, 7 figures, 2 tables. v3: merged introduction and literature review; clarified convergence proof; increased the number of trials in the numerical experiments; and other improvements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.07812" title="Abstract">arXiv:1912.07812</a> (replaced) [<a href="/pdf/1912.07812" title="Download PDF">pdf</a>, <a href="/format/1912.07812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capsule Attention for Multimodal EEG-EOG Representation Learning with  Application to Driver Vigilance Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guangyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Etemad%2C+A">Ali Etemad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Neural Systems and Rehabilitation Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.09522" title="Abstract">arXiv:1912.09522</a> (replaced) [<a href="/pdf/1912.09522" title="Download PDF">pdf</a>, <a href="/format/1912.09522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event Outlier Detection in Continuous Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hauskrecht%2C+M">Milos Hauskrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2001.06372" title="Abstract">arXiv:2001.06372</a> (replaced) [<a href="/e-print/2001.06372" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BigEarthNet Dataset with A New Class-Nomenclature for Remote Sensing  Image Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sumbul%2C+G">Gencer Sumbul</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jian Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kreuziger%2C+T">Tristan Kreuziger</a>, 
<a href="/search/cs?searchtype=author&query=Marcelino%2C+F">Filipe Marcelino</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+H">Hugo Costa</a>, 
<a href="/search/cs?searchtype=author&query=Benevides%2C+P">Pedro Benevides</a>, 
<a href="/search/cs?searchtype=author&query=Caetano%2C+M">Mario Caetano</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+B">Beg&#xfc;m Demir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been withdrawn by the authors. This paper has been superseded by <a href="/abs/2105.07921">arXiv:2105.07921</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.00526" title="Abstract">arXiv:2002.00526</a> (replaced) [<a href="/pdf/2002.00526" title="Download PDF">pdf</a>, <a href="/format/2002.00526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DANCE: Enhancing saliency maps using decoys
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenbo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+X">Xinyu Xing</a>, 
<a href="/search/cs?searchtype=author&query=Noble%2C+W+S">William Stafford Noble</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.03629" title="Abstract">arXiv:2002.03629</a> (replaced) [<a href="/pdf/2002.03629" title="Download PDF">pdf</a>, <a href="/format/2002.03629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Feedforward Computation via Parallel Nonlinear Equation  Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yang Song</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Chenlin Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+R">Renjie Liao</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.04839" title="Abstract">arXiv:2002.04839</a> (replaced) [<a href="/pdf/2002.04839" title="Download PDF">pdf</a>, <a href="/format/2002.04839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaProp: Separating Momentum and Adaptivity in Adam
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziyin%2C+L">Liu Ziyin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z+T">Zhikang T.Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ueda%2C+M">Masahito Ueda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.07285" title="Abstract">arXiv:2002.07285</a> (replaced) [<a href="/pdf/2002.07285" title="Download PDF">pdf</a>, <a href="/format/2002.07285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Double/Debiased Machine Learning for Dynamic Treatment Effects via  g-Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Lewis%2C+G">Greg Lewis</a>, 
<a href="/search/econ?searchtype=author&query=Syrgkanis%2C+V">Vasilis Syrgkanis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.08187" title="Abstract">arXiv:2002.08187</a> (replaced) [<a href="/pdf/2002.08187" title="Download PDF">pdf</a>, <a href="/format/2002.08187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An adaptive virtual element method for the polymer self-consistent field  theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wei%2C+H">Huayi Wei</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chunyu Chen</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+K">Kai Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.10904" title="Abstract">arXiv:2002.10904</a> (replaced) [<a href="/pdf/2002.10904" title="Download PDF">pdf</a>, <a href="/format/2002.10904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Apprenticeship Learning via Kernel-based Inverse Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rucker%2C+M+A">Mark A. Rucker</a>, 
<a href="/search/cs?searchtype=author&query=Watson%2C+L+T">Layne T. Watson</a>, 
<a href="/search/cs?searchtype=author&query=Barnes%2C+L+E">Laura E. Barnes</a>, 
<a href="/search/cs?searchtype=author&query=Gerber%2C+M+S">Matthew S. Gerber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 23 figures, Submitted to Journal of Artificial Intelligence Research, "for source code, see <a href="https://github.com/mrucker/kpirl-kla">this https URL</a>"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.00578" title="Abstract">arXiv:2003.00578</a> (replaced) [<a href="/pdf/2003.00578" title="Download PDF">pdf</a>, <a href="/ps/2003.00578" title="Download PostScript">ps</a>, <a href="/format/2003.00578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Indistinguishability for Public Key Encryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gagliardoni%2C+T">Tommaso Gagliardoni</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%A4mer%2C+J">Juliane Kr&#xe4;mer</a>, 
<a href="/search/cs?searchtype=author&query=Struck%2C+P">Patrick Struck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.01926" title="Abstract">arXiv:2003.01926</a> (replaced) [<a href="/pdf/2003.01926" title="Download PDF">pdf</a>, <a href="/format/2003.01926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformation Importance with Applications to Cosmology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Singh%2C+C">Chandan Singh</a>, 
<a href="/search/stat?searchtype=author&query=Ha%2C+W">Wooseok Ha</a>, 
<a href="/search/stat?searchtype=author&query=Lanusse%2C+F">Francois Lanusse</a>, 
<a href="/search/stat?searchtype=author&query=Boehm%2C+V">Vanessa Boehm</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+J">Jia Liu</a>, 
<a href="/search/stat?searchtype=author&query=Yu%2C+B">Bin Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ICLR 2020 Workshop on Fundamental Science in the era of AI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.03196" title="Abstract">arXiv:2003.03196</a> (replaced) [<a href="/pdf/2003.03196" title="Download PDF">pdf</a>, <a href="/format/2003.03196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Continual Learning with Weighted Inter-client Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+W">Wonyong Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Giwoong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Eunho Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.06601" title="Abstract">arXiv:2003.06601</a> (replaced) [<a href="/pdf/2003.06601" title="Download PDF">pdf</a>, <a href="/format/2003.06601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lattice protein design using Bayesian learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Takahashi%2C+T">Tomoei Takahashi</a>, 
<a href="/search/physics?searchtype=author&query=Chikenji%2C+G">George Chikenji</a>, 
<a href="/search/physics?searchtype=author&query=Tokita%2C+K">Kei Tokita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biological Physics (physics.bio-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.11991" title="Abstract">arXiv:2003.11991</a> (replaced) [<a href="/pdf/2003.11991" title="Download PDF">pdf</a>, <a href="/ps/2003.11991" title="Download PostScript">ps</a>, <a href="/format/2003.11991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Treatment Effects with Observed Confounders and Mediators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gupta%2C+S">Shantanu Gupta</a>, 
<a href="/search/stat?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>, 
<a href="/search/stat?searchtype=author&query=Childers%2C+D">David Childers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2004.00202" title="Abstract">arXiv:2004.00202</a> (replaced) [<a href="/pdf/2004.00202" title="Download PDF">pdf</a>, <a href="/format/2004.00202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shared Cross-Modal Trajectory Prediction for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+C">Chiho Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J+H">Joon Hee Choi</a>, 
<a href="/search/cs?searchtype=author&query=Malla%2C+S">Srikanth Malla</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiachen Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2021 [Oral]. arXiv admin note: substantial text overlap with <a href="/abs/2011.08436">arXiv:2011.08436</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2004.08891" title="Abstract">arXiv:2004.08891</a> (replaced) [<a href="/pdf/2004.08891" title="Download PDF">pdf</a>, <a href="/format/2004.08891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hedging with Linear Regressions and Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Ruf%2C+J">Johannes Ruf</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+W">Weiguan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Forthcoming in the Journal of Business &amp; Economic Statistics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Machine Learning (cs.LG); Mathematical Finance (q-fin.MF); Statistical Finance (q-fin.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.00613" title="Abstract">arXiv:2005.00613</a> (replaced) [<a href="/pdf/2005.00613" title="Download PDF">pdf</a>, <a href="/format/2005.00613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Controllable Model of Grounded Response Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zeqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Galley%2C+M">Michel Galley</a>, 
<a href="/search/cs?searchtype=author&query=Brockett%2C+C">Chris Brockett</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Quirk%2C+C">Chris Quirk</a>, 
<a href="/search/cs?searchtype=author&query=Koncel-Kedziorski%2C+R">Rik Koncel-Kedziorski</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Ostendorf%2C+M">Mari Ostendorf</a>, 
<a href="/search/cs?searchtype=author&query=Dolan%2C+B">Bill Dolan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.04507" title="Abstract">arXiv:2005.04507</a> (replaced) [<a href="/pdf/2005.04507" title="Download PDF">pdf</a>, <a href="/format/2005.04507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PGDOT -- Perturbed Gradient Descent Adapted with Occupation Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guo%2C+X">Xin Guo</a>, 
<a href="/search/math?searchtype=author&query=Han%2C+J">Jiequn Han</a>, 
<a href="/search/math?searchtype=author&query=Tajrobehkar%2C+M">Mahan Tajrobehkar</a>, 
<a href="/search/math?searchtype=author&query=Tang%2C+W">Wenpin Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.07496" title="Abstract">arXiv:2005.07496</a> (replaced) [<a href="/pdf/2005.07496" title="Download PDF">pdf</a>, <a href="/format/2005.07496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundations and modelling of dynamic networks using Dynamic Graph Neural  Networks: A survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Skarding%2C+J">Joakim Skarding</a>, 
<a href="/search/cs?searchtype=author&query=Gabrys%2C+B">Bogdan Gabrys</a>, 
<a href="/search/cs?searchtype=author&query=Musial%2C+K">Katarzyna Musial</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 9 figures, 8 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Access, vol. 9, pp. 79143-79168, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.11092" title="Abstract">arXiv:2005.11092</a> (replaced) [<a href="/pdf/2005.11092" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Co-registration for Sentinel-1 SAR and Sentinel-2 Optical  images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ye%2C+Y">Yuanxin Ye</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+B">Bai Zhu</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+Y">Youquan He</a>, 
<a href="/search/eess?searchtype=author&query=Jia%2C+H">Huarong Jia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.12604" title="Abstract">arXiv:2005.12604</a> (replaced) [<a href="/pdf/2005.12604" title="Download PDF">pdf</a>, <a href="/format/2005.12604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An adaptive block Bregman proximal gradient method for computing  stationary states of multicomponent phase-field crystal model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bao%2C+C">Chenglong Bao</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chang Chen</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+K">Kai Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.01771" title="Abstract">arXiv:2006.01771</a> (replaced) [<a href="/pdf/2006.01771" title="Download PDF">pdf</a>, <a href="/format/2006.01771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Database of Power Grid Frequency Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jumar%2C+R">Richard Jumar</a>, 
<a href="/search/eess?searchtype=author&query=Maa%C3%9F%2C+H">Heiko Maa&#xdf;</a>, 
<a href="/search/eess?searchtype=author&query=Sch%C3%A4fer%2C+B">Benjamin Sch&#xe4;fer</a>, 
<a href="/search/eess?searchtype=author&query=Gorj%C3%A3o%2C+L+R">Leonardo Rydin Gorj&#xe3;o</a>, 
<a href="/search/eess?searchtype=author&query=Hagenmeyer%2C+V">Veit Hagenmeyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 10 figures. Data associated with this paper are located here: <a href="https://osf.io/by5hu/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.02138" title="Abstract">arXiv:2006.02138</a> (replaced) [<a href="/pdf/2006.02138" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Deep Learning into CAD/CAE System: Generative Design and  Evaluation of 3D Conceptual Wheel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Soyoung Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sunghee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seongsin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+K+H">Kwang Hyeon Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+H">Jong Ho Park</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+N">Namwoo Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.04222" title="Abstract">arXiv:2006.04222</a> (replaced) [<a href="/pdf/2006.04222" title="Download PDF">pdf</a>, <a href="/format/2006.04222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Entity-wise Factorization for Multi-Agent Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iqbal%2C+S">Shariq Iqbal</a>, 
<a href="/search/cs?searchtype=author&query=de+Witt%2C+C+A+S">Christian A. Schroeder de Witt</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bei Peng</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hmer%2C+W">Wendelin B&#xf6;hmer</a>, 
<a href="/search/cs?searchtype=author&query=Whiteson%2C+S">Shimon Whiteson</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+F">Fei Sha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021 Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.04648" title="Abstract">arXiv:2006.04648</a> (replaced) [<a href="/pdf/2006.04648" title="Download PDF">pdf</a>, <a href="/format/2006.04648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based Visual-Semantic Entanglement Network for Zero-shot Image  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+G">Guihua Wen</a>, 
<a href="/search/cs?searchtype=author&query=Chapman%2C+A">Adriane Chapman</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Pei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Mingnan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yingxue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+D">Dan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+W">Wendy Hall</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures, on IEEE Transactions on Multimedia
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> [J]. IEEE Transactions on Multimedia, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.04740" title="Abstract">arXiv:2006.04740</a> (replaced) [<a href="/pdf/2006.04740" title="Download PDF">pdf</a>, <a href="/format/2006.04740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Heavy-Tail Phenomenon in SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gurbuzbalaban%2C+M">Mert Gurbuzbalaban</a>, 
<a href="/search/math?searchtype=author&query=%C5%9Eim%C5%9Fekli%2C+U">Umut &#x15e;im&#x15f;ekli</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+L">Lingjiong Zhu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published as a conference paper at International Conference on
  Machine Learning (ICML) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.05145" title="Abstract">arXiv:2006.05145</a> (replaced) [<a href="/pdf/2006.05145" title="Download PDF">pdf</a>, <a href="/format/2006.05145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix games with bandit feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=O%27Donoghue%2C+B">Brendan O&#x27;Donoghue</a>, 
<a href="/search/cs?searchtype=author&query=Lattimore%2C+T">Tor Lattimore</a>, 
<a href="/search/cs?searchtype=author&query=Osband%2C+I">Ian Osband</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.05468" title="Abstract">arXiv:2006.05468</a> (replaced) [<a href="/pdf/2006.05468" title="Download PDF">pdf</a>, <a href="/format/2006.05468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Auto-Regressive Gaussian Processes for Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kapoor%2C+S">Sanyam Kapoor</a>, 
<a href="/search/stat?searchtype=author&query=Karaletsos%2C+T">Theofanis Karaletsos</a>, 
<a href="/search/stat?searchtype=author&query=Bui%2C+T+D">Thang D. Bui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Machine Learning (ICML), 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.06987" title="Abstract">arXiv:2006.06987</a> (replaced) [<a href="/pdf/2006.06987" title="Download PDF">pdf</a>, <a href="/format/2006.06987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved estimations of stochastic chemical kinetics by finite state  expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Waizmann%2C+T">Tabea Waizmann</a>, 
<a href="/search/q-bio?searchtype=author&query=Bortolussi%2C+L">Luca Bortolussi</a>, 
<a href="/search/q-bio?searchtype=author&query=Vandin%2C+A">Andrea Vandin</a>, 
<a href="/search/q-bio?searchtype=author&query=Tribastone%2C+M">Mirco Tribastone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Molecular Networks (q-bio.MN)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.07942" title="Abstract">arXiv:2006.07942</a> (replaced) [<a href="/pdf/2006.07942" title="Download PDF">pdf</a>, <a href="/format/2006.07942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Duplicity Games for Deception Design with an Application to Insider  Threat Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Linan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Quanyan Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.08836" title="Abstract">arXiv:2006.08836</a> (replaced) [<a href="/pdf/2006.08836" title="Download PDF">pdf</a>, <a href="/format/2006.08836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extension complexity of low-dimensional polytopes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kwan%2C+M">Matthew Kwan</a>, 
<a href="/search/math?searchtype=author&query=Sauermann%2C+L">Lisa Sauermann</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+Y">Yufei Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We fixed an issue with Lemma 6.9 (the exponential Efron-Stein inequality was previously used incorrectly)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.09179" title="Abstract">arXiv:2006.09179</a> (replaced) [<a href="/pdf/2006.09179" title="Download PDF">pdf</a>, <a href="/format/2006.09179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstruction of turbulent data with deep generative models for  semantic inpainting from TURB-Rot database
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Buzzicotti%2C+M">M. Buzzicotti</a>, 
<a href="/search/physics?searchtype=author&query=Bonaccorso%2C+F">F. Bonaccorso</a>, 
<a href="/search/physics?searchtype=author&query=Di+Leoni%2C+P+C">P. Clark Di Leoni</a>, 
<a href="/search/physics?searchtype=author&query=Biferale%2C+L">L. Biferale</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. Fluids 6, 050503 (2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Statistical Mechanics (cond-mat.stat-mech); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Chaotic Dynamics (nlin.CD)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.10529" title="Abstract">arXiv:2006.10529</a> (replaced) [<a href="/pdf/2006.10529" title="Download PDF">pdf</a>, <a href="/format/2006.10529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Path Features and Neural Path Kernel : Understanding the role of  gates in deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lakshminarayanan%2C+C">Chandrashekar Lakshminarayanan</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+V">Amit Vikram Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared in NeurIPS 2020 (23 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.14512" title="Abstract">arXiv:2006.14512</a> (replaced) [<a href="/pdf/2006.14512" title="Download PDF">pdf</a>, <a href="/format/2006.14512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering the Connections Between Adversarial Transferability and  Knowledge Transferability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kaizhao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+Y">Jacky Y. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+O">Oluwasanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.16205" title="Abstract">arXiv:2006.16205</a> (replaced) [<a href="/pdf/2006.16205" title="Download PDF">pdf</a>, <a href="/format/2006.16205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for  Improved Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+S+M">Sang Michael Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tengyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.01612" title="Abstract">arXiv:2007.01612</a> (replaced) [<a href="/pdf/2007.01612" title="Download PDF">pdf</a>, <a href="/ps/2007.01612" title="Download PostScript">ps</a>, <a href="/format/2007.01612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online learning in MDPs with linear function approximation and bandit  feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neu%2C+G">Gergely Neu</a>, 
<a href="/search/cs?searchtype=author&query=Olkhovskaya%2C+J">Julia Olkhovskaya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.03767" title="Abstract">arXiv:2007.03767</a> (replaced) [<a href="/pdf/2007.03767" title="Download PDF">pdf</a>, <a href="/format/2007.03767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending against Backdoors in Federated Learning with Robust Learning  Rate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozdayi%2C+M+S">Mustafa Safa Ozdayi</a>, 
<a href="/search/cs?searchtype=author&query=Kantarcioglu%2C+M">Murat Kantarcioglu</a>, 
<a href="/search/cs?searchtype=author&query=Gel%2C+Y+R">Yulia R. Gel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at AAAI 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.04938" title="Abstract">arXiv:2007.04938</a> (replaced) [<a href="/pdf/2007.04938" title="Download PDF">pdf</a>, <a href="/format/2007.04938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kimin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Laskin%2C+M">Michael Laskin</a>, 
<a href="/search/cs?searchtype=author&query=Srinivas%2C+A">Aravind Srinivas</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.05724" title="Abstract">arXiv:2007.05724</a> (replaced) [<a href="/pdf/2007.05724" title="Download PDF">pdf</a>, <a href="/format/2007.05724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Randomly Perturbed Structured Predictors for Direct Loss  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Indelman%2C+H+C">Hedda Cohen Indelman</a>, 
<a href="/search/stat?searchtype=author&query=Hazan%2C+T">Tamir Hazan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 38th International Conference on Machine Learning, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.09453" title="Abstract">arXiv:2007.09453</a> (replaced) [<a href="/pdf/2007.09453" title="Download PDF">pdf</a>, <a href="/format/2007.09453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Image Classification Using A Low-Pass Activation Function and DCT  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+M+T">Md Tahmid Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+S+W">Shyh Wei Teng</a>, 
<a href="/search/cs?searchtype=author&query=Sohel%2C+F">Ferdous Sohel</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+G">Guojun Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.15190" title="Abstract">arXiv:2007.15190</a> (replaced) [<a href="/pdf/2007.15190" title="Download PDF">pdf</a>, <a href="/format/2007.15190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantitative Understanding of VAE as a Non-linearly Scaled Isometric  Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nakagawa%2C+A">Akira Nakagawa</a>, 
<a href="/search/stat?searchtype=author&query=Kato%2C+K">Keizo Kato</a>, 
<a href="/search/stat?searchtype=author&query=Suzuki%2C+T">Taiji Suzuki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 29 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.00483" title="Abstract">arXiv:2008.00483</a> (replaced) [<a href="/pdf/2008.00483" title="Download PDF">pdf</a>, <a href="/ps/2008.00483" title="Download PostScript">ps</a>, <a href="/format/2008.00483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zuyue Fu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.00553" title="Abstract">arXiv:2008.00553</a> (replaced) [<a href="/pdf/2008.00553" title="Download PDF">pdf</a>, <a href="/format/2008.00553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unifying Framework for Parallel and Distributed Processing in R using  Futures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bengtsson%2C+H">Henrik Bengtsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 1 figure, accepted The R Journal, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.01158" title="Abstract">arXiv:2008.01158</a> (replaced) [<a href="/pdf/2008.01158" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Disease Classification of 13,667 Body CT Scans Using Weakly  Supervised Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tushar%2C+F+I">Fakrul Islam Tushar</a>, 
<a href="/search/cs?searchtype=author&query=D%27Anniballe%2C+V+M">Vincent M. D&#x27;Anniballe</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+R">Rui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Mazurowski%2C+M+A">Maciej A. Mazurowski</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+W">Wanyi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Samei%2C+E">Ehsan Samei</a>, 
<a href="/search/cs?searchtype=author&query=Rubin%2C+G+D">Geoffrey D. Rubin</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+J+Y">Joseph Y. Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.01558" title="Abstract">arXiv:2008.01558</a> (replaced) [<a href="/pdf/2008.01558" title="Download PDF">pdf</a>, <a href="/format/2008.01558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with Sparsification-Amplified Privacy and Adaptive  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Rui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yanmin Gong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuanxiong Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IJCAI 2021, this is the full version with appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.03194" title="Abstract">arXiv:2008.03194</a> (replaced) [<a href="/pdf/2008.03194" title="Download PDF">pdf</a>, <a href="/format/2008.03194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Low-Rank Tensor Learning for Spatiotemporal Traffic Data  Imputation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+X">Xinyu Chen</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+Y">Yixian Chen</a>, 
<a href="/search/stat?searchtype=author&query=Saunier%2C+N">Nicolas Saunier</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+L">Lijun Sun</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transportation Research Part C Emerging Technologies (2021)
  129:103226
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.05221" title="Abstract">arXiv:2008.05221</a> (replaced) [<a href="/pdf/2008.05221" title="Download PDF">pdf</a>, <a href="/format/2008.05221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression of Deep Learning Models for Text: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Manish Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Puneet Agrawal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at TKDD for publication. 53 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.06910" title="Abstract">arXiv:2008.06910</a> (replaced) [<a href="/pdf/2008.06910" title="Download PDF">pdf</a>, <a href="/format/2008.06910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Descent for Visual 3D Human Pose and Shape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zanfir%2C+A">Andrei Zanfir</a>, 
<a href="/search/cs?searchtype=author&query=Bazavan%2C+E+G">Eduard Gabriel Bazavan</a>, 
<a href="/search/cs?searchtype=author&query=Zanfir%2C+M">Mihai Zanfir</a>, 
<a href="/search/cs?searchtype=author&query=Freeman%2C+W+T">William T. Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Sukthankar%2C+R">Rahul Sukthankar</a>, 
<a href="/search/cs?searchtype=author&query=Sminchisescu%2C+C">Cristian Sminchisescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.09887" title="Abstract">arXiv:2008.09887</a> (replaced) [<a href="/pdf/2008.09887" title="Download PDF">pdf</a>, <a href="/format/2008.09887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Data Programming with Subset Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maheshwari%2C+A">Ayush Maheshwari</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+O">Oishik Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Killamsetty%2C+K">KrishnaTeja Killamsetty</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+R">Rishabh Iyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of ACL, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.10898" title="Abstract">arXiv:2008.10898</a> (replaced) [<a href="/pdf/2008.10898" title="Download PDF">pdf</a>, <a href="/format/2008.10898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAGE: A Simple and Optimal Probabilistic Gradient Estimator for  Nonconvex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhize Li</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hongyan Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Richt%C3%A1rik%2C+P">Peter Richt&#xe1;rik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages; accepted by ICML 2021 (long talk)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.13305" title="Abstract">arXiv:2008.13305</a> (replaced) [<a href="/pdf/2008.13305" title="Download PDF">pdf</a>, <a href="/format/2008.13305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Integrated Approach to Produce Robust Models with High Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhijian Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Jack Xin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.00102" title="Abstract">arXiv:2009.00102</a> (replaced) [<a href="/pdf/2009.00102" title="Download PDF">pdf</a>, <a href="/format/2009.00102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete conservation laws for finite element discretisations of  multisymplectic PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Celledoni%2C+E">Elena Celledoni</a>, 
<a href="/search/math?searchtype=author&query=Jackaman%2C+J">James Jackaman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.01454" title="Abstract">arXiv:2009.01454</a> (replaced) [<a href="/pdf/2009.01454" title="Download PDF">pdf</a>, <a href="/format/2009.01454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Say No to the Discrimination: Learning Fair Graph Neural Networks with  Limited Sensitive Attribute Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+E">Enyan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.01716" title="Abstract">arXiv:2009.01716</a> (replaced) [<a href="/pdf/2009.01716" title="Download PDF">pdf</a>, <a href="/format/2009.01716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Software-Defined Networking Solution for Transparent Session and  Service Continuity in Dynamic Multi-Access Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fondo-Ferreiro%2C+P">Pablo Fondo-Ferreiro</a>, 
<a href="/search/cs?searchtype=author&query=Gil-Casti%C3%B1eira%2C+F">Felipe Gil-Casti&#xf1;eira</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Casta%C3%B1o%2C+F+J">Francisco Javier Gonz&#xe1;lez-Casta&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Candal-Ventureira%2C+D">David Candal-Ventureira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version of the article published in IEEE Transactions on Network and Service Management
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Network and Service Management, vol. 18, no.
  2, pp. 1401-1414, June 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.05567" title="Abstract">arXiv:2009.05567</a> (replaced) [<a href="/pdf/2009.05567" title="Download PDF">pdf</a>, <a href="/format/2009.05567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Unlearning for Random Forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brophy%2C+J">Jonathan Brophy</a>, 
<a href="/search/cs?searchtype=author&query=Lowd%2C+D">Daniel Lowd</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 5 figures, 9 tables, and 3 algorithms. Accepted at ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.06040" title="Abstract">arXiv:2009.06040</a> (replaced) [<a href="/pdf/2009.06040" title="Download PDF">pdf</a>, <a href="/ps/2009.06040" title="Download PostScript">ps</a>, <a href="/format/2009.06040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Span-based Semantic Parsing for Compositional Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herzig%2C+J">Jonathan Herzig</a>, 
<a href="/search/cs?searchtype=author&query=Berant%2C+J">Jonathan Berant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2021 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.06701" title="Abstract">arXiv:2009.06701</a> (replaced) [<a href="/pdf/2009.06701" title="Download PDF">pdf</a>, <a href="/format/2009.06701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dirty Road Can Attack: Security of Deep Learning based Automated Lane  Centering under Physical-World Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sato%2C+T">Takami Sato</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Junjie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Ningfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y+J">Yunhan Jack Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xue Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q+A">Qi Alfred Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Usenix Security '21
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.09162" title="Abstract">arXiv:2009.09162</a> (replaced) [<a href="/pdf/2009.09162" title="Download PDF">pdf</a>, <a href="/format/2009.09162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extracting Summary Knowledge Graphs from Long Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zeqiu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Koncel-Kedziorski%2C+R">Rik Koncel-Kedziorski</a>, 
<a href="/search/cs?searchtype=author&query=Ostendorf%2C+M">Mari Ostendorf</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.12480" title="Abstract">arXiv:2009.12480</a> (replaced) [<a href="/pdf/2009.12480" title="Download PDF">pdf</a>, <a href="/format/2009.12480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bandwidth-Agile Image Transmission with Deep Joint Source-Channel Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurka%2C+D+B">David Burth Kurka</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnd%C3%BCz%2C+D">Deniz G&#xfc;nd&#xfc;z</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2009.13504" title="Abstract">arXiv:2009.13504</a> (replaced) [<a href="/pdf/2009.13504" title="Download PDF">pdf</a>, <a href="/format/2009.13504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Obfuscation of Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+P">Peiyuan Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Keyulu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>, 
<a href="/search/cs?searchtype=author&query=Gordon%2C+G">Geoffrey Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Jegelka%2C+S">Stefanie Jegelka</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021; Code is available at <a href="https://github.com/liaopeiyuan/GAL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.01755" title="Abstract">arXiv:2010.01755</a> (replaced) [<a href="/pdf/2010.01755" title="Download PDF">pdf</a>, <a href="/format/2010.01755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Distributed Model-Free Ride-Sharing Approach for Joint Matching,  Pricing, and Dispatching using Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haliem%2C+M">Marina Haliem</a>, 
<a href="/search/cs?searchtype=author&query=Mani%2C+G">Ganapathy Mani</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Bhargava%2C+B">Bharat Bhargava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.02068" title="Abstract">arXiv:2010.02068</a> (replaced) [<a href="/pdf/2010.02068" title="Download PDF">pdf</a>, <a href="/format/2010.02068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning for Electric Vehicle Routing Problem with  Time Windows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ghaddar%2C+B">Bissan Ghaddar</a>, 
<a href="/search/cs?searchtype=author&query=Nathwani%2C+J">Jatin Nathwani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.03934" title="Abstract">arXiv:2010.03934</a> (replaced) [<a href="/pdf/2010.03934" title="Download PDF">pdf</a>, <a href="/format/2010.03934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prioritized Level Replay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Minqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Grefenstette%2C+E">Edward Grefenstette</a>, 
<a href="/search/cs?searchtype=author&query=Rockt%C3%A4schel%2C+T">Tim Rockt&#xe4;schel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.04223" title="Abstract">arXiv:2010.04223</a> (replaced) [<a href="/pdf/2010.04223" title="Download PDF">pdf</a>, <a href="/format/2010.04223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fictitious play in zero-sum stochastic games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayin%2C+M+O">Muhammed O. Sayin</a>, 
<a href="/search/cs?searchtype=author&query=Parise%2C+F">Francesca Parise</a>, 
<a href="/search/cs?searchtype=author&query=Ozdaglar%2C+A">Asuman Ozdaglar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.04627" title="Abstract">arXiv:2010.04627</a> (replaced) [<a href="/pdf/2010.04627" title="Download PDF">pdf</a>, <a href="/format/2010.04627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Binary Decision Trees by Argmin Differentiation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zantedeschi%2C+V">Valentina Zantedeschi</a>, 
<a href="/search/cs?searchtype=author&query=Kusner%2C+M+J">Matt J. Kusner</a>, 
<a href="/search/cs?searchtype=author&query=Niculae%2C+V">Vlad Niculae</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.05153" title="Abstract">arXiv:2010.05153</a> (replaced) [<a href="/pdf/2010.05153" title="Download PDF">pdf</a>, <a href="/format/2010.05153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Learning and Distributed Control for Residential Demand Response
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yingying Li</a>, 
<a href="/search/eess?searchtype=author&query=Shimada%2C+J">Jun Shimada</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+N">Na Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.05594" title="Abstract">arXiv:2010.05594</a> (replaced) [<a href="/pdf/2010.05594" title="Download PDF">pdf</a>, <a href="/format/2010.05594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiWOZ 2.3: A multi-domain task-oriented dialogue dataset enhanced  with annotation corrections and co-reference annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Ting Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Ximing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Takanobu%2C+R">Ryuichi Takanobu</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Y">Yixin Lian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chongxuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+D">Dazhen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wei Peng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.07003" title="Abstract">arXiv:2010.07003</a> (replaced) [<a href="/pdf/2010.07003" title="Download PDF">pdf</a>, <a href="/format/2010.07003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime  with Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gyuwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2021; 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.08175" title="Abstract">arXiv:2010.08175</a> (replaced) [<a href="/pdf/2010.08175" title="Download PDF">pdf</a>, <a href="/format/2010.08175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anisotropic Stroke Control for Multiple Artists Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuanhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xirui Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Naiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+T">Ting Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+B">Bingbing Ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACMMM2020
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Multimedia 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.08581" title="Abstract">arXiv:2010.08581</a> (replaced) [<a href="/pdf/2010.08581" title="Download PDF">pdf</a>, <a href="/format/2010.08581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sampling-Based Method for Tensor Ring Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Malik%2C+O+A">Osman Asif Malik</a>, 
<a href="/search/math?searchtype=author&query=Becker%2C+S">Stephen Becker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.09029" title="Abstract">arXiv:2010.09029</a> (replaced) [<a href="/pdf/2010.09029" title="Download PDF">pdf</a>, <a href="/format/2010.09029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHECKED: Chinese COVID-19 Fake News Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zafarani%2C+R">Reza Zafarani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Social Network Analysis and Mining (SNAM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.09116" title="Abstract">arXiv:2010.09116</a> (replaced) [<a href="/pdf/2010.09116" title="Download PDF">pdf</a>, <a href="/format/2010.09116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topic Recommendation for Software Repositories using Multi-label  Classification Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Izadi%2C+M">Maliheh Izadi</a>, 
<a href="/search/cs?searchtype=author&query=Heydarnoori%2C+A">Abbas Heydarnoori</a>, 
<a href="/search/cs?searchtype=author&query=Gousios%2C+G">Georgios Gousios</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.09317" title="Abstract">arXiv:2010.09317</a> (replaced) [<a href="/pdf/2010.09317" title="Download PDF">pdf</a>, <a href="/format/2010.09317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Overview on 5G-and-Beyond Networks with UAVs: From  Communications to Sensing and Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+D+W+K">Derrick Wing Kwan Ng</a>, 
<a href="/search/cs?searchtype=author&query=Al-Dhahir%2C+N">Naofal Al-Dhahir</a>, 
<a href="/search/cs?searchtype=author&query=Schober%2C+R">Robert Schober</a>, 
<a href="/search/cs?searchtype=author&query=Swindlehurst%2C+A+L">A. Lee Swindlehurst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE JSAC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.09670" title="Abstract">arXiv:2010.09670</a> (replaced) [<a href="/pdf/2010.09670" title="Download PDF">pdf</a>, <a href="/format/2010.09670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RobustBench: a standardized adversarial robustness benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Croce%2C+F">Francesco Croce</a>, 
<a href="/search/cs?searchtype=author&query=Andriushchenko%2C+M">Maksym Andriushchenko</a>, 
<a href="/search/cs?searchtype=author&query=Sehwag%2C+V">Vikash Sehwag</a>, 
<a href="/search/cs?searchtype=author&query=Debenedetti%2C+E">Edoardo Debenedetti</a>, 
<a href="/search/cs?searchtype=author&query=Flammarion%2C+N">Nicolas Flammarion</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+M">Mung Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+P">Prateek Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Hein%2C+M">Matthias Hein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Version 2: 90+ evaluations, 60+ models, 5 leaderboards (Linf, L2, common corruptions), significantly expanded analysis part (calibration, fairness, privacy leakage, smoothness, transferability)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.11387" title="Abstract">arXiv:2010.11387</a> (replaced) [<a href="/pdf/2010.11387" title="Download PDF">pdf</a>, <a href="/format/2010.11387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boateng%2C+G">George Boateng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages. Accepted and presented at NeurIPS 2020 workshop (Black in AI) and AIED 2021 (international conference on AI in Education)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.12919" title="Abstract">arXiv:2010.12919</a> (replaced) [<a href="/pdf/2010.12919" title="Download PDF">pdf</a>, <a href="/format/2010.12919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Effects of Linguistic Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pryzant%2C+R">Reid Pryzant</a>, 
<a href="/search/cs?searchtype=author&query=Card%2C+D">Dallas Card</a>, 
<a href="/search/cs?searchtype=author&query=Jurafsky%2C+D">Dan Jurafsky</a>, 
<a href="/search/cs?searchtype=author&query=Veitch%2C+V">Victor Veitch</a>, 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+D">Dhanya Sridhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NAACL 2021 (Annual Conference of the North American Chapter of the Association for Computational Linguistics)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.14318" title="Abstract">arXiv:2010.14318</a> (replaced) [<a href="/pdf/2010.14318" title="Download PDF">pdf</a>, <a href="/ps/2010.14318" title="Download PostScript">ps</a>, <a href="/format/2010.14318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multitask Training with Text Data for End-to-End Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sainath%2C+T+N">Tara N. Sainath</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+R+J">Ron J. Weiss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.14535" title="Abstract">arXiv:2010.14535</a> (replaced) [<a href="/pdf/2010.14535" title="Download PDF">pdf</a>, <a href="/format/2010.14535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Architecture Search of SPD Manifold Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukthanker%2C+R+S">Rhea Sanjay Sukthanker</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiwu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Suryansh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Endsjo%2C+E+G">Erik Goron Endsjo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted for publication at IJCAI 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.14824" title="Abstract">arXiv:2010.14824</a> (replaced) [<a href="/pdf/2010.14824" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Artificial Intelligence for Manufacturing Cost Estimation  and Machining Feature Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Soyoung Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+N">Namwoo Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.14860" title="Abstract">arXiv:2010.14860</a> (replaced) [<a href="/pdf/2010.14860" title="Download PDF">pdf</a>, <a href="/format/2010.14860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Evidence Lower Bound of Variational Autoencoders Converges to a Sum  of Three Entropies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=L%C3%BCcke%2C+J">J&#xf6;rg L&#xfc;cke</a>, 
<a href="/search/stat?searchtype=author&query=Forster%2C+D">Dennis Forster</a>, 
<a href="/search/stat?searchtype=author&query=Dai%2C+Z">Zhenwen Dai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.00344" title="Abstract">arXiv:2011.00344</a> (replaced) [<a href="/pdf/2011.00344" title="Download PDF">pdf</a>, <a href="/format/2011.00344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Distribution-Dependent Analysis of Meta-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Konobeev%2C+M">Mikhail Konobeev</a>, 
<a href="/search/stat?searchtype=author&query=Kuzborskij%2C+I">Ilja Kuzborskij</a>, 
<a href="/search/stat?searchtype=author&query=Szepesv%C3%A1ri%2C+C">Csaba Szepesv&#xe1;ri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.00382" title="Abstract">arXiv:2011.00382</a> (replaced) [<a href="/pdf/2011.00382" title="Download PDF">pdf</a>, <a href="/format/2011.00382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Policy Gradient Algorithm for Learning to Learn in Multiagent  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dong-Ki Kim</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Miao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Riemer%2C+M">Matthew Riemer</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chuangchuang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Abdulhai%2C+M">Marwa Abdulhai</a>, 
<a href="/search/cs?searchtype=author&query=Habibi%2C+G">Golnaz Habibi</a>, 
<a href="/search/cs?searchtype=author&query=Lopez-Cot%2C+S">Sebastian Lopez-Cot</a>, 
<a href="/search/cs?searchtype=author&query=Tesauro%2C+G">Gerald Tesauro</a>, 
<a href="/search/cs?searchtype=author&query=How%2C+J+P">Jonathan P. How</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2021. Code at <a href="https://github.com/dkkim93/meta-mapg">this https URL</a> and Videos at <a href="https://sites.google.com/view/meta-mapg/home">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.00751" title="Abstract">arXiv:2011.00751</a> (replaced) [<a href="/pdf/2011.00751" title="Download PDF">pdf</a>, <a href="/format/2011.00751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Critical Correspondence on Humpty Dumpty&#x27;s Funding for European  Journalism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruohonen%2C+J">Jukka Ruohonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.01192" title="Abstract">arXiv:2011.01192</a> (replaced) [<a href="/pdf/2011.01192" title="Download PDF">pdf</a>, <a href="/format/2011.01192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Budget Sharing for Multi-Analyst Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pujol%2C+D">David Pujol</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yikai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fain%2C+B">Brandon Fain</a>, 
<a href="/search/cs?searchtype=author&query=Machanavajjhala%2C+A">Ashwin Machanavajjhala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, accepted to PVLDB Vol. 14, to be presented at VLDB 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.01489" title="Abstract">arXiv:2011.01489</a> (replaced) [<a href="/pdf/2011.01489" title="Download PDF">pdf</a>, <a href="/format/2011.01489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Computing Stable Extensions of Abstract Argumentation Frameworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nofal%2C+S">Samer Nofal</a>, 
<a href="/search/cs?searchtype=author&query=Jabal%2C+A+A">Amani Abu Jabal</a>, 
<a href="/search/cs?searchtype=author&query=Alfarrarjeh%2C+A">Abdullah Alfarrarjeh</a>, 
<a href="/search/cs?searchtype=author&query=Hababeh%2C+I">Ismail Hababeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.03853" title="Abstract">arXiv:2011.03853</a> (replaced) [<a href="/pdf/2011.03853" title="Download PDF">pdf</a>, <a href="/format/2011.03853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A fast randomized incremental gradient method for decentralized  non-convex optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xin%2C+R">Ran Xin</a>, 
<a href="/search/math?searchtype=author&query=Khan%2C+U+A">Usman A. Khan</a>, 
<a href="/search/math?searchtype=author&query=Kar%2C+S">Soummya Kar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added more numerical experiments, expanded discussion on technical results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.06569" title="Abstract">arXiv:2011.06569</a> (replaced) [<a href="/pdf/2011.06569" title="Download PDF">pdf</a>, <a href="/format/2011.06569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When are Adaptive Strategies in Asymptotic Quantum Channel  Discrimination Useful?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Salek%2C+F">Farzin Salek</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hayashi%2C+M">Masahito Hayashi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Winter%2C+A">Andreas Winter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V2 has more complete references, an improved presentation, including clearer figures and a summary of the classes of strategies considered, also an additional example of an asymptotic separation between adaptive and non-adaptive strategies for two qc-channels, and finally a cleaned-up presentation of the discrimination power of a quantum channel
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.07435" title="Abstract">arXiv:2011.07435</a> (replaced) [<a href="/pdf/2011.07435" title="Download PDF">pdf</a>, <a href="/format/2011.07435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functorial Manifold Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shiebler%2C+D">Dan Shiebler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.07585" title="Abstract">arXiv:2011.07585</a> (replaced) [<a href="/pdf/2011.07585" title="Download PDF">pdf</a>, <a href="/ps/2011.07585" title="Download PostScript">ps</a>, <a href="/format/2011.07585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An acceleration of decentralized SGD under general assumptions with low  stochastic noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ekaterina%2C+T">Trimbach Ekaterina</a>, 
<a href="/search/math?searchtype=author&query=Alexander%2C+R">Rogozin Alexander</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MOTOR 2021 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.10977" title="Abstract">arXiv:2011.10977</a> (replaced) [<a href="/pdf/2011.10977" title="Download PDF">pdf</a>, <a href="/format/2011.10977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel NOMA Solution with RIS Partitioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khaleel%2C+A">Aymen Khaleel</a>, 
<a href="/search/eess?searchtype=author&query=Basar%2C+E">Ertugrul Basar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.11057" title="Abstract">arXiv:2011.11057</a> (replaced) [<a href="/pdf/2011.11057" title="Download PDF">pdf</a>, <a href="/format/2011.11057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Gaussian Process Regression Based on Iterative Trimming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhao-Zhou Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lu Li</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhengyi Shao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> major revision, 11 pages, 8 figures, 2 tables; accepted by Astronomy and Computing; code available at <a href="https://github.com/syrte/robustgp/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.11256" title="Abstract">arXiv:2011.11256</a> (replaced) [<a href="/pdf/2011.11256" title="Download PDF">pdf</a>, <a href="/format/2011.11256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Quantized Neural Nets by Coarse Gradient Method for Non-linear  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Ziang Long</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+P">Penghang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Jack Xin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.12114" title="Abstract">arXiv:2011.12114</a> (replaced) [<a href="/pdf/2011.12114" title="Download PDF">pdf</a>, <a href="/format/2011.12114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking solar photovoltaic parameter estimation: global optimality  analysis and a simple efficient differential evolution method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gao%2C+S">Shuhua Gao</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+C">Cheng Xiang</a>, 
<a href="/search/eess?searchtype=author&query=Ming%2C+Y">Yu Ming</a>, 
<a href="/search/eess?searchtype=author&query=Tak%2C+T+K">Tan Kuan Tak</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+T+H">Tong Heng Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2, see source code at <a href="https://github.com/ShuhuaGao/rePVest">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.14372" title="Abstract">arXiv:2011.14372</a> (replaced) [<a href="/pdf/2011.14372" title="Download PDF">pdf</a>, <a href="/format/2011.14372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CNN-based Lung CT Registration with Multiple Anatomical Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hering%2C+A">Alessa Hering</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4ger%2C+S">Stephanie H&#xe4;ger</a>, 
<a href="/search/cs?searchtype=author&query=Moltz%2C+J">Jan Moltz</a>, 
<a href="/search/cs?searchtype=author&query=Lessmann%2C+N">Nikolas Lessmann</a>, 
<a href="/search/cs?searchtype=author&query=Heldmann%2C+S">Stefan Heldmann</a>, 
<a href="/search/cs?searchtype=author&query=van+Ginneken%2C+B">Bram van Ginneken</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.14804" title="Abstract">arXiv:2011.14804</a> (replaced) [<a href="/pdf/2011.14804" title="Download PDF">pdf</a>, <a href="/format/2011.14804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extremal Set Theory and LWE Based Access Structure Hiding Verifiable  Secret Sharing with Malicious-Majority and Free Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sehrawat%2C+V+S">Vipin Singh Sehrawat</a>, 
<a href="/search/cs?searchtype=author&query=Yeo%2C+F+Y">Foo Yee Yeo</a>, 
<a href="/search/cs?searchtype=author&query=Desmedt%2C+Y">Yvo Desmedt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.01750" title="Abstract">arXiv:2012.01750</a> (replaced) [<a href="/pdf/2012.01750" title="Download PDF">pdf</a>, <a href="/format/2012.01750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Failures of Deep Networks via Robust Feature Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singla%2C+S">Sahil Singla</a>, 
<a href="/search/cs?searchtype=author&query=Nushi%2C+B">Besmira Nushi</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S">Shital Shah</a>, 
<a href="/search/cs?searchtype=author&query=Kamar%2C+E">Ece Kamar</a>, 
<a href="/search/cs?searchtype=author&query=Horvitz%2C+E">Eric Horvitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CVPR, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.01926" title="Abstract">arXiv:2012.01926</a> (replaced) [<a href="/pdf/2012.01926" title="Download PDF">pdf</a>, <a href="/format/2012.01926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COVID-19 Cough Classification using Machine Learning and Global  Smartphone Recordings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pahar%2C+M">Madhurananda Pahar</a>, 
<a href="/search/cs?searchtype=author&query=Klopper%2C+M">Marisa Klopper</a>, 
<a href="/search/cs?searchtype=author&query=Warren%2C+R">Robin Warren</a>, 
<a href="/search/cs?searchtype=author&query=Niesler%2C+T">Thomas Niesler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted in "Computers in Medicine and Biology" and currently under production
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.02724" title="Abstract">arXiv:2012.02724</a> (replaced) [<a href="/pdf/2012.02724" title="Download PDF">pdf</a>, <a href="/ps/2012.02724" title="Download PostScript">ps</a>, <a href="/format/2012.02724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Treachery of Images in the Digital Sovereignty Debate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruohonen%2C+J">Jukka Ruohonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.03408" title="Abstract">arXiv:2012.03408</a> (replaced) [<a href="/pdf/2012.03408" title="Download PDF">pdf</a>, <a href="/format/2012.03408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PMP-Net: Point Cloud Completion by Learning Multi-step Point Moving  Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+P">Peng Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhizhong Han</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan-Pei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+P">Pengfei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu-Shen Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.03461" title="Abstract">arXiv:2012.03461</a> (replaced) [<a href="/pdf/2012.03461" title="Download PDF">pdf</a>, <a href="/ps/2012.03461" title="Download PostScript">ps</a>, <a href="/format/2012.03461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Distributed and Secure Algorithm for Computing Dominant SVD Based on  Projection Splitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.05390" title="Abstract">arXiv:2012.05390</a> (replaced) [<a href="/pdf/2012.05390" title="Download PDF">pdf</a>, <a href="/format/2012.05390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble Squared: A Meta AutoML System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jason Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Joseph%2C+T">Tony Joseph</a>, 
<a href="/search/cs?searchtype=author&query=Yung%2C+D">Dylan Yung</a>, 
<a href="/search/cs?searchtype=author&query=Nasseri%2C+S+A">S. Ali Nasseri</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+F">Frank Wood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.05522" title="Abstract">arXiv:2012.05522</a> (replaced) [<a href="/pdf/2012.05522" title="Download PDF">pdf</a>, <a href="/format/2012.05522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiashun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sifei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.05766" title="Abstract">arXiv:2012.05766</a> (replaced) [<a href="/pdf/2012.05766" title="Download PDF">pdf</a>, <a href="/format/2012.05766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Argumentative Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Albini%2C+E">Emanuele Albini</a>, 
<a href="/search/cs?searchtype=author&query=Lertvittayakumjorn%2C+P">Piyawat Lertvittayakumjorn</a>, 
<a href="/search/cs?searchtype=author&query=Rago%2C+A">Antonio Rago</a>, 
<a href="/search/cs?searchtype=author&query=Toni%2C+F">Francesca Toni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.05874" title="Abstract">arXiv:2012.05874</a> (replaced) [<a href="/pdf/2012.05874" title="Download PDF">pdf</a>, <a href="/format/2012.05874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hindsight and Sequential Rationality of Correlated Play
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morrill%2C+D">Dustin Morrill</a>, 
<a href="/search/cs?searchtype=author&query=D%27Orazio%2C+R">Ryan D&#x27;Orazio</a>, 
<a href="/search/cs?searchtype=author&query=Sarfati%2C+R">Reca Sarfati</a>, 
<a href="/search/cs?searchtype=author&query=Lanctot%2C+M">Marc Lanctot</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+J+R">James R. Wright</a>, 
<a href="/search/cs?searchtype=author&query=Greenwald%2C+A">Amy Greenwald</a>, 
<a href="/search/cs?searchtype=author&query=Bowling%2C+M">Michael Bowling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report for a paper in the proceedings of the thirty-fifth AAAI Conference on Artificial Intelligence (AAAI-21), February 2-9, 2021, Virtual. 26 pages and 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.06860" title="Abstract">arXiv:2012.06860</a> (replaced) [<a href="/pdf/2012.06860" title="Download PDF">pdf</a>, <a href="/ps/2012.06860" title="Download PostScript">ps</a>, <a href="/format/2012.06860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age-Optimal Power Allocation in Industrial IoT: A Risk-Sensitive  Federated Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+Y">Yung-Lin Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chen-Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Samarakoon%2C+S">Sumudu Samarakoon</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hung-Yu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE PIMRC 2021 with 6 pages and 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.07983" title="Abstract">arXiv:2012.07983</a> (replaced) [<a href="/pdf/2012.07983" title="Download PDF">pdf</a>, <a href="/format/2012.07983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Continuous Local BDD-Based Search for Hybrid SAT Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kyrillidis%2C+A">Anastasios Kyrillidis</a>, 
<a href="/search/cs?searchtype=author&query=Vardi%2C+M+Y">Moshe Y. Vardi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiwei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 21
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Logic in Computer Science (cs.LO); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.08156" title="Abstract">arXiv:2012.08156</a> (replaced) [<a href="/pdf/2012.08156" title="Download PDF">pdf</a>, <a href="/format/2012.08156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidential Machine Learning on Untrusted Platforms: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Sagar Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Keke Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Cybersecurity Journal, Springer, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.10333" title="Abstract">arXiv:2012.10333</a> (replaced) [<a href="/pdf/2012.10333" title="Download PDF">pdf</a>, <a href="/format/2012.10333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from History for Byzantine Robust Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karimireddy%2C+S+P">Sai Praneeth Karimireddy</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lie He</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021; v2 contains stronger theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.10630" title="Abstract">arXiv:2012.10630</a> (replaced) [<a href="/pdf/2012.10630" title="Download PDF">pdf</a>, <a href="/format/2012.10630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLISTER: Generalization based Data Subset Selection for Efficient and  Robust Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Killamsetty%2C+K">Krishnateja Killamsetty</a>, 
<a href="/search/cs?searchtype=author&query=Sivasubramanian%2C+D">Durga Sivasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+R">Rishabh Iyer</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the AAAI Conference on Artificial Intelligence 35.
  9(2021): 8110-8118
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.10911" title="Abstract">arXiv:2012.10911</a> (replaced) [<a href="/pdf/2012.10911" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-adaptive Fall Detection Using Deep Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+K">Kai-Chun Liu</a>, 
<a href="/search/eess?searchtype=author&query=Can%2C+M">Michael Can</a>, 
<a href="/search/eess?searchtype=author&query=Kuo%2C+H">Heng-Cheng Kuo</a>, 
<a href="/search/eess?searchtype=author&query=Hsieh%2C+C">Chia-Yeh Hsieh</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+H">Hsiang-Yun Huang</a>, 
<a href="/search/eess?searchtype=author&query=Chan%2C+C">Chia-Tai Chan</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Neural Systems and Rehabilitation Engineering, 10 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.12779" title="Abstract">arXiv:2012.12779</a> (replaced) [<a href="/pdf/2012.12779" title="Download PDF">pdf</a>, <a href="/format/2012.12779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal and Low-Memory Near-Optimal Preconditioning of Fully Implicit  Runge-Kutta Schemes for Parabolic PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jiao%2C+X">Xiangmin Jiao</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xuebin Wang</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+Q">Qiao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.12854" title="Abstract">arXiv:2012.12854</a> (replaced) [<a href="/pdf/2012.12854" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep manifold learning reveals hidden dynamics of proteasome  autoregulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+Z">Zhaolong Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+S">Shuwen Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+W+L">Wei Li Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Ma%2C+Y">Yinping Ma</a>, 
<a href="/search/q-bio?searchtype=author&query=Dong%2C+Y">Yuanchen Dong</a>, 
<a href="/search/q-bio?searchtype=author&query=Mao%2C+Y">Youdong Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 81 pages, 16 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Statistical Mechanics (cond-mat.stat-mech); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.13052" title="Abstract">arXiv:2012.13052</a> (replaced) [<a href="/pdf/2012.13052" title="Download PDF">pdf</a>, <a href="/format/2012.13052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Crowds by Modeling Common Confusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhendong Chu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.13658" title="Abstract">arXiv:2012.13658</a> (replaced) [<a href="/pdf/2012.13658" title="Download PDF">pdf</a>, <a href="/format/2012.13658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Persistent Exploration in Continuous Control Tasks with Sparse  Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amin%2C+S">Susan Amin</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Gomrokchi%2C+M">Maziar Gomrokchi</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Aboutalebi%2C+H">Hossein Aboutalebi</a> (3), 
<a href="/search/cs?searchtype=author&query=Satija%2C+H">Harsh Satija</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a> (1 and 2) ((1) McGill University, (2) Mila- Quebec Artificial Intelligence Institute, (3) University of Waterloo)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in ICML, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.00148" title="Abstract">arXiv:2101.00148</a> (replaced) [<a href="/pdf/2101.00148" title="Download PDF">pdf</a>, <a href="/format/2101.00148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bilingual Lexicon Induction via Unsupervised Bitext Construction and  Word Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haoyue Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S+I">Sida I. Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL-IJCNLP 2021 camera-ready version, with full supplementary material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.00151" title="Abstract">arXiv:2101.00151</a> (replaced) [<a href="/pdf/2101.00151" title="Download PDF">pdf</a>, <a href="/format/2101.00151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded  Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hung Le</a>, 
<a href="/search/cs?searchtype=author&query=Sankar%2C+C">Chinnadhurai Sankar</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Seungwhan Moon</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>, 
<a href="/search/cs?searchtype=author&query=Geramifard%2C+A">Alborz Geramifard</a>, 
<a href="/search/cs?searchtype=author&query=Kottur%2C+S">Satwik Kottur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 14 figures, 8 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Association for Computational Linguistics (2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.02523" title="Abstract">arXiv:2101.02523</a> (replaced) [<a href="/pdf/2101.02523" title="Download PDF">pdf</a>, <a href="/format/2101.02523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Learning with Class Imbalance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ochal%2C+M">Mateusz Ochal</a>, 
<a href="/search/cs?searchtype=author&query=Patacchiola%2C+M">Massimiliano Patacchiola</a>, 
<a href="/search/cs?searchtype=author&query=Storkey%2C+A">Amos Storkey</a>, 
<a href="/search/cs?searchtype=author&query=Vazquez%2C+J">Jose Vazquez</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.03419" title="Abstract">arXiv:2101.03419</a> (replaced) [<a href="/pdf/2101.03419" title="Download PDF">pdf</a>, <a href="/format/2101.03419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Deep Architectures Without End-to-End Backpropagation: A Brief  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+S">Shiyu Duan</a>, 
<a href="/search/cs?searchtype=author&query=Principe%2C+J+C">Jose C. Principe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.04097" title="Abstract">arXiv:2101.04097</a> (replaced) [<a href="/pdf/2101.04097" title="Download PDF">pdf</a>, <a href="/format/2101.04097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlated Weights in Infinite Limits of Deep Convolutional Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Garriga-Alonso%2C+A">Adri&#xe0; Garriga-Alonso</a>, 
<a href="/search/stat?searchtype=author&query=van+der+Wilk%2C+M">Mark van der Wilk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the 37th Conference on Uncertainty in Artificial Intelligence (UAI 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.04230" title="Abstract">arXiv:2101.04230</a> (replaced) [<a href="/pdf/2101.04230" title="Download PDF">pdf</a>, <a href="/format/2101.04230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining the Black-box Smoothly- A Counterfactual Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singla%2C+S">Sumedha Singla</a>, 
<a href="/search/cs?searchtype=author&query=Pollack%2C+B">Brian Pollack</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+S">Stephen Wallace</a>, 
<a href="/search/cs?searchtype=author&query=Batmanghelich%2C+K">Kayhan Batmanghelich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review for IEEE-TMI journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.05068" title="Abstract">arXiv:2101.05068</a> (replaced) [<a href="/pdf/2101.05068" title="Download PDF">pdf</a>, <a href="/format/2101.05068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Embeddings for Cross-Modal Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chun%2C+S">Sanghyuk Chun</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S+J">Seong Joon Oh</a>, 
<a href="/search/cs?searchtype=author&query=de+Rezende%2C+R+S">Rafael Sampaio de Rezende</a>, 
<a href="/search/cs?searchtype=author&query=Kalantidis%2C+Y">Yannis Kalantidis</a>, 
<a href="/search/cs?searchtype=author&query=Larlus%2C+D">Diane Larlus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2021; Code is available at <a href="https://github.com/naver-ai/pcme">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.06561" title="Abstract">arXiv:2101.06561</a> (replaced) [<a href="/pdf/2101.06561" title="Download PDF">pdf</a>, <a href="/format/2101.06561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khashabi%2C+D">Daniel Khashabi</a>, 
<a href="/search/cs?searchtype=author&query=Stanovsky%2C+G">Gabriel Stanovsky</a>, 
<a href="/search/cs?searchtype=author&query=Bragg%2C+J">Jonathan Bragg</a>, 
<a href="/search/cs?searchtype=author&query=Lourie%2C+N">Nicholas Lourie</a>, 
<a href="/search/cs?searchtype=author&query=Kasai%2C+J">Jungo Kasai</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Weld%2C+D+S">Daniel S. Weld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.06969" title="Abstract">arXiv:2101.06969</a> (replaced) [<a href="/pdf/2101.06969" title="Download PDF">pdf</a>, <a href="/format/2101.06969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Red Alarm for Pre-trained Models: Universal Vulnerability to  Neuron-Level Backdoor Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+G">Guangxuan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+T">Tian Lv</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+F">Fanchao Qi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.06983" title="Abstract">arXiv:2101.06983</a> (replaced) [<a href="/pdf/2101.06983" title="Download PDF">pdf</a>, <a href="/format/2101.06983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Luyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>, 
<a href="/search/cs?searchtype=author&query=Callan%2C+J">Jamie Callan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> RepL4NLP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.07393" title="Abstract">arXiv:2101.07393</a> (replaced) [<a href="/pdf/2101.07393" title="Download PDF">pdf</a>, <a href="/format/2101.07393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grounding Language to Entities and Dynamics for Generalization in  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanjie%2C+A+W">Austin W. Hanjie</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+V">Victor Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+K">Karthik Narasimhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2021. Note author list and name changes from previous version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.08143" title="Abstract">arXiv:2101.08143</a> (replaced) [<a href="/pdf/2101.08143" title="Download PDF">pdf</a>, <a href="/format/2101.08143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Evaluation for Relevant Quantities of Opinion Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wanyue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+Q">Qi Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongzhi Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of The Web Conference 2021, pp.2037-2045
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.08477" title="Abstract">arXiv:2101.08477</a> (replaced) [<a href="/pdf/2101.08477" title="Download PDF">pdf</a>, <a href="/format/2101.08477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Cardiovascular Modelling with Deep Reinforcement Learning for  Uncertainty Aware Control of Sepsis Treatment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nanayakkara%2C+T">Thesath Nanayakkara</a>, 
<a href="/search/cs?searchtype=author&query=Clermont%2C+G">Gilles Clermont</a>, 
<a href="/search/cs?searchtype=author&query=Langmead%2C+C+J">Christopher James Langmead</a>, 
<a href="/search/cs?searchtype=author&query=Swigon%2C+D">David Swigon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.08524" title="Abstract">arXiv:2101.08524</a> (replaced) [<a href="/pdf/2101.08524" title="Download PDF">pdf</a>, <a href="/format/2101.08524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Robust Certifiable Estimation of the Relative Pose Between Two  Calibrated Cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia-Salguero%2C+M">Mercedes Garcia-Salguero</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez-Jimenez%2C+J">Javier Gonzalez-Jimenez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.09678" title="Abstract">arXiv:2101.09678</a> (replaced) [<a href="/pdf/2101.09678" title="Download PDF">pdf</a>, <a href="/format/2101.09678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A symmetric fractional-order reduction method for direct nonuniform  approximations of semilinear diffusion-wave equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lyu%2C+P">Pin Lyu</a>, 
<a href="/search/math?searchtype=author&query=Vong%2C+S">Seakweng Vong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.10025" title="Abstract">arXiv:2101.10025</a> (replaced) [<a href="/pdf/2101.10025" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Graph Neural Networks and Their Applications in Power  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Wenlong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Bak-Jensen%2C+B">Birgitte Bak-Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+J+R">Jayakrishnan Radhakrishna Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuelong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yusen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.10040" title="Abstract">arXiv:2101.10040</a> (replaced) [<a href="/pdf/2101.10040" title="Download PDF">pdf</a>, <a href="/format/2101.10040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity of Linear Minimization and Projection on Some Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Combettes%2C+C+W">Cyrille W. Combettes</a>, 
<a href="/search/math?searchtype=author&query=Pokutta%2C+S">Sebastian Pokutta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.10902" title="Abstract">arXiv:2101.10902</a> (replaced) [<a href="/pdf/2101.10902" title="Download PDF">pdf</a>, <a href="/format/2101.10902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum n-times Coverage for Vaccine Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+G">Ge Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Dimitrakakis%2C+A">Alexander Dimitrakakis</a>, 
<a href="/search/q-bio?searchtype=author&query=Carter%2C+B">Brandon Carter</a>, 
<a href="/search/q-bio?searchtype=author&query=Gifford%2C+D">David Gifford</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.11139" title="Abstract">arXiv:2101.11139</a> (replaced) [<a href="/pdf/2101.11139" title="Download PDF">pdf</a>, <a href="/format/2101.11139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Strengthened Cutset Upper Bound on the Capacity of the Relay Channel  and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gamal%2C+A+E">Abbas El Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Gohari%2C+A">Amin Gohari</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+C">Chandra Nair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.11783" title="Abstract">arXiv:2101.11783</a> (replaced) [<a href="/pdf/2101.11783" title="Download PDF">pdf</a>, <a href="/ps/2101.11783" title="Download PostScript">ps</a>, <a href="/format/2101.11783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Graph Matching with Improved Noise Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Cheng Mao</a>, 
<a href="/search/cs?searchtype=author&query=Rudelson%2C+M">Mark Rudelson</a>, 
<a href="/search/cs?searchtype=author&query=Tikhomirov%2C+K">Konstantin Tikhomirov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages. Accepted for presentation at Conference on Learning Theory (COLT) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Probability (math.PR); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.12242" title="Abstract">arXiv:2101.12242</a> (replaced) [<a href="/pdf/2101.12242" title="Download PDF">pdf</a>, <a href="/ps/2101.12242" title="Download PostScript">ps</a>, <a href="/format/2101.12242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D3DLO: Deep 3D LiDAR Odometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adis%2C+P">Philipp Adis</a>, 
<a href="/search/cs?searchtype=author&query=Horst%2C+N">Nicolas Horst</a>, 
<a href="/search/cs?searchtype=author&query=Wien%2C+M">Mathias Wien</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, accepted at IEEE ICIP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.12475" title="Abstract">arXiv:2101.12475</a> (replaced) [<a href="/pdf/2101.12475" title="Download PDF">pdf</a>, <a href="/format/2101.12475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on 6G Networks:Applications, Core Services,  Enabling Technologies, and Future Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahraki%2C+A">Amin Shahraki</a>, 
<a href="/search/cs?searchtype=author&query=Abbasi%2C+M">Mahmoud Abbasi</a>, 
<a href="/search/cs?searchtype=author&query=Piran%2C+M+J">Md. Jalil Piran</a>, 
<a href="/search/cs?searchtype=author&query=Taherkordi%2C+A">Amir Taherkordi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.00087" title="Abstract">arXiv:2102.00087</a> (replaced) [<a href="/pdf/2102.00087" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Work Zones and Connected Automated Vehicles Ready for a Harmonious  Coexistence? A Scoping Review and Research Agenda
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dehman%2C+A">Amjad Dehman</a>, 
<a href="/search/cs?searchtype=author&query=Farooq%2C+B">Bilal Farooq</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.01391" title="Abstract">arXiv:2102.01391</a> (replaced) [<a href="/pdf/2102.01391" title="Download PDF">pdf</a>, <a href="/format/2102.01391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Neural Networks for Virtual Flow Metering: An Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grimstad%2C+B">Bjarne Grimstad</a>, 
<a href="/search/cs?searchtype=author&query=Hotvedt%2C+M">Mathilde Hotvedt</a>, 
<a href="/search/cs?searchtype=author&query=Sandnes%2C+A+T">Anders T. Sandnes</a>, 
<a href="/search/cs?searchtype=author&query=Kolbj%C3%B8rnsen%2C+O">Odd Kolbj&#xf8;rnsen</a>, 
<a href="/search/cs?searchtype=author&query=Imsland%2C+L+S">Lars S. Imsland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.01956" title="Abstract">arXiv:2102.01956</a> (replaced) [<a href="/pdf/2102.01956" title="Download PDF">pdf</a>, <a href="/format/2102.01956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Series Classification via Topological Data Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Karan%2C+A">Alperen Karan</a>, 
<a href="/search/stat?searchtype=author&query=Kaygun%2C+A">Atabey Kaygun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.02414" title="Abstract">arXiv:2102.02414</a> (replaced) [<a href="/pdf/2102.02414" title="Download PDF">pdf</a>, <a href="/format/2102.02414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Noise Transition Matrix from Only Noisy Labels via Total  Variation Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Y">Yivan Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Niu%2C+G">Gang Niu</a>, 
<a href="/search/stat?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021 camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.02959" title="Abstract">arXiv:2102.02959</a> (replaced) [<a href="/pdf/2102.02959" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text  Reports Using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Anniballe%2C+V+M">Vincent M. D&#x27;Anniballe</a>, 
<a href="/search/cs?searchtype=author&query=Tushar%2C+F+I">Fakrul I. Tushar</a>, 
<a href="/search/cs?searchtype=author&query=Faryna%2C+K">Khrystyna Faryna</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Songyue Han</a>, 
<a href="/search/cs?searchtype=author&query=Mazurowski%2C+M+A">Maciej A. Mazurowski</a>, 
<a href="/search/cs?searchtype=author&query=Rubin%2C+G+D">Geoffrey D. Rubin</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+J+Y">Joseph Y. Lo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.03129" title="Abstract">arXiv:2102.03129</a> (replaced) [<a href="/pdf/2102.03129" title="Download PDF">pdf</a>, <a href="/format/2102.03129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integer Programming for Causal Structure Learning in the Presence of  Latent Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dash%2C+S">Sanjeeb Dash</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tian Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.03198" title="Abstract">arXiv:2102.03198</a> (replaced) [<a href="/pdf/2102.03198" title="Download PDF">pdf</a>, <a href="/format/2102.03198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bias-Variance Reduced Local SGD for Less Heterogeneous Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murata%2C+T">Tomoya Murata</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Taiji Suzuki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.03360" title="Abstract">arXiv:2102.03360</a> (replaced) [<a href="/pdf/2102.03360" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scenario Generation for Cooling, Heating, and Power Loads Using  Generative Moment Matching Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liao%2C+W">Wenlong Liao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yusen Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yuelong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Powell%2C+K">Kody Powell</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Z">Zhe Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by CSEE Journal of Power and Energy Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.03716" title="Abstract">arXiv:2102.03716</a> (replaced) [<a href="/pdf/2102.03716" title="Download PDF">pdf</a>, <a href="/format/2102.03716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPADE: A Spectral Method for Black-Box Adversarial Robustness Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wuxinlin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chenhui Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhiqiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yaohui Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhuo Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 2021 International Conference on Machine Learning (ICML)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.03895" title="Abstract">arXiv:2102.03895</a> (replaced) [<a href="/pdf/2102.03895" title="Download PDF">pdf</a>, <a href="/format/2102.03895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional optimal transport: map estimation and domain adaptation for  functional data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhu%2C+J">Jiacheng Zhu</a>, 
<a href="/search/stat?searchtype=author&query=Guha%2C+A">Aritra Guha</a>, 
<a href="/search/stat?searchtype=author&query=Do%2C+D">Dat Do</a>, 
<a href="/search/stat?searchtype=author&query=Xu%2C+M">Mengdi Xu</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+X">XuanLong Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.04216" title="Abstract">arXiv:2102.04216</a> (replaced) [<a href="/pdf/2102.04216" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social and behavioral determinants of health in the era of artificial  intelligence with electronic health records: A scoping review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bompelli%2C+A">Anusha Bompelli</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanshan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+R">Ruyuan Wan</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+E">Esha Singh</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuqi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Oniani%2C+D">David Oniani</a>, 
<a href="/search/cs?searchtype=author&query=Kshatriya%2C+B+S+A">Bhavani Singh Agnikula Kshatriya</a>, 
<a href="/search/cs?searchtype=author&query=Joyce">Joyce</a> (Joy)
<a href="/search/cs?searchtype=author&query=Balls-Berry%2C+E">E. Balls-Berry</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.05140" title="Abstract">arXiv:2102.05140</a> (replaced) [<a href="/pdf/2102.05140" title="Download PDF">pdf</a>, <a href="/format/2102.05140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Adaptive Label Smoothing for Predictive Churn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bahri%2C+D">Dara Bahri</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Heinrich Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.05185" title="Abstract">arXiv:2102.05185</a> (replaced) [<a href="/pdf/2102.05185" title="Download PDF">pdf</a>, <a href="/format/2102.05185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ross%2C+A+S">Andrew Slavin Ross</a>, 
<a href="/search/cs?searchtype=author&query=Doshi-Velez%2C+F">Finale Doshi-Velez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021 accepted paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.05284" title="Abstract">arXiv:2102.05284</a> (replaced) [<a href="/pdf/2102.05284" title="Download PDF">pdf</a>, <a href="/ps/2102.05284" title="Download PostScript">ps</a>, <a href="/format/2102.05284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding the Stochastic Shortest Path with Low Regret: The Adversarial  Cost and Unknown Transition Case
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haipeng Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.05363" title="Abstract">arXiv:2102.05363</a> (replaced) [<a href="/pdf/2102.05363" title="Download PDF">pdf</a>, <a href="/format/2102.05363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Certifying L-infinity Robustness using Neural Networks with  L-inf-dist Neurons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bohang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Tianle Cai</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhou Lu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Di He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liwei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appearing at International Conference on Machine Learning (ICML) 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.05858" title="Abstract">arXiv:2102.05858</a> (replaced) [<a href="/pdf/2102.05858" title="Download PDF">pdf</a>, <a href="/ps/2102.05858" title="Download PostScript">ps</a>, <a href="/format/2102.05858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Near Instance-Optimality and Minimax-Optimality in Stochastic  and Adversarial Linear Bandits Simultaneously
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chung-Wei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haipeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chen-Yu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaojin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.05912" title="Abstract">arXiv:2102.05912</a> (replaced) [<a href="/pdf/2102.05912" title="Download PDF">pdf</a>, <a href="/format/2102.05912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BoMb-OT: On Batch of Mini-batches Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+K">Khai Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+Q">Quoc Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>, 
<a href="/search/stat?searchtype=author&query=Pham%2C+T">Tung Pham</a>, 
<a href="/search/stat?searchtype=author&query=Bui%2C+H">Hung Bui</a>, 
<a href="/search/stat?searchtype=author&query=Phung%2C+D">Dinh Phung</a>, 
<a href="/search/stat?searchtype=author&query=Le%2C+T">Trung Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.06192" title="Abstract">arXiv:2102.06192</a> (replaced) [<a href="/pdf/2102.06192" title="Download PDF">pdf</a>, <a href="/format/2102.06192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Segmentation Loss for Sketch Colorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hicsonmez%2C+S">Samet Hicsonmez</a>, 
<a href="/search/cs?searchtype=author&query=Samet%2C+N">Nermin Samet</a>, 
<a href="/search/cs?searchtype=author&query=Akbas%2C+E">Emre Akbas</a>, 
<a href="/search/cs?searchtype=author&query=Duygulu%2C+P">Pinar Duygulu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICIP 2021 camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.06752" title="Abstract">arXiv:2102.06752</a> (replaced) [<a href="/pdf/2102.06752" title="Download PDF">pdf</a>, <a href="/format/2102.06752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Variance-Reduced Method for Decentralized Stochastic Non-Convex  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xin%2C+R">Ran Xin</a>, 
<a href="/search/math?searchtype=author&query=Khan%2C+U+A">Usman A. Khan</a>, 
<a href="/search/math?searchtype=author&query=Kar%2C+S">Soummya Kar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.06810" title="Abstract">arXiv:2102.06810</a> (replaced) [<a href="/pdf/2102.06810" title="Download PDF">pdf</a>, <a href="/format/2102.06810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding self-supervised Learning Dynamics without Contrastive  Pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinlei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ganguli%2C+S">Surya Ganguli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.06828" title="Abstract">arXiv:2102.06828</a> (replaced) [<a href="/pdf/2102.06828" title="Download PDF">pdf</a>, <a href="/format/2102.06828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Adaptation for Time Series Forecasting via Attention Sharing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaoyong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Youngsuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Maddix%2C+D+C">Danielle C. Maddix</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xifeng Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.06961" title="Abstract">arXiv:2102.06961</a> (replaced) [<a href="/pdf/2102.06961" title="Download PDF">pdf</a>, <a href="/format/2102.06961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PerSim: Data-Efficient Offline Reinforcement Learning with Heterogeneous  Agents via Personalized Simulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Anish Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Alomar%2C+A">Abdullah Alomar</a>, 
<a href="/search/cs?searchtype=author&query=Alumootil%2C+V">Varkey Alumootil</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Devavrat Shah</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+D">Dennis Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cindy Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.06973" title="Abstract">arXiv:2102.06973</a> (replaced) [<a href="/pdf/2102.06973" title="Download PDF">pdf</a>, <a href="/format/2102.06973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Deviation Types and Learning for Hindsight Rationality in  Extensive-Form Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morrill%2C+D">Dustin Morrill</a>, 
<a href="/search/cs?searchtype=author&query=D%27Orazio%2C+R">Ryan D&#x27;Orazio</a>, 
<a href="/search/cs?searchtype=author&query=Lanctot%2C+M">Marc Lanctot</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+J+R">James R. Wright</a>, 
<a href="/search/cs?searchtype=author&query=Bowling%2C+M">Michael Bowling</a>, 
<a href="/search/cs?searchtype=author&query=Greenwald%2C+A">Amy Greenwald</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report for a paper in the proceedings of the thirty-eighth International Conference on Machine Learning (ICML 2021), virtual. 39 pages and 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.07060" title="Abstract">arXiv:2102.07060</a> (replaced) [<a href="/pdf/2102.07060" title="Download PDF">pdf</a>, <a href="/format/2102.07060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Efficiency in Black Box Simulation of Distribution Tails with  Self-structuring Importance Samplers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Deo%2C+A">Anand Deo</a>, 
<a href="/search/stat?searchtype=author&query=Murthy%2C+K">Karthyek Murthy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.07074" title="Abstract">arXiv:2102.07074</a> (replaced) [<a href="/pdf/2102.07074" title="Download PDF">pdf</a>, <a href="/format/2102.07074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can  Scale Up
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yifan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shiyu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.07108" title="Abstract">arXiv:2102.07108</a> (replaced) [<a href="/pdf/2102.07108" title="Download PDF">pdf</a>, <a href="/format/2102.07108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CATE: Computation-aware Neural Architecture Encoding with Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaiqiang Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021 camera-ready. Code: <a href="https://github.com/MSU-MLSys-Lab/CATE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.07238" title="Abstract">arXiv:2102.07238</a> (replaced) [<a href="/pdf/2102.07238" title="Download PDF">pdf</a>, <a href="/format/2102.07238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Double-descent curves in neural networks: a new perspective using  Gaussian processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Harzli%2C+O+E">Ouns El Harzli</a>, 
<a href="/search/stat?searchtype=author&query=Valle-P%C3%A9rez%2C+G">Guillermo Valle-P&#xe9;rez</a>, 
<a href="/search/stat?searchtype=author&query=Louis%2C+A+A">Ard A. Louis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.07475" title="Abstract">arXiv:2102.07475</a> (replaced) [<a href="/pdf/2102.07475" title="Download PDF">pdf</a>, <a href="/format/2102.07475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Multi-Agent Reinforcement Learning with Selective Parameter  Sharing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christianos%2C+F">Filippos Christianos</a>, 
<a href="/search/cs?searchtype=author&query=Papoudakis%2C+G">Georgios Papoudakis</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+A">Arrasy Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.07758" title="Abstract">arXiv:2102.07758</a> (replaced) [<a href="/pdf/2102.07758" title="Download PDF">pdf</a>, <a href="/format/2102.07758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Distributed Optimization for Saddle Point Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rogozin%2C+A">Alexander Rogozin</a>, 
<a href="/search/math?searchtype=author&query=Beznosikov%2C+A">Aleksandr Beznosikov</a>, 
<a href="/search/math?searchtype=author&query=Dvinskikh%2C+D">Darina Dvinskikh</a>, 
<a href="/search/math?searchtype=author&query=Kovalev%2C+D">Dmitry Kovalev</a>, 
<a href="/search/math?searchtype=author&query=Dvurechensky%2C+P">Pavel Dvurechensky</a>, 
<a href="/search/math?searchtype=author&query=Gasnikov%2C+A">Alexander Gasnikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.07850" title="Abstract">arXiv:2102.07850</a> (replaced) [<a href="/pdf/2102.07850" title="Download PDF">pdf</a>, <a href="/format/2102.07850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Particle Filtering via Entropy-Regularized Optimal  Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Corenflos%2C+A">Adrien Corenflos</a>, 
<a href="/search/stat?searchtype=author&query=Thornton%2C+J">James Thornton</a>, 
<a href="/search/stat?searchtype=author&query=Doucet%2C+A">Arnaud Doucet</a>, 
<a href="/search/stat?searchtype=author&query=Deligiannidis%2C+G">George Deligiannidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages of content + 11 pages supplementary, accepted for presentation at ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.07923" title="Abstract">arXiv:2102.07923</a> (replaced) [<a href="/pdf/2102.07923" title="Download PDF">pdf</a>, <a href="/format/2102.07923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Darboux-Frame-Based Parametrization for a Spin-Rolling Sphere on a  Plane: A Nonlinear Transformation of Underactuated System to Fully-Actuated  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tafrishi%2C+S+A">Seyed Amir Tafrishi</a>, 
<a href="/search/cs?searchtype=author&query=Svinin%2C+M">Mikhail Svinin</a>, 
<a href="/search/cs?searchtype=author&query=Yamamoto%2C+M">Motoji Yamamoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures, Accepted at Mechanism and Machine Theory Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Differential Geometry (math.DG)

</div>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.08329" title="Abstract">arXiv:2102.08329</a> (replaced) [<a href="/pdf/2102.08329" title="Download PDF">pdf</a>, <a href="/format/2102.08329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rate-Distortion Theoretic Model Compression: Successive Refinement for  Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Isik%2C+B">Berivan Isik</a>, 
<a href="/search/cs?searchtype=author&query=No%2C+A">Albert No</a>, 
<a href="/search/cs?searchtype=author&query=Weissman%2C+T">Tsachy Weissman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Title changed. Previous title: Successive pruning for model compression via rate distortion theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.08452" title="Abstract">arXiv:2102.08452</a> (replaced) [<a href="/pdf/2102.08452" title="Download PDF">pdf</a>, <a href="/format/2102.08452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Globally-Robust Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leino%2C+K">Klas Leino</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fredrikson%2C+M">Matt Fredrikson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appearing in ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.08554" title="Abstract">arXiv:2102.08554</a> (replaced) [<a href="/pdf/2102.08554" title="Download PDF">pdf</a>, <a href="/format/2102.08554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recoverability Landscape of Tree Structured Markov Random Fields under  Symmetric Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Katiyar%2C+A">Ashish Katiyar</a>, 
<a href="/search/stat?searchtype=author&query=Basu%2C+S">Soumya Basu</a>, 
<a href="/search/stat?searchtype=author&query=Shah%2C+V">Vatsal Shah</a>, 
<a href="/search/stat?searchtype=author&query=Caramanis%2C+C">Constantine Caramanis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.08622" title="Abstract">arXiv:2102.08622</a> (replaced) [<a href="/pdf/2102.08622" title="Download PDF">pdf</a>, <a href="/format/2102.08622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sinkhorn Label Allocation: Semi-Supervised Classification via Annealed  Self-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tai%2C+K+S">Kai Sheng Tai</a>, 
<a href="/search/cs?searchtype=author&query=Bailis%2C+P">Peter Bailis</a>, 
<a href="/search/cs?searchtype=author&query=Valiant%2C+G">Gregory Valiant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021 camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.09626" title="Abstract">arXiv:2102.09626</a> (replaced) [<a href="/pdf/2102.09626" title="Download PDF">pdf</a>, <a href="/format/2102.09626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Unified Framework for High Dimensional Bandit Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Barik%2C+A">Adarsh Barik</a>, 
<a href="/search/cs?searchtype=author&query=Honorio%2C+J">Jean Honorio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.09695" title="Abstract">arXiv:2102.09695</a> (replaced) [<a href="/pdf/2102.09695" title="Download PDF">pdf</a>, <a href="/format/2102.09695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fortify Machine Learning Production Systems: Detect and Classify  Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ciolino%2C+M">Matthew Ciolino</a>, 
<a href="/search/cs?searchtype=author&query=Kalin%2C+J">Josh Kalin</a>, 
<a href="/search/cs?searchtype=author&query=Noever%2C+D">David Noever</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 Pages, 5 Figures, 5 Tables, 17 References, ICMLA 2021, IEEE Conference Format
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.09768" title="Abstract">arXiv:2102.09768</a> (replaced) [<a href="/pdf/2102.09768" title="Download PDF">pdf</a>, <a href="/format/2102.09768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Generating Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Honghua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Juba%2C+B">Brendan Juba</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Broeck%2C+G">Guy Van den Broeck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.10255" title="Abstract">arXiv:2102.10255</a> (replaced) [<a href="/pdf/2102.10255" title="Download PDF">pdf</a>, <a href="/format/2102.10255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Link Prediction with Persistent Homology: An Interactive View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zuoyu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tengfei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Liangcai Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICML2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.10271" title="Abstract">arXiv:2102.10271</a> (replaced) [<a href="/pdf/2102.10271" title="Download PDF">pdf</a>, <a href="/format/2102.10271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning Dynamics Forecasting Using Task Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.10884" title="Abstract">arXiv:2102.10884</a> (replaced) [<a href="/pdf/2102.10884" title="Download PDF">pdf</a>, <a href="/format/2102.10884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Classification Perspective on Scene Text Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Hongxiang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yichao Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.11005" title="Abstract">arXiv:2102.11005</a> (replaced) [<a href="/pdf/2102.11005" title="Download PDF">pdf</a>, <a href="/format/2102.11005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LogME: Practical Assessment of Pre-trained Models for Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+K">Kaichao You</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages (ICML 2021 camera ready version)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.11592" title="Abstract">arXiv:2102.11592</a> (replaced) [<a href="/pdf/2102.11592" title="Download PDF">pdf</a>, <a href="/format/2102.11592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Classification in the Dark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghalme%2C+G">Ganesh Ghalme</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+V">Vineet Nair</a>, 
<a href="/search/cs?searchtype=author&query=Eilat%2C+I">Itay Eilat</a>, 
<a href="/search/cs?searchtype=author&query=Talgam-Cohen%2C+I">Inbal Talgam-Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Rosenfeld%2C+N">Nir Rosenfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.12013" title="Abstract">arXiv:2102.12013</a> (replaced) [<a href="/pdf/2102.12013" title="Download PDF">pdf</a>, <a href="/format/2102.12013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Mitigating Accuracy Disparity in Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+J">Jianfeng Chi</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Gordon%2C+G+J">Geoffrey J. Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.12668" title="Abstract">arXiv:2102.12668</a> (replaced) [<a href="/pdf/2102.12668" title="Download PDF">pdf</a>, <a href="/format/2102.12668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-based Robust Motion Planning with Guaranteed Stability: A  Contraction Theory Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsukamoto%2C+H">Hiroyasu Tsukamoto</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+S">Soon-Jo Chung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Robotics and Automation Letters (RA-L), Accepted June 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.00123" title="Abstract">arXiv:2103.00123</a> (replaced) [<a href="/pdf/2103.00123" title="Download PDF">pdf</a>, <a href="/format/2103.00123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient  Deep Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Killamsetty%2C+K">Krishnateja Killamsetty</a>, 
<a href="/search/cs?searchtype=author&query=Sivasubramanian%2C+D">Durga Sivasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=De%2C+A">Abir De</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+R">Rishabh Iyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.00164" title="Abstract">arXiv:2103.00164</a> (replaced) [<a href="/pdf/2103.00164" title="Download PDF">pdf</a>, <a href="/format/2103.00164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FeatureNorm: L2 Feature Normalization for Dynamic Graph Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Menglin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Ziqiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+I">Irwin King</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICDM 2020;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.01338" title="Abstract">arXiv:2103.01338</a> (replaced) [<a href="/pdf/2103.01338" title="Download PDF">pdf</a>, <a href="/format/2103.01338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acceleration via Fractal Learning Rate Schedules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+N">Naman Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+S">Surbhi Goel</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cyril Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: revisions for ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.01458" title="Abstract">arXiv:2103.01458</a> (replaced) [<a href="/pdf/2103.01458" title="Download PDF">pdf</a>, <a href="/format/2103.01458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Probabilistic Models for 3D Point Cloud Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shitong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wei Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.01638" title="Abstract">arXiv:2103.01638</a> (replaced) [<a href="/pdf/2103.01638" title="Download PDF">pdf</a>, <a href="/format/2103.01638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning disentangled representations via product manifold projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fumero%2C+M">Marco Fumero</a>, 
<a href="/search/cs?searchtype=author&query=Cosmo%2C+L">Luca Cosmo</a>, 
<a href="/search/cs?searchtype=author&query=Melzi%2C+S">Simone Melzi</a>, 
<a href="/search/cs?searchtype=author&query=Rodol%C3%A0%2C+E">Emanuele Rodol&#xe0;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 38th International Conference on Machine
  Learning, PMLR 139, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.01826" title="Abstract">arXiv:2103.01826</a> (replaced) [<a href="/pdf/2103.01826" title="Download PDF">pdf</a>, <a href="/format/2103.01826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Classification Made Practical
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levanon%2C+S">Sagi Levanon</a>, 
<a href="/search/cs?searchtype=author&query=Rosenfeld%2C+N">Nir Rosenfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.02475" title="Abstract">arXiv:2103.02475</a> (replaced) [<a href="/pdf/2103.02475" title="Download PDF">pdf</a>, <a href="/format/2103.02475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Blockingness Verification of Bounded Petri Nets Using Basis  Reachability Graphs -- An Extended Version With Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gu%2C+C">Chao Gu</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+Z">Ziyue Ma</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhiwu Li</a>, 
<a href="/search/eess?searchtype=author&query=Giua%2C+A">Alessandro Giua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article is an extended version of the paper "C. Gu, Z. Ma, Z. Li and A. Giua. Non-blockingness verification of bounded Petri nets using basis reachability graphs. IEEE Control Systems Letters, doi:10.1109/LCSYS.2021.3087937, 2021" with benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.02700" title="Abstract">arXiv:2103.02700</a> (replaced) [<a href="/pdf/2103.02700" title="Download PDF">pdf</a>, <a href="/ps/2103.02700" title="Download PostScript">ps</a>, <a href="/format/2103.02700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding supercodes of Gabidulin codes and applications to cryptanalysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bombar%2C+M">Maxime Bombar</a>, 
<a href="/search/cs?searchtype=author&query=Couvreur%2C+A">Alain Couvreur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PQCrypto 2021. The Sage code is available on Github: <a href="https://github.com/mbombar/Attack_on_LIGA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.03212" title="Abstract">arXiv:2103.03212</a> (replaced) [<a href="/pdf/2103.03212" title="Download PDF">pdf</a>, <a href="/format/2103.03212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weisfeiler and Lehman Go Topological: Message Passing Simplicial  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bodnar%2C+C">Cristian Bodnar</a>, 
<a href="/search/cs?searchtype=author&query=Frasca%2C+F">Fabrizio Frasca</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y+G">Yu Guang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Otter%2C+N">Nina Otter</a>, 
<a href="/search/cs?searchtype=author&query=Mont%C3%BAfar%2C+G">Guido Mont&#xfa;far</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021. Contains 27 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.03216" title="Abstract">arXiv:2103.03216</a> (replaced) [<a href="/pdf/2103.03216" title="Download PDF">pdf</a>, <a href="/format/2103.03216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Coordination As a Realistic Scenario for Lifelong Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nekoei%2C+H">Hadi Nekoei</a>, 
<a href="/search/cs?searchtype=author&query=Badrinaaraayanan%2C+A">Akilesh Badrinaaraayanan</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+A">Aaron Courville</a>, 
<a href="/search/cs?searchtype=author&query=Chandar%2C+S">Sarath Chandar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages with supplementary materials. Added results for Lifelong RL methods and some future work. Accepted to ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.03230" title="Abstract">arXiv:2103.03230</a> (replaced) [<a href="/pdf/2103.03230" title="Download PDF">pdf</a>, <a href="/format/2103.03230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Barlow Twins: Self-Supervised Learning via Redundancy Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zbontar%2C+J">Jure Zbontar</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+L">Li Jing</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+I">Ishan Misra</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>, 
<a href="/search/cs?searchtype=author&query=Deny%2C+S">St&#xe9;phane Deny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures, to appear at ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.03323" title="Abstract">arXiv:2103.03323</a> (replaced) [<a href="/pdf/2103.03323" title="Download PDF">pdf</a>, <a href="/format/2103.03323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-free uncertainty quantification for classification under  label shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Podkopaev%2C+A">Aleksandr Podkopaev</a>, 
<a href="/search/stat?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.04850" title="Abstract">arXiv:2103.04850</a> (replaced) [<a href="/pdf/2103.04850" title="Download PDF">pdf</a>, <a href="/format/2103.04850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Ignorance in Individual-Level Causal-Effect Estimates under  Hidden Confounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jesson%2C+A">Andrew Jesson</a>, 
<a href="/search/cs?searchtype=author&query=Mindermann%2C+S">S&#xf6;ren Mindermann</a>, 
<a href="/search/cs?searchtype=author&query=Gal%2C+Y">Yarin Gal</a>, 
<a href="/search/cs?searchtype=author&query=Shalit%2C+U">Uri Shalit</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures, ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.04941" title="Abstract">arXiv:2103.04941</a> (replaced) [<a href="/pdf/2103.04941" title="Download PDF">pdf</a>, <a href="/format/2103.04941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InFillmore: Frame-Guided Language Generation with Bidirectional Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ou%2C+J">Jiefu Ou</a>, 
<a href="/search/cs?searchtype=author&query=Weir%2C+N">Nathaniel Weir</a>, 
<a href="/search/cs?searchtype=author&query=Belyy%2C+A">Anton Belyy</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Felix Yu</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appearing in *SEM 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.05224" title="Abstract">arXiv:2103.05224</a> (replaced) [<a href="/pdf/2103.05224" title="Download PDF">pdf</a>, <a href="/format/2103.05224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A review of flywheel energy storage systems: state of the art and  opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiaojun Li</a>, 
<a href="/search/eess?searchtype=author&query=Palazzolo%2C+A">Alan Palazzolo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.05331" title="Abstract">arXiv:2103.05331</a> (replaced) [<a href="/pdf/2103.05331" title="Download PDF">pdf</a>, <a href="/format/2103.05331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Testing: Sample-Efficient Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kossen%2C+J">Jannik Kossen</a>, 
<a href="/search/stat?searchtype=author&query=Farquhar%2C+S">Sebastian Farquhar</a>, 
<a href="/search/stat?searchtype=author&query=Gal%2C+Y">Yarin Gal</a>, 
<a href="/search/stat?searchtype=author&query=Rainforth%2C+T">Tom Rainforth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at the 38th International Conference on Machine Learning (ICML 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.05632" title="Abstract">arXiv:2103.05632</a> (replaced) [<a href="/pdf/2103.05632" title="Download PDF">pdf</a>, <a href="/format/2103.05632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Prediction of General Hamiltonian Dynamics via Learning  Exactly-Symplectic Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Renyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Molei Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures. ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS); Numerical Analysis (math.NA); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.07838" title="Abstract">arXiv:2103.07838</a> (replaced) [<a href="/pdf/2103.07838" title="Download PDF">pdf</a>, <a href="/format/2103.07838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cycle4Completion: Unpaired Point Cloud Completion using Cycle  Transformation with Missing Region Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhizhong Han</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan-Pei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+P">Pengfei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu-Shen Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.09284" title="Abstract">arXiv:2103.09284</a> (replaced) [<a href="/pdf/2103.09284" title="Download PDF">pdf</a>, <a href="/format/2103.09284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning in Nonzero-Sum Stochastic Games with Potentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mguni%2C+D">David Mguni</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yutong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minne Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Jennings%2C+J">Joel Jennings</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.09947" title="Abstract">arXiv:2103.09947</a> (replaced) [<a href="/pdf/2103.09947" title="Download PDF">pdf</a>, <a href="/format/2103.09947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Generalization in Adversarial Training via the  Bias-Variance Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yaodong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zitong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>, 
<a href="/search/cs?searchtype=author&query=Steinhardt%2C+J">Jacob Steinhardt</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V2 adds new results and improves organization and presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.10626" title="Abstract">arXiv:2103.10626</a> (replaced) [<a href="/pdf/2103.10626" title="Download PDF">pdf</a>, <a href="/format/2103.10626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning  for Whole Slide Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sharma%2C+Y">Yash Sharma</a>, 
<a href="/search/eess?searchtype=author&query=Shrivastava%2C+A">Aman Shrivastava</a>, 
<a href="/search/eess?searchtype=author&query=Ehsan%2C+L">Lubaina Ehsan</a>, 
<a href="/search/eess?searchtype=author&query=Moskaluk%2C+C+A">Christopher A. Moskaluk</a>, 
<a href="/search/eess?searchtype=author&query=Syed%2C+S">Sana Syed</a>, 
<a href="/search/eess?searchtype=author&query=Brown%2C+D+E">Donald E. Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at MIDL, 2021 - <a href="https://openreview.net/forum?id=7i1-2oKIELU">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.10685" title="Abstract">arXiv:2103.10685</a> (replaced) [<a href="/pdf/2103.10685" title="Download PDF">pdf</a>, <a href="/format/2103.10685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllable Generation from Pre-trained Language Models via Inverse  Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+X">Xu Zou</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Da Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Q">Qingyang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhilin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.11244" title="Abstract">arXiv:2103.11244</a> (replaced) [<a href="/pdf/2103.11244" title="Download PDF">pdf</a>, <a href="/ps/2103.11244" title="Download PostScript">ps</a>, <a href="/format/2103.11244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Impossibility of Post-Quantum Black-Box Zero-Knowledge in  Constant Rounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chia%2C+N">Nai-Hui Chia</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+K">Kai-Min Chung</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qipeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yamakawa%2C+T">Takashi Yamakawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.13308" title="Abstract">arXiv:2103.13308</a> (replaced) [<a href="/pdf/2103.13308" title="Download PDF">pdf</a>, <a href="/format/2103.13308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Power Modeling for Effective Datacenter Planning and Compute Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radovanovic%2C+A">Ana Radovanovic</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bokan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Talukdar%2C+S">Saurav Talukdar</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+B">Binz Roy</a>, 
<a href="/search/cs?searchtype=author&query=Duarte%2C+A">Alexandre Duarte</a>, 
<a href="/search/cs?searchtype=author&query=Shahbazi%2C+M">Mahya Shahbazi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.13740" title="Abstract">arXiv:2103.13740</a> (replaced) [<a href="/pdf/2103.13740" title="Download PDF">pdf</a>, <a href="/format/2103.13740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECG-TCN: Wearable Cardiac Arrhythmia Detection with a Temporal  Convolutional Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ingolfsson%2C+T+M">Thorir Mar Ingolfsson</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaying Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hersche%2C+M">Michael Hersche</a>, 
<a href="/search/cs?searchtype=author&query=Burrello%2C+A">Alessio Burrello</a>, 
<a href="/search/cs?searchtype=author&query=Cavigelli%2C+L">Lukas Cavigelli</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 1 figure, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.13906" title="Abstract">arXiv:2103.13906</a> (replaced) [<a href="/pdf/2103.13906" title="Download PDF">pdf</a>, <a href="/ps/2103.13906" title="Download PostScript">ps</a>, <a href="/format/2103.13906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> About exchanging expectation and supremum for conditional Wasserstein  GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martin%2C+J">J&#xf6;rg Martin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.14122" title="Abstract">arXiv:2103.14122</a> (replaced) [<a href="/pdf/2103.14122" title="Download PDF">pdf</a>, <a href="/format/2103.14122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private and Resource-Bounded Locally Decodable Codes for Insertions and  Deletions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Block%2C+A+R">Alexander R. Block</a> (Purdue University), 
<a href="/search/cs?searchtype=author&query=Blocki%2C+J">Jeremiah Blocki</a> (Purdue University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.00203" title="Abstract">arXiv:2104.00203</a> (replaced) [<a href="/pdf/2104.00203" title="Download PDF">pdf</a>, <a href="/format/2104.00203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaPool: A Diurnal-Adaptive Fleet Management Framework using Model-Free  Deep Reinforcement Learning and Change Point Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haliem%2C+M">Marina Haliem</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Bhargava%2C+B">Bharat Bhargava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2010.01755">arXiv:2010.01755</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.00550" title="Abstract">arXiv:2104.00550</a> (replaced) [<a href="/pdf/2104.00550" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agent-based simulations for protecting nursing homes with prevention and  vaccination strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lasser%2C+J">Jana Lasser</a>, 
<a href="/search/physics?searchtype=author&query=Zuber%2C+J">Johannes Zuber</a>, 
<a href="/search/physics?searchtype=author&query=Sorger%2C+J">Johannes Sorger</a>, 
<a href="/search/physics?searchtype=author&query=Dervic%2C+E">Elma Dervic</a>, 
<a href="/search/physics?searchtype=author&query=Ledebur%2C+K">Katharina Ledebur</a>, 
<a href="/search/physics?searchtype=author&query=Lindner%2C+S+D">Simon David Lindner</a>, 
<a href="/search/physics?searchtype=author&query=Klager%2C+E">Elisabeth Klager</a>, 
<a href="/search/physics?searchtype=author&query=Klete%C4%8Dka-Pulker%2C+M">Maria Klete&#x10d;ka-Pulker</a>, 
<a href="/search/physics?searchtype=author&query=Willschke%2C+H">Harald Willschke</a>, 
<a href="/search/physics?searchtype=author&query=Stangl%2C+K">Katrin Stangl</a>, 
<a href="/search/physics?searchtype=author&query=Stadtmann%2C+S">Sarah Stadtmann</a>, 
<a href="/search/physics?searchtype=author&query=Haslinger%2C+C">Christian Haslinger</a>, 
<a href="/search/physics?searchtype=author&query=Klimek%2C+P">Peter Klimek</a>, 
<a href="/search/physics?searchtype=author&query=Wochele-Thoma%2C+T">Thomas Wochele-Thoma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary material is included in the manuscript PDF
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.01320" title="Abstract">arXiv:2104.01320</a> (replaced) [<a href="/pdf/2104.01320" title="Download PDF">pdf</a>, <a href="/format/2104.01320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study on Channel Effects for Synthetic Voice Spoofing  Countermeasure Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">You Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+G">Ge Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+F">Fei Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Duan%2C+Z">Zhiyao Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures, to appear in INTERSPEECH 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.01408" title="Abstract">arXiv:2104.01408</a> (replaced) [<a href="/pdf/2104.01408" title="Download PDF">pdf</a>, <a href="/format/2104.01408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning for Emotional Text-to-Speech Synthesis with  Improved Emotion Discriminability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sisman%2C+B">Berrak Sisman</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, in Proceedings of INTERSPEECH 2021 conference, Speech Samples: <a href="https://ttslr.github.io/i-ETTS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.01440" title="Abstract">arXiv:2104.01440</a> (replaced) [<a href="/pdf/2104.01440" title="Download PDF">pdf</a>, <a href="/format/2104.01440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COHORTNEY: Non-Parametric Clustering of Event Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuzhel%2C+V">Vladislav Zhuzhel</a>, 
<a href="/search/cs?searchtype=author&query=Rivera-Castro%2C+R">Rodrigo Rivera-Castro</a>, 
<a href="/search/cs?searchtype=author&query=Kaploukhaya%2C+N">Nina Kaploukhaya</a>, 
<a href="/search/cs?searchtype=author&query=Mironova%2C+L">Liliya Mironova</a>, 
<a href="/search/cs?searchtype=author&query=Zaytsev%2C+A">Alexey Zaytsev</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.01714" title="Abstract">arXiv:2104.01714</a> (replaced) [<a href="/pdf/2104.01714" title="Download PDF">pdf</a>, <a href="/ps/2104.01714" title="Download PostScript">ps</a>, <a href="/format/2104.01714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Urysohn Forest for Aleatoric Uncertainty Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Polar%2C+A">Andrew Polar</a>, 
<a href="/search/cs?searchtype=author&query=Poluektov%2C+M">Michael Poluektov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.02194" title="Abstract">arXiv:2104.02194</a> (replaced) [<a href="/pdf/2104.02194" title="Download PDF">pdf</a>, <a href="/format/2104.02194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextualized Streaming End-to-End Speech Recognition with Trie-Based  Deep Biasing and Shallow Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+D">Duc Le</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+M">Mahaveer Jain</a>, 
<a href="/search/cs?searchtype=author&query=Keren%2C+G">Gil Keren</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Suyoun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yangyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Mahadeokar%2C+J">Jay Mahadeokar</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+J">Julian Chan</a>, 
<a href="/search/cs?searchtype=author&query=Shangguan%2C+Y">Yuan Shangguan</a>, 
<a href="/search/cs?searchtype=author&query=Fuegen%2C+C">Christian Fuegen</a>, 
<a href="/search/cs?searchtype=author&query=Kalinli%2C+O">Ozlem Kalinli</a>, 
<a href="/search/cs?searchtype=author&query=Saraf%2C+Y">Yatharth Saraf</a>, 
<a href="/search/cs?searchtype=author&query=Seltzer%2C+M+L">Michael L. Seltzer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at INTERSPEECH 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.02703" title="Abstract">arXiv:2104.02703</a> (replaced) [<a href="/pdf/2104.02703" title="Download PDF">pdf</a>, <a href="/format/2104.02703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Robustness under Long-Tailed Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingqiu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2021 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.02901" title="Abstract">arXiv:2104.02901</a> (replaced) [<a href="/pdf/2104.02901" title="Download PDF">pdf</a>, <a href="/format/2104.02901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised  Pretrained Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+J">Jheng-hao Lin</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y+Y">Yist Y. Lin</a>, 
<a href="/search/eess?searchtype=author&query=Chien%2C+C">Chung-Ming Chien</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by INTERSPEECH 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.03006" title="Abstract">arXiv:2104.03006</a> (replaced) [<a href="/pdf/2104.03006" title="Download PDF">pdf</a>, <a href="/format/2104.03006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Librispeech Transducer Model with Internal Language Model Prior  Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeyer%2C+A">Albert Zeyer</a>, 
<a href="/search/cs?searchtype=author&query=Merboldt%2C+A">Andr&#xe9; Merboldt</a>, 
<a href="/search/cs?searchtype=author&query=Michel%2C+W">Wilfried Michel</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%BCter%2C+R">Ralf Schl&#xfc;ter</a>, 
<a href="/search/cs?searchtype=author&query=Ney%2C+H">Hermann Ney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.03019" title="Abstract">arXiv:2104.03019</a> (replaced) [<a href="/pdf/2104.03019" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Vehicle Cooperation on Prediction-Level: Enhancing Automated  Driving with Human Foresight
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Weisswange%2C+T+H">Thomas H. Weisswange</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+M">Matti Krueger</a>, 
<a href="/search/cs?searchtype=author&query=Wiebel-Herboth%2C+C+B">Christiane B. Wiebel-Herboth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.04654" title="Abstract">arXiv:2104.04654</a> (replaced) [<a href="/pdf/2104.04654" title="Download PDF">pdf</a>, <a href="/format/2104.04654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regression Networks For Calculating Englacial Layer Thickness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varshney%2C+D">Debvrat Varshney</a>, 
<a href="/search/cs?searchtype=author&query=Rahnemoonfar%2C+M">Maryam Rahnemoonfar</a>, 
<a href="/search/cs?searchtype=author&query=Yari%2C+M">Masoud Yari</a>, 
<a href="/search/cs?searchtype=author&query=Paden%2C+J">John Paden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.04657" title="Abstract">arXiv:2104.04657</a> (replaced) [<a href="/pdf/2104.04657" title="Download PDF">pdf</a>, <a href="/format/2104.04657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning Bidirectional Update Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sandler%2C+M">Mark Sandler</a>, 
<a href="/search/cs?searchtype=author&query=Vladymyrov%2C+M">Max Vladymyrov</a>, 
<a href="/search/cs?searchtype=author&query=Zhmoginov%2C+A">Andrey Zhmoginov</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+N">Nolan Miller</a>, 
<a href="/search/cs?searchtype=author&query=Jackson%2C+A">Andrew Jackson</a>, 
<a href="/search/cs?searchtype=author&query=Madams%2C+T">Tom Madams</a>, 
<a href="/search/cs?searchtype=author&query=Arcas%2C+B+A+y">Blaise Aguera y Arcas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021, 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.05017" title="Abstract">arXiv:2104.05017</a> (replaced) [<a href="/pdf/2104.05017" title="Download PDF">pdf</a>, <a href="/format/2104.05017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating articulatory movements in speech production with transformer  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Udupa%2C+S">Sathvik Udupa</a>, 
<a href="/search/eess?searchtype=author&query=Roy%2C+A">Anwesha Roy</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+A">Abhayjeet Singh</a>, 
<a href="/search/eess?searchtype=author&query=Illa%2C+A">Aravind Illa</a>, 
<a href="/search/eess?searchtype=author&query=Ghosh%2C+P+K">Prasanta Kumar Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted for oral presentation at INTERSPEECH 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.05077" title="Abstract">arXiv:2104.05077</a> (replaced) [<a href="/pdf/2104.05077" title="Download PDF">pdf</a>, <a href="/format/2104.05077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoPE: Conditional image generation using Polynomial Expansions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chrysos%2C+G+G">Grigorios G Chrysos</a>, 
<a href="/search/cs?searchtype=author&query=Georgopoulos%2C+M">Markos Georgopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Panagakis%2C+Y">Yannis Panagakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.05632" title="Abstract">arXiv:2104.05632</a> (replaced) [<a href="/pdf/2104.05632" title="Download PDF">pdf</a>, <a href="/format/2104.05632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmented World Models Facilitate Zero-Shot Dynamics Generalization From  a Single Offline Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ball%2C+P+J">Philip J. Ball</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Parker-Holder%2C+J">Jack Parker-Holder</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+S">Stephen Roberts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted @ ICML 2021; Spotlight @ ICLR 2021 "Self-Supervision for Reinforcement Learning Workshop"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.05752" title="Abstract">arXiv:2104.05752</a> (replaced) [<a href="/pdf/2104.05752" title="Download PDF">pdf</a>, <a href="/format/2104.05752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speak or Chat with Me: End-to-End Spoken Language Understanding System  with Flexible Inputs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cha%2C+S">Sujeong Cha</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+W">Wangrui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hyun Jung</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+M">My Phung</a>, 
<a href="/search/cs?searchtype=author&query=Picheny%2C+M">Michael Picheny</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+H">Hong-Kwang Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+S">Samuel Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Morais%2C+E">Edmilson Morais</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.06570" title="Abstract">arXiv:2104.06570</a> (replaced) [<a href="/pdf/2104.06570" title="Download PDF">pdf</a>, <a href="/ps/2104.06570" title="Download PostScript">ps</a>, <a href="/format/2104.06570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $q$-Polymatroids and Their Relation to Rank-Metric Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gluesing-Luerssen%2C+H">Heide Gluesing-Luerssen</a>, 
<a href="/search/cs?searchtype=author&query=Jany%2C+B">Benjamin Jany</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.06648" title="Abstract">arXiv:2104.06648</a> (replaced) [<a href="/pdf/2104.06648" title="Download PDF">pdf</a>, <a href="/format/2104.06648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Root-finding Approaches for Computing Conformal Prediction Set
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ndiaye%2C+E">Eugene Ndiaye</a>, 
<a href="/search/stat?searchtype=author&query=Takeuchi%2C+I">Ichiro Takeuchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.07639" title="Abstract">arXiv:2104.07639</a> (replaced) [<a href="/pdf/2104.07639" title="Download PDF">pdf</a>, <a href="/format/2104.07639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Optimization for Multilingual Translation with Imbalanced Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xian Li</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+H">Hongyu Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.08593" title="Abstract">arXiv:2104.08593</a> (replaced) [<a href="/pdf/2104.08593" title="Download PDF">pdf</a>, <a href="/format/2104.08593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: Design Tools for Side-Channel-Aware Implementations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buhan%2C+I">Ileana Buhan</a>, 
<a href="/search/cs?searchtype=author&query=Batina%2C+L">Lejla Batina</a>, 
<a href="/search/cs?searchtype=author&query=Yarom%2C+Y">Yuval Yarom</a>, 
<a href="/search/cs?searchtype=author&query=Schaumont%2C+P">Patrick Schaumont</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.08620" title="Abstract">arXiv:2104.08620</a> (replaced) [<a href="/pdf/2104.08620" title="Download PDF">pdf</a>, <a href="/format/2104.08620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as  a Target for NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rozner%2C+J">Josh Rozner</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Mahowald%2C+K">Kyle Mahowald</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.09261" title="Abstract">arXiv:2104.09261</a> (replaced) [<a href="/pdf/2104.09261" title="Download PDF">pdf</a>, <a href="/format/2104.09261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+C">Chunyan Miao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures, published at NAACL-HLT 2021 conference, see <a href="https://www.aclweb.org/anthology/2021.naacl-main.425/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.10414" title="Abstract">arXiv:2104.10414</a> (replaced) [<a href="/pdf/2104.10414" title="Download PDF">pdf</a>, <a href="/format/2104.10414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orderly Dual-Teacher Knowledge Distillation for Lightweight Human Pose  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhong-Qiu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yuchen Ge</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+W">Weidong Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.10683" title="Abstract">arXiv:2104.10683</a> (replaced) [<a href="/pdf/2104.10683" title="Download PDF">pdf</a>, <a href="/format/2104.10683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable artificial intelligence for mechanics: physics-informing  neural networks for constitutive models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koeppe%2C+A">Arnd Koeppe</a>, 
<a href="/search/cs?searchtype=author&query=Bamer%2C+F">Franz Bamer</a>, 
<a href="/search/cs?searchtype=author&query=Selzer%2C+M">Michael Selzer</a>, 
<a href="/search/cs?searchtype=author&query=Nestler%2C+B">Britta Nestler</a>, 
<a href="/search/cs?searchtype=author&query=Markert%2C+B">Bernd Markert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint - Rev-1 (minor grammatical changes; prevent Google from archiving)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.10799" title="Abstract">arXiv:2104.10799</a> (replaced) [<a href="/pdf/2104.10799" title="Download PDF">pdf</a>, <a href="/ps/2104.10799" title="Download PostScript">ps</a>, <a href="/format/2104.10799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Negative Dependence Properties of Latin Hypercube Samples and  Scrambled Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Doerr%2C+B">Benjamin Doerr</a>, 
<a href="/search/math?searchtype=author&query=Gnewuch%2C+M">Michael Gnewuch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Discrete Mathematics (cs.DM); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.11014" title="Abstract">arXiv:2104.11014</a> (replaced) [<a href="/pdf/2104.11014" title="Download PDF">pdf</a>, <a href="/format/2104.11014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Space Search for Pareto-Efficient Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+M">Min-Fong Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao-Yun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min-Hung Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yu-Syuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+H">Hsien-Kai Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yi-Min Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hung-Jen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jou%2C+K">Kevin Jou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR2021 Workshop (Efficient Deep Learning for Computer Vision). Website: <a href="https://minhungchen.netlify.app/publication/nss">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.12100" title="Abstract">arXiv:2104.12100</a> (replaced) [<a href="/pdf/2104.12100" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Scale Hourglass Hierarchical Fusion Network for Single Image  Deraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yufeng Huang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+L">Lei Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in CVPRW 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.12672" title="Abstract">arXiv:2104.12672</a> (replaced) [<a href="/pdf/2104.12672" title="Download PDF">pdf</a>, <a href="/format/2104.12672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Interaction-based Methodology Towards Explainable AI with Better  Understanding of Pneumonia Chest X-ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+S">Shaw-Hwa Lo</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yiqiao Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.13352" title="Abstract">arXiv:2104.13352</a> (replaced) [<a href="/pdf/2104.13352" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking Peaceful Tractors on Social Media -- XAI-enabled analysis of  Red Fort Riots 2021
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Ajay Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+B">Basant Agarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.13744" title="Abstract">arXiv:2104.13744</a> (replaced) [<a href="/pdf/2104.13744" title="Download PDF">pdf</a>, <a href="/format/2104.13744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bio-SODA: Enabling Natural Language Question Answering over Knowledge  Graphs without Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sima%2C+A+C">Ana Claudia Sima</a>, 
<a href="/search/cs?searchtype=author&query=de+Farias%2C+T+M">Tarcisio Mendes de Farias</a>, 
<a href="/search/cs?searchtype=author&query=Anisimova%2C+M">Maria Anisimova</a>, 
<a href="/search/cs?searchtype=author&query=Dessimoz%2C+C">Christophe Dessimoz</a>, 
<a href="/search/cs?searchtype=author&query=Robinson-Rechavi%2C+M">Marc Robinson-Rechavi</a>, 
<a href="/search/cs?searchtype=author&query=Zbinden%2C+E">Erich Zbinden</a>, 
<a href="/search/cs?searchtype=author&query=Stockinger%2C+K">Kurt Stockinger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 33rd International Conference on Scientific and Statistical
  Database Management (SSDBM 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.14461" title="Abstract">arXiv:2104.14461</a> (replaced) [<a href="/pdf/2104.14461" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based  Reasoning Pairings for Explanation and Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keane%2C+M+T">Mark T Keane</a>, 
<a href="/search/cs?searchtype=author&query=Kenny%2C+E+M">Eoin M Kenny</a>, 
<a href="/search/cs?searchtype=author&query=Temraz%2C+M">Mohammed Temraz</a>, 
<a href="/search/cs?searchtype=author&query=Greene%2C+D">Derek Greene</a>, 
<a href="/search/cs?searchtype=author&query=Smyth%2C+B">Barry Smyth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages,4 figures, 2 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IJCAI-21 Workshop on DL-CBR-AML, July 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.14470" title="Abstract">arXiv:2104.14470</a> (replaced) [<a href="/pdf/2104.14470" title="Download PDF">pdf</a>, <a href="/format/2104.14470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Encoding and Segmentation Strategies on End-to-End  Simultaneous Speech Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Ha Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Est%C3%A8ve%2C+Y">Yannick Est&#xe8;ve</a>, 
<a href="/search/cs?searchtype=author&query=Besacier%2C+L">Laurent Besacier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.00268" title="Abstract">arXiv:2105.00268</a> (replaced) [<a href="/pdf/2105.00268" title="Download PDF">pdf</a>, <a href="/format/2105.00268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lite-FPN for Keypoint-based Monocular 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Minghan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.00455" title="Abstract">arXiv:2105.00455</a> (replaced) [<a href="/pdf/2105.00455" title="Download PDF">pdf</a>, <a href="/format/2105.00455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesized Difference in Differences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Strobl%2C+E+V">Eric V. Strobl</a>, 
<a href="/search/stat?searchtype=author&query=Lasko%2C+T+A">Thomas A. Lasko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM BCB 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.00648" title="Abstract">arXiv:2105.00648</a> (replaced) [<a href="/pdf/2105.00648" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel hybrid methodology of measuring sentence similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoo%2C+Y">Yongmin Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Heo%2C+T">Tak-Sung Heo</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Yeongjoon Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.00793" title="Abstract">arXiv:2105.00793</a> (replaced) [<a href="/pdf/2105.00793" title="Download PDF">pdf</a>, <a href="/ps/2105.00793" title="Download PostScript">ps</a>, <a href="/format/2105.00793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tubal Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Qi%2C+L">Liqun Qi</a>, 
<a href="/search/math?searchtype=author&query=Luo%2C+Z">Ziyan Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.00885" title="Abstract">arXiv:2105.00885</a> (replaced) [<a href="/pdf/2105.00885" title="Download PDF">pdf</a>, <a href="/ps/2105.00885" title="Download PostScript">ps</a>, <a href="/format/2105.00885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Extended Resolution Proofs with a BDD-Based SAT Solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bryant%2C+R+E">Randal E. Bryant</a>, 
<a href="/search/cs?searchtype=author&query=Heule%2C+M+J+H">Marijn J. H. Heule</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of paper published at TACAS 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.00981" title="Abstract">arXiv:2105.00981</a> (replaced) [<a href="/pdf/2105.00981" title="Download PDF">pdf</a>, <a href="/format/2105.00981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Russian News Clustering and Headline Selection Shared Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gusev%2C+I">Ilya Gusev</a>, 
<a href="/search/cs?searchtype=author&query=Smurov%2C+I">Ivan Smurov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Dialogue 2021 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.02266" title="Abstract">arXiv:2105.02266</a> (replaced) [<a href="/pdf/2105.02266" title="Download PDF">pdf</a>, <a href="/format/2105.02266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Stochastic Variance-Reduced Methods for Multi-Task Stochastic  Bilevel Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guo%2C+Z">Zhishuai Guo</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+Q">Quanqi Hu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+T">Tianbao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.03052" title="Abstract">arXiv:2105.03052</a> (replaced) [<a href="/pdf/2105.03052" title="Download PDF">pdf</a>, <a href="/format/2105.03052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Informational Design of Dynamic Multi-Agent System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Quanyan Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2102.07152">arXiv:2102.07152</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.03743" title="Abstract">arXiv:2105.03743</a> (replaced) [<a href="/pdf/2105.03743" title="Download PDF">pdf</a>, <a href="/format/2105.03743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certified Robustness to Text Adversarial Attacks by Randomized [MASK]
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Jiehang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianhan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Liping Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.03773" title="Abstract">arXiv:2105.03773</a> (replaced) [<a href="/pdf/2105.03773" title="Download PDF">pdf</a>, <a href="/ps/2105.03773" title="Download PostScript">ps</a>, <a href="/format/2105.03773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separations for Estimating Large Frequency Moments on Data Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D+P">David P. Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Samson Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICALP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.03818" title="Abstract">arXiv:2105.03818</a> (replaced) [<a href="/pdf/2105.03818" title="Download PDF">pdf</a>, <a href="/format/2105.03818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneous Risk Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiashuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zheyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+P">Peng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zheyan Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 38th International Conference on Machine Learning, PMLR 139, 2021. (ICML2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.03821" title="Abstract">arXiv:2105.03821</a> (replaced) [<a href="/pdf/2105.03821" title="Download PDF">pdf</a>, <a href="/format/2105.03821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Inference Representation: Learning Graph Positional Embeddings  with Anchor Path Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinpeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">ChuXiong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jie Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.04779" title="Abstract">arXiv:2105.04779</a> (replaced) [<a href="/pdf/2105.04779" title="Download PDF">pdf</a>, <a href="/format/2105.04779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EL-Attention: Memory Efficient Lossless Attention for Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiusheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+W">Weizhen Qi</a>, 
<a href="/search/cs?searchtype=author&query=Bhendawade%2C+N">Nikhil Bhendawade</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruofei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021. Version 2: add pseudocode
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.05366" title="Abstract">arXiv:2105.05366</a> (replaced) [<a href="/pdf/2105.05366" title="Download PDF">pdf</a>, <a href="/format/2105.05366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rearrangement on Lattices with Pick-n-Swaps: Optimality Structures and  Efficient Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingjin Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in R:SS 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.05980" title="Abstract">arXiv:2105.05980</a> (replaced) [<a href="/pdf/2105.05980" title="Download PDF">pdf</a>, <a href="/format/2105.05980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DONet: Dual-Octave Network for Fast MR Image Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Feng%2C+C">Chun-Mei Feng</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Z">Zhanyuan Yang</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+H">Huazhu Fu</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yong Xu</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/eess?searchtype=author&query=Shao%2C+L">Ling Shao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2104.05345">arXiv:2104.05345</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Neural Networks and Learning Systems, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.06548" title="Abstract">arXiv:2105.06548</a> (replaced) [<a href="/pdf/2105.06548" title="Download PDF">pdf</a>, <a href="/format/2105.06548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not All Memories are Created Equal: Learning to Forget by Expiring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukhbaatar%2C+S">Sainbayar Sukhbaatar</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+D">Da Ju</a>, 
<a href="/search/cs?searchtype=author&query=Poff%2C+S">Spencer Poff</a>, 
<a href="/search/cs?searchtype=author&query=Roller%2C+S">Stephen Roller</a>, 
<a href="/search/cs?searchtype=author&query=Szlam%2C+A">Arthur Szlam</a>, 
<a href="/search/cs?searchtype=author&query=Weston%2C+J">Jason Weston</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+A">Angela Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.08266" title="Abstract">arXiv:2105.08266</a> (replaced) [<a href="/pdf/2105.08266" title="Download PDF">pdf</a>, <a href="/format/2105.08266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Inference Attacks from Log-loss Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+A">Abhinav Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Kasiviswanathan%2C+S+P">Shiva Prasad Kasiviswanathan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zekun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Feyisetan%2C+O">Oluwaseyi Feyisetan</a>, 
<a href="/search/cs?searchtype=author&query=Teissier%2C+N">Nathanael Teissier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.08633" title="Abstract">arXiv:2105.08633</a> (replaced) [<a href="/pdf/2105.08633" title="Download PDF">pdf</a>, <a href="/format/2105.08633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PDE-constrained Models with Neural Network Terms: Optimization and  Global Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sirignano%2C+J">Justin Sirignano</a>, 
<a href="/search/cs?searchtype=author&query=MacArt%2C+J">Jonathan MacArt</a>, 
<a href="/search/cs?searchtype=author&query=Spiliopoulos%2C+K">Konstantinos Spiliopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.08645" title="Abstract">arXiv:2105.08645</a> (replaced) [<a href="/pdf/2105.08645" title="Download PDF">pdf</a>, <a href="/format/2105.08645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoTexT: Multi-task Learning with Code-Text Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phan%2C+L">Long Phan</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+H">Hieu Tran</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D">Daniel Le</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Hieu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Anibal%2C+J">James Anibal</a>, 
<a href="/search/cs?searchtype=author&query=Peltekian%2C+A">Alec Peltekian</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yanfang Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.08949" title="Abstract">arXiv:2105.08949</a> (replaced) [<a href="/pdf/2105.08949" title="Download PDF">pdf</a>, <a href="/format/2105.08949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Feng%2C+C">Chun-Mei Feng</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+H">Huazhu Fu</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+S">Shuhao Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yong Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.09157" title="Abstract">arXiv:2105.09157</a> (replaced) [<a href="/pdf/2105.09157" title="Download PDF">pdf</a>, <a href="/format/2105.09157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hunter in the Dark: Discover Anomalous Network Activity Using Deep  Ensemble Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shiyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Moustafa%2C+N">Nour Moustafa</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peilun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hui Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.09356" title="Abstract">arXiv:2105.09356</a> (replaced) [<a href="/pdf/2105.09356" title="Download PDF">pdf</a>, <a href="/format/2105.09356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Adversarial Neural Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+S+S+C">Seyed Saeed Changiz Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+F+X">Fred X. Han</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+D">Di Niu</a>, 
<a href="/search/cs?searchtype=author&query=Salameh%2C+M">Mohammad Salameh</a>, 
<a href="/search/cs?searchtype=author&query=Mills%2C+K">Keith Mills</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+S">Shuo Lian</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jui%2C+S">Shangling Jui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures, 13 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.09369" title="Abstract">arXiv:2105.09369</a> (replaced) [<a href="/pdf/2105.09369" title="Download PDF">pdf</a>, <a href="/format/2105.09369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Label Leakage from Gradients in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wainakh%2C+A">Aidmar Wainakh</a>, 
<a href="/search/cs?searchtype=author&query=Ventola%2C+F">Fabrizio Ventola</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BC%C3%9Fig%2C+T">Till M&#xfc;&#xdf;ig</a>, 
<a href="/search/cs?searchtype=author&query=Keim%2C+J">Jens Keim</a>, 
<a href="/search/cs?searchtype=author&query=Cordero%2C+C+G">Carlos Garcia Cordero</a>, 
<a href="/search/cs?searchtype=author&query=Zimmer%2C+E">Ephraim Zimmer</a>, 
<a href="/search/cs?searchtype=author&query=Grube%2C+T">Tim Grube</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BChlh%C3%A4user%2C+M">Max M&#xfc;hlh&#xe4;user</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.09970" title="Abstract">arXiv:2105.09970</a> (replaced) [<a href="/e-print/2105.09970" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forest languages defined by counting maximal paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beaudry%2C+M">Martin Beaudry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The proof of the main Lemma (3.11, section 3.4) is incomplete: in the middle of page 22, the fact that $\gamma$ is weakly distributive is not sufficient to justify the chain of two inclusions used to invoke Proposition 2.1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.10255" title="Abstract">arXiv:2105.10255</a> (replaced) [<a href="/pdf/2105.10255" title="Download PDF">pdf</a>, <a href="/format/2105.10255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the dimension of real algebraic sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lairez%2C+P">Piere Lairez</a>, 
<a href="/search/cs?searchtype=author&query=Din%2C+M+S+E">Mohab Safey El Din</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: title change
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISSAC 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>

</div>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.10440" title="Abstract">arXiv:2105.10440</a> (replaced) [<a href="/pdf/2105.10440" title="Download PDF">pdf</a>, <a href="/format/2105.10440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nori: Concealing the Concealed Identifier in 5G
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mattsson%2C+J+P">John Preu&#xdf; Mattsson</a>, 
<a href="/search/cs?searchtype=author&query=Nakarmi%2C+P+K">Prajwol Kumar Nakarmi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.10620" title="Abstract">arXiv:2105.10620</a> (replaced) [<a href="/pdf/2105.10620" title="Download PDF">pdf</a>, <a href="/format/2105.10620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HPNet: Deep Primitive Segmentation Using Hybrid Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Siming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhenpei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chongyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haibin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Vouga%2C+E">Etienne Vouga</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qixing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.10682" title="Abstract">arXiv:2105.10682</a> (replaced) [<a href="/pdf/2105.10682" title="Download PDF">pdf</a>, <a href="/format/2105.10682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feasible Actor-Critic: Constrained Reinforcement Learning for Ensuring  Statewise Safety
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haitong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yang Guan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+E">Shegnbo Eben Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangteng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sifa Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianyu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.10879" title="Abstract">arXiv:2105.10879</a> (replaced) [<a href="/pdf/2105.10879" title="Download PDF">pdf</a>, <a href="/format/2105.10879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise Approximation of Convolutional Neural Networks for  Homomorphically Encrypted Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E">Eunsang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joon-Woo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yongjune Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Sik Kim</a>, 
<a href="/search/cs?searchtype=author&query=No%2C+J">Jong-Seon No</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Typos corrected, supplementary added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.11367" title="Abstract">arXiv:2105.11367</a> (replaced) [<a href="/pdf/2105.11367" title="Download PDF">pdf</a>, <a href="/format/2105.11367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedScale: Benchmarking Model and System Performance of Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+F">Fan Lai</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yinwei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiangfeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+M">Mosharaf Chowdhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.11417" title="Abstract">arXiv:2105.11417</a> (replaced) [<a href="/pdf/2105.11417" title="Download PDF">pdf</a>, <a href="/format/2105.11417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skew Orthogonal Convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singla%2C+S">Sahil Singla</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICML, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.11688" title="Abstract">arXiv:2105.11688</a> (replaced) [<a href="/pdf/2105.11688" title="Download PDF">pdf</a>, <a href="/format/2105.11688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Configuration Model with Triadic Closure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+R">Ruhui Chang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Duan-Shin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Cheng-Shang Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.11982" title="Abstract">arXiv:2105.11982</a> (replaced) [<a href="/pdf/2105.11982" title="Download PDF">pdf</a>, <a href="/format/2105.11982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Uncertainty in Deep Spatiotemporal Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dongxia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Liyao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+X">Xinyue Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chinazzi%2C+M">Matteo Chinazzi</a>, 
<a href="/search/cs?searchtype=author&query=Vespignani%2C+A">Alessandro Vespignani</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi-An Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2102.06684">arXiv:2102.06684</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.12856" title="Abstract">arXiv:2105.12856</a> (replaced) [<a href="/pdf/2105.12856" title="Download PDF">pdf</a>, <a href="/format/2105.12856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Algorithmic Bias: A Socio-Computational Interrogation of the  Google Search by Image Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papakyriakopoulos%2C+O">Orestis Papakyriakopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Mboya%2C+A+M">Arwa Michelle Mboya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.12924" title="Abstract">arXiv:2105.12924</a> (replaced) [<a href="/pdf/2105.12924" title="Download PDF">pdf</a>, <a href="/ps/2105.12924" title="Download PostScript">ps</a>, <a href="/format/2105.12924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Ensembling Contrastive Learning for Semi-Supervised Medical Image  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jinxi Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuowei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenji Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Q">Qing Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaoting Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.13010" title="Abstract">arXiv:2105.13010</a> (replaced) [<a href="/pdf/2105.13010" title="Download PDF">pdf</a>, <a href="/ps/2105.13010" title="Download PostScript">ps</a>, <a href="/format/2105.13010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An error analysis of generative adversarial networks for learning  distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+Y">Yuling Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yunfei Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.13096" title="Abstract">arXiv:2105.13096</a> (replaced) [<a href="/pdf/2105.13096" title="Download PDF">pdf</a>, <a href="/ps/2105.13096" title="Download PostScript">ps</a>, <a href="/format/2105.13096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lattice-Based Minimum-Distortion Data Hiding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jieni Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Junren Qin</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Shanxiang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+B">Bingwen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiabo Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, to appear in IEEE communications letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.13648" title="Abstract">arXiv:2105.13648</a> (replaced) [<a href="/pdf/2105.13648" title="Download PDF">pdf</a>, <a href="/format/2105.13648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Lingual Abstractive Summarization with Limited Parallel Resources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yu Bai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.13856" title="Abstract">arXiv:2105.13856</a> (replaced) [<a href="/pdf/2105.13856" title="Download PDF">pdf</a>, <a href="/format/2105.13856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight Cross-Lingual Sentence Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhuoyuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Prakhar Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+C">Chenhui Chu</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>, 
<a href="/search/cs?searchtype=author&query=Kurohashi%2C+S">Sadao Kurohashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.13878" title="Abstract">arXiv:2105.13878</a> (replaced) [<a href="/pdf/2105.13878" title="Download PDF">pdf</a>, <a href="/format/2105.13878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating BERT Inference for Sequence Labeling via Early-Exit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yunfan Shao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.14152" title="Abstract">arXiv:2105.14152</a> (replaced) [<a href="/pdf/2105.14152" title="Download PDF">pdf</a>, <a href="/format/2105.14152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Radar Odometry Combining Probabilistic Estimation and Unsupervised  Feature Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burnett%2C+K">Keenan Burnett</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+D+J">David J. Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Schoellig%2C+A+P">Angela P. Schoellig</a>, 
<a href="/search/cs?searchtype=author&query=Barfoot%2C+T+D">Timothy D. Barfoot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Robotics Science and Systems 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.14188" title="Abstract">arXiv:2105.14188</a> (replaced) [<a href="/pdf/2105.14188" title="Download PDF">pdf</a>, <a href="/format/2105.14188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> We Know What You Want: An Advertising Strategy Recommender System for  Online Advertising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Liyi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Junqi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhenzhe Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiye Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Z">Zhizhuang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+F">Fei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Lvyin Niu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuning Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaoqiang Zhu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining (KDD '21), August 14--18, 2021, Virtual Event,
  Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.14710" title="Abstract">arXiv:2105.14710</a> (replaced) [<a href="/pdf/2105.14710" title="Download PDF">pdf</a>, <a href="/format/2105.14710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustifying $\ell_\infty$ Adversarial Training to the Union of  Perturbation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patil%2C+A+D">Ameya D. Patil</a>, 
<a href="/search/cs?searchtype=author&query=Tuttle%2C+M">Michael Tuttle</a>, 
<a href="/search/cs?searchtype=author&query=Schwing%2C+A+G">Alexander G. Schwing</a>, 
<a href="/search/cs?searchtype=author&query=Shanbhag%2C+N+R">Naresh R. Shanbhag</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.15039" title="Abstract">arXiv:2105.15039</a> (replaced) [<a href="/pdf/2105.15039" title="Download PDF">pdf</a>, <a href="/ps/2105.15039" title="Download PostScript">ps</a>, <a href="/format/2105.15039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequenceable Event Recorders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cardelli%2C+L">Luca Cardelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.15082" title="Abstract">arXiv:2105.15082</a> (replaced) [<a href="/pdf/2105.15082" title="Download PDF">pdf</a>, <a href="/format/2105.15082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Sparse Expert Models and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+A">An Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Junyang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Men%2C+R">Rui Men</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Le Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xianyan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Ang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiamang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Lin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.15134" title="Abstract">arXiv:2105.15134</a> (replaced) [<a href="/pdf/2105.15134" title="Download PDF">pdf</a>, <a href="/format/2105.15134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Understanding the Feature Learning Process of Self-supervised  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zixin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V2 polished writing and added citations. Accepted to ICML2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.00273" title="Abstract">arXiv:2106.00273</a> (replaced) [<a href="/pdf/2106.00273" title="Download PDF">pdf</a>, <a href="/format/2106.00273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Adversarial Robustness for Speaker Verification by  Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haibin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xu Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A+T">Andy T. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H">Helen Meng</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to TASLP on 19 April 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.00305" title="Abstract">arXiv:2106.00305</a> (replaced) [<a href="/pdf/2106.00305" title="Download PDF">pdf</a>, <a href="/format/2106.00305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Independent Prototype Propagation for Zero-Shot Compositionality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruis%2C+F">Frank Ruis</a>, 
<a href="/search/cs?searchtype=author&query=Burghouts%2C+G">Gertjan Burghouts</a>, 
<a href="/search/cs?searchtype=author&query=Bucur%2C+D">Doina Bucur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.00467" title="Abstract">arXiv:2106.00467</a> (replaced) [<a href="/pdf/2106.00467" title="Download PDF">pdf</a>, <a href="/format/2106.00467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The zoo of Fairness metrics in Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castelnovo%2C+A">Alessandro Castelnovo</a>, 
<a href="/search/cs?searchtype=author&query=Crupi%2C+R">Riccardo Crupi</a>, 
<a href="/search/cs?searchtype=author&query=Greco%2C+G">Greta Greco</a>, 
<a href="/search/cs?searchtype=author&query=Regoli%2C+D">Daniele Regoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.00671" title="Abstract">arXiv:2106.00671</a> (replaced) [<a href="/pdf/2106.00671" title="Download PDF">pdf</a>, <a href="/format/2106.00671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Can I Do Here? Learning New Skills by Imagining Visual Affordances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khazatsky%2C+A">Alexander Khazatsky</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+A">Ashvin Nair</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+D">Daniel Jing</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 10 figures. Presented at ICRA 2021. Project website: <a href="https://sites.google.com/view/val-rl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.00922" title="Abstract">arXiv:2106.00922</a> (replaced) [<a href="/pdf/2106.00922" title="Download PDF">pdf</a>, <a href="/format/2106.00922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Comparison of Off-policy Prediction Learning Algorithms on  the Collision Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghiassian%2C+S">Sina Ghiassian</a>, 
<a href="/search/cs?searchtype=author&query=Sutton%2C+R+S">Richard S. Sutton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.00961" title="Abstract">arXiv:2106.00961</a> (replaced) [<a href="/pdf/2106.00961" title="Download PDF">pdf</a>, <a href="/format/2106.00961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Control-Estimation Synthesis for Stochastic Multi-Agent  Systems via Virtual Interaction between Non-neighboring Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hojin Lee</a>, 
<a href="/search/eess?searchtype=author&query=Kwon%2C+C">Cheolhyeon Kwon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, to be published in IEEE/Control Systems Letters (L-CSS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.01726" title="Abstract">arXiv:2106.01726</a> (replaced) [<a href="/pdf/2106.01726" title="Download PDF">pdf</a>, <a href="/format/2106.01726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting co-execution with oneAPI: heterogeneity from a modern  perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nozal%2C+R">Ra&#xfa;l Nozal</a>, 
<a href="/search/cs?searchtype=author&query=Bosque%2C+J+L">Jose Luis Bosque</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Euro-Par 2021 (27th International Conference on Parallel and Distributed Computing). 16 pages, 9 figures, 1 listing. Conference paper - extended with API
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.01797" title="Abstract">arXiv:2106.01797</a> (replaced) [<a href="/pdf/2106.01797" title="Download PDF">pdf</a>, <a href="/format/2106.01797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+P">Pengda Qin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhong Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+K">Kefeng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiang Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02096" title="Abstract">arXiv:2106.02096</a> (replaced) [<a href="/pdf/2106.02096" title="Download PDF">pdf</a>, <a href="/ps/2106.02096" title="Download PostScript">ps</a>, <a href="/format/2106.02096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shape-Preserving Dimensionality Reduction : An Algorithm and Measures of  Topological Equivalence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yu%2C+B">Byeongsu Yu</a>, 
<a href="/search/stat?searchtype=author&query=You%2C+K">Kisung You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02245" title="Abstract">arXiv:2106.02245</a> (replaced) [<a href="/pdf/2106.02245" title="Download PDF">pdf</a>, <a href="/format/2106.02245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards offensive language detection and reduction in four Software  Engineering communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheriyan%2C+J">Jithin Cheriyan</a>, 
<a href="/search/cs?searchtype=author&query=Savarimuthu%2C+B+T+R">Bastin Tony Roy Savarimuthu</a>, 
<a href="/search/cs?searchtype=author&query=Cranefield%2C+S">Stephen Cranefield</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02297" title="Abstract">arXiv:2106.02297</a> (replaced) [<a href="/pdf/2106.02297" title="Download PDF">pdf</a>, <a href="/format/2106.02297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fre-GAN: Adversarial Frequency-consistent Audio Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kim%2C+J">Ji-Hoon Kim</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+S">Sang-Hoon Lee</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J">Ji-Hyun Lee</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted paper in Interspeech 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02391" title="Abstract">arXiv:2106.02391</a> (replaced) [<a href="/pdf/2106.02391" title="Download PDF">pdf</a>, <a href="/ps/2106.02391" title="Download PostScript">ps</a>, <a href="/format/2106.02391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Control Design with LMIs and Dynamic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lee%2C+D">Donghwan Lee</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+D+W">Do Wan Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02549" title="Abstract">arXiv:2106.02549</a> (replaced) [<a href="/pdf/2106.02549" title="Download PDF">pdf</a>, <a href="/format/2106.02549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detect the Interactions that Matter in Matter: Geometric Attention for  Many-Body Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frank%2C+T">Thorben Frank</a>, 
<a href="/search/cs?searchtype=author&query=Chmiela%2C+S">Stefan Chmiela</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph)

</div>
</div>
</dd>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02556" title="Abstract">arXiv:2106.02556</a> (replaced) [<a href="/pdf/2106.02556" title="Download PDF">pdf</a>, <a href="/format/2106.02556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Musical Prosody-Driven Emotion Classification: Interpreting Vocalists  Portrayal of Emotions Through Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farris%2C+N">Nicholas Farris</a>, 
<a href="/search/cs?searchtype=author&query=Model%2C+B">Brian Model</a>, 
<a href="/search/cs?searchtype=author&query=Savery%2C+R">Richard Savery</a>, 
<a href="/search/cs?searchtype=author&query=Weinberg%2C+G">Gil Weinberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02604" title="Abstract">arXiv:2106.02604</a> (replaced) [<a href="/pdf/2106.02604" title="Download PDF">pdf</a>, <a href="/format/2106.02604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Persuasive Technology Make Smartphones More Addictive? -- An  Empirical Study of Chinese University Students
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaowei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02639" title="Abstract">arXiv:2106.02639</a> (replaced) [<a href="/pdf/2106.02639" title="Download PDF">pdf</a>, <a href="/ps/2106.02639" title="Download PostScript">ps</a>, <a href="/format/2106.02639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Singular Dynamic Mode Decompositions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rosenfeld%2C+J+A">Joel A. Rosenfeld</a>, 
<a href="/search/eess?searchtype=author&query=Kamalapurkar%2C+R">Rushikesh Kamalapurkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages. YouTube playlist supporting this manuscript can be found here: <a href="https://youtube.com/playlist?list=PLldiDnQu2phsZdFP3nHoGnk_Aq-kp_4nE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS); Functional Analysis (math.FA); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02968" title="Abstract">arXiv:2106.02968</a> (replaced) [<a href="/pdf/2106.02968" title="Download PDF">pdf</a>, <a href="/format/2106.02968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low Budget Active Learning via Wasserstein Distance: An Integer  Programming Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+R">Rafid Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Fidler%2C+S">Sanja Fidler</a>, 
<a href="/search/cs?searchtype=author&query=Law%2C+M+T">Marc T. Law</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03039" title="Abstract">arXiv:2106.03039</a> (replaced) [<a href="/pdf/2106.03039" title="Download PDF">pdf</a>, <a href="/format/2106.03039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-facet Contextual Bandits: A Neural Network Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ban%2C+Y">Yikun Ban</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jingrui He</a>, 
<a href="/search/cs?searchtype=author&query=Cook%2C+C+B">Curtiss B. Cook</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> KDD'21, 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03107" title="Abstract">arXiv:2106.03107</a> (replaced) [<a href="/pdf/2106.03107" title="Download PDF">pdf</a>, <a href="/format/2106.03107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New complexity results and algorithms for min-max-min robust  combinatorial optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kurtz%2C+J">Jannis Kurtz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03153" title="Abstract">arXiv:2106.03153</a> (replaced) [<a href="/pdf/2106.03153" title="Download PDF">pdf</a>, <a href="/format/2106.03153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Min%2C+D">Dongchan Min</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+D+B">Dong Bok Lee</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+E">Eunho Yang</a>, 
<a href="/search/eess?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03207" title="Abstract">arXiv:2106.03207</a> (replaced) [<a href="/pdf/2106.03207" title="Download PDF">pdf</a>, <a href="/format/2106.03207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Covariate Shift in Imitation Learning via Offline Data  Without Great Coverage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+J+D">Jonathan D. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Uehara%2C+M">Masatoshi Uehara</a>, 
<a href="/search/cs?searchtype=author&query=Sreenivas%2C+D">Dhruv Sreenivas</a>, 
<a href="/search/cs?searchtype=author&query=Kidambi%2C+R">Rahul Kidambi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 5 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03210" title="Abstract">arXiv:2106.03210</a> (replaced) [<a href="/pdf/2106.03210" title="Download PDF">pdf</a>, <a href="/format/2106.03210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alpha Matte Generation from Single Input for Portrait Matting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yaman%2C+D">Dogucan Yaman</a>, 
<a href="/search/cs?searchtype=author&query=Ekenel%2C+H+K">Haz&#x131;m Kemal Ekenel</a>, 
<a href="/search/cs?searchtype=author&query=Waibel%2C+A">Alexander Waibel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03279" title="Abstract">arXiv:2106.03279</a> (replaced) [<a href="/pdf/2106.03279" title="Download PDF">pdf</a>, <a href="/format/2106.03279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning MDPs from Features: Predict-Then-Optimize for Sequential  Decision Problems by Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S">Sanket Shah</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haipeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Perrault%2C+A">Andrew Perrault</a>, 
<a href="/search/cs?searchtype=author&query=Doshi-Velez%2C+F">Finale Doshi-Velez</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+M">Milind Tambe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03469" title="Abstract">arXiv:2106.03469</a> (replaced) [<a href="/pdf/2106.03469" title="Download PDF">pdf</a>, <a href="/format/2106.03469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Neural Semantic Parsing for Low-Resourced Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Menglin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Monti%2C+E">Emilio Monti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at *SEM2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03569" title="Abstract">arXiv:2106.03569</a> (replaced) [<a href="/pdf/2106.03569" title="Download PDF">pdf</a>, <a href="/format/2106.03569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Socially-Aware Self-Supervised Tri-Training for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Junliang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Min Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+N+Q+V">Nguyen Quoc Viet Hung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, accepted by KDD'21
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03840" title="Abstract">arXiv:2106.03840</a> (replaced) [<a href="/pdf/2106.03840" title="Download PDF">pdf</a>, <a href="/format/2106.03840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing Garbage Collection vs I/O Amplification using hybrid Key-Value  Placement in LSM-based Key-Value Stores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xanthakis%2C+G">Giorgos Xanthakis</a>, 
<a href="/search/cs?searchtype=author&query=Saloustros%2C+G">Giorgos Saloustros</a>, 
<a href="/search/cs?searchtype=author&query=Batsaras%2C+N">Nikos Batsaras</a>, 
<a href="/search/cs?searchtype=author&query=Papagiannis%2C+A">Anastasios Papagiannis</a>, 
<a href="/search/cs?searchtype=author&query=Bilas%2C+A">Angelos Bilas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03954" title="Abstract">arXiv:2106.03954</a> (replaced) [<a href="/pdf/2106.03954" title="Download PDF">pdf</a>, <a href="/format/2106.03954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Meta-Feature Selection for the Algorithm Recommendation  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pereira%2C+G+T">Geand Trindade Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+M+R+d">Moises Rocha dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=de+Leon+Ferreira+de+Carvalho%2C+A+C+P">Andre Carlos Ponce de Leon Ferreira de Carvalho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04067" title="Abstract">arXiv:2106.04067</a> (replaced) [<a href="/pdf/2106.04067" title="Download PDF">pdf</a>, <a href="/format/2106.04067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution  Homography Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+R">Ruizhi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Gaochang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuemei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Ying Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yebin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04102" title="Abstract">arXiv:2106.04102</a> (replaced) [<a href="/pdf/2106.04102" title="Download PDF">pdf</a>, <a href="/format/2106.04102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Swords: A Benchmark for Lexical Substitution with Improved Data Coverage  and Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Mina Lee</a>, 
<a href="/search/cs?searchtype=author&query=Donahue%2C+C">Chris Donahue</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Iyabor%2C+A">Alexander Iyabor</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NAACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04179" title="Abstract">arXiv:2106.04179</a> (replaced) [<a href="/pdf/2106.04179" title="Download PDF">pdf</a>, <a href="/format/2106.04179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic $(1+\varepsilon)$-Approximate Maximum Matching with  $\mathsf{poly}(1/\varepsilon)$ Passes in the Semi-Streaming Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fischer%2C+M">Manuela Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Mitrovi%C4%87%2C+S">Slobodan Mitrovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Uitto%2C+J">Jara Uitto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04289" title="Abstract">arXiv:2106.04289</a> (replaced) [<a href="/pdf/2106.04289" title="Download PDF">pdf</a>, <a href="/format/2106.04289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Morphing tree drawings in a small 3D grid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arseneva%2C+E">Elena Arseneva</a>, 
<a href="/search/cs?searchtype=author&query=Gangopadhyay%2C+R">Rahul Gangopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Istomina%2C+A">Aleksandra Istomina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, corrected version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04292" title="Abstract">arXiv:2106.04292</a> (replaced) [<a href="/pdf/2106.04292" title="Download PDF">pdf</a>, <a href="/format/2106.04292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Principled Hyperedge Prediction with Structural Spectral Features and  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+C">Changlin Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+W">Wei Hao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Sha Cao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04392" title="Abstract">arXiv:2106.04392</a> (replaced) [<a href="/pdf/2106.04392" title="Download PDF">pdf</a>, <a href="/format/2106.04392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signal Transformer: Complex-valued Attention and Meta-Learning for  Signal Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yihong Dong</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Ying Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Muqiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Songtao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Q">Qingjiang Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04476" title="Abstract">arXiv:2106.04476</a> (replaced) [<a href="/pdf/2106.04476" title="Download PDF">pdf</a>, <a href="/ps/2106.04476" title="Download PostScript">ps</a>, <a href="/format/2106.04476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task  Learning on Semantic Parsing Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Damonte%2C+M">Marco Damonte</a>, 
<a href="/search/cs?searchtype=author&query=Monti%2C+E">Emilio Monti</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Tenth Joint Conference on Lexical and Computational Semantics
  (*SEM 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04496" title="Abstract">arXiv:2106.04496</a> (replaced) [<a href="/pdf/2106.04496" title="Download PDF">pdf</a>, <a href="/format/2106.04496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Theoretical Framework of Out-of-Distribution Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haotian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chuanlong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Tianle Cai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruichen Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liwei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04527" title="Abstract">arXiv:2106.04527</a> (replaced) [<a href="/pdf/2106.04527" title="Download PDF">pdf</a>, <a href="/format/2106.04527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sellars%2C+P">Philip Sellars</a>, 
<a href="/search/cs?searchtype=author&query=Aviles-Rivero%2C+A+I">Angelica I. Aviles-Rivero</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6nlieb%2C+C">Carola-Bibiane Sch&#xf6;nlieb</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04563" title="Abstract">arXiv:2106.04563</a> (replaced) [<a href="/pdf/2106.04563" title="Download PDF">pdf</a>, <a href="/ps/2106.04563" title="Download PostScript">ps</a>, <a href="/format/2106.04563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Subhabrata Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A+H">Ahmed Hassan Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and checkpoints released (links in draft)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.05064" title="Abstract">arXiv:2106.05064</a> (replaced) [<a href="/pdf/2106.05064" title="Download PDF">pdf</a>, <a href="/ps/2106.05064" title="Download PostScript">ps</a>, <a href="/format/2106.05064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharp Elements and the Scott Topology of Continuous Dcpos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=de+Jong%2C+T">Tom de Jong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Corrected a typo in a section title
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.05234" title="Abstract">arXiv:2106.05234</a> (replaced) [<a href="/pdf/2106.05234" title="Download PDF">pdf</a>, <a href="/format/2106.05234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Transformers Really Perform Bad for Graph Representation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+C">Chengxuan Ying</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Tianle Cai</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shengjie Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuxin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+G">Guolin Ke</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Di He</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yanming Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tie-Yan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.05239" title="Abstract">arXiv:2106.05239</a> (replaced) [<a href="/pdf/2106.05239" title="Download PDF">pdf</a>, <a href="/format/2106.05239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XBNet : An Extremely Boosted Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+T">Tushar Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.05430" title="Abstract">arXiv:2106.05430</a> (replaced) [<a href="/pdf/2106.05430" title="Download PDF">pdf</a>, <a href="/format/2106.05430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Very Compact Clusters with Structural Regularization via Similarity and  Connectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W+H">Won Hwa Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.05819" title="Abstract">arXiv:2106.05819</a> (replaced) [<a href="/pdf/2106.05819" title="Download PDF">pdf</a>, <a href="/format/2106.05819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Graph Augmentation to Improve Graph Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suresh%2C+S">Susheel Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+C">Cong Hao</a>, 
<a href="/search/cs?searchtype=author&query=Neville%2C+J">Jennifer Neville</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> typo fix in theorem 1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06046" title="Abstract">arXiv:2106.06046</a> (replaced) [<a href="/pdf/2106.06046" title="Download PDF">pdf</a>, <a href="/ps/2106.06046" title="Download PostScript">ps</a>, <a href="/format/2106.06046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Theoretic Evaluation of Privacy-Leakage, Interpretability,  and Transferability for a Novel Trustworthy AI Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+M">Mohit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Moser%2C+B+A">Bernhard A. Moser</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+L">Lukas Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Freudenthaler%2C+B">Bernhard Freudenthaler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2105.04615">arXiv:2105.04615</a>, <a href="/abs/2104.07060">arXiv:2104.07060</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06054" title="Abstract">arXiv:2106.06054</a> (replaced) [<a href="/pdf/2106.06054" title="Download PDF">pdf</a>, <a href="/format/2106.06054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Preprocessing: Towards Understanding Compositional Fairness of Data  Transformers in Machine Learning Pipeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sumon Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Rajan%2C+H">Hridesh Rajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ESEC/FSE'2021: The 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the ESEC/FSE 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06169" title="Abstract">arXiv:2106.06169</a> (replaced) [<a href="/pdf/2106.06169" title="Download PDF">pdf</a>, <a href="/format/2106.06169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BoB: BERT Over BERT for Training Persona-based Dialogue Models from  Limited Personalized Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Haoyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei-Nan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06189" title="Abstract">arXiv:2106.06189</a> (replaced) [<a href="/pdf/2106.06189" title="Download PDF">pdf</a>, <a href="/format/2106.06189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Order Matters: Probabilistic Modeling of Node Sequence for Graph  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+X">Xiaohui Chen</a>, 
<a href="/search/stat?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/stat?searchtype=author&query=Hu%2C+J">Jiajing Hu</a>, 
<a href="/search/stat?searchtype=author&query=Ruiz%2C+F+J+R">Francisco J. R. Ruiz</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+L">Liping Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06242" title="Abstract">arXiv:2106.06242</a> (replaced) [<a href="/pdf/2106.06242" title="Download PDF">pdf</a>, <a href="/format/2106.06242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where to Encode: A Performance Analysis of x86 and Arm-based Amazon EC2  Instances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Math%C3%A1%2C+R">Roland Math&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Kimovski%2C+D">Dragi Kimovski</a>, 
<a href="/search/cs?searchtype=author&query=Zabrovskiy%2C+A">Anatoliy Zabrovskiy</a>, 
<a href="/search/cs?searchtype=author&query=Timmerer%2C+C">Christian Timmerer</a>, 
<a href="/search/cs?searchtype=author&query=Prodan%2C+R">Radu Prodan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06333" title="Abstract">arXiv:2106.06333</a> (replaced) [<a href="/pdf/2106.06333" title="Download PDF">pdf</a>, <a href="/format/2106.06333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariant Information Bottleneck for Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yifei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yezhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenzhen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Reed%2C+C+J">Colorado J. Reed</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The work is in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06415" title="Abstract">arXiv:2106.06415</a> (replaced) [<a href="/pdf/2106.06415" title="Download PDF">pdf</a>, <a href="/ps/2106.06415" title="Download PostScript">ps</a>, <a href="/format/2106.06415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-based Partial Face Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%B6rmann%2C+S">Stefan H&#xf6;rmann</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Knoche%2C+M">Martin Knoche</a>, 
<a href="/search/cs?searchtype=author&query=Teepe%2C+T">Torben Teepe</a>, 
<a href="/search/cs?searchtype=author&query=Rigoll%2C+G">Gerhard Rigoll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in IEEE ICIP 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06482" title="Abstract">arXiv:2106.06482</a> (replaced) [<a href="/pdf/2106.06482" title="Download PDF">pdf</a>, <a href="/format/2106.06482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Network Modeling of Probabilities for Coding the Octree  Representation of Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaya%2C+E+C">Emre Can Kaya</a>, 
<a href="/search/cs?searchtype=author&query=Tabus%2C+I">Ioan Tabus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, Submitted to MMSP 2021, a few typos fixed in v2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06500" title="Abstract">arXiv:2106.06500</a> (replaced) [<a href="/pdf/2106.06500" title="Download PDF">pdf</a>, <a href="/ps/2106.06500" title="Download PostScript">ps</a>, <a href="/format/2106.06500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Benchmark of Dynamical Variational Autoencoders applied to Speech  Spectrogram Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bie%2C+X">Xiaoyu Bie</a>, 
<a href="/search/cs?searchtype=author&query=Girin%2C+L">Laurent Girin</a>, 
<a href="/search/cs?searchtype=author&query=Leglaive%2C+S">Simon Leglaive</a>, 
<a href="/search/cs?searchtype=author&query=Hueber%2C+T">Thomas Hueber</a>, 
<a href="/search/cs?searchtype=author&query=Alameda-Pineda%2C+X">Xavier Alameda-Pineda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Interspeech 2021. arXiv admin note: text overlap with <a href="/abs/2008.12595">arXiv:2008.12595</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item525">Cross-lists</a></li>
<li><a href="#item585">Replacements</a></li>
</ul>
<small>[ total of 951 entries:  <b>1-951</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2106">2106</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
